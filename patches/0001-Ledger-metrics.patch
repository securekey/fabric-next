From 400ccff09ae4b857a6a6da03750180b1e949c651 Mon Sep 17 00:00:00 2001
From: Firas Qutishat <firas.qutishat@securekey.com>
Date: Mon, 22 Oct 2018 11:03:59 -0400
Subject: [PATCH] Ledger metrics

Signed-off-by: Firas Qutishat <firas.qutishat@securekey.com>
---
 bccsp/pkcs11/impl.go                          |  21 +
 bccsp/sw/impl.go                              |  25 +
 .../cdbblkstorage/cdb_blockstore.go           |  59 +++
 .../blkstorage/fsblkstorage/blockfile_mgr.go  |  16 +
 .../blkstorage/fsblkstorage/blockindex.go     |   7 +
 .../blkstorage/fsblkstorage/blocks_itr.go     |   8 +
 .../util/leveldbhelper/leveldb_helper.go      |  69 +++
 .../util/leveldbhelper/leveldb_provider.go    |  11 +-
 common/metrics/server.go                      | 271 +++++-----
 common/metrics/server_test.go                 | 242 ---------
 common/metrics/tally_provider.go              | 374 ++------------
 common/metrics/tally_provider_test.go         | 462 ------------------
 common/metrics/types.go                       |  45 --
 .../blocksprovider/blocksprovider.go          |  10 +
 .../historycouchdb/historycouchdb.go          |   2 +-
 .../historyleveldb/historyleveldb.go          |   7 +
 .../kvledger/inventory/cdbid/cdb_store.go     |   7 +-
 core/ledger/kvledger/kv_ledger.go             |  26 +
 .../privacyenabledstate/common_storage_db.go  |   9 +
 .../txmgmt/pvtstatepurgemgmt/expiry_keeper.go |   7 +-
 .../txmgmt/statedb/statecouchdb/batch_util.go |  10 +
 .../statedb/statecouchdb/commit_handling.go   |   7 +
 .../statedb/statecouchdb/statecouchdb.go      |   7 +
 .../statedb/stateleveldb/stateleveldb.go      |   7 +
 .../txmgmt/txmgr/lockbasedtxmgr/helper.go     |   1 +
 .../txmgr/lockbasedtxmgr/lockbased_txmgr.go   |  35 ++
 core/ledger/ledgerstorage/store.go            |  28 ++
 .../cdbpvtdata/cdb_pvtprovider.go             |   2 +-
 .../cdbpvtdata/common_store_impl.go           |  24 +
 core/ledger/util/couchdb/couchdb.go           |  44 +-
 gossip/privdata/coordinator.go                |  11 +
 gossip/state/payloads_buffer.go               |   9 +-
 gossip/state/state.go                         |  24 +-
 peer/node/start.go                            |   4 +
 34 files changed, 671 insertions(+), 1220 deletions(-)
 delete mode 100644 common/metrics/server_test.go
 delete mode 100644 common/metrics/tally_provider_test.go
 delete mode 100644 common/metrics/types.go

diff --git a/bccsp/pkcs11/impl.go b/bccsp/pkcs11/impl.go
index 148c9e198..9b0c77fe7 100644
--- a/bccsp/pkcs11/impl.go
+++ b/bccsp/pkcs11/impl.go
@@ -19,6 +19,7 @@ import (
 	"github.com/hyperledger/fabric/bccsp/sw"
 	"github.com/hyperledger/fabric/bccsp/utils"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/miekg/pkcs11"
 	"github.com/pkg/errors"
 )
@@ -384,6 +385,10 @@ func (csp *impl) KeyImport(raw interface{}, opts bccsp.KeyImportOpts) (k bccsp.K
 // GetKey returns the key this CSP associates to
 // the Subject Key Identifier ski.
 func (csp *impl) GetKey(ski []byte) (k bccsp.Key, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_getkey_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	pubKey, isPriv, err := csp.getECKey(ski)
 	if err == nil {
 		if isPriv {
@@ -402,6 +407,10 @@ func (csp *impl) GetKey(ski []byte) (k bccsp.Key, err error) {
 // the caller is responsible for hashing the larger message and passing
 // the hash (as digest).
 func (csp *impl) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) (signature []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_sign_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil")
@@ -421,6 +430,10 @@ func (csp *impl) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) (signat
 
 // Verify verifies signature against key k and digest
 func (csp *impl) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.SignerOpts) (valid bool, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_verify_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return false, errors.New("Invalid Key. It must not be nil")
@@ -446,6 +459,10 @@ func (csp *impl) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.Signer
 // Encrypt encrypts plaintext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *impl) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts) (ciphertext []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_encrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// TODO: Add PKCS11 support for encryption, when fabric starts requiring it
 	return csp.BCCSP.Encrypt(k, plaintext, opts)
 }
@@ -453,6 +470,10 @@ func (csp *impl) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts
 // Decrypt decrypts ciphertext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *impl) Decrypt(k bccsp.Key, ciphertext []byte, opts bccsp.DecrypterOpts) (plaintext []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_decrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	return csp.BCCSP.Decrypt(k, ciphertext, opts)
 }
 
diff --git a/bccsp/sw/impl.go b/bccsp/sw/impl.go
index d6a9da6c0..a172735da 100644
--- a/bccsp/sw/impl.go
+++ b/bccsp/sw/impl.go
@@ -21,6 +21,7 @@ import (
 
 	"github.com/hyperledger/fabric/bccsp"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/pkg/errors"
 )
 
@@ -165,6 +166,10 @@ func (csp *CSP) KeyImport(raw interface{}, opts bccsp.KeyImportOpts) (k bccsp.Ke
 // GetKey returns the key this CSP associates to
 // the Subject Key Identifier ski.
 func (csp *CSP) GetKey(ski []byte) (k bccsp.Key, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_getkey_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	k, err = csp.ks.GetKey(ski)
 	if err != nil {
 		return nil, errors.Wrapf(err, "Failed getting key for SKI [%v]", ski)
@@ -175,6 +180,10 @@ func (csp *CSP) GetKey(ski []byte) (k bccsp.Key, err error) {
 
 // Hash hashes messages msg using options opts.
 func (csp *CSP) Hash(msg []byte, opts bccsp.HashOpts) (digest []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_hash_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if opts == nil {
 		return nil, errors.New("Invalid opts. It must not be nil.")
@@ -221,6 +230,10 @@ func (csp *CSP) GetHash(opts bccsp.HashOpts) (h hash.Hash, err error) {
 // the caller is responsible for hashing the larger message and passing
 // the hash (as digest).
 func (csp *CSP) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) (signature []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_sign_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil.")
@@ -245,6 +258,10 @@ func (csp *CSP) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) (signatu
 
 // Verify verifies signature against key k and digest
 func (csp *CSP) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.SignerOpts) (valid bool, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_verify_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return false, errors.New("Invalid Key. It must not be nil.")
@@ -272,6 +289,10 @@ func (csp *CSP) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.SignerO
 // Encrypt encrypts plaintext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *CSP) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts) (ciphertext []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_encrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil.")
@@ -288,6 +309,10 @@ func (csp *CSP) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts)
 // Decrypt decrypts ciphertext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *CSP) Decrypt(k bccsp.Key, ciphertext []byte, opts bccsp.DecrypterOpts) (plaintext []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_decrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil.")
diff --git a/common/ledger/blkstorage/cdbblkstorage/cdb_blockstore.go b/common/ledger/blkstorage/cdbblkstorage/cdb_blockstore.go
index 6dfb4a7ab..f365649e2 100644
--- a/common/ledger/blkstorage/cdbblkstorage/cdb_blockstore.go
+++ b/common/ledger/blkstorage/cdbblkstorage/cdb_blockstore.go
@@ -18,6 +18,7 @@ import (
 
 	"github.com/hyperledger/fabric/common/ledger"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
 	"github.com/hyperledger/fabric/protos/common"
@@ -73,6 +74,11 @@ func (s *cdbBlockStore) AddBlock(block *common.Block) error {
 		// Nothing to do if not a committer
 		return nil
 	}
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_addBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 
 	logger.Debugf("Storing block %d", block.Header.Number)
 	err := s.storeBlock(block)
@@ -118,6 +124,12 @@ func (s *cdbBlockStore) storeTransactions(block *common.Block) error {
 func (s *cdbBlockStore) CheckpointBlock(block *common.Block) error {
 	logger.Debugf("[%s] Updating checkpoint for block [%d]", s.ledgerID, block.Header.Number)
 
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_checkpointBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	//Update the checkpoint info with the results of adding the new block
 	newCPInfo := &checkpointInfo{
 		isChainEmpty:    false,
@@ -141,6 +153,12 @@ func (s *cdbBlockStore) CheckpointBlock(block *common.Block) error {
 
 // GetBlockchainInfo returns the current info about blockchain
 func (s *cdbBlockStore) GetBlockchainInfo() (*common.BlockchainInfo, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_getBlockchainInfo_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	cpInfo, err := s.cp.getCheckpointInfo()
 	if err != nil {
 		return nil, err
@@ -171,11 +189,22 @@ func (s *cdbBlockStore) GetBlockchainInfo() (*common.BlockchainInfo, error) {
 
 // RetrieveBlocks returns an iterator that can be used for iterating over a range of blocks
 func (s *cdbBlockStore) RetrieveBlocks(startNum uint64) (ledger.ResultsIterator, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_retrieveBlocks_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	return newBlockItr(s, startNum), nil
 }
 
 // RetrieveBlockByHash returns the block for given block-hash
 func (s *cdbBlockStore) RetrieveBlockByHash(blockHash []byte) (*common.Block, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_retrieveBlockByHash_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	blockHashHex := hex.EncodeToString(blockHash)
 	const queryFmt = `
 	{
@@ -197,6 +226,12 @@ func (s *cdbBlockStore) RetrieveBlockByHash(blockHash []byte) (*common.Block, er
 
 // RetrieveBlockByNumber returns the block at a given blockchain height
 func (s *cdbBlockStore) RetrieveBlockByNumber(blockNum uint64) (*common.Block, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_retrieveBlockByNumber_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// interpret math.MaxUint64 as a request for last block
 	if blockNum == math.MaxUint64 {
 		bcinfo, err := s.GetBlockchainInfo()
@@ -226,6 +261,12 @@ func (s *cdbBlockStore) RetrieveBlockByNumber(blockNum uint64) (*common.Block, e
 
 // RetrieveTxByID returns a transaction for given transaction id
 func (s *cdbBlockStore) RetrieveTxByID(txID string) (*common.Envelope, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_retrieveTxByID_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	doc, _, err := s.txnStore.ReadDoc(txID)
 	if err != nil {
 		// note: allow ErrNotFoundInIndex to pass through
@@ -257,6 +298,12 @@ func (s *cdbBlockStore) RetrieveTxByID(txID string) (*common.Envelope, error) {
 
 // RetrieveTxByBlockNumTranNum returns a transaction for given block ID and transaction ID
 func (s *cdbBlockStore) RetrieveTxByBlockNumTranNum(blockNum uint64, tranNum uint64) (*common.Envelope, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_retrieveTxByBlockNumTranNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	block, err := s.RetrieveBlockByNumber(blockNum)
 	if err != nil {
 		// note: allow ErrNotFoundInIndex to pass through
@@ -269,6 +316,12 @@ func (s *cdbBlockStore) RetrieveTxByBlockNumTranNum(blockNum uint64, tranNum uin
 
 // RetrieveBlockByTxID returns a block for a given transaction ID
 func (s *cdbBlockStore) RetrieveBlockByTxID(txID string) (*common.Block, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_retrieveBlockByTxID_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	blockHash, err := s.retrieveBlockHashByTxID(txID)
 	if err != nil {
 		// note: allow ErrNotFoundInIndex to pass through
@@ -305,6 +358,12 @@ func (s *cdbBlockStore) retrieveBlockHashByTxID(txID string) ([]byte, error) {
 
 // RetrieveTxValidationCodeByTxID returns a TX validation code for a given transaction ID
 func (s *cdbBlockStore) RetrieveTxValidationCodeByTxID(txID string) (peer.TxValidationCode, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("blkstorage_couchdb_retrieveTxValidationCodeByTxID_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	jsonResult, err := retrieveJSONQuery(s.txnStore, txID)
 	if err != nil {
 		// note: allow ErrNotFoundInIndex to pass through
diff --git a/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go b/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
index f2a1a522b..f781d3fba 100644
--- a/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
+++ b/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
@@ -29,6 +29,7 @@ import (
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
 	putil "github.com/hyperledger/fabric/protos/utils"
@@ -247,6 +248,11 @@ func (mgr *blockfileMgr) moveToNextFile() {
 }
 
 func (mgr *blockfileMgr) addBlock(block *common.Block) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_addBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	if block.Header.Number != mgr.getBlockchainInfo().Height {
 		return fmt.Errorf("Block number should have been %d but was %d", mgr.getBlockchainInfo().Height, block.Header.Number)
 	}
@@ -428,6 +434,12 @@ func (mgr *blockfileMgr) getBlockchainInfo() *common.BlockchainInfo {
 }
 
 func (mgr *blockfileMgr) updateCheckpoint(cpInfo *checkpointInfo) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_updateCheckpoint_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	mgr.cpInfoCond.L.Lock()
 	defer mgr.cpInfoCond.L.Unlock()
 	mgr.cpInfo = cpInfo
@@ -595,6 +607,10 @@ func (mgr *blockfileMgr) loadCurrentInfo() (*checkpointInfo, error) {
 }
 
 func (mgr *blockfileMgr) saveCurrentInfo(i *checkpointInfo, sync bool) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_saveCurrentInfo_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	b, err := i.marshal()
 	if err != nil {
 		return err
diff --git a/common/ledger/blkstorage/fsblkstorage/blockindex.go b/common/ledger/blkstorage/fsblkstorage/blockindex.go
index 3d21b7e45..2060bb9e5 100644
--- a/common/ledger/blkstorage/fsblkstorage/blockindex.go
+++ b/common/ledger/blkstorage/fsblkstorage/blockindex.go
@@ -25,6 +25,7 @@ import (
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	ledgerUtil "github.com/hyperledger/fabric/core/ledger/util"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
@@ -98,6 +99,12 @@ func (index *blockIndex) getLastBlockIndexed() (uint64, error) {
 }
 
 func (index *blockIndex) indexBlock(blockIdxInfo *blockIdxInfo) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_indexBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// do not index anything
 	if len(index.indexItemsMap) == 0 {
 		logger.Debug("Not indexing block... as nothing to index")
diff --git a/common/ledger/blkstorage/fsblkstorage/blocks_itr.go b/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
index 3e06887b6..28d00d944 100644
--- a/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
+++ b/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
@@ -20,6 +20,7 @@ import (
 	"sync"
 
 	"github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/metrics"
 )
 
 // blocksItr - an iterator for iterating over a sequence of blocks
@@ -37,6 +38,13 @@ func newBlockItr(mgr *blockfileMgr, startBlockNum uint64) *blocksItr {
 }
 
 func (itr *blocksItr) waitForBlock(blockNum uint64) uint64 {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_waitForBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	itr.mgr.cpInfoCond.L.Lock()
 	defer itr.mgr.cpInfoCond.L.Unlock()
 	for itr.mgr.cpInfo.lastBlockNumber < blockNum && !itr.shouldClose() {
diff --git a/common/ledger/util/leveldbhelper/leveldb_helper.go b/common/ledger/util/leveldbhelper/leveldb_helper.go
index 9e2ab17df..0a65722ef 100644
--- a/common/ledger/util/leveldbhelper/leveldb_helper.go
+++ b/common/ledger/util/leveldbhelper/leveldb_helper.go
@@ -20,8 +20,15 @@ import (
 	"fmt"
 	"sync"
 
+	"strings"
+
+	"bytes"
+	"time"
+
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util"
+	"github.com/hyperledger/fabric/common/metrics"
+	"github.com/spf13/viper"
 	"github.com/syndtr/goleveldb/leveldb"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
 	"github.com/syndtr/goleveldb/leveldb/opt"
@@ -88,6 +95,9 @@ func (dbInst *DB) Open() {
 		panic(fmt.Sprintf("Error while trying to open DB: %s", err))
 	}
 	dbInst.dbState = opened
+	if viper.GetBool("logging.leveldbState") {
+		go dbInst.getState()
+	}
 }
 
 // Close closes the underlying db
@@ -105,6 +115,18 @@ func (dbInst *DB) Close() {
 
 // Get returns the value for the given key
 func (dbInst *DB) Get(key []byte) ([]byte, error) {
+	if metrics.IsDebug() {
+		dbName := dbInst.conf.DBPath
+		if strings.Contains(dbName, "ledgersData/") {
+			dbName = metrics.FilterMetricName(strings.Split(dbName, "ledgersData/")[1])
+		} else {
+			dbName = metrics.FilterMetricName(dbName)
+		}
+		ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_get_%s_processing_time_seconds", dbName))
+		ccStopWatch := ccTimer.Start()
+		defer ccStopWatch.Stop()
+	}
+
 	value, err := dbInst.db.Get(key, dbInst.readOpts)
 	if err == leveldb.ErrNotFound {
 		value = nil
@@ -119,6 +141,17 @@ func (dbInst *DB) Get(key []byte) ([]byte, error) {
 
 // Put saves the key/value
 func (dbInst *DB) Put(key []byte, value []byte, sync bool) error {
+	if metrics.IsDebug() {
+		dbName := dbInst.conf.DBPath
+		if strings.Contains(dbName, "ledgersData/") {
+			dbName = metrics.FilterMetricName(strings.Split(dbName, "ledgersData/")[1])
+		} else {
+			dbName = metrics.FilterMetricName(dbName)
+		}
+		ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_put_%s_processing_time_seconds", dbName))
+		ccStopWatch := ccTimer.Start()
+		defer ccStopWatch.Stop()
+	}
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -133,6 +166,17 @@ func (dbInst *DB) Put(key []byte, value []byte, sync bool) error {
 
 // Delete deletes the given key
 func (dbInst *DB) Delete(key []byte, sync bool) error {
+	if metrics.IsDebug() {
+		dbName := dbInst.conf.DBPath
+		if strings.Contains(dbName, "ledgersData/") {
+			dbName = metrics.FilterMetricName(strings.Split(dbName, "ledgersData/")[1])
+		} else {
+			dbName = metrics.FilterMetricName(dbName)
+		}
+		ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_delete_%s_processing_time_seconds", dbName))
+		ccStopWatch := ccTimer.Start()
+		defer ccStopWatch.Stop()
+	}
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -154,6 +198,10 @@ func (dbInst *DB) GetIterator(startKey []byte, endKey []byte) iterator.Iterator
 
 // WriteBatch writes a batch
 func (dbInst *DB) WriteBatch(batch *leveldb.Batch, sync bool) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("leveldbhelper_db_WriteBatch_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -163,3 +211,24 @@ func (dbInst *DB) WriteBatch(batch *leveldb.Batch, sync bool) error {
 	}
 	return nil
 }
+
+func (dbInst *DB) getState() {
+	for {
+		time.Sleep(5 * time.Second)
+		if dbInst.dbState == closed {
+			logger.Info("leveldb is closed exit the getState")
+			break
+		}
+		levelDBStats := []string{"stats", "iostats", "writedelay", "sstables", "blockpool", "cachedblock", "openedtables", "alivesnaps", "aliveiters"}
+		var b bytes.Buffer
+		for _, stats := range levelDBStats {
+			res, err := dbInst.db.GetProperty(fmt.Sprintf("leveldb.%s", stats))
+			if err != nil {
+				logger.Errorf("leveldb getState %s return error %s", stats, err)
+				continue
+			}
+			b.WriteString(res)
+		}
+		logger.Infof("******* leveldb getState %s", strings.Replace(b.String(), "\n", " ", -1))
+	}
+}
diff --git a/common/ledger/util/leveldbhelper/leveldb_provider.go b/common/ledger/util/leveldbhelper/leveldb_provider.go
index db41651d7..8bcc91a2c 100644
--- a/common/ledger/util/leveldbhelper/leveldb_provider.go
+++ b/common/ledger/util/leveldbhelper/leveldb_provider.go
@@ -20,6 +20,7 @@ import (
 	"bytes"
 	"sync"
 
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/syndtr/goleveldb/leveldb"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
 )
@@ -32,13 +33,14 @@ type Provider struct {
 	db        *DB
 	dbHandles map[string]*DBHandle
 	mux       sync.Mutex
+	dbPath    string
 }
 
 // NewProvider constructs a Provider
 func NewProvider(conf *Conf) *Provider {
 	db := CreateDB(conf)
 	db.Open()
-	return &Provider{db, make(map[string]*DBHandle), sync.Mutex{}}
+	return &Provider{db, make(map[string]*DBHandle), sync.Mutex{}, conf.DBPath}
 }
 
 // GetDBHandle returns a handle to a named db
@@ -47,7 +49,7 @@ func (p *Provider) GetDBHandle(dbName string) *DBHandle {
 	defer p.mux.Unlock()
 	dbHandle := p.dbHandles[dbName]
 	if dbHandle == nil {
-		dbHandle = &DBHandle{dbName, p.db}
+		dbHandle = &DBHandle{dbName, p.db, p.dbPath}
 		p.dbHandles[dbName] = dbHandle
 	}
 	return dbHandle
@@ -62,6 +64,7 @@ func (p *Provider) Close() {
 type DBHandle struct {
 	dbName string
 	db     *DB
+	dbPath string
 }
 
 // Get returns the value for the given key
@@ -81,6 +84,10 @@ func (h *DBHandle) Delete(key []byte, sync bool) error {
 
 // WriteBatch writes a batch in an atomic way
 func (h *DBHandle) WriteBatch(batch *UpdateBatch, sync bool) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("leveldbhelper_dbhandle_WriteBatch_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	levelBatch := &leveldb.Batch{}
 	for k, v := range batch.KVs {
 		key := constructLevelKey(h.dbName, []byte(k))
diff --git a/common/metrics/server.go b/common/metrics/server.go
index 677151e78..fc42eac09 100644
--- a/common/metrics/server.go
+++ b/common/metrics/server.go
@@ -8,11 +8,21 @@ package metrics
 
 import (
 	"fmt"
-	"sync"
+	"io"
 	"time"
 
+	"sync"
+
+	"strings"
+
+	"runtime"
+
+	"regexp"
+
+	"github.com/pkg/errors"
 	"github.com/spf13/viper"
 	"github.com/uber-go/tally"
+	promreporter "github.com/uber-go/tally/prometheus"
 )
 
 const (
@@ -28,21 +38,112 @@ const (
 	defaultStatsdReporterFlushBytes    = 1432
 )
 
-var RootScope Scope
-var once sync.Once
+const (
+	peerConfigFileName = "core"
+	peerConfigPath     = "/etc/hyperledger/fabric"
+	cmdRootPrefix      = "core"
+)
+
+var peerConfig *viper.Viper
+var peerConfigPathOverride string
+
+// RootScope tally.NoopScope is a scope that does nothing
+var RootScope = tally.NoopScope
 var rootScopeMutex = &sync.Mutex{}
 var running bool
+var debugOn bool
+
+// StatsdReporterOpts ...
+type StatsdReporterOpts struct {
+	Address       string
+	Prefix        string
+	FlushInterval time.Duration
+	FlushBytes    int
+}
+
+// PromReporterOpts ...
+type PromReporterOpts struct {
+	ListenAddress string
+}
+
+// Opts ...
+type Opts struct {
+	Reporter           string
+	Interval           time.Duration
+	Enabled            bool
+	StatsdReporterOpts StatsdReporterOpts
+	PromReporterOpts   PromReporterOpts
+}
+
+// IsDebug ...
+func IsDebug() bool {
+	return debugOn
+}
+
+var reg *regexp.Regexp
 
-// NewOpts create metrics options based config file
-func NewOpts() Opts {
+// Initialize ...
+func Initialize() {
+
+	// load peer config
+	if err := initPeerConfig(); err != nil {
+		panic(fmt.Sprintf("error initPeerConfig %v", err))
+	}
+
+	if peerConfig.GetBool("peer.profile.enabled") {
+		runtime.SetMutexProfileFraction(5)
+	}
+	if peerConfig.GetBool("metrics.enabled") {
+		debugOn = peerConfig.GetBool("metrics.debug.enabled")
+	}
+
+	// start metric server
+	opts := NewOpts(peerConfig)
+	err := Start(opts)
+	if err != nil {
+		logger.Errorf("Failed to start metrics collection: %s", err)
+	}
+
+	reg = regexp.MustCompile("[^a-zA-Z0-9_]+")
+
+	logger.Info("Fabric Bootstrap filter initialized")
+}
+
+func FilterMetricName(name string) string {
+	return reg.ReplaceAllString(name, "_")
+}
+
+func initPeerConfig() error {
+	peerConfig = viper.New()
+	peerConfig.AddConfigPath(peerConfigPath)
+	if peerConfigPathOverride != "" {
+		peerConfig.AddConfigPath(peerConfigPathOverride)
+	}
+	peerConfig.SetConfigName(peerConfigFileName)
+	peerConfig.SetEnvPrefix(cmdRootPrefix)
+	peerConfig.AutomaticEnv()
+	peerConfig.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
+	err := peerConfig.ReadInConfig()
+	if err != nil {
+		return err
+	}
+
+	return nil
+}
+
+// NewOpts create metrics options based config file.
+// TODO: Currently this is only for peer node which uses global viper.
+// As for orderer, which uses its local viper, we are unable to get
+// metrics options with the function NewOpts()
+func NewOpts(peerConfig *viper.Viper) Opts {
 	opts := Opts{}
-	opts.Enabled = viper.GetBool("metrics.enabled")
-	if report := viper.GetString("metrics.reporter"); report != "" {
+	opts.Enabled = peerConfig.GetBool("metrics.enabled")
+	if report := peerConfig.GetString("metrics.reporter"); report != "" {
 		opts.Reporter = report
 	} else {
 		opts.Reporter = defaultReporterType
 	}
-	if interval := viper.GetDuration("metrics.interval"); interval > 0 {
+	if interval := peerConfig.GetDuration("metrics.interval"); interval > 0 {
 		opts.Interval = interval
 	} else {
 		opts.Interval = defaultInterval
@@ -50,13 +151,17 @@ func NewOpts() Opts {
 
 	if opts.Reporter == statsdReporterType {
 		statsdOpts := StatsdReporterOpts{}
-		statsdOpts.Address = viper.GetString("metrics.statsdReporter.address")
-		if flushInterval := viper.GetDuration("metrics.statsdReporter.flushInterval"); flushInterval > 0 {
+		statsdOpts.Address = peerConfig.GetString("metrics.statsdReporter.address")
+		statsdOpts.Prefix = peerConfig.GetString("metrics.statsdReporter.prefix")
+		if statsdOpts.Prefix == "" && !peerConfig.IsSet("peer.id") {
+			statsdOpts.Prefix = peerConfig.GetString("peer.id")
+		}
+		if flushInterval := peerConfig.GetDuration("metrics.statsdReporter.flushInterval"); flushInterval > 0 {
 			statsdOpts.FlushInterval = flushInterval
 		} else {
 			statsdOpts.FlushInterval = defaultStatsdReporterFlushInterval
 		}
-		if flushBytes := viper.GetInt("metrics.statsdReporter.flushBytes"); flushBytes > 0 {
+		if flushBytes := peerConfig.GetInt("metrics.statsdReporter.flushBytes"); flushBytes > 0 {
 			statsdOpts.FlushBytes = flushBytes
 		} else {
 			statsdOpts.FlushBytes = defaultStatsdReporterFlushBytes
@@ -66,45 +171,47 @@ func NewOpts() Opts {
 
 	if opts.Reporter == promReporterType {
 		promOpts := PromReporterOpts{}
-		promOpts.ListenAddress = viper.GetString("metrics.promReporter.listenAddress")
+		promOpts.ListenAddress = peerConfig.GetString("metrics.fabric.PromReporter.listenAddress")
 		opts.PromReporterOpts = promOpts
 	}
 
 	return opts
 }
 
-//Init initializes global root metrics scope instance, all callers can only use it to extend sub scope
-func Init(opts Opts) (err error) {
-	once.Do(func() {
-		RootScope, err = create(opts)
-	})
-
-	return
-}
-
-//Start starts metrics server
-func Start() error {
+// Start starts metrics server
+func Start(opts Opts) error {
+	if !opts.Enabled {
+		return errors.New("Unable to start metrics server because is disbled")
+	}
 	rootScopeMutex.Lock()
 	defer rootScopeMutex.Unlock()
-	if running {
-		return nil
+	if !running {
+		rootScope, err := create(opts)
+		if err == nil {
+			running = true
+			RootScope = rootScope
+		}
+		return err
 	}
-	running = true
-	return RootScope.Start()
+	return errors.New("metrics server was already started")
 }
 
-//Shutdown closes underlying resources used by metrics server
+// Shutdown closes underlying resources used by metrics server
 func Shutdown() error {
 	rootScopeMutex.Lock()
 	defer rootScopeMutex.Unlock()
-	if !running {
-		return nil
+	if running {
+		var err error
+		if closer, ok := RootScope.(io.Closer); ok {
+			if err = closer.Close(); err != nil {
+				return err
+			}
+		}
+		running = false
+		RootScope = tally.NoopScope
+		return err
 	}
-
-	err := RootScope.Close()
-	RootScope = nil
-	running = false
-	return err
+	return nil
 }
 
 func isRunning() bool {
@@ -113,109 +220,35 @@ func isRunning() bool {
 	return running
 }
 
-type StatsdReporterOpts struct {
-	Address       string
-	FlushInterval time.Duration
-	FlushBytes    int
-}
-
-type PromReporterOpts struct {
-	ListenAddress string
-}
-
-type Opts struct {
-	Reporter           string
-	Interval           time.Duration
-	Enabled            bool
-	StatsdReporterOpts StatsdReporterOpts
-	PromReporterOpts   PromReporterOpts
-}
-
-type noOpCounter struct {
-}
-
-func (c *noOpCounter) Inc(v int64) {
-
-}
-
-type noOpGauge struct {
-}
-
-func (g *noOpGauge) Update(v float64) {
-
-}
-
-type noOpScope struct {
-	counter *noOpCounter
-	gauge   *noOpGauge
-}
-
-func (s *noOpScope) Counter(name string) Counter {
-	return s.counter
-}
-
-func (s *noOpScope) Gauge(name string) Gauge {
-	return s.gauge
-}
-
-func (s *noOpScope) Tagged(tags map[string]string) Scope {
-	return s
-}
-
-func (s *noOpScope) SubScope(prefix string) Scope {
-	return s
-}
-
-func (s *noOpScope) Close() error {
-	return nil
-}
-
-func (s *noOpScope) Start() error {
-	return nil
-}
-
-func newNoOpScope() Scope {
-	return &noOpScope{
-		counter: &noOpCounter{},
-		gauge:   &noOpGauge{},
-	}
-}
-
-func create(opts Opts) (rootScope Scope, e error) {
+func create(opts Opts) (rootScope tally.Scope, e error) {
 	if !opts.Enabled {
-		rootScope = newNoOpScope()
-		return
+		rootScope = tally.NoopScope
 	} else {
 		if opts.Interval <= 0 {
 			e = fmt.Errorf("invalid Interval option %d", opts.Interval)
 			return
 		}
-
-		if opts.Reporter != statsdReporterType && opts.Reporter != promReporterType {
-			e = fmt.Errorf("not supported Reporter type %s", opts.Reporter)
-			return
-		}
-
 		var reporter tally.StatsReporter
 		var cachedReporter tally.CachedStatsReporter
-		if opts.Reporter == statsdReporterType {
+		switch opts.Reporter {
+		case statsdReporterType:
 			reporter, e = newStatsdReporter(opts.StatsdReporterOpts)
-		}
-
-		if opts.Reporter == promReporterType {
+		case promReporterType:
 			cachedReporter, e = newPromReporter(opts.PromReporterOpts)
+		default:
+			e = fmt.Errorf("not supported Reporter type %s", opts.Reporter)
+			return
 		}
-
 		if e != nil {
 			return
 		}
-
 		rootScope = newRootScope(
 			tally.ScopeOptions{
 				Prefix:         namespace,
 				Reporter:       reporter,
 				CachedReporter: cachedReporter,
+				Separator:      promreporter.DefaultSeparator,
 			}, opts.Interval)
-		return
 	}
+	return
 }
diff --git a/common/metrics/server_test.go b/common/metrics/server_test.go
deleted file mode 100644
index 07d010ca8..000000000
--- a/common/metrics/server_test.go
+++ /dev/null
@@ -1,242 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import (
-	"fmt"
-	"strings"
-	"testing"
-	"time"
-
-	"github.com/hyperledger/fabric/core/config/configtest"
-	. "github.com/onsi/gomega"
-	"github.com/spf13/viper"
-	"github.com/stretchr/testify/assert"
-)
-
-func TestStartSuccessStatsd(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: statsdReporterType,
-		Interval: 1 * time.Second,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		}}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartSuccessProm(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: promReporterType,
-		Interval: 1 * time.Second,
-		PromReporterOpts: PromReporterOpts{
-			ListenAddress: "127.0.0.1:0",
-		}}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartDisabled(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled: false,
-	}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartInvalidInterval(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 0,
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidAddress(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidFlushInterval(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 0,
-			FlushBytes:    512,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartPromInvalidListernAddress(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		PromReporterOpts: PromReporterOpts{
-			ListenAddress: "",
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidFlushBytes(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    0,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartInvalidReporter(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: "test",
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartAndClose(t *testing.T) {
-	t.Parallel()
-	gt := NewGomegaWithT(t)
-	defer Shutdown()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: statsdReporterType,
-		Interval: 1 * time.Second,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		}}
-	Init(opts)
-	assert.NotNil(t, RootScope)
-	go Start()
-	gt.Eventually(isRunning).Should(BeTrue())
-}
-
-func TestNoOpScopeMetrics(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled: false,
-	}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-
-	// make sure no error throws when invoke noOpScope
-	subScope := s.SubScope("test")
-	subScope.Counter("foo").Inc(2)
-	subScope.Gauge("bar").Update(1.33)
-	tagSubScope := subScope.Tagged(map[string]string{"env": "test"})
-	tagSubScope.Counter("foo").Inc(2)
-	tagSubScope.Gauge("bar").Update(1.33)
-}
-
-func TestNewOpts(t *testing.T) {
-	t.Parallel()
-	defer viper.Reset()
-	setupTestConfig()
-	opts := NewOpts()
-	assert.False(t, opts.Enabled)
-	assert.Equal(t, 1*time.Second, opts.Interval)
-	assert.Equal(t, statsdReporterType, opts.Reporter)
-	assert.Equal(t, 1432, opts.StatsdReporterOpts.FlushBytes)
-	assert.Equal(t, 2*time.Second, opts.StatsdReporterOpts.FlushInterval)
-	assert.Equal(t, "0.0.0.0:8125", opts.StatsdReporterOpts.Address)
-	viper.Reset()
-
-	setupTestConfig()
-	viper.Set("metrics.Reporter", promReporterType)
-	opts1 := NewOpts()
-	assert.False(t, opts1.Enabled)
-	assert.Equal(t, 1*time.Second, opts1.Interval)
-	assert.Equal(t, promReporterType, opts1.Reporter)
-	assert.Equal(t, "0.0.0.0:8080", opts1.PromReporterOpts.ListenAddress)
-}
-
-func TestNewOptsDefaultVar(t *testing.T) {
-	t.Parallel()
-	opts := NewOpts()
-	assert.False(t, opts.Enabled)
-	assert.Equal(t, 1*time.Second, opts.Interval)
-	assert.Equal(t, statsdReporterType, opts.Reporter)
-	assert.Equal(t, 1432, opts.StatsdReporterOpts.FlushBytes)
-	assert.Equal(t, 2*time.Second, opts.StatsdReporterOpts.FlushInterval)
-}
-
-func setupTestConfig() {
-	viper.SetConfigName("core")
-	viper.SetEnvPrefix("CORE")
-	viper.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
-	viper.AutomaticEnv()
-
-	err := configtest.AddDevConfigPath(nil)
-	if err != nil {
-		panic(fmt.Errorf("Fatal error adding dev dir: %s \n", err))
-	}
-
-	err = viper.ReadInConfig()
-	if err != nil { // Handle errors reading the config file
-		panic(fmt.Errorf("Fatal error config file: %s \n", err))
-	}
-}
diff --git a/common/metrics/tally_provider.go b/common/metrics/tally_provider.go
index cf1e3deff..aca8d54ab 100644
--- a/common/metrics/tally_provider.go
+++ b/common/metrics/tally_provider.go
@@ -7,17 +7,16 @@ SPDX-License-Identifier: Apache-2.0
 package metrics
 
 import (
-	"context"
 	"errors"
-	"fmt"
-	"io"
 	"net/http"
-	"sort"
-	"sync"
 	"time"
 
+	"net"
+
+	"sort"
+
 	"github.com/cactus/go-statsd-client/statsd"
-	"github.com/op/go-logging"
+	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/prometheus/client_golang/prometheus"
 	"github.com/prometheus/client_golang/prometheus/promhttp"
 	"github.com/uber-go/tally"
@@ -25,74 +24,11 @@ import (
 	statsdreporter "github.com/uber-go/tally/statsd"
 )
 
-var logger = logging.MustGetLogger("common/metrics/tally")
-
-var scopeRegistryKey = tally.KeyForPrefixedStringMap
-
-type counter struct {
-	tallyCounter tally.Counter
-}
-
-func newCounter(tallyCounter tally.Counter) *counter {
-	return &counter{tallyCounter: tallyCounter}
-}
-
-func (c *counter) Inc(v int64) {
-	c.tallyCounter.Inc(v)
-}
-
-type gauge struct {
-	tallyGauge tally.Gauge
-}
-
-func newGauge(tallyGauge tally.Gauge) *gauge {
-	return &gauge{tallyGauge: tallyGauge}
-}
+var logger = flogging.MustGetLogger("common/metrics/tally")
 
-func (g *gauge) Update(v float64) {
-	g.tallyGauge.Update(v)
-}
-
-type scopeRegistry struct {
-	sync.RWMutex
-	subScopes map[string]*scope
-}
-
-type scope struct {
-	separator    string
-	prefix       string
-	tags         map[string]string
-	tallyScope   tally.Scope
-	registry     *scopeRegistry
-	baseReporter tally.BaseStatsReporter
-
-	cm sync.RWMutex
-	gm sync.RWMutex
-
-	counters map[string]*counter
-	gauges   map[string]*gauge
-}
-
-func newRootScope(opts tally.ScopeOptions, interval time.Duration) Scope {
+func newRootScope(opts tally.ScopeOptions, interval time.Duration) tally.Scope {
 	s, _ := tally.NewRootScope(opts, interval)
-
-	var baseReporter tally.BaseStatsReporter
-	if opts.Reporter != nil {
-		baseReporter = opts.Reporter
-	} else if opts.CachedReporter != nil {
-		baseReporter = opts.CachedReporter
-	}
-
-	return &scope{
-		prefix:     opts.Prefix,
-		separator:  opts.Separator,
-		tallyScope: s,
-		registry: &scopeRegistry{
-			subScopes: make(map[string]*scope),
-		},
-		baseReporter: baseReporter,
-		counters:     make(map[string]*counter),
-		gauges:       make(map[string]*gauge)}
+	return s
 }
 
 func newStatsdReporter(statsdReporterOpts StatsdReporterOpts) (tally.StatsReporter, error) {
@@ -109,13 +45,13 @@ func newStatsdReporter(statsdReporterOpts StatsdReporterOpts) (tally.StatsReport
 	}
 
 	statter, err := statsd.NewBufferedClient(statsdReporterOpts.Address,
-		"", statsdReporterOpts.FlushInterval, statsdReporterOpts.FlushBytes)
+		statsdReporterOpts.Prefix, statsdReporterOpts.FlushInterval, statsdReporterOpts.FlushBytes)
 	if err != nil {
 		return nil, err
 	}
 	opts := statsdreporter.Options{}
 	reporter := statsdreporter.NewReporter(statter, opts)
-	statsdReporter := &statsdReporter{reporter: reporter, statter: statter}
+	statsdReporter := &statsdReporter{StatsReporter: reporter, statter: statter}
 	return statsdReporter, nil
 }
 
@@ -127,159 +63,52 @@ func newPromReporter(promReporterOpts PromReporterOpts) (promreporter.Reporter,
 	opts := promreporter.Options{Registerer: prometheus.NewRegistry()}
 	reporter := promreporter.NewReporter(opts)
 	mux := http.NewServeMux()
-	handler := promReporterHttpHandler(opts.Registerer.(*prometheus.Registry))
+	handler := promReporterHTTPHandler(opts.Registerer.(*prometheus.Registry))
 	mux.Handle("/metrics", handler)
-	server := &http.Server{Addr: promReporterOpts.ListenAddress, Handler: mux}
+	server := &http.Server{Handler: mux}
+	addr := promReporterOpts.ListenAddress
+	if addr == "" {
+		addr = ":http"
+	}
+	listener, err := net.Listen("tcp", addr)
+	if err != nil {
+		return nil, err
+	}
 	promReporter := &promReporter{
-		reporter: reporter,
+		Reporter: reporter,
 		server:   server,
-		registry: opts.Registerer.(*prometheus.Registry)}
+		registry: opts.Registerer.(*prometheus.Registry),
+		listener: listener}
+	go server.Serve(listener)
 	return promReporter, nil
 }
 
-func (s *scope) Counter(name string) Counter {
-	s.cm.RLock()
-	val, ok := s.counters[name]
-	s.cm.RUnlock()
-	if !ok {
-		s.cm.Lock()
-		val, ok = s.counters[name]
-		if !ok {
-			counter := s.tallyScope.Counter(name)
-			val = newCounter(counter)
-			s.counters[name] = val
-		}
-		s.cm.Unlock()
-	}
-	return val
-}
-
-func (s *scope) Gauge(name string) Gauge {
-	s.gm.RLock()
-	val, ok := s.gauges[name]
-	s.gm.RUnlock()
-	if !ok {
-		s.gm.Lock()
-		val, ok = s.gauges[name]
-		if !ok {
-			gauge := s.tallyScope.Gauge(name)
-			val = newGauge(gauge)
-			s.gauges[name] = val
-		}
-		s.gm.Unlock()
-	}
-	return val
-}
-
-func (s *scope) Tagged(tags map[string]string) Scope {
-	originTags := tags
-	tags = mergeRightTags(s.tags, tags)
-	key := scopeRegistryKey(s.prefix, tags)
-
-	s.registry.RLock()
-	existing, ok := s.registry.subScopes[key]
-	if ok {
-		s.registry.RUnlock()
-		return existing
-	}
-	s.registry.RUnlock()
-
-	s.registry.Lock()
-	defer s.registry.Unlock()
-
-	existing, ok = s.registry.subScopes[key]
-	if ok {
-		return existing
-	}
-
-	subScope := &scope{
-		separator: s.separator,
-		prefix:    s.prefix,
-		// NB(r): Take a copy of the tags on creation
-		// so that it cannot be modified after set.
-		tags:       copyStringMap(tags),
-		tallyScope: s.tallyScope.Tagged(originTags),
-		registry:   s.registry,
-
-		counters: make(map[string]*counter),
-		gauges:   make(map[string]*gauge),
-	}
-
-	s.registry.subScopes[key] = subScope
-	return subScope
-}
-
-func (s *scope) SubScope(prefix string) Scope {
-	key := scopeRegistryKey(s.fullyQualifiedName(prefix), s.tags)
-
-	s.registry.RLock()
-	existing, ok := s.registry.subScopes[key]
-	if ok {
-		s.registry.RUnlock()
-		return existing
-	}
-	s.registry.RUnlock()
-
-	s.registry.Lock()
-	defer s.registry.Unlock()
-
-	existing, ok = s.registry.subScopes[key]
-	if ok {
-		return existing
-	}
-
-	subScope := &scope{
-		separator: s.separator,
-		prefix:    s.prefix,
-		// NB(r): Take a copy of the tags on creation
-		// so that it cannot be modified after set.
-		tags:       copyStringMap(s.tags),
-		tallyScope: s.tallyScope.SubScope(prefix),
-		registry:   s.registry,
-
-		counters: make(map[string]*counter),
-		gauges:   make(map[string]*gauge),
-	}
-
-	s.registry.subScopes[key] = subScope
-	return subScope
-}
-
-func (s *scope) Close() error {
-	if closer, ok := s.tallyScope.(io.Closer); ok {
-		return closer.Close()
-	}
-	return nil
-}
-
-func (s *scope) Start() error {
-	if server, ok := s.baseReporter.(serve); ok {
-		return server.Start()
-	}
-	return nil
-}
-
 type statsdReporter struct {
-	reporter tally.StatsReporter
-	statter  statsd.Statter
+	tally.StatsReporter
+	statter statsd.Statter
 }
 
 type promReporter struct {
-	reporter promreporter.Reporter
+	promreporter.Reporter
 	server   *http.Server
+	listener net.Listener
 	registry *prometheus.Registry
 }
 
+func (r *statsdReporter) Close() error {
+	return r.statter.Close()
+}
+
 func (r *statsdReporter) ReportCounter(name string, tags map[string]string, value int64) {
-	r.reporter.ReportCounter(tagsToName(name, tags), tags, value)
+	r.StatsReporter.ReportCounter(tagsToName(name, tags), tags, value)
 }
 
 func (r *statsdReporter) ReportGauge(name string, tags map[string]string, value float64) {
-	r.reporter.ReportGauge(tagsToName(name, tags), tags, value)
+	r.StatsReporter.ReportGauge(tagsToName(name, tags), tags, value)
 }
 
 func (r *statsdReporter) ReportTimer(name string, tags map[string]string, interval time.Duration) {
-	r.reporter.ReportTimer(tagsToName(name, tags), tags, interval)
+	r.StatsReporter.ReportTimer(tagsToName(name, tags), tags, interval)
 }
 
 func (r *statsdReporter) ReportHistogramValueSamples(
@@ -290,7 +119,7 @@ func (r *statsdReporter) ReportHistogramValueSamples(
 	bucketUpperBound float64,
 	samples int64,
 ) {
-	r.reporter.ReportHistogramValueSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
+	r.StatsReporter.ReportHistogramValueSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
 }
 
 func (r *statsdReporter) ReportHistogramDurationSamples(
@@ -301,7 +130,7 @@ func (r *statsdReporter) ReportHistogramDurationSamples(
 	bucketUpperBound time.Duration,
 	samples int64,
 ) {
-	r.reporter.ReportHistogramDurationSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
+	r.StatsReporter.ReportHistogramDurationSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
 }
 
 func (r *statsdReporter) Capabilities() tally.Capabilities {
@@ -316,127 +145,20 @@ func (r *statsdReporter) Tagging() bool {
 	return true
 }
 
-func (r *statsdReporter) Flush() {
-	// no-op
-}
-
-func (r *statsdReporter) Close() error {
-	return r.statter.Close()
-}
-
-func (r *promReporter) RegisterCounter(
-	name string,
-	tagKeys []string,
-	desc string,
-) (*prometheus.CounterVec, error) {
-	return r.reporter.RegisterCounter(name, tagKeys, desc)
-}
-
-// AllocateCounter implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateCounter(name string, tags map[string]string) tally.CachedCount {
-	return r.reporter.AllocateCounter(name, tags)
-}
-
-func (r *promReporter) RegisterGauge(
-	name string,
-	tagKeys []string,
-	desc string,
-) (*prometheus.GaugeVec, error) {
-	return r.reporter.RegisterGauge(name, tagKeys, desc)
-}
-
-// AllocateGauge implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateGauge(name string, tags map[string]string) tally.CachedGauge {
-	return r.reporter.AllocateGauge(name, tags)
-}
-
-func (r *promReporter) RegisterTimer(
-	name string,
-	tagKeys []string,
-	desc string,
-	opts *promreporter.RegisterTimerOptions,
-) (promreporter.TimerUnion, error) {
-	return r.reporter.RegisterTimer(name, tagKeys, desc, opts)
-}
-
-// AllocateTimer implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateTimer(name string, tags map[string]string) tally.CachedTimer {
-	return r.reporter.AllocateTimer(name, tags)
-}
-
-func (r *promReporter) AllocateHistogram(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-) tally.CachedHistogram {
-	return r.reporter.AllocateHistogram(name, tags, buckets)
-}
-
-func (r *promReporter) Capabilities() tally.Capabilities {
-	return r
-}
-
-func (r *promReporter) Reporting() bool {
-	return true
-}
-
-func (r *promReporter) Tagging() bool {
-	return true
-}
-
-// Flush does nothing for prometheus
-func (r *promReporter) Flush() {
-
-}
-
 func (r *promReporter) Close() error {
-	//TODO: Timeout here?
-	return r.server.Shutdown(context.Background())
-}
-
-func (r *promReporter) Start() error {
-	return r.server.ListenAndServe()
+	//TODO: Shutdown server gracefully?
+	// Close() is not a graceful way since it closes server immediately
+	err := r.server.Close()
+	r.listener.Close()
+	return err
 }
 
 func (r *promReporter) HTTPHandler() http.Handler {
-	return promReporterHttpHandler(r.registry)
+	return promReporterHTTPHandler(r.registry)
 }
 
-func (s *scope) fullyQualifiedName(name string) string {
-	if len(s.prefix) == 0 {
-		return name
-	}
-	return fmt.Sprintf("%s%s%s", s.prefix, s.separator, name)
-}
-
-// mergeRightTags merges 2 sets of tags with the tags from tagsRight overriding values from tagsLeft
-func mergeRightTags(tagsLeft, tagsRight map[string]string) map[string]string {
-	if tagsLeft == nil && tagsRight == nil {
-		return nil
-	}
-	if len(tagsRight) == 0 {
-		return tagsLeft
-	}
-	if len(tagsLeft) == 0 {
-		return tagsRight
-	}
-
-	result := make(map[string]string, len(tagsLeft)+len(tagsRight))
-	for k, v := range tagsLeft {
-		result[k] = v
-	}
-	for k, v := range tagsRight {
-		result[k] = v
-	}
-	return result
-}
-
-func copyStringMap(stringMap map[string]string) map[string]string {
-	result := make(map[string]string, len(stringMap))
-	for k, v := range stringMap {
-		result[k] = v
-	}
-	return result
+func promReporterHTTPHandler(registry *prometheus.Registry) http.Handler {
+	return promhttp.HandlerFor(registry, promhttp.HandlerOpts{})
 }
 
 func tagsToName(name string, tags map[string]string) string {
@@ -447,12 +169,8 @@ func tagsToName(name string, tags map[string]string) string {
 	sort.Strings(keys)
 
 	for _, k := range keys {
-		name = name + tally.DefaultSeparator + k + "-" + tags[k]
+		name = name + promreporter.DefaultSeparator + k + "-" + tags[k]
 	}
 
 	return name
 }
-
-func promReporterHttpHandler(registry *prometheus.Registry) http.Handler {
-	return promhttp.HandlerFor(registry, promhttp.HandlerOpts{})
-}
diff --git a/common/metrics/tally_provider_test.go b/common/metrics/tally_provider_test.go
deleted file mode 100644
index 9e911c275..000000000
--- a/common/metrics/tally_provider_test.go
+++ /dev/null
@@ -1,462 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import (
-	"fmt"
-	"io"
-	"io/ioutil"
-	"net"
-	"net/http"
-	"strings"
-	"sync"
-	"sync/atomic"
-	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/uber-go/tally"
-	promreporter "github.com/uber-go/tally/prometheus"
-)
-
-const (
-	statsdAddress = "127.0.0.1:8125"
-	promAddress   = "127.0.0.1:8082"
-)
-
-type testIntValue struct {
-	val      int64
-	tags     map[string]string
-	reporter *testStatsReporter
-}
-
-func (m *testIntValue) ReportCount(value int64) {
-	m.val = value
-	m.reporter.cg.Done()
-}
-
-type testFloatValue struct {
-	val      float64
-	tags     map[string]string
-	reporter *testStatsReporter
-}
-
-func (m *testFloatValue) ReportGauge(value float64) {
-	m.val = value
-	m.reporter.gg.Done()
-}
-
-type testStatsReporter struct {
-	cg sync.WaitGroup
-	gg sync.WaitGroup
-
-	scope Scope
-
-	counters map[string]*testIntValue
-	gauges   map[string]*testFloatValue
-
-	flushes int32
-}
-
-// newTestStatsReporter returns a new TestStatsReporter
-func newTestStatsReporter() *testStatsReporter {
-	return &testStatsReporter{
-		counters: make(map[string]*testIntValue),
-		gauges:   make(map[string]*testFloatValue)}
-}
-
-func (r *testStatsReporter) WaitAll() {
-	r.cg.Wait()
-	r.gg.Wait()
-}
-
-func (r *testStatsReporter) AllocateCounter(
-	name string, tags map[string]string,
-) tally.CachedCount {
-	counter := &testIntValue{
-		val:      0,
-		tags:     tags,
-		reporter: r,
-	}
-	r.counters[name] = counter
-	return counter
-}
-
-func (r *testStatsReporter) ReportCounter(name string, tags map[string]string, value int64) {
-	r.counters[name] = &testIntValue{
-		val:  value,
-		tags: tags,
-	}
-	r.cg.Done()
-}
-
-func (r *testStatsReporter) AllocateGauge(
-	name string, tags map[string]string,
-) tally.CachedGauge {
-	gauge := &testFloatValue{
-		val:      0,
-		tags:     tags,
-		reporter: r,
-	}
-	r.gauges[name] = gauge
-	return gauge
-}
-
-func (r *testStatsReporter) ReportGauge(name string, tags map[string]string, value float64) {
-	r.gauges[name] = &testFloatValue{
-		val:  value,
-		tags: tags,
-	}
-	r.gg.Done()
-}
-
-func (r *testStatsReporter) AllocateTimer(
-	name string, tags map[string]string,
-) tally.CachedTimer {
-	return nil
-}
-
-func (r *testStatsReporter) ReportTimer(name string, tags map[string]string, interval time.Duration) {
-
-}
-
-func (r *testStatsReporter) AllocateHistogram(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-) tally.CachedHistogram {
-	return nil
-}
-
-func (r *testStatsReporter) ReportHistogramValueSamples(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-	bucketLowerBound,
-	bucketUpperBound float64,
-	samples int64,
-) {
-
-}
-
-func (r *testStatsReporter) ReportHistogramDurationSamples(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-	bucketLowerBound,
-	bucketUpperBound time.Duration,
-	samples int64,
-) {
-
-}
-
-func (r *testStatsReporter) Capabilities() tally.Capabilities {
-	return nil
-}
-
-func (r *testStatsReporter) Flush() {
-	atomic.AddInt32(&r.flushes, 1)
-}
-
-func TestCounter(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.cg.Add(1)
-	s.Counter("foo").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".foo"].val)
-
-	defer func() {
-		if r := recover(); r == nil {
-			t.Errorf("Should panic when wrong key used")
-		}
-	}()
-	assert.Equal(t, int64(1), r.counters[namespace+".foo1"].val)
-}
-
-func TestMultiCounterReport(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 2*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.cg.Add(1)
-	go s.Counter("foo").Inc(1)
-	go s.Counter("foo").Inc(3)
-	go s.Counter("foo").Inc(5)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(9), r.counters[namespace+".foo"].val)
-}
-
-func TestGauge(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.gg.Add(1)
-	s.Gauge("foo").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".foo"].val)
-}
-
-func TestMultiGaugeReport(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-
-	r.gg.Add(1)
-	s.Gauge("foo").Update(float64(1.33))
-	s.Gauge("foo").Update(float64(3.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(3.33), r.gauges[namespace+".foo"].val)
-}
-
-func TestSubScope(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("foo")
-
-	r.gg.Add(1)
-	subs.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".foo.bar"].val)
-
-	r.cg.Add(1)
-	subs.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".foo.haha"].val)
-}
-
-func TestTagged(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.Tagged(map[string]string{"env": "test"})
-
-	r.gg.Add(1)
-	subs.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".bar"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.gauges[namespace+".bar"].tags)
-
-	r.cg.Add(1)
-	subs.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".haha"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.counters[namespace+".haha"].tags)
-}
-
-func TestTaggedExistingReturnsSameScope(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-
-	for _, initialTags := range []map[string]string{
-		nil,
-		{"env": "test"},
-	} {
-		root := newRootScope(tally.ScopeOptions{Prefix: "foo", Tags: initialTags, Reporter: r}, 0)
-		go root.Start()
-		rootScope := root.(*scope)
-		fooScope := root.Tagged(map[string]string{"foo": "bar"}).(*scope)
-
-		assert.NotEqual(t, rootScope, fooScope)
-		assert.Equal(t, fooScope, fooScope.Tagged(nil))
-
-		fooBarScope := fooScope.Tagged(map[string]string{"bar": "baz"}).(*scope)
-
-		assert.NotEqual(t, fooScope, fooBarScope)
-		assert.Equal(t, fooBarScope, fooScope.Tagged(map[string]string{"bar": "baz"}).(*scope))
-		root.Close()
-	}
-}
-
-func TestSubScopeTagged(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("sub")
-	subtags := subs.Tagged(map[string]string{"env": "test"})
-
-	r.gg.Add(1)
-	subtags.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".sub.bar"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.gauges[namespace+".sub.bar"].tags)
-
-	r.cg.Add(1)
-	subtags.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".sub.haha"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.counters[namespace+".sub.haha"].tags)
-}
-
-func TestMetricsByStatsdReporter(t *testing.T) {
-	t.Parallel()
-	udpAddr, err := net.ResolveUDPAddr("udp", statsdAddress)
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	server, err := net.ListenUDP("udp", udpAddr)
-	if err != nil {
-		t.Fatal(err)
-	}
-	defer server.Close()
-
-	r, _ := newTestStatsdReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("peer").Tagged(map[string]string{"component": "committer", "env": "test"})
-	subs.Counter("success_total").Inc(1)
-	subs.Gauge("channel_total").Update(4)
-
-	buffer := make([]byte, 4096)
-	n, _ := io.ReadAtLeast(server, buffer, 1)
-	result := string(buffer[:n])
-
-	expected := []string{
-		`hyperledger_fabric.peer.success_total.component-committer.env-test:1|c`,
-		`hyperledger_fabric.peer.channel_total.component-committer.env-test:4|g`,
-	}
-
-	for i, res := range strings.Split(result, "\n") {
-		if res != expected[i] {
-			t.Errorf("Got `%s`, expected `%s`", res, expected[i])
-		}
-	}
-}
-
-func TestMetricsByPrometheusReporter(t *testing.T) {
-	t.Parallel()
-	r, _ := newTestPrometheusReporter()
-
-	opts := tally.ScopeOptions{
-		Prefix:         namespace,
-		Separator:      promreporter.DefaultSeparator,
-		CachedReporter: r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-
-	scrape := func() string {
-		resp, _ := http.Get(fmt.Sprintf("http://%s/metrics", promAddress))
-		buf, _ := ioutil.ReadAll(resp.Body)
-		return string(buf)
-	}
-	subs := s.SubScope("peer").Tagged(map[string]string{"component": "committer", "env": "test"})
-	subs.Counter("success_total").Inc(1)
-	subs.Gauge("channel_total").Update(4)
-
-	time.Sleep(2 * time.Second)
-
-	expected := []string{
-		`# HELP hyperledger_fabric_peer_channel_total hyperledger_fabric_peer_channel_total gauge`,
-		`# TYPE hyperledger_fabric_peer_channel_total gauge`,
-		`hyperledger_fabric_peer_channel_total{component="committer",env="test"} 4`,
-		`# HELP hyperledger_fabric_peer_success_total hyperledger_fabric_peer_success_total counter`,
-		`# TYPE hyperledger_fabric_peer_success_total counter`,
-		`hyperledger_fabric_peer_success_total{component="committer",env="test"} 1`,
-		``,
-	}
-
-	result := strings.Split(scrape(), "\n")
-
-	for i, res := range result {
-		if res != expected[i] {
-			t.Errorf("Got `%s`, expected `%s`", res, expected[i])
-		}
-	}
-}
-
-func newTestStatsdReporter() (tally.StatsReporter, error) {
-	opts := StatsdReporterOpts{
-		Address:       statsdAddress,
-		FlushInterval: defaultStatsdReporterFlushInterval,
-		FlushBytes:    defaultStatsdReporterFlushBytes,
-	}
-	return newStatsdReporter(opts)
-}
-
-func newTestPrometheusReporter() (promreporter.Reporter, error) {
-	opts := PromReporterOpts{
-		ListenAddress: promAddress,
-	}
-	return newPromReporter(opts)
-}
diff --git a/common/metrics/types.go b/common/metrics/types.go
deleted file mode 100644
index c70001ea1..000000000
--- a/common/metrics/types.go
+++ /dev/null
@@ -1,45 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import "io"
-
-// Counter is the interface for emitting Counter type metrics.
-type Counter interface {
-	// Inc increments the Counter by a delta.
-	Inc(delta int64)
-}
-
-// Gauge is the interface for emitting Gauge metrics.
-type Gauge interface {
-	// Update sets the gauges absolute value.
-	Update(value float64)
-}
-
-// Scope is a namespace wrapper around a stats Reporter, ensuring that
-// all emitted values have a given prefix or set of tags.
-type Scope interface {
-	serve
-	// Counter returns the Counter object corresponding to the name.
-	Counter(name string) Counter
-
-	// Gauge returns the Gauge object corresponding to the name.
-	Gauge(name string) Gauge
-
-	// Tagged returns a new child Scope with the given tags and current tags.
-	Tagged(tags map[string]string) Scope
-
-	// SubScope returns a new child Scope appending a further name prefix.
-	SubScope(name string) Scope
-}
-
-// serve is the interface represents who can provide service
-type serve interface {
-	io.Closer
-	// Start starts the server
-	Start() error
-}
diff --git a/core/deliverservice/blocksprovider/blocksprovider.go b/core/deliverservice/blocksprovider/blocksprovider.go
index 4a7441c58..6cc295487 100644
--- a/core/deliverservice/blocksprovider/blocksprovider.go
+++ b/core/deliverservice/blocksprovider/blocksprovider.go
@@ -11,8 +11,11 @@ import (
 	"sync/atomic"
 	"time"
 
+	"fmt"
+
 	"github.com/golang/protobuf/proto"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/gossip/api"
 	gossipcommon "github.com/hyperledger/fabric/gossip/common"
 	"github.com/hyperledger/fabric/gossip/discovery"
@@ -167,6 +170,10 @@ func (b *blocksProviderImpl) DeliverBlocks() {
 			statusCounter = 0
 			blockNum := t.Block.Header.Number
 
+			//if metrics.IsDebug() {
+			metrics.RootScope.Gauge(fmt.Sprintf("blocksprovider_%s_received_block_number", metrics.FilterMetricName(b.chainID))).Update(float64(blockNum))
+			//}
+
 			marshaledBlock, err := proto.Marshal(t.Block)
 			if err != nil {
 				logger.Errorf("[%s] Error serializing block with sequence number %d, due to %s", b.chainID, blockNum, err)
@@ -189,6 +196,9 @@ func (b *blocksProviderImpl) DeliverBlocks() {
 				logger.Warningf("Block [%d] received from ordering service wasn't added to payload buffer: %v", blockNum, err)
 			}
 
+			if metrics.IsDebug() {
+				metrics.RootScope.Gauge(fmt.Sprintf("blocksprovider_%s_gossiping_block_number", metrics.FilterMetricName(b.chainID))).Update(float64(payload.SeqNum))
+			}
 			// Gossip messages with other nodes
 			logger.Debugf("[%s] Gossiping block [%d], peers number [%d]", b.chainID, blockNum, numberOfPeers)
 			if !b.isDone() {
diff --git a/core/ledger/kvledger/history/historydb/historycouchdb/historycouchdb.go b/core/ledger/kvledger/history/historydb/historycouchdb/historycouchdb.go
index 146c48562..21aa6b06a 100644
--- a/core/ledger/kvledger/history/historydb/historycouchdb/historycouchdb.go
+++ b/core/ledger/kvledger/history/historydb/historycouchdb/historycouchdb.go
@@ -107,7 +107,7 @@ func (sets writeSets) asCouchDbDocs() ([]*couchdb.CouchDoc, error) {
 func NewHistoryDBProvider(couchDBDef *couchdb.CouchDBDef) (historydb.HistoryDBProvider, error) {
 	logger.Debugf("constructing CouchDB historyDB storage provider")
 	couchInstance, err := couchdb.CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
-		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout)
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB)
 	if err != nil {
 		return nil, errors.WithMessage(err, "obtaining CouchDB HistoryDB provider failed")
 	}
diff --git a/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go b/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
index 7b029648e..b6423c123 100644
--- a/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
+++ b/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
@@ -10,6 +10,7 @@ import (
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/history/historydb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
@@ -69,6 +70,12 @@ func (historyDB *historyDB) Close() {
 // Commit implements method in HistoryDB interface
 func (historyDB *historyDB) Commit(block *common.Block) error {
 
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("historyleveldb_Commit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// Get the history batch from the block
 	keys, height, err := historydb.ConstructHistoryBatch(historyDB.dbName, block)
 	if err != nil {
diff --git a/core/ledger/kvledger/inventory/cdbid/cdb_store.go b/core/ledger/kvledger/inventory/cdbid/cdb_store.go
index 5dea87fdc..837de6a22 100644
--- a/core/ledger/kvledger/inventory/cdbid/cdb_store.go
+++ b/core/ledger/kvledger/inventory/cdbid/cdb_store.go
@@ -8,6 +8,7 @@ package cdbid
 
 import (
 	"fmt"
+
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
@@ -77,7 +78,7 @@ func newCommitterStore(couchInstance *couchdb.CouchInstance, dbName string) (*St
 		return nil, err
 	}
 
-	s := Store {db}
+	s := Store{db}
 	return &s, nil
 }
 
@@ -93,7 +94,7 @@ func createCouchInstance() (*couchdb.CouchInstance, error) {
 	logger.Debugf("constructing CouchDB block storage provider")
 	couchDBDef := couchdb.GetCouchDBDefinition()
 	couchInstance, err := couchdb.CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
-		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout)
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB)
 	if err != nil {
 		return nil, errors.WithMessage(err, "obtaining CouchDB instance failed")
 	}
@@ -224,4 +225,4 @@ func (s *Store) GetAllLedgerIds() ([]string, error) {
 }
 
 func (s *Store) Close() {
-}
\ No newline at end of file
+}
diff --git a/core/ledger/kvledger/kv_ledger.go b/core/ledger/kvledger/kv_ledger.go
index 85d80ad67..a9d833897 100644
--- a/core/ledger/kvledger/kv_ledger.go
+++ b/core/ledger/kvledger/kv_ledger.go
@@ -16,6 +16,7 @@ import (
 
 	"github.com/hyperledger/fabric/common/flogging"
 	commonledger "github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
@@ -261,6 +262,13 @@ func (l *kvLedger) NewHistoryQueryExecutor() (ledger.HistoryQueryExecutor, error
 
 // CommitWithPvtData commits the block and the corresponding pvt data in an atomic operation
 func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) error {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("kvledger_CommitWithPvtData_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	var err error
 	block := pvtdataAndBlock.Block
 	blockNo := pvtdataAndBlock.Block.Header.Number
@@ -282,6 +290,10 @@ func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) er
 	}
 	elapsedCommitBlockStorage := time.Since(startCommitBlockStorage) / time.Millisecond // duration in ms
 
+	// KEEP EVEN WHEN metrics.debug IS OFF
+	metrics.RootScope.Gauge(fmt.Sprintf("kvledger_%s_commited_block_number", metrics.FilterMetricName(l.ledgerID))).Update(float64(block.Header.Number))
+	logger.Infof("Channel [%s]: Committed block [%d] with %d transaction(s)", metrics.FilterMetricName(l.ledgerID), block.Header.Number, len(block.Data.Data))
+
 	startCommitState := time.Now()
 	logger.Debugf("[%s] Committing block [%d] transactions to state database", l.ledgerID, blockNo)
 	if err = l.txtmgmt.Commit(); err != nil {
@@ -315,6 +327,13 @@ func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) er
 // GetPvtDataAndBlockByNum returns the block and the corresponding pvt data.
 // The pvt data is filtered by the list of 'collections' supplied
 func (l *kvLedger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsCollFilter) (*ledger.BlockAndPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("kvledger_GetPvtDataAndBlockByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	blockAndPvtdata, err := l.blockStore.GetPvtDataAndBlockByNum(blockNum, filter)
 	l.blockAPIsRWLock.RLock()
 	l.blockAPIsRWLock.RUnlock()
@@ -324,6 +343,13 @@ func (l *kvLedger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsC
 // GetPvtDataByNum returns only the pvt data  corresponding to the given block number
 // The pvt data is filtered by the list of 'collections' supplied
 func (l *kvLedger) GetPvtDataByNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("kvledger_GetPvtDataByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	pvtdata, err := l.blockStore.GetPvtDataByNum(blockNum, filter)
 	l.blockAPIsRWLock.RLock()
 	l.blockAPIsRWLock.RUnlock()
diff --git a/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go b/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
index f522ab2cc..9ca7313bf 100644
--- a/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
+++ b/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
@@ -12,6 +12,7 @@ import (
 	"strings"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
 
@@ -257,6 +258,10 @@ func deriveHashedDataNs(namespace, collection string) string {
 }
 
 func addPvtUpdates(pubUpdateBatch *PubUpdateBatch, pvtUpdateBatch *PvtUpdateBatch) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("privacyenabledstate_addPvtUpdates_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	for ns, nsBatch := range pvtUpdateBatch.UpdateMap {
 		for _, coll := range nsBatch.GetCollectionNames() {
 			for key, vv := range nsBatch.GetUpdates(coll) {
@@ -267,6 +272,10 @@ func addPvtUpdates(pubUpdateBatch *PubUpdateBatch, pvtUpdateBatch *PvtUpdateBatc
 }
 
 func addHashedUpdates(pubUpdateBatch *PubUpdateBatch, hashedUpdateBatch *HashedUpdateBatch, base64Key bool) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("privacyenabledstate_addHashedUpdatesTimer_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	for ns, nsBatch := range hashedUpdateBatch.UpdateMap {
 		for _, coll := range nsBatch.GetCollectionNames() {
 			for key, vv := range nsBatch.GetUpdates(coll) {
diff --git a/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go b/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
index 1d1436cb5..0f94a7ec3 100644
--- a/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
+++ b/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
@@ -7,10 +7,11 @@ SPDX-License-Identifier: Apache-2.0
 package pvtstatepurgemgmt
 
 import (
-	proto "github.com/golang/protobuf/proto"
+	"github.com/golang/protobuf/proto"
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/bookkeeping"
 )
 
@@ -64,6 +65,10 @@ type expKeeper struct {
 // at the time of the commit of the block number 45 and the second entry was created at the time of the commit of the block number 40, however
 // both are expiring with the commit of block number 50.
 func (ek *expKeeper) updateBookkeeping(toTrack []*expiryInfo, toClear []*expiryInfoKey) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("pvtstatepurgemgmt_updateBookkeepingTimer_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	updateBatch := leveldbhelper.NewUpdateBatch()
 	for _, expinfo := range toTrack {
 		k, v, err := encodeKV(expinfo)
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
index 2dd88237c..a80f2f3a3 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
@@ -7,6 +7,8 @@ package statecouchdb
 
 import (
 	"sync"
+
+	"github.com/hyperledger/fabric/common/metrics"
 )
 
 // batch is executed in a separate goroutine.
@@ -18,7 +20,15 @@ type batch interface {
 // any of the batches return error during its execution
 func executeBatches(batches []batch) error {
 	logger.Debugf("Executing batches = %s", batches)
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("statecouchdb_executeBatches_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	numBatches := len(batches)
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge("statecouchdb_numBatches").Update(float64(numBatches))
+	}
 	if numBatches == 0 {
 		return nil
 	}
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
index 240667a73..e328658dd 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
@@ -9,6 +9,7 @@ import (
 	"errors"
 	"fmt"
 
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
@@ -151,6 +152,12 @@ type nsFlusher struct {
 }
 
 func (vdb *VersionedDB) ensureFullCommit(dbs []*couchdb.CouchDatabase) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("statecouchdb_ensureFullCommit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	var flushers []batch
 	for _, db := range dbs {
 		flushers = append(flushers, &nsFlusher{db})
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
index 999b91a72..b26d6f25b 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
@@ -12,6 +12,7 @@ import (
 	"sync"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
@@ -348,6 +349,12 @@ func (vdb *VersionedDB) ExecuteQuery(namespace, query string) (statedb.ResultsIt
 
 // ApplyUpdates implements method in VersionedDB interface
 func (vdb *VersionedDB) ApplyUpdates(updates *statedb.UpdateBatch, height *version.Height) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("statecouchdb_ApplyUpdates_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// TODO a note about https://jira.hyperledger.org/browse/FAB-8622
 	// the function `Apply update can be split into three functions. Each carrying out one of the following three stages`.
 	// The write lock is needed only for the stage 2.
diff --git a/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go b/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
index ba3be0ea3..6adaf3bd5 100644
--- a/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
+++ b/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
@@ -11,6 +11,7 @@ import (
 
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
@@ -138,6 +139,12 @@ func (vdb *versionedDB) ExecuteQuery(namespace, query string) (statedb.ResultsIt
 
 // ApplyUpdates implements method in VersionedDB interface
 func (vdb *versionedDB) ApplyUpdates(batch *statedb.UpdateBatch, height *version.Height) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("stateleveldb_ApplyUpdates_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	dbBatch := leveldbhelper.NewUpdateBatch()
 	namespaces := batch.GetUpdatedNamespaces()
 	for _, ns := range namespaces {
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
index f13febb53..75ec58933 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
@@ -185,6 +185,7 @@ func (h *queryHelper) done() {
 
 	defer func() {
 		h.txmgr.commitRWLock.RUnlock()
+		h.txmgr.StopWatch.Stop()
 		h.doneInvoked = true
 		for _, itr := range h.itrs {
 			itr.Close()
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
index d03bd0535..9ea18b441 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
@@ -11,6 +11,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/bookkeeping"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate"
@@ -20,6 +21,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	"github.com/uber-go/tally"
 )
 
 var logger = flogging.MustGetLogger("lockbasedtxmgr")
@@ -34,6 +36,7 @@ type LockBasedTxMgr struct {
 	stateListeners  []ledger.StateListener
 	commitRWLock    sync.RWMutex
 	current         *current
+	StopWatch       tally.Stopwatch
 }
 
 type current struct {
@@ -73,6 +76,7 @@ func (txmgr *LockBasedTxMgr) GetLastSavepoint() (*version.Height, error) {
 // NewQueryExecutor implements method in interface `txmgmt.TxMgr`
 func (txmgr *LockBasedTxMgr) NewQueryExecutor(txid string) (ledger.QueryExecutor, error) {
 	qe := newQueryExecutor(txmgr, txid)
+	txmgr.StopWatch = metrics.RootScope.Timer("lockbasedtxmgr_NewQueryExecutor_commitRWLock_RLock_time_seconds").Start()
 	txmgr.commitRWLock.RLock()
 	return qe, nil
 }
@@ -84,6 +88,7 @@ func (txmgr *LockBasedTxMgr) NewTxSimulator(txid string) (ledger.TxSimulator, er
 	if err != nil {
 		return nil, err
 	}
+	txmgr.StopWatch = metrics.RootScope.Timer("lockbasedtxmgr_NewTxSimulator_commitRWLock_RLock_time_seconds").Start()
 	txmgr.commitRWLock.RLock()
 	return s, nil
 }
@@ -129,18 +134,29 @@ func (txmgr *LockBasedTxMgr) Shutdown() {
 
 // Commit implements method in interface `txmgmt.TxMgr`
 func (txmgr *LockBasedTxMgr) Commit() error {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("lockbasedtxmgr_Commit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// When using the purge manager for the first block commit after peer start, the asynchronous function
 	// 'PrepareForExpiringKeys' is invoked in-line. However, for the subsequent blocks commits, this function is invoked
 	// in advance for the next block
 	if !txmgr.pvtdataPurgeMgr.usedOnce {
+		stopWatch := metrics.RootScope.Timer("lockbasedtxmgr_Commit_PrepareForExpiringKeys_time_seconds").Start()
 		txmgr.pvtdataPurgeMgr.PrepareForExpiringKeys(txmgr.current.blockNum())
 		txmgr.pvtdataPurgeMgr.usedOnce = true
+		stopWatch.Stop()
 	}
 	defer func() {
+		stopWatch := metrics.RootScope.Timer("lockbasedtxmgr_Commit_defer_time_seconds").Start()
 		txmgr.clearCache()
 		txmgr.pvtdataPurgeMgr.PrepareForExpiringKeys(txmgr.current.blockNum() + 1)
 		logger.Debugf("Cleared version cache and launched the background routine for preparing keys to purge with the next block")
 		txmgr.reset()
+		stopWatch.Stop()
+
 	}()
 
 	logger.Debugf("Committing updates to state database")
@@ -148,27 +164,42 @@ func (txmgr *LockBasedTxMgr) Commit() error {
 		panic("validateAndPrepare() method should have been called before calling commit()")
 	}
 
+	stopWatch := metrics.RootScope.Timer("lockbasedtxmgr_Commit_DeleteExpiredAndUpdateBookkeeping_time_seconds").Start()
 	if err := txmgr.pvtdataPurgeMgr.DeleteExpiredAndUpdateBookkeeping(
 		txmgr.current.batch.PvtUpdates, txmgr.current.batch.HashUpdates); err != nil {
+		stopWatch.Stop()
 		return err
 	}
+	stopWatch.Stop()
 
+	stopWatch = metrics.RootScope.Timer("lockbasedtxmgr_Commit_commitRWLock_time_seconds").Start()
 	txmgr.commitRWLock.Lock()
+	stopWatch.Stop()
 	defer txmgr.commitRWLock.Unlock()
 	logger.Debugf("Write lock acquired for committing updates to state database")
 	commitHeight := version.NewHeight(txmgr.current.blockNum(), txmgr.current.maxTxNumber())
+	stopWatch = metrics.RootScope.Timer("lockbasedtxmgr_Commit_ApplyPrivacyAwareUpdates_time_seconds").Start()
 	if err := txmgr.db.ApplyPrivacyAwareUpdates(txmgr.current.batch, commitHeight); err != nil {
+		stopWatch.Stop()
 		return err
 	}
+	stopWatch.Stop()
+
 	logger.Debugf("Updates committed to state database")
 
+	stopWatch = metrics.RootScope.Timer("lockbasedtxmgr_Commit_BlockCommitDone_time_seconds").Start()
 	// purge manager should be called (in this call the purge mgr removes the expiry entries from schedules) after committing to statedb
 	if err := txmgr.pvtdataPurgeMgr.BlockCommitDone(); err != nil {
+		stopWatch.Stop()
 		return err
 	}
+	stopWatch.Stop()
+
+	stopWatch = metrics.RootScope.Timer("lockbasedtxmgr_Commit_updateStateListeners_time_seconds").Start()
 	// In the case of error state listeners will not recieve this call - instead a peer panic is caused by the ledger upon receiveing
 	// an error from this function
 	txmgr.updateStateListeners()
+	stopWatch.Stop()
 	return nil
 }
 
@@ -223,6 +254,10 @@ func extractStateUpdates(batch *privacyenabledstate.UpdateBatch, namespaces []st
 }
 
 func (txmgr *LockBasedTxMgr) updateStateListeners() {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("lockbasedtxmgr_updateStateListenersTimer_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	for _, l := range txmgr.current.listeners {
 		l.StateCommitDone(txmgr.ledgerid)
 	}
diff --git a/core/ledger/ledgerstorage/store.go b/core/ledger/ledgerstorage/store.go
index 33629096e..f5e397cb3 100644
--- a/core/ledger/ledgerstorage/store.go
+++ b/core/ledger/ledgerstorage/store.go
@@ -30,6 +30,7 @@ import (
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage/cdbblkstorage"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage/cdbpvtdata"
 )
@@ -135,6 +136,13 @@ func (s *Store) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
 
 // CommitWithPvtData commits the block and the corresponding pvt data in an atomic operation
 func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("ledgerstorage_CommitWithPvtData_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	blockNum := blockAndPvtdata.Block.Header.Number
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
@@ -143,6 +151,9 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 	if err != nil {
 		return err
 	}
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge("ledgerstorage_CommitWithPvtData_BlockDiff").Update(float64(blockNum - pvtBlkStoreHt))
+	}
 
 	writtenToPvtStore := false
 	if pvtBlkStoreHt < blockNum+1 { // The pvt data store sanity check does not allow rewriting the pvt data.
@@ -158,6 +169,9 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 		}
 		writtenToPvtStore = true
 	} else {
+		if metrics.IsDebug() {
+			metrics.RootScope.Counter("ledgerstorage_CommitWithPvtData_SkipCount").Inc(1)
+		}
 		logger.Debugf("Skipping writing block [%d] to pvt block store as the store height is [%d]", blockNum, pvtBlkStoreHt)
 	}
 
@@ -175,6 +189,13 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 // GetPvtDataAndBlockByNum returns the block and the corresponding pvt data.
 // The pvt data is filtered by the list of 'collections' supplied
 func (s *Store) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsCollFilter) (*ledger.BlockAndPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("ledgerstorage_GetPvtDataAndBlockByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
 
@@ -194,6 +215,13 @@ func (s *Store) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsColl
 // The pvt data is filtered by the list of 'ns/collections' supplied in the filter
 // A nil filter does not filter any results
 func (s *Store) GetPvtDataByNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("ledgerstorage_GetPvtDataByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
 	return s.getPvtDataByNumWithoutLock(blockNum, filter)
diff --git a/core/ledger/pvtdatastorage/cdbpvtdata/cdb_pvtprovider.go b/core/ledger/pvtdatastorage/cdbpvtdata/cdb_pvtprovider.go
index bd1f059e2..37adf4347 100644
--- a/core/ledger/pvtdatastorage/cdbpvtdata/cdb_pvtprovider.go
+++ b/core/ledger/pvtdatastorage/cdbpvtdata/cdb_pvtprovider.go
@@ -29,7 +29,7 @@ func NewProvider() (*Provider, error) {
 	logger.Debugf("constructing CouchDB private data storage provider")
 	couchDBDef := couchdb.GetCouchDBDefinition()
 	couchInstance, err := couchdb.CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
-		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout)
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB)
 	if err != nil {
 		return nil, errors.WithMessage(err, "obtaining CouchDB instance failed")
 	}
diff --git a/core/ledger/pvtdatastorage/cdbpvtdata/common_store_impl.go b/core/ledger/pvtdatastorage/cdbpvtdata/common_store_impl.go
index ef673f9d4..f5784d31b 100644
--- a/core/ledger/pvtdatastorage/cdbpvtdata/common_store_impl.go
+++ b/core/ledger/pvtdatastorage/cdbpvtdata/common_store_impl.go
@@ -12,6 +12,7 @@ import (
 
 	"encoding/hex"
 
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
@@ -72,6 +73,12 @@ func (s *store) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData) error {
 		panic("calling Prepare on a peer that is not a committer")
 	}
 
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("pvtdatastorage_couchdb_prepare_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	if s.batchPending {
 		return pvtdatastorage.NewErrIllegalCall(`A pending batch exists as as result of last invoke to "Prepare" call.
 			 Invoke "Commit" or "Rollback" on the pending batch before invoking "Prepare" function`)
@@ -97,6 +104,12 @@ func (s *store) Commit() error {
 		panic("calling Commit on a peer that is not a committer")
 	}
 
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("pvtdatastorage_couchdb_commit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	if !s.batchPending {
 		return pvtdatastorage.NewErrIllegalCall("No pending batch to commit")
 	}
@@ -117,6 +130,11 @@ func (s *store) Commit() error {
 }
 
 func (s *store) InitLastCommittedBlock(blockNum uint64) error {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("pvtdatastorage_couchdb_initLastCommittedBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	if !(s.isEmpty && !s.batchPending) {
 		return pvtdatastorage.NewErrIllegalCall("The private data store is not empty. InitLastCommittedBlock() function call is not allowed")
 	}
@@ -133,6 +151,12 @@ func (s *store) InitLastCommittedBlock(blockNum uint64) error {
 }
 
 func (s *store) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("pvtdatastorage_couchdb_getPvtDataByBlockNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	logger.Debugf("Get private data for block [%d] from DB [%s], filter=%#v", blockNum, s.db.DBName, filter)
 	if s.isEmpty {
 		return nil, pvtdatastorage.NewErrOutOfRange("The store is empty")
diff --git a/core/ledger/util/couchdb/couchdb.go b/core/ledger/util/couchdb/couchdb.go
index 81cbaaef9..c8ed26d7e 100644
--- a/core/ledger/util/couchdb/couchdb.go
+++ b/core/ledger/util/couchdb/couchdb.go
@@ -12,7 +12,6 @@ import (
 	"encoding/base64"
 	"encoding/json"
 	"fmt"
-	"github.com/pkg/errors"
 	"io"
 	"io/ioutil"
 	"log"
@@ -29,7 +28,10 @@ import (
 	"time"
 	"unicode/utf8"
 
+	"github.com/pkg/errors"
+
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/op/go-logging"
 )
@@ -194,11 +196,11 @@ type NamedCouchDoc struct {
 //BatchRetrieveDocResponse is used for processing REST batch responses from CouchDB
 type BatchRetrieveDocResponse struct {
 	Rows []struct {
-		ID          string `json:"id"`
+		ID  string `json:"id"`
 		Doc struct {
-			ID      string `json:"_id"`
-			Rev     string `json:"_rev"`
-			Version string `json:"~version"`
+			ID              string          `json:"_id"`
+			Rev             string          `json:"_rev"`
+			Version         string          `json:"~version"`
 			AttachmentsInfo json.RawMessage `json:"_attachments"`
 		} `json:"doc"`
 	} `json:"rows"`
@@ -206,7 +208,7 @@ type BatchRetrieveDocResponse struct {
 
 type BatchRetreiveDocValueResponse struct {
 	Rows []struct {
-		ID          string `json:"id"`
+		ID  string                     `json:"id"`
 		Doc map[string]json.RawMessage `json:"doc"`
 	} `json:"rows"`
 }
@@ -439,7 +441,6 @@ func (couchInstance *CouchInstance) VerifyCouchConfig() (*ConnectionInfo, *DBRet
 	logger.Debugf("Entering VerifyCouchConfig()")
 	defer logger.Debugf("Exiting VerifyCouchConfig()")
 
-
 	dbResponse, couchDBReturn, err := couchInstance.getConnectionInfo()
 	if err != nil {
 		return nil, couchDBReturn, err
@@ -540,6 +541,10 @@ func (dbclient *CouchDatabase) DropDatabase() (*DBOperationResponse, error) {
 // EnsureFullCommit calls _ensure_full_commit for explicit fsync
 func (dbclient *CouchDatabase) EnsureFullCommit() (*DBOperationResponse, error) {
 
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("couchdb_ensureFullCommit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	logger.Debugf("Entering EnsureFullCommit()")
 
 	dbResponse, err := dbclient.dbOperation("/_ensure_full_commit")
@@ -606,6 +611,11 @@ func (dbclient *CouchDatabase) dbOperation(op string) (*DBOperationResponse, err
 //SaveDoc method provides a function to save a document, id and byte array
 func (dbclient *CouchDatabase) SaveDoc(id string, rev string, couchDoc *CouchDoc) (string, error) {
 
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("couchdb_saveDoc_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	logger.Debugf("Entering SaveDoc()  id=[%s]", id)
 
 	if !utf8.ValidString(id) {
@@ -966,7 +976,7 @@ func (dbclient *CouchDatabase) ReadDocRange(startKey, endKey string, limit, skip
 
 	jsonResponse, err := dbclient.rangeQuery(startKey, endKey, limit, skip)
 	if err != nil {
-		return nil ,err
+		return nil, err
 	}
 
 	for _, row := range jsonResponse.Rows {
@@ -1135,7 +1145,7 @@ func (dbclient *CouchDatabase) QueryDocuments(query string) ([]*QueryResult, err
 
 	jsonResponse, err := dbclient.query(query)
 	if err != nil {
-		return nil ,err
+		return nil, err
 	}
 
 	for _, row := range jsonResponse.Docs {
@@ -1424,6 +1434,11 @@ func (dbclient *CouchDatabase) WarmIndex(designdoc, indexname string) error {
 	if err != nil {
 		return err
 	}
+	if metrics.IsDebug() {
+		timer := metrics.RootScope.Timer("couchdb_WarmIndex")
+		stopWatch := timer.Start()
+		defer stopWatch.Stop()
+	}
 
 	queryParms := indexURL.Query()
 	//Query parameter that allows the execution of the URL to return immediately
@@ -1644,7 +1659,7 @@ func (dbclient *CouchDatabase) BatchRetrieveDocument(keys []string) ([]*NamedCou
 			return nil, err
 		}
 
-		doc := CouchDoc{JSONValue:jsonValue, Attachments: attachments}
+		doc := CouchDoc{JSONValue: jsonValue, Attachments: attachments}
 		namedDoc := NamedCouchDoc{ID: row.ID, Doc: &doc}
 		docs = append(docs, &namedDoc)
 	}
@@ -1680,8 +1695,8 @@ func createAttachmentsFromBatchResponse(attachmentsInfo json.RawMessage) ([]*Att
 		}
 
 		attachment := AttachmentInfo{
-			Name: name,
-			ContentType: attachmentResponse.ContentType,
+			Name:            name,
+			ContentType:     attachmentResponse.ContentType,
 			AttachmentBytes: bytes,
 		}
 		attachments = append(attachments, &attachment)
@@ -1922,6 +1937,11 @@ func (dbclient *CouchDatabase) handleRequestWithRevisionRetry(id, method string,
 func (couchInstance *CouchInstance) handleRequest(method, connectURL string, data []byte, rev string,
 	multipartBoundary string, maxRetries int, keepConnectionOpen bool) (*http.Response, *DBReturn, error) {
 
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("couchdb_handleRequest_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	logger.Debugf("Entering handleRequest()  method=%s  url=%v", method, connectURL)
 
 	//create the return objects for couchDB
diff --git a/gossip/privdata/coordinator.go b/gossip/privdata/coordinator.go
index 39cb9c1a2..3f4a13ed8 100644
--- a/gossip/privdata/coordinator.go
+++ b/gossip/privdata/coordinator.go
@@ -13,6 +13,7 @@ import (
 	"time"
 
 	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/metrics"
 	util2 "github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/committer"
 	"github.com/hyperledger/fabric/core/committer/txvalidator"
@@ -31,6 +32,7 @@ import (
 	"github.com/op/go-logging"
 	"github.com/pkg/errors"
 	"github.com/spf13/viper"
+	"github.com/uber-go/tally"
 )
 
 const (
@@ -195,6 +197,12 @@ func (c *coordinator) StoreBlock(block *common.Block, privateDataSets util.PvtDa
 	}
 	startPull := time.Now()
 	limit := startPull.Add(retryThresh)
+
+	var waitingForMissingKeysStopWatch tally.Stopwatch
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge("privdata_gossipMissingKeys").Update(float64(len(privateInfo.missingKeys)))
+		waitingForMissingKeysStopWatch = metrics.RootScope.Timer("privdata_gossipWaitingForMissingKeys_time_seconds").Start()
+	}
 	for len(privateInfo.missingKeys) > 0 && time.Now().Before(limit) {
 		c.fetchFromPeers(block.Header.Number, ownedRWsets, privateInfo)
 		// If succeeded to fetch everything, no need to sleep before
@@ -206,6 +214,9 @@ func (c *coordinator) StoreBlock(block *common.Block, privateDataSets util.PvtDa
 	}
 	elapsedPull := int64(time.Since(startPull) / time.Millisecond) // duration in ms
 
+	if metrics.IsDebug() {
+		waitingForMissingKeysStopWatch.Stop()
+	}
 	// Only log results if we actually attempted to fetch
 	if bFetchFromPeers {
 		if len(privateInfo.missingKeys) == 0 {
diff --git a/gossip/state/payloads_buffer.go b/gossip/state/payloads_buffer.go
index e7fbf5b18..232485341 100644
--- a/gossip/state/payloads_buffer.go
+++ b/gossip/state/payloads_buffer.go
@@ -21,7 +21,7 @@ import (
 // to signal whenever expected block has arrived.
 type PayloadsBuffer interface {
 	// Adds new block into the buffer
-	Push(payload *proto.Payload)
+	Push(payload *proto.Payload) bool
 
 	// Returns next expected sequence number
 	Next() uint64
@@ -74,7 +74,7 @@ func (b *PayloadsBufferImpl) Ready() chan struct{} {
 // sequence number is below the expected next block number payload will be
 // thrown away.
 // TODO return bool to indicate if payload was added or not, so that caller can log result.
-func (b *PayloadsBufferImpl) Push(payload *proto.Payload) {
+func (b *PayloadsBufferImpl) Push(payload *proto.Payload) bool {
 	b.mutex.Lock()
 	defer b.mutex.Unlock()
 
@@ -82,15 +82,18 @@ func (b *PayloadsBufferImpl) Push(payload *proto.Payload) {
 
 	if seqNum < b.next || b.buf[seqNum] != nil {
 		logger.Debugf("Payload with sequence number = %d has been already processed", payload.SeqNum)
-		return
+		return false
 	}
 
 	b.buf[seqNum] = payload
 
 	// Send notification that next sequence has arrived
+
 	if seqNum == b.next && len(b.readyChan) == 0 {
 		b.readyChan <- struct{}{}
 	}
+
+	return true
 }
 
 // Next function provides the number of the next expected block
diff --git a/gossip/state/state.go b/gossip/state/state.go
index 55f51d255..50c046027 100644
--- a/gossip/state/state.go
+++ b/gossip/state/state.go
@@ -15,6 +15,7 @@ import (
 
 	pb "github.com/golang/protobuf/proto"
 	vsccErrors "github.com/hyperledger/fabric/common/errors"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/common/util/retry"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
@@ -494,6 +495,10 @@ func (s *GossipStateProviderImpl) handleStateResponse(msg proto.ReceivedMessage)
 		return uint64(0), errors.New("Received state transfer response without payload")
 	}
 	for _, payload := range response.GetPayloads() {
+
+		if metrics.IsDebug() {
+			metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_handleStateResponse_block_number", metrics.FilterMetricName(s.chainID))).Update(float64(payload.SeqNum))
+		}
 		logger.Debugf("Received payload with sequence number %d.", payload.SeqNum)
 		if err := s.mediator.VerifyBlock(common2.ChainID(s.chainID), payload.SeqNum, payload.Data); err != nil {
 			err = errors.WithStack(err)
@@ -538,6 +543,11 @@ func (s *GossipStateProviderImpl) queueNewMessage(msg *proto.GossipMessage) {
 
 	dataMsg := msg.GetDataMsg()
 	if dataMsg != nil {
+
+		if metrics.IsDebug() {
+			metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_adding_payload_number", metrics.FilterMetricName(s.chainID))).Update(float64(dataMsg.Payload.SeqNum))
+		}
+
 		if err := s.addPayload(dataMsg.GetPayload(), nonBlocking); err != nil {
 			logger.Warningf("Block [%d] received from gossip wasn't added to payload buffer: %v", dataMsg.Payload.SeqNum, err)
 			return
@@ -555,6 +565,9 @@ func (s *GossipStateProviderImpl) deliverPayloads() {
 		select {
 		// Wait for notification that next seq has arrived
 		case <-s.payloads.Ready():
+			// KEEP EVEN WHEN metrics.debug IS OFF
+			metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_next_sequence_arrived", metrics.FilterMetricName(s.chainID))).Update(float64(s.payloads.Next()))
+
 			logger.Debugf("[%s] Ready to transfer payloads (blocks) to the ledger, next block number is = [%d]", s.chainID, s.payloads.Next())
 			// Collect all subsequent payloads
 			for payload := s.payloads.Pop(); payload != nil; payload = s.payloads.Pop() {
@@ -854,6 +867,9 @@ func (s *GossipStateProviderImpl) addPayload(payload *proto.Payload, blockingMod
 	if payload == nil {
 		return errors.New("Given payload is nil")
 	}
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_addPayload", metrics.FilterMetricName(s.chainID))).Update(float64(payload.SeqNum))
+	}
 	logger.Debugf("[%s] Adding payload to local buffer, blockNum = [%d]", s.chainID, payload.SeqNum)
 	height, err := s.ledger.LedgerHeight()
 	if err != nil {
@@ -868,7 +884,10 @@ func (s *GossipStateProviderImpl) addPayload(payload *proto.Payload, blockingMod
 		time.Sleep(enqueueRetryInterval)
 	}
 
-	s.payloads.Push(payload)
+	if s.payloads.Push(payload) {
+		metrics.RootScope.Gauge(fmt.Sprintf("payloadbuffer_%s_push_block_number", metrics.FilterMetricName(s.chainID))).Update(float64(payload.SeqNum))
+		logger.Debugf("[%s] Adding Payload to local buffer done, blockNum = [%d]", s.chainID, payload.SeqNum)
+	}
 	return nil
 }
 
@@ -884,6 +903,9 @@ func (s *GossipStateProviderImpl) addPayloads(payloads []*proto.Payload) error {
 }
 
 func (s *GossipStateProviderImpl) commitBlock(block *common.Block, pvtData util.PvtDataCollections) error {
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_about_to_store_block_number", metrics.FilterMetricName(s.chainID))).Update(float64(block.Header.Number))
+	}
 
 	if ledgerconfig.IsCommitter() {
 		// Commit block with available private transactions
diff --git a/peer/node/start.go b/peer/node/start.go
index 710281f5f..af799d278 100644
--- a/peer/node/start.go
+++ b/peer/node/start.go
@@ -25,6 +25,7 @@ import (
 	"github.com/hyperledger/fabric/common/deliver"
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/localmsp"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/common/policies"
 	"github.com/hyperledger/fabric/common/viperutil"
 	"github.com/hyperledger/fabric/core/aclmgmt"
@@ -115,6 +116,9 @@ var nodeStartCmd = &cobra.Command{
 }
 
 func serve(args []string) error {
+
+	metrics.Initialize()
+
 	// currently the peer only works with the standard MSP
 	// because in certain scenarios the MSP has to make sure
 	// that from a single credential you only have a single 'identity'.
-- 
2.17.1 (Apple Git-112)

