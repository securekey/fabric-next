diff a/core/ledger/kvledger/kv_ledger.go b/core/ledger/kvledger/kv_ledger.go	(rejected hunks)
@@ -7,15 +7,13 @@ SPDX-License-Identifier: Apache-2.0
 package kvledger
 
 import (
-	"errors"
 	"fmt"
 	"sync"
 	"time"
 
-	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
-
 	"github.com/hyperledger/fabric/common/flogging"
 	commonledger "github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
@@ -23,12 +21,17 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/kvledger/bookkeeping"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/history/historydb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb/kvcache"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/txmgr"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/ledgerstorage"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	ledgerutil "github.com/hyperledger/fabric/core/ledger/util"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/pkg/errors"
+	"golang.org/x/net/context"
 )
 
 var logger = flogging.MustGetLogger("kvledger")
@@ -38,10 +41,20 @@ var logger = flogging.MustGetLogger("kvledger")
 type kvLedger struct {
 	ledgerID               string
 	blockStore             *ledgerstorage.Store
-	txtmgmt                txmgr.TxMgr
+	txtmgmt                txmgr.LockBasedTxMgr
 	historyDB              historydb.HistoryDB
+	versionedDB            privacyenabledstate.DB
+	kvCacheProvider        *kvcache.KVCacheProvider
 	configHistoryRetriever ledger.ConfigHistoryRetriever
 	blockAPIsRWLock        *sync.RWMutex
+	bcInfo                 *common.BlockchainInfo
+
+	stateCommitDoneCh chan *ledger.BlockAndPvtData
+	commitCh          chan *ledger.BlockAndPvtData
+	indexCh           chan *indexUpdate
+	stoppedCommitCh   chan struct{}
+	stoppedIndexCh    chan struct{}
+	doneCh            chan struct{}
 }
 
 // NewKVLedger constructs new `KVLedger`
@@ -58,7 +71,20 @@ func newKVLedger(
 	stateListeners = append(stateListeners, configHistoryMgr)
 	// Create a kvLedger for this chain/ledger, which encasulates the underlying
 	// id store, blockstore, txmgr (state database), history database
-	l := &kvLedger{ledgerID: ledgerID, blockStore: blockStore, historyDB: historyDB, blockAPIsRWLock: &sync.RWMutex{}}
+	l := &kvLedger{
+		ledgerID:          ledgerID,
+		blockStore:        blockStore,
+		historyDB:         historyDB,
+		versionedDB:       versionedDB,
+		kvCacheProvider:   versionedDB.GetKVCacheProvider(),
+		blockAPIsRWLock:   &sync.RWMutex{},
+		stateCommitDoneCh: make(chan *ledger.BlockAndPvtData),
+		commitCh:          make(chan *ledger.BlockAndPvtData),
+		indexCh:           make(chan *indexUpdate),
+		stoppedCommitCh:   make(chan struct{}),
+		stoppedIndexCh:    make(chan struct{}),
+		doneCh:            make(chan struct{}),
+	}
 
 	// TODO Move the function `GetChaincodeEventListener` to ledger interface and
 	// this functionality of regiserting for events to ledgermgmt package so that this
@@ -73,18 +99,36 @@ func newKVLedger(
 		return nil, err
 	}
 	l.initBlockStore(btlPolicy)
-	//Recover both state DB and history DB if they are out of sync with block storage
-	if err := l.recoverDBs(); err != nil {
-		panic(fmt.Errorf(`Error during state DB recovery:%s`, err))
+	if ledgerconfig.IsCommitter() {
+		//Recover both state DB and history DB if they are out of sync with block storage
+		if err := l.recoverDBs(); err != nil {
+			panic(fmt.Errorf(`Error during state DB recovery:%s`, err))
+		}
 	}
 	l.configHistoryRetriever = configHistoryMgr.GetRetriever(ledgerID, l)
+
+	// pre populate non durable private data cache
+	var err error
+	l.bcInfo, err = blockStore.GetBlockchainInfo()
+	if err != nil {
+		logger.Warningf("Skipping pre populate of non-durable private data cache due to failed fetching BlockChainInfo [ledgerID:%s] - error : %s", ledgerID, err)
+	} else {
+		err = l.populateNonDurablePvtCache(l.bcInfo.Height)
+		if err != nil {
+			logger.Warningf("Skipping pre populate of non-durable private data cache on new KV ledger because it failed [ledgerID:%s] - error : %s", ledgerID, err)
+		}
+	}
+
+	go l.commitWatcher(btlPolicy)
+	go l.indexWriter()
+
 	return l, nil
 }
 
 func (l *kvLedger) initTxMgr(versionedDB privacyenabledstate.DB, stateListeners []ledger.StateListener,
 	btlPolicy pvtdatapolicy.BTLPolicy, bookkeeperProvider bookkeeping.Provider) error {
 	var err error
-	l.txtmgmt, err = lockbasedtxmgr.NewLockBasedTxMgr(l.ledgerID, versionedDB, stateListeners, btlPolicy, bookkeeperProvider)
+	l.txtmgmt, err = lockbasedtxmgr.NewLockBasedTxMgr(l.ledgerID, versionedDB, stateListeners, btlPolicy, bookkeeperProvider, l.stateCommitDoneCh)
 	return err
 }
 
@@ -250,55 +359,74 @@ func (l *kvLedger) NewHistoryQueryExecutor() (ledger.HistoryQueryExecutor, error
 
 // CommitWithPvtData commits the block and the corresponding pvt data in an atomic operation
 func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) error {
-	var err error
-	block := pvtdataAndBlock.Block
-	blockNo := pvtdataAndBlock.Block.Header.Number
+	stopWatch := metrics.StopWatch("kvledger_CommitWithPvtData_duration")
+	defer stopWatch()
 
-	startStateValidation := time.Now()
-	logger.Debugf("[%s] Validating state for block [%d]", l.ledgerID, blockNo)
-	err = l.txtmgmt.ValidateAndPrepare(pvtdataAndBlock, true)
+	l.blockAPIsRWLock.Lock()
+	indexUpdate, err := l.cacheBlock(pvtdataAndBlock)
 	if err != nil {
-		return err
+		l.blockAPIsRWLock.Unlock()
+		panic(fmt.Errorf("block was not cached [%s]", err))
 	}
-	elapsedStateValidation := time.Since(startStateValidation) / time.Millisecond // duration in ms
+	l.blockAPIsRWLock.Unlock()
+
+	l.indexCh <- indexUpdate
+	l.commitCh <- pvtdataAndBlock
+
+	return nil
+}
+
+func (l *kvLedger) commitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) error {
+	stopWatch := metrics.StopWatch("kvledger_CommitWithPvtData_worker_duration")
+	defer stopWatch()
+
+	block := pvtdataAndBlock.Block
+	blockNo := pvtdataAndBlock.Block.Header.Number
 
-	startCommitBlockStorage := time.Now()
 	logger.Debugf("[%s] Committing block [%d] to storage", l.ledgerID, blockNo)
-	l.blockAPIsRWLock.Lock()
-	defer l.blockAPIsRWLock.Unlock()
-	if err = l.blockStore.CommitWithPvtData(pvtdataAndBlock); err != nil {
-		return err
+
+	if err := l.blockStore.CommitWithPvtData(pvtdataAndBlock); err != nil {
+		return errors.WithMessage(err, `Error during commit to block store`)
 	}
-	elapsedCommitBlockStorage := time.Since(startCommitBlockStorage) / time.Millisecond // duration in ms
 
-	startCommitState := time.Now()
 	logger.Debugf("[%s] Committing block [%d] transactions to state database", l.ledgerID, blockNo)
-	if err = l.txtmgmt.Commit(); err != nil {
-		panic(fmt.Errorf(`Error during commit to txmgr:%s`, err))
+	if err := l.txtmgmt.Commit(); err != nil {
+		return errors.WithMessage(err, `Error during commit to txmgr`)
 	}
-	elapsedCommitState := time.Since(startCommitState) / time.Millisecond // duration in ms
 
 	// History database could be written in parallel with state and/or async as a future optimization,
 	// although it has not been a bottleneck...no need to clutter the log with elapsed duration.
 	if ledgerconfig.IsHistoryDBEnabled() {
 		logger.Debugf("[%s] Committing block [%d] transactions to history database", l.ledgerID, blockNo)
 		if err := l.historyDB.Commit(block); err != nil {
-			panic(fmt.Errorf(`Error during commit to history db:%s`, err))
+			return errors.WithMessage(err, `Error during commit to history db`)
 		}
 	}
 
-	elapsedCommitWithPvtData := time.Since(startStateValidation) / time.Millisecond // total duration in ms
-
-	logger.Infof("[%s] Committed block [%d] with %d transaction(s) in %dms (state_validation=%dms block_commit=%dms state_commit=%dms)",
-		l.ledgerID, block.Header.Number, len(block.Data.Data), elapsedCommitWithPvtData,
-		elapsedStateValidation, elapsedCommitBlockStorage, elapsedCommitState)
+	// Set the checkpoint now that all of the data has been successfully committed
+	if err := l.blockStore.CheckpointBlock(block); err != nil {
+		return errors.WithMessage(err, `Error during checkpoint`)
+	}
 
 	return nil
 }
 
+// ValidateMVCC validates block for MVCC conflicts and phantom reads against committed data
+func (l *kvLedger) ValidateMVCC(ctx context.Context, block *common.Block, txFlags ledgerutil.TxValidationFlags, filter ledgerutil.TxFilter) error {
+	return l.txtmgmt.ValidateMVCC(ctx, block, txFlags, filter)
+}
+
+// ValidateBlockWithPvtData validate commit with pvt data
+func (l *kvLedger) ValidateBlockWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) error {
+	return l.txtmgmt.ValidateAndPrepare(pvtdataAndBlock, true)
+}
+
 // GetPvtDataAndBlockByNum returns the block and the corresponding pvt data.
 // The pvt data is filtered by the list of 'collections' supplied
 func (l *kvLedger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsCollFilter) (*ledger.BlockAndPvtData, error) {
+	stopWatch := metrics.StopWatch("kvledger_GetPvtDataAndBlockByNum_duration")
+	defer stopWatch()
+
 	blockAndPvtdata, err := l.blockStore.GetPvtDataAndBlockByNum(blockNum, filter)
 	l.blockAPIsRWLock.RLock()
 	l.blockAPIsRWLock.RUnlock()
