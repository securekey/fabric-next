From b10cb5b946a3972dafa5109b5b353b5b83621956 Mon Sep 17 00:00:00 2001
From: manish <manish.sethi@gmail.com>
Date: Thu, 12 Oct 2017 15:54:05 -0400
Subject: [PATCH] [FAB-6619] purge pvtdata from pvt block store

This CR includes
 - Make the  pvtdata key based on collection so that purge does not
   have to perform a read and can simply issue delete for the keys
 - Store the expiry schedule based on the expirying block for collections
 - Purge the expired pvt data periodically (default every 500 block)

Change-Id: I8700de8a3d24436ac88472b4f46e8f994475e6c6
Signed-off-by: manish <manish.sethi@gmail.com>
---
 core/ledger/ledgerconfig/ledger_config.go        |  10 +
 core/ledger/pvtdatastorage/expiry_data.pb.go     | 107 +++++++++++
 core/ledger/pvtdatastorage/expiry_data.proto     |  23 +++
 core/ledger/pvtdatastorage/expiry_data_helper.go |  29 +++
 core/ledger/pvtdatastorage/helper.go             | 122 ++++++++++++
 core/ledger/pvtdatastorage/kv_encoding.go        |  78 ++++++--
 core/ledger/pvtdatastorage/kv_encoding_test.go   |  19 ++
 core/ledger/pvtdatastorage/store_impl.go         | 225 ++++++++++++++--------
 core/ledger/pvtdatastorage/store_impl_test.go    | 226 ++++++++++++++++++-----
 core/ledger/pvtdatastorage/test_exports.go       |  13 +-
 core/transientstore/store.go                     |   3 +-
 core/transientstore/store_helper.go              |  36 ++++
 12 files changed, 733 insertions(+), 158 deletions(-)
 create mode 100644 core/ledger/pvtdatastorage/expiry_data.pb.go
 create mode 100644 core/ledger/pvtdatastorage/expiry_data.proto
 create mode 100644 core/ledger/pvtdatastorage/expiry_data_helper.go
 create mode 100644 core/ledger/pvtdatastorage/helper.go
 create mode 100644 core/ledger/pvtdatastorage/kv_encoding_test.go

diff --git a/core/ledger/ledgerconfig/ledger_config.go b/core/ledger/ledgerconfig/ledger_config.go
index c27206f7..70dd2075 100644
--- a/core/ledger/ledgerconfig/ledger_config.go
+++ b/core/ledger/ledgerconfig/ledger_config.go
@@ -94,6 +94,16 @@ func GetMaxBatchUpdateSize() int {
 	return maxBatchUpdateSize
 }
 
+// GetPvtdataStorePurgeInterval returns the interval in the terms of number of blocks
+// when the purge for the expired data would be performed
+func GetPvtdataStorePurgeInterval() uint64 {
+	purgeInterval := viper.GetInt("ledger.pvtdataStore.purgeInterval")
+	if purgeInterval <= 0 {
+		purgeInterval = 100
+	}
+	return uint64(purgeInterval)
+}
+
 //IsHistoryDBEnabled exposes the historyDatabase variable
 func IsHistoryDBEnabled() bool {
 	return viper.GetBool("ledger.history.enableHistoryDatabase")
diff --git a/core/ledger/pvtdatastorage/expiry_data.pb.go b/core/ledger/pvtdatastorage/expiry_data.pb.go
new file mode 100644
index 00000000..42726ce8
--- /dev/null
+++ b/core/ledger/pvtdatastorage/expiry_data.pb.go
@@ -0,0 +1,107 @@
+// Code generated by protoc-gen-go. DO NOT EDIT.
+// source: expiry_data.proto
+
+/*
+Package pvtdatastorage is a generated protocol buffer package.
+
+It is generated from these files:
+	expiry_data.proto
+
+It has these top-level messages:
+	ExpiryData
+	Collections
+	TxNums
+*/
+package pvtdatastorage
+
+import proto "github.com/golang/protobuf/proto"
+import fmt "fmt"
+import math "math"
+
+// Reference imports to suppress errors if they are not otherwise used.
+var _ = proto.Marshal
+var _ = fmt.Errorf
+var _ = math.Inf
+
+// This is a compile-time assertion to ensure that this generated file
+// is compatible with the proto package it is being compiled against.
+// A compilation error at this line likely means your copy of the
+// proto package needs to be updated.
+const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package
+
+type ExpiryData struct {
+	Map map[string]*Collections `protobuf:"bytes,1,rep,name=map" json:"map,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
+}
+
+func (m *ExpiryData) Reset()                    { *m = ExpiryData{} }
+func (m *ExpiryData) String() string            { return proto.CompactTextString(m) }
+func (*ExpiryData) ProtoMessage()               {}
+func (*ExpiryData) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{0} }
+
+func (m *ExpiryData) GetMap() map[string]*Collections {
+	if m != nil {
+		return m.Map
+	}
+	return nil
+}
+
+type Collections struct {
+	Map map[string]*TxNums `protobuf:"bytes,1,rep,name=map" json:"map,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
+}
+
+func (m *Collections) Reset()                    { *m = Collections{} }
+func (m *Collections) String() string            { return proto.CompactTextString(m) }
+func (*Collections) ProtoMessage()               {}
+func (*Collections) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{1} }
+
+func (m *Collections) GetMap() map[string]*TxNums {
+	if m != nil {
+		return m.Map
+	}
+	return nil
+}
+
+type TxNums struct {
+	List []uint64 `protobuf:"varint,1,rep,packed,name=list" json:"list,omitempty"`
+}
+
+func (m *TxNums) Reset()                    { *m = TxNums{} }
+func (m *TxNums) String() string            { return proto.CompactTextString(m) }
+func (*TxNums) ProtoMessage()               {}
+func (*TxNums) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{2} }
+
+func (m *TxNums) GetList() []uint64 {
+	if m != nil {
+		return m.List
+	}
+	return nil
+}
+
+func init() {
+	proto.RegisterType((*ExpiryData)(nil), "pvtdatastorage.ExpiryData")
+	proto.RegisterType((*Collections)(nil), "pvtdatastorage.Collections")
+	proto.RegisterType((*TxNums)(nil), "pvtdatastorage.TxNums")
+}
+
+func init() { proto.RegisterFile("expiry_data.proto", fileDescriptor0) }
+
+var fileDescriptor0 = []byte{
+	// 263 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xe2, 0x12, 0x4c, 0xad, 0x28, 0xc8,
+	0x2c, 0xaa, 0x8c, 0x4f, 0x49, 0x2c, 0x49, 0xd4, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0xe2, 0x2b,
+	0x28, 0x2b, 0x01, 0x71, 0x8b, 0x4b, 0xf2, 0x8b, 0x12, 0xd3, 0x53, 0x95, 0x66, 0x30, 0x72, 0x71,
+	0xb9, 0x82, 0x55, 0xb9, 0x24, 0x96, 0x24, 0x0a, 0x99, 0x72, 0x31, 0xe7, 0x26, 0x16, 0x48, 0x30,
+	0x2a, 0x30, 0x6b, 0x70, 0x1b, 0x29, 0xeb, 0xa1, 0x2a, 0xd6, 0x43, 0x28, 0xd4, 0xf3, 0x4d, 0x2c,
+	0x70, 0xcd, 0x2b, 0x29, 0xaa, 0x0c, 0x02, 0xa9, 0x97, 0x0a, 0xe6, 0xe2, 0x80, 0x09, 0x08, 0x09,
+	0x70, 0x31, 0x67, 0xa7, 0x56, 0x4a, 0x30, 0x2a, 0x30, 0x6a, 0x70, 0x06, 0x81, 0x98, 0x42, 0x86,
+	0x5c, 0xac, 0x65, 0x89, 0x39, 0xa5, 0xa9, 0x12, 0x4c, 0x0a, 0x8c, 0x1a, 0xdc, 0x46, 0xd2, 0xe8,
+	0xc6, 0x3a, 0xe7, 0xe7, 0xe4, 0xa4, 0x26, 0x97, 0x64, 0xe6, 0xe7, 0x15, 0x07, 0x41, 0x54, 0x5a,
+	0x31, 0x59, 0x30, 0x2a, 0x4d, 0x65, 0xe4, 0xe2, 0x46, 0x92, 0x12, 0x32, 0x43, 0x76, 0x9b, 0x0a,
+	0x1e, 0x43, 0xd0, 0x1c, 0xe7, 0x87, 0xd7, 0x71, 0x3a, 0xa8, 0x8e, 0x13, 0x43, 0x37, 0x37, 0xa4,
+	0xc2, 0xaf, 0x34, 0x17, 0xc5, 0x5d, 0x32, 0x5c, 0x6c, 0x10, 0x41, 0x21, 0x21, 0x2e, 0x96, 0x9c,
+	0xcc, 0xe2, 0x12, 0xb0, 0x93, 0x58, 0x82, 0xc0, 0x6c, 0x27, 0xab, 0x28, 0x8b, 0xf4, 0xcc, 0x92,
+	0x8c, 0xd2, 0x24, 0xbd, 0xe4, 0xfc, 0x5c, 0xfd, 0x8c, 0xca, 0x82, 0xd4, 0xa2, 0x9c, 0xd4, 0x94,
+	0xf4, 0xd4, 0x22, 0xfd, 0xb4, 0xc4, 0xa4, 0xa2, 0xcc, 0x64, 0xfd, 0xe4, 0xfc, 0xa2, 0x54, 0x7d,
+	0xa8, 0x10, 0xaa, 0x5d, 0x49, 0x6c, 0xe0, 0x38, 0x32, 0x06, 0x04, 0x00, 0x00, 0xff, 0xff, 0xb7,
+	0xd4, 0xf8, 0x9d, 0xb8, 0x01, 0x00, 0x00,
+}
diff --git a/core/ledger/pvtdatastorage/expiry_data.proto b/core/ledger/pvtdatastorage/expiry_data.proto
new file mode 100644
index 00000000..e5d58e99
--- /dev/null
+++ b/core/ledger/pvtdatastorage/expiry_data.proto
@@ -0,0 +1,23 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+syntax = "proto3";
+
+option go_package = "github.com/hyperledger/fabric/core/ledger/pvtdatastorage";
+
+package pvtdatastorage;
+
+message ExpiryData {
+    map<string, Collections>  map = 1;
+}
+
+message Collections {
+    map<string, TxNums> map = 1;
+}
+
+message TxNums {
+    repeated uint64 list = 1;
+}
\ No newline at end of file
diff --git a/core/ledger/pvtdatastorage/expiry_data_helper.go b/core/ledger/pvtdatastorage/expiry_data_helper.go
new file mode 100644
index 00000000..89d5cea9
--- /dev/null
+++ b/core/ledger/pvtdatastorage/expiry_data_helper.go
@@ -0,0 +1,29 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+func newExpiryData() *ExpiryData {
+	return &ExpiryData{make(map[string]*Collections)}
+}
+
+func newCollections() *Collections {
+	return &Collections{make(map[string]*TxNums)}
+}
+
+func (e *ExpiryData) add(ns, coll string, txNum uint64) {
+	collections, ok := e.Map[ns]
+	if !ok {
+		collections = newCollections()
+		e.Map[ns] = collections
+	}
+	txNums, ok := collections.Map[coll]
+	if !ok {
+		txNums = &TxNums{}
+		collections.Map[coll] = txNums
+	}
+	txNums.List = append(txNums.List, txNum)
+}
diff --git a/core/ledger/pvtdatastorage/helper.go b/core/ledger/pvtdatastorage/helper.go
new file mode 100644
index 00000000..28d909c2
--- /dev/null
+++ b/core/ledger/pvtdatastorage/helper.go
@@ -0,0 +1,122 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"math"
+
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+)
+
+func prepareStoreEntries(blockNum uint64, pvtdata []*ledger.TxPvtData, btlPolicy pvtdatapolicy.BTLPolicy) ([]*dataEntry, []*expiryEntry) {
+	dataEntries := prepareDataEntries(blockNum, pvtdata)
+	expiryEntries := prepareExpiryEntries(blockNum, dataEntries, btlPolicy)
+	return dataEntries, expiryEntries
+}
+
+func prepareDataEntries(blockNum uint64, pvtData []*ledger.TxPvtData) []*dataEntry {
+	var dataEntries []*dataEntry
+	for _, txPvtdata := range pvtData {
+		for _, nsPvtdata := range txPvtdata.WriteSet.NsPvtRwset {
+			for _, collPvtdata := range nsPvtdata.CollectionPvtRwset {
+				txnum := txPvtdata.SeqInBlock
+				ns := nsPvtdata.Namespace
+				coll := collPvtdata.CollectionName
+				dataKey := &dataKey{blockNum, txnum, ns, coll}
+				dataEntries = append(dataEntries, &dataEntry{key: dataKey, value: collPvtdata})
+			}
+		}
+	}
+	return dataEntries
+}
+
+func prepareExpiryEntries(committingBlk uint64, dataEntries []*dataEntry, btlPolicy pvtdatapolicy.BTLPolicy) []*expiryEntry {
+	mapByExpiringBlk := make(map[uint64]*ExpiryData)
+	for _, dataEntry := range dataEntries {
+		expiringBlk := btlPolicy.GetExpiringBlock(dataEntry.key.ns, dataEntry.key.coll, dataEntry.key.blkNum)
+		if neverExpires(expiringBlk) {
+			continue
+		}
+		expiryData, ok := mapByExpiringBlk[expiringBlk]
+		if !ok {
+			expiryData = newExpiryData()
+			mapByExpiringBlk[expiringBlk] = expiryData
+		}
+		expiryData.add(dataEntry.key.ns, dataEntry.key.coll, dataEntry.key.txNum)
+	}
+	var expiryEntries []*expiryEntry
+	for expiryBlk, expiryData := range mapByExpiringBlk {
+		expiryKey := &expiryKey{expiringBlk: expiryBlk, committingBlk: committingBlk}
+		expiryEntries = append(expiryEntries, &expiryEntry{key: expiryKey, value: expiryData})
+	}
+	return expiryEntries
+}
+
+func deriveDataKeys(expiryEntry *expiryEntry) []*dataKey {
+	var dataKeys []*dataKey
+	for ns, colls := range expiryEntry.value.Map {
+		for coll, txNums := range colls.Map {
+			for _, txNum := range txNums.List {
+				dataKeys = append(dataKeys, &dataKey{expiryEntry.key.committingBlk, txNum, ns, coll})
+			}
+		}
+	}
+	return dataKeys
+}
+
+func passesFilter(dataKey *dataKey, filter ledger.PvtNsCollFilter) bool {
+	return filter == nil || filter.Has(dataKey.ns, dataKey.coll)
+}
+
+func isExpired(dataKey *dataKey, btl pvtdatapolicy.BTLPolicy, latestBlkNum uint64) bool {
+	return latestBlkNum >= btl.GetExpiringBlock(dataKey.ns, dataKey.coll, dataKey.blkNum)
+}
+
+func neverExpires(expiringBlkNum uint64) bool {
+	return expiringBlkNum == math.MaxUint64
+}
+
+type txPvtdataAssembler struct {
+	blockNum, txNum uint64
+	txWset          *rwset.TxPvtReadWriteSet
+	currentNsWSet   *rwset.NsPvtReadWriteSet
+	firstCall       bool
+}
+
+func newTxPvtdataAssembler(blockNum, txNum uint64) *txPvtdataAssembler {
+	return &txPvtdataAssembler{blockNum, txNum, &rwset.TxPvtReadWriteSet{}, nil, true}
+}
+
+func (a *txPvtdataAssembler) add(ns string, collPvtWset *rwset.CollectionPvtReadWriteSet) {
+	// start a NsWset
+	if a.firstCall {
+		a.currentNsWSet = &rwset.NsPvtReadWriteSet{Namespace: ns}
+		a.firstCall = false
+	}
+
+	// if a new ns started, add the existing NsWset to TxWset and start a new one
+	if a.currentNsWSet.Namespace != ns {
+		a.txWset.NsPvtRwset = append(a.txWset.NsPvtRwset, a.currentNsWSet)
+		a.currentNsWSet = &rwset.NsPvtReadWriteSet{Namespace: ns}
+	}
+	// add the collWset to the current NsWset
+	a.currentNsWSet.CollectionPvtRwset = append(a.currentNsWSet.CollectionPvtRwset, collPvtWset)
+}
+
+func (a *txPvtdataAssembler) done() {
+	if a.currentNsWSet != nil {
+		a.txWset.NsPvtRwset = append(a.txWset.NsPvtRwset, a.currentNsWSet)
+	}
+	a.currentNsWSet = nil
+}
+
+func (a *txPvtdataAssembler) getTxPvtdata() *ledger.TxPvtData {
+	a.done()
+	return &ledger.TxPvtData{SeqInBlock: a.txNum, WriteSet: a.txWset}
+}
diff --git a/core/ledger/pvtdatastorage/kv_encoding.go b/core/ledger/pvtdatastorage/kv_encoding.go
index 69951a52..d7b5bfae 100644
--- a/core/ledger/pvtdatastorage/kv_encoding.go
+++ b/core/ledger/pvtdatastorage/kv_encoding.go
@@ -7,7 +7,7 @@ SPDX-License-Identifier: Apache-2.0
 package pvtdatastorage
 
 import (
-	"math"
+	"bytes"
 
 	"github.com/golang/protobuf/proto"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
@@ -18,39 +18,77 @@ var (
 	pendingCommitKey    = []byte{0}
 	lastCommittedBlkkey = []byte{1}
 	pvtDataKeyPrefix    = []byte{2}
+	expiryKeyPrefix     = []byte{3}
 
+	nilByte    = byte(0)
 	emptyValue = []byte{}
 )
 
-func encodePK(blockNum uint64, tranNum uint64) blkTranNumKey {
-	return append(pvtDataKeyPrefix, version.NewHeight(blockNum, tranNum).ToBytes()...)
+func getDataKeysForRangeScanByBlockNum(blockNum uint64) (startKey, endKey []byte) {
+	startKey = append(pvtDataKeyPrefix, version.NewHeight(blockNum, 0).ToBytes()...)
+	endKey = append(pvtDataKeyPrefix, version.NewHeight(blockNum+1, 0).ToBytes()...)
+	return
 }
 
-func decodePK(key blkTranNumKey) (blockNum uint64, tranNum uint64) {
-	height, _ := version.NewHeightFromBytes(key[1:])
-	return height.BlockNum, height.TxNum
+func getExpiryKeysForRangeScan(minBlkNum, maxBlkNum uint64) (startKey, endKey []byte) {
+	startKey = append(expiryKeyPrefix, version.NewHeight(minBlkNum, 0).ToBytes()...)
+	endKey = append(expiryKeyPrefix, version.NewHeight(maxBlkNum+1, 0).ToBytes()...)
+	return
 }
 
-func getKeysForRangeScanByBlockNum(blockNum uint64) (startKey []byte, endKey []byte) {
-	startKey = encodePK(blockNum, 0)
-	endKey = encodePK(blockNum, math.MaxUint64)
-	return
+func encodeLastCommittedBlockVal(blockNum uint64) []byte {
+	return proto.EncodeVarint(blockNum)
 }
 
-func encodePvtRwSet(txPvtRwSet *rwset.TxPvtReadWriteSet) ([]byte, error) {
-	return proto.Marshal(txPvtRwSet)
+func decodeLastCommittedBlockVal(blockNumBytes []byte) uint64 {
+	s, _ := proto.DecodeVarint(blockNumBytes)
+	return s
 }
 
-func decodePvtRwSet(encodedBytes []byte) (*rwset.TxPvtReadWriteSet, error) {
-	writeset := &rwset.TxPvtReadWriteSet{}
-	return writeset, proto.Unmarshal(encodedBytes, writeset)
+func encodeDataKey(key *dataKey) []byte {
+	dataKeyBytes := append(pvtDataKeyPrefix, version.NewHeight(key.blkNum, key.txNum).ToBytes()...)
+	dataKeyBytes = append(dataKeyBytes, []byte(key.ns)...)
+	dataKeyBytes = append(dataKeyBytes, nilByte)
+	return append(dataKeyBytes, []byte(key.coll)...)
 }
 
-func encodeBlockNum(blockNum uint64) []byte {
-	return proto.EncodeVarint(blockNum)
+func encodeDataValue(collData *rwset.CollectionPvtReadWriteSet) ([]byte, error) {
+	return proto.Marshal(collData)
 }
 
-func decodeBlockNum(blockNumBytes []byte) uint64 {
-	s, _ := proto.DecodeVarint(blockNumBytes)
-	return s
+func encodeExpiryKey(expiryKey *expiryKey) []byte {
+	// reusing version encoding scheme here
+	return append(expiryKeyPrefix, version.NewHeight(expiryKey.expiringBlk, expiryKey.committingBlk).ToBytes()...)
+}
+
+func encodeExpiryValue(expiryData *ExpiryData) ([]byte, error) {
+	return proto.Marshal(expiryData)
+}
+
+func decodeExpiryKey(expiryKeyBytes []byte) *expiryKey {
+	height, _ := version.NewHeightFromBytes(expiryKeyBytes[1:])
+	return &expiryKey{expiringBlk: height.BlockNum, committingBlk: height.TxNum}
+}
+
+func decodeExpiryValue(expiryValueBytes []byte) (*ExpiryData, error) {
+	expiryData := &ExpiryData{}
+	err := proto.Unmarshal(expiryValueBytes, expiryData)
+	return expiryData, err
+}
+
+func decodeDatakey(datakeyBytes []byte) *dataKey {
+	v, n := version.NewHeightFromBytes(datakeyBytes[1:])
+	blkNum := v.BlockNum
+	tranNum := v.TxNum
+	remainingBytes := datakeyBytes[n+1:]
+	nilByteIndex := bytes.IndexByte(remainingBytes, nilByte)
+	ns := string(remainingBytes[:nilByteIndex])
+	coll := string(remainingBytes[nilByteIndex+1:])
+	return &dataKey{blkNum: blkNum, txNum: tranNum, ns: ns, coll: coll}
+}
+
+func decodeDataValue(datavalueBytes []byte) (*rwset.CollectionPvtReadWriteSet, error) {
+	collPvtdata := &rwset.CollectionPvtReadWriteSet{}
+	err := proto.Unmarshal(datavalueBytes, collPvtdata)
+	return collPvtdata, err
 }
diff --git a/core/ledger/pvtdatastorage/kv_encoding_test.go b/core/ledger/pvtdatastorage/kv_encoding_test.go
new file mode 100644
index 00000000..eeffaf01
--- /dev/null
+++ b/core/ledger/pvtdatastorage/kv_encoding_test.go
@@ -0,0 +1,19 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+)
+
+func TestDataKeyEncoding(t *testing.T) {
+	dataKey1 := &dataKey{blkNum: 2, txNum: 5, ns: "ns1", coll: "coll1"}
+	datakey2 := decodeDatakey(encodeDataKey(dataKey1))
+	assert.Equal(t, dataKey1, datakey2)
+}
diff --git a/core/ledger/pvtdatastorage/store_impl.go b/core/ledger/pvtdatastorage/store_impl.go
index a7a270cf..6cd0c41e 100644
--- a/core/ledger/pvtdatastorage/store_impl.go
+++ b/core/ledger/pvtdatastorage/store_impl.go
@@ -8,11 +8,13 @@ package pvtdatastorage
 
 import (
 	"fmt"
+	"sync"
 
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
 	"github.com/hyperledger/fabric/protos/ledger/rwset"
 )
 
@@ -23,15 +25,42 @@ type provider struct {
 }
 
 type store struct {
-	db                 *leveldbhelper.DBHandle
-	ledgerid           string
+	db        *leveldbhelper.DBHandle
+	ledgerid  string
+	btlPolicy pvtdatapolicy.BTLPolicy
+
 	isEmpty            bool
 	lastCommittedBlock uint64
 	batchPending       bool
+	purgerLock         sync.Mutex
 }
 
 type blkTranNumKey []byte
 
+type dataEntry struct {
+	key   *dataKey
+	value *rwset.CollectionPvtReadWriteSet
+}
+
+type expiryEntry struct {
+	key   *expiryKey
+	value *ExpiryData
+}
+
+type expiryKey struct {
+	expiringBlk   uint64
+	committingBlk uint64
+}
+
+type dataKey struct {
+	blkNum   uint64
+	txNum    uint64
+	ns, coll string
+}
+
+//////// Provider functions  /////////////
+//////////////////////////////////////////
+
 // NewProvider instantiates a StoreProvider
 func NewProvider() Provider {
 	dbPath := ledgerconfig.GetPvtdataStorePath()
@@ -42,7 +71,11 @@ func NewProvider() Provider {
 // OpenStore returns a handle to a store
 func (p *provider) OpenStore(ledgerid string) (Store, error) {
 	dbHandle := p.dbProvider.GetDBHandle(ledgerid)
-	s := &store{db: dbHandle, ledgerid: ledgerid}
+	btlPolicy, err := pvtdatapolicy.GetBTLPolicy(ledgerid)
+	if err != nil {
+		return nil, err
+	}
+	s := &store{db: dbHandle, ledgerid: ledgerid, btlPolicy: btlPolicy}
 	if err := s.initState(); err != nil {
 		return nil, err
 	}
@@ -54,6 +87,9 @@ func (p *provider) Close() {
 	p.dbProvider.Close()
 }
 
+//////// store functions  ////////////////
+//////////////////////////////////////////
+
 func (s *store) initState() error {
 	var err error
 	if s.isEmpty, s.lastCommittedBlock, err = s.getLastCommittedBlockNum(); err != nil {
@@ -77,15 +113,22 @@ func (s *store) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData) error {
 	}
 
 	batch := leveldbhelper.NewUpdateBatch()
-	var key, value []byte
 	var err error
-	for _, txPvtData := range pvtData {
-		key = encodePK(blockNum, txPvtData.SeqInBlock)
-		if value, err = encodePvtRwSet(txPvtData.WriteSet); err != nil {
+	var keyBytes, valBytes []byte
+	dataEntries, expiryEnteries := prepareStoreEntries(blockNum, pvtData, s.btlPolicy)
+	for _, dataEntry := range dataEntries {
+		keyBytes = encodeDataKey(dataEntry.key)
+		if valBytes, err = encodeDataValue(dataEntry.value); err != nil {
 			return err
 		}
-		logger.Debugf("Adding private data to LevelDB batch for block [%d], tran [%d]", blockNum, txPvtData.SeqInBlock)
-		batch.Put(key, value)
+		batch.Put(keyBytes, valBytes)
+	}
+	for _, expiryEntry := range expiryEnteries {
+		keyBytes = encodeExpiryKey(expiryEntry.key)
+		if valBytes, err = encodeExpiryValue(expiryEntry.value); err != nil {
+			return err
+		}
+		batch.Put(keyBytes, valBytes)
 	}
 	batch.Put(pendingCommitKey, emptyValue)
 	if err := s.db.WriteBatch(batch, true); err != nil {
@@ -105,7 +148,7 @@ func (s *store) Commit() error {
 	logger.Debugf("Committing private data for block [%d]", committingBlockNum)
 	batch := leveldbhelper.NewUpdateBatch()
 	batch.Delete(pendingCommitKey)
-	batch.Put(lastCommittedBlkkey, encodeBlockNum(committingBlockNum))
+	batch.Put(lastCommittedBlkkey, encodeLastCommittedBlockVal(committingBlockNum))
 	if err := s.db.WriteBatch(batch, true); err != nil {
 		return err
 	}
@@ -113,32 +156,19 @@ func (s *store) Commit() error {
 	s.isEmpty = false
 	s.lastCommittedBlock = committingBlockNum
 	logger.Debugf("Committed private data for block [%d]", committingBlockNum)
+	s.performPurgeIfScheduled(committingBlockNum)
 	return nil
 }
 
 // Rollback implements the function in the interface `Store`
+// Not deleting the existing data entries and expiry entries for now
+// Because the next try would have exact same entries and will overwrite those
 func (s *store) Rollback() error {
-	var pendingBatchKeys []blkTranNumKey
-	var err error
 	if !s.batchPending {
 		return &ErrIllegalCall{"No pending batch to rollback"}
 	}
-	rollingbackBlockNum := s.nextBlockNum()
-	logger.Debugf("Rolling back private data for block [%d]", rollingbackBlockNum)
 
-	if pendingBatchKeys, err = s.retrievePendingBatchKeys(); err != nil {
-		return err
-	}
-	batch := leveldbhelper.NewUpdateBatch()
-	for _, key := range pendingBatchKeys {
-		batch.Delete(key)
-	}
-	batch.Delete(pendingCommitKey)
-	if err := s.db.WriteBatch(batch, true); err != nil {
-		return err
-	}
 	s.batchPending = false
-	logger.Debugf("Rolled back private data for block [%d]", rollingbackBlockNum)
 	return nil
 }
 
@@ -150,28 +180,48 @@ func (s *store) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFil
 	if s.isEmpty {
 		return nil, &ErrOutOfRange{"The store is empty"}
 	}
-
 	if blockNum > s.lastCommittedBlock {
 		return nil, &ErrOutOfRange{fmt.Sprintf("Last committed block=%d, block requested=%d", s.lastCommittedBlock, blockNum)}
 	}
-	var pvtData []*ledger.TxPvtData
-	startKey, endKey := getKeysForRangeScanByBlockNum(blockNum)
+	startKey, endKey := getDataKeysForRangeScanByBlockNum(blockNum)
 	logger.Debugf("Querying private data storage for write sets using startKey=%#v, endKey=%#v", startKey, endKey)
 	itr := s.db.GetIterator(startKey, endKey)
 	defer itr.Release()
 
-	var pvtWSet *rwset.TxPvtReadWriteSet
-	var err error
+	var blockPvtdata []*ledger.TxPvtData
+	var currentTxNum uint64
+	var currentTxWsetAssember *txPvtdataAssembler
+	firstItr := true
+
 	for itr.Next() {
-		bNum, tNum := decodePK(itr.Key())
-		if pvtWSet, err = decodePvtRwSet(itr.Value()); err != nil {
+		dataKeyBytes := itr.Key()
+		dataValueBytes := itr.Value()
+		dataKey := decodeDatakey(dataKeyBytes)
+		if isExpired(dataKey, s.btlPolicy, s.lastCommittedBlock) || !passesFilter(dataKey, filter) {
+			continue
+		}
+		dataValue, err := decodeDataValue(dataValueBytes)
+		if err != nil {
 			return nil, err
 		}
-		logger.Debugf("Retrieved private data write set for block [%d] tran [%d]", bNum, tNum)
-		filteredWSet := TrimPvtWSet(pvtWSet, filter)
-		pvtData = append(pvtData, &ledger.TxPvtData{SeqInBlock: tNum, WriteSet: filteredWSet})
+
+		if firstItr {
+			currentTxNum = dataKey.txNum
+			currentTxWsetAssember = newTxPvtdataAssembler(blockNum, currentTxNum)
+			firstItr = false
+		}
+
+		if dataKey.txNum != currentTxNum {
+			blockPvtdata = append(blockPvtdata, currentTxWsetAssember.getTxPvtdata())
+			currentTxNum = dataKey.txNum
+			currentTxWsetAssember = newTxPvtdataAssembler(blockNum, currentTxNum)
+		}
+		currentTxWsetAssember.add(dataKey.ns, dataValue)
+	}
+	if currentTxWsetAssember != nil {
+		blockPvtdata = append(blockPvtdata, currentTxWsetAssember.getTxPvtdata())
 	}
-	return pvtData, nil
+	return blockPvtdata, nil
 }
 
 // InitLastCommittedBlock implements the function in the interface `Store`
@@ -180,7 +230,7 @@ func (s *store) InitLastCommittedBlock(blockNum uint64) error {
 		return &ErrIllegalCall{"The private data store is not empty. InitLastCommittedBlock() function call is not allowed"}
 	}
 	batch := leveldbhelper.NewUpdateBatch()
-	batch.Put(lastCommittedBlkkey, encodeBlockNum(blockNum))
+	batch.Put(lastCommittedBlkkey, encodeLastCommittedBlockVal(blockNum))
 	if err := s.db.WriteBatch(batch, true); err != nil {
 		return err
 	}
@@ -190,6 +240,60 @@ func (s *store) InitLastCommittedBlock(blockNum uint64) error {
 	return nil
 }
 
+func (s *store) performPurgeIfScheduled(latestCommittedBlk uint64) {
+	if latestCommittedBlk%ledgerconfig.GetPvtdataStorePurgeInterval() != 0 {
+		return
+	}
+	go func() {
+		s.purgerLock.Lock()
+		logger.Info("Purger started")
+		defer s.purgerLock.Unlock()
+		err := s.purgeExpiredData(0, latestCommittedBlk)
+		if err != nil {
+			logger.Warningf("Could not purge data from pvtdata store:%s", err)
+		}
+		logger.Info("Purger finished")
+	}()
+}
+
+func (s *store) purgeExpiredData(minBlkNum, maxBlkNum uint64) error {
+	batch := leveldbhelper.NewUpdateBatch()
+	expiryEntries, err := s.retrieveExpiryEntries(minBlkNum, maxBlkNum)
+	if err != nil {
+		return nil
+	}
+	for _, expiryEntry := range expiryEntries {
+		// this encoding could have been saved if the function retrieveExpiryEntries also returns the encoded expiry keys.
+		// However, keeping it for better readability
+		batch.Delete(encodeExpiryKey(expiryEntry.key))
+		for _, dataKey := range deriveDataKeys(expiryEntry) {
+			batch.Delete(encodeDataKey(dataKey))
+		}
+		s.db.WriteBatch(batch, false)
+	}
+	return nil
+}
+
+func (s *store) retrieveExpiryEntries(minBlkNum, maxBlkNum uint64) ([]*expiryEntry, error) {
+	startKey, endKey := getExpiryKeysForRangeScan(minBlkNum, maxBlkNum)
+	logger.Debugf("GetPvtDataByBlockNum(): startKey=%#v, endKey=%#v", startKey, endKey)
+	itr := s.db.GetIterator(startKey, endKey)
+	defer itr.Release()
+
+	var expiryEntries []*expiryEntry
+	for itr.Next() {
+		expiryKeyBytes := itr.Key()
+		expiryValueBytes := itr.Value()
+		expiryKey := decodeExpiryKey(expiryKeyBytes)
+		expiryValue, err := decodeExpiryValue(expiryValueBytes)
+		if err != nil {
+			return nil, err
+		}
+		expiryEntries = append(expiryEntries, &expiryEntry{key: expiryKey, value: expiryValue})
+	}
+	return expiryEntries, nil
+}
+
 // LastCommittedBlockHeight implements the function in the interface `Store`
 func (s *store) LastCommittedBlockHeight() (uint64, error) {
 	if s.isEmpty {
@@ -220,15 +324,6 @@ func (s *store) nextBlockNum() uint64 {
 	return s.lastCommittedBlock + 1
 }
 
-func (s *store) retrievePendingBatchKeys() ([]blkTranNumKey, error) {
-	var pendingBatchKeys []blkTranNumKey
-	itr := s.db.GetIterator(encodePK(s.nextBlockNum(), 0), nil)
-	for itr.Next() {
-		pendingBatchKeys = append(pendingBatchKeys, itr.Key())
-	}
-	return pendingBatchKeys, nil
-}
-
 func (s *store) hasPendingCommit() (bool, error) {
 	var v []byte
 	var err error
@@ -244,39 +339,5 @@ func (s *store) getLastCommittedBlockNum() (bool, uint64, error) {
 	if v, err = s.db.Get(lastCommittedBlkkey); v == nil || err != nil {
 		return true, 0, err
 	}
-	return false, decodeBlockNum(v), nil
-}
-
-// TrimPvtWSet returns a `TxPvtReadWriteSet` that retains only list of 'ns/collections' supplied in the filter
-// A nil filter does not filter any results and returns the original `pvtWSet` as is
-func TrimPvtWSet(pvtWSet *rwset.TxPvtReadWriteSet, filter ledger.PvtNsCollFilter) *rwset.TxPvtReadWriteSet {
-	if filter == nil {
-		return pvtWSet
-	}
-
-	var filteredNsRwSet []*rwset.NsPvtReadWriteSet
-	for _, ns := range pvtWSet.NsPvtRwset {
-		var filteredCollRwSet []*rwset.CollectionPvtReadWriteSet
-		for _, coll := range ns.CollectionPvtRwset {
-			if filter.Has(ns.Namespace, coll.CollectionName) {
-				filteredCollRwSet = append(filteredCollRwSet, coll)
-			}
-		}
-		if filteredCollRwSet != nil {
-			filteredNsRwSet = append(filteredNsRwSet,
-				&rwset.NsPvtReadWriteSet{
-					Namespace:          ns.Namespace,
-					CollectionPvtRwset: filteredCollRwSet,
-				},
-			)
-		}
-	}
-	var filteredTxPvtRwSet *rwset.TxPvtReadWriteSet
-	if filteredNsRwSet != nil {
-		filteredTxPvtRwSet = &rwset.TxPvtReadWriteSet{
-			DataModel:  pvtWSet.GetDataModel(),
-			NsPvtRwset: filteredNsRwSet,
-		}
-	}
-	return filteredTxPvtRwSet
+	return false, decodeLastCommittedBlockVal(v), nil
 }
diff --git a/core/ledger/pvtdatastorage/store_impl_test.go b/core/ledger/pvtdatastorage/store_impl_test.go
index 8c9ff5ac..e095ec25 100644
--- a/core/ledger/pvtdatastorage/store_impl_test.go
+++ b/core/ledger/pvtdatastorage/store_impl_test.go
@@ -7,12 +7,16 @@ SPDX-License-Identifier: Apache-2.0
 package pvtdatastorage
 
 import (
+	"fmt"
 	"os"
+	"strings"
 	"testing"
+	"time"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/testutil"
 	"github.com/hyperledger/fabric/core/ledger"
-	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
 	"github.com/spf13/viper"
 	"github.com/stretchr/testify/assert"
 )
@@ -24,7 +28,7 @@ func TestMain(m *testing.M) {
 }
 
 func TestEmptyStore(t *testing.T) {
-	env := NewTestStoreEnv(t)
+	env := NewTestStoreEnv(t, "TestEmptyStore")
 	defer env.Cleanup()
 	assert := assert.New(t)
 	store := env.TestStore
@@ -33,11 +37,14 @@ func TestEmptyStore(t *testing.T) {
 }
 
 func TestStoreBasicCommitAndRetrieval(t *testing.T) {
-	env := NewTestStoreEnv(t)
+	env := NewTestStoreEnv(t, "TestStoreBasicCommitAndRetrieval")
 	defer env.Cleanup()
 	assert := assert.New(t)
 	store := env.TestStore
-	testData := samplePvtData(t, []uint64{2, 4})
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
 
 	// no pvt data with block 0
 	assert.NoError(store.Prepare(0, nil))
@@ -67,10 +74,11 @@ func TestStoreBasicCommitAndRetrieval(t *testing.T) {
 	filter.Add("ns-1", "coll-1")
 	filter.Add("ns-2", "coll-2")
 	retrievedData, err = store.GetPvtDataByBlockNum(1, filter)
-	assert.Equal(1, len(retrievedData[0].WriteSet.NsPvtRwset[0].CollectionPvtRwset))
-	assert.Equal(1, len(retrievedData[0].WriteSet.NsPvtRwset[1].CollectionPvtRwset))
-	assert.True(retrievedData[0].Has("ns-1", "coll-1"))
-	assert.True(retrievedData[0].Has("ns-2", "coll-2"))
+	expectedRetrievedData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+	}
+	testutil.AssertEquals(t, retrievedData, expectedRetrievedData)
 
 	// pvt data retrieval for block 2 should return ErrOutOfRange
 	retrievedData, err = store.GetPvtDataByBlockNum(2, nilFilter)
@@ -79,13 +87,149 @@ func TestStoreBasicCommitAndRetrieval(t *testing.T) {
 	assert.Nil(retrievedData)
 }
 
-func TestStoreState(t *testing.T) {
-	env := NewTestStoreEnv(t)
+func TestExpiryDataNotIncluded(t *testing.T) {
+	ledgerid := "TestExpiryDataNotIncluded"
+	viper.Set(fmt.Sprintf("ledger.pvtdata.btlpolicy.%s.ns-1.coll-1", ledgerid), 1)
+	viper.Set(fmt.Sprintf("ledger.pvtdata.btlpolicy.%s.ns-2.coll-2", ledgerid), 2)
+
+	env := NewTestStoreEnv(t, ledgerid)
 	defer env.Cleanup()
 	assert := assert.New(t)
 	store := env.TestStore
-	testData := samplePvtData(t, []uint64{0})
 
+	// no pvt data with block 0
+	assert.NoError(store.Prepare(0, nil))
+	assert.NoError(store.Commit())
+
+	// write pvt data for block 1
+	testDataForBlk1 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	assert.NoError(store.Prepare(1, testDataForBlk1))
+	assert.NoError(store.Commit())
+
+	// write pvt data for block 2
+	testDataForBlk2 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 3, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 5, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	assert.NoError(store.Prepare(2, testDataForBlk2))
+	assert.NoError(store.Commit())
+
+	retrievedData, _ := store.GetPvtDataByBlockNum(1, nil)
+	// block 1 data should still be not expired
+	testutil.AssertEquals(t, retrievedData, testDataForBlk1)
+
+	// Commit block 3 with no pvtdata
+	assert.NoError(store.Prepare(3, nil))
+	assert.NoError(store.Commit())
+
+	// After committing block 3, the data for "ns-1:coll1" of block 1 should have expired and should not be returned by the store
+	expectedPvtdataFromBlock1 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	retrievedData, _ = store.GetPvtDataByBlockNum(1, nil)
+	testutil.AssertEquals(t, retrievedData, expectedPvtdataFromBlock1)
+
+	// Commit block 4 with no pvtdata
+	assert.NoError(store.Prepare(4, nil))
+	assert.NoError(store.Commit())
+
+	// After committing block 4, the data for "ns-2:coll2" of block 1 should also have expired and should not be returned by the store
+	expectedPvtdataFromBlock1 = []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-2", "ns-2:coll-1"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-2", "ns-2:coll-1"}),
+	}
+	retrievedData, _ = store.GetPvtDataByBlockNum(1, nil)
+	testutil.AssertEquals(t, retrievedData, expectedPvtdataFromBlock1)
+
+	// Now, for block 2, "ns-1:coll1" should also have expired
+	expectedPvtdataFromBlock2 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 3, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 5, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	retrievedData, _ = store.GetPvtDataByBlockNum(2, nil)
+	testutil.AssertEquals(t, retrievedData, expectedPvtdataFromBlock2)
+}
+
+func TestStorePurge(t *testing.T) {
+	ledgerid := "TestStorePurge"
+	viper.Set("ledger.pvtdataStore.purgeInterval", 2)
+	viper.Set(fmt.Sprintf("ledger.pvtdata.btlpolicy.%s.ns-1.coll-1", ledgerid), 1)
+	viper.Set(fmt.Sprintf("ledger.pvtdata.btlpolicy.%s.ns-2.coll-2", ledgerid), 4)
+
+	env := NewTestStoreEnv(t, "TestStorePurge")
+	defer env.Cleanup()
+	assert := assert.New(t)
+	s := env.TestStore
+
+	// no pvt data with block 0
+	assert.NoError(s.Prepare(0, nil))
+	assert.NoError(s.Commit())
+
+	// write pvt data for block 1
+	testDataForBlk1 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	assert.NoError(s.Prepare(1, testDataForBlk1))
+	assert.NoError(s.Commit())
+
+	// write pvt data for block 2
+	assert.NoError(s.Prepare(2, nil))
+	assert.NoError(s.Commit())
+	// data for ns-1:coll-1 and ns-2:coll-2 should exist in store
+	testWaitForPurgerRoutineToFinish(s)
+	assert.True(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-1", coll: "coll-1"}))
+	assert.True(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-2", coll: "coll-2"}))
+
+	// write pvt data for block 3
+	assert.NoError(s.Prepare(3, nil))
+	assert.NoError(s.Commit())
+	// data for ns-1:coll-1 and ns-2:coll-2 should exist in store (because purger should not be launched at block 3)
+	testWaitForPurgerRoutineToFinish(s)
+	assert.True(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-1", coll: "coll-1"}))
+	assert.True(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-2", coll: "coll-2"}))
+
+	// write pvt data for block 4
+	assert.NoError(s.Prepare(4, nil))
+	assert.NoError(s.Commit())
+	// data for ns-1:coll-1 should not exist in store (because purger should be launched at block 4) but ns-2:coll-2 should exist because it
+	// expires at block 5
+	testWaitForPurgerRoutineToFinish(s)
+	assert.False(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-1", coll: "coll-1"}))
+	assert.True(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-2", coll: "coll-2"}))
+
+	// write pvt data for block 5
+	assert.NoError(s.Prepare(5, nil))
+	assert.NoError(s.Commit())
+	// ns-2:coll-2 should exist because though the data expires at block 5 but purger is launched every second block
+	testWaitForPurgerRoutineToFinish(s)
+	assert.False(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-1", coll: "coll-1"}))
+	assert.True(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-2", coll: "coll-2"}))
+
+	// write pvt data for block 6
+	assert.NoError(s.Prepare(6, nil))
+	assert.NoError(s.Commit())
+	// ns-2:coll-2 should not exists now (because purger should be launched at block 6)
+	testWaitForPurgerRoutineToFinish(s)
+	assert.False(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-1", coll: "coll-1"}))
+	assert.False(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-2", coll: "coll-2"}))
+
+	// "ns-2:coll-1" should never have been purged (because, it was no btl was declared for this)
+	assert.True(testDataKeyExists(t, s, &dataKey{blkNum: 1, txNum: 2, ns: "ns-1", coll: "coll-2"}))
+}
+
+func TestStoreState(t *testing.T) {
+	env := NewTestStoreEnv(t, "TestStoreState")
+	defer env.Cleanup()
+	assert := assert.New(t)
+	store := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 0, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+	}
 	_, ok := store.Prepare(1, testData).(*ErrIllegalArgs)
 	assert.True(ok)
 
@@ -98,7 +242,7 @@ func TestStoreState(t *testing.T) {
 }
 
 func TestInitLastCommittedBlock(t *testing.T) {
-	env := NewTestStoreEnv(t)
+	env := NewTestStoreEnv(t, "TestStoreState")
 	defer env.Cleanup()
 	assert := assert.New(t)
 	store := env.TestStore
@@ -139,40 +283,28 @@ func testLastCommittedBlockHeight(expectedBlockHt uint64, assert *assert.Asserti
 	assert.Equal(expectedBlockHt, blkHt)
 }
 
-func samplePvtData(t *testing.T, txNums []uint64) []*ledger.TxPvtData {
-	pvtWriteSet := &rwset.TxPvtReadWriteSet{DataModel: rwset.TxReadWriteSet_KV}
-	pvtWriteSet.NsPvtRwset = []*rwset.NsPvtReadWriteSet{
-		{
-			Namespace: "ns-1",
-			CollectionPvtRwset: []*rwset.CollectionPvtReadWriteSet{
-				{
-					CollectionName: "coll-1",
-					Rwset:          []byte("RandomBytes-PvtRWSet-ns1-coll1"),
-				},
-				{
-					CollectionName: "coll-2",
-					Rwset:          []byte("RandomBytes-PvtRWSet-ns1-coll2"),
-				},
-			},
-		},
-
-		{
-			Namespace: "ns-2",
-			CollectionPvtRwset: []*rwset.CollectionPvtReadWriteSet{
-				{
-					CollectionName: "coll-1",
-					Rwset:          []byte("RandomBytes-PvtRWSet-ns2-coll1"),
-				},
-				{
-					CollectionName: "coll-2",
-					Rwset:          []byte("RandomBytes-PvtRWSet-ns2-coll2"),
-				},
-			},
-		},
-	}
-	var pvtData []*ledger.TxPvtData
-	for _, txNum := range txNums {
-		pvtData = append(pvtData, &ledger.TxPvtData{SeqInBlock: txNum, WriteSet: pvtWriteSet})
+func testDataKeyExists(t *testing.T, s Store, dataKey *dataKey) bool {
+	dataKeyBytes := encodeDataKey(dataKey)
+	val, err := s.(*store).db.Get(dataKeyBytes)
+	assert.NoError(t, err)
+	return len(val) != 0
+}
+
+func testWaitForPurgerRoutineToFinish(s Store) {
+	time.Sleep(1 * time.Second)
+	s.(*store).purgerLock.Lock()
+	s.(*store).purgerLock.Unlock()
+}
+
+func produceSamplePvtdata(t *testing.T, txNum uint64, nsColls []string) *ledger.TxPvtData {
+	builder := rwsetutil.NewRWSetBuilder()
+	for _, nsColl := range nsColls {
+		nsCollSplit := strings.Split(nsColl, ":")
+		ns := nsCollSplit[0]
+		coll := nsCollSplit[1]
+		builder.AddToPvtAndHashedWriteSet(ns, coll, fmt.Sprintf("key-%s-%s", ns, coll), []byte(fmt.Sprintf("value-%s-%s", ns, coll)))
 	}
-	return pvtData
+	simRes, err := builder.GetTxSimulationResults()
+	assert.NoError(t, err)
+	return &ledger.TxPvtData{SeqInBlock: txNum, WriteSet: simRes.PvtSimulationResults}
 }
diff --git a/core/ledger/pvtdatastorage/test_exports.go b/core/ledger/pvtdatastorage/test_exports.go
index 7c63e83c..f25a8956 100644
--- a/core/ledger/pvtdatastorage/test_exports.go
+++ b/core/ledger/pvtdatastorage/test_exports.go
@@ -14,23 +14,22 @@ import (
 	"github.com/stretchr/testify/assert"
 )
 
-const testStoreid = "TestStore"
-
 // StoreEnv provides the  store env for testing
 type StoreEnv struct {
 	t                 testing.TB
 	TestStoreProvider Provider
 	TestStore         Store
+	ledgerid          string
 }
 
 // NewTestStoreEnv construct a StoreEnv for testing
-func NewTestStoreEnv(t *testing.T) *StoreEnv {
+func NewTestStoreEnv(t *testing.T, ledgerid string) *StoreEnv {
 	removeStorePath(t)
 	assert := assert.New(t)
 	testStoreProvider := NewProvider()
-	testStore, err := testStoreProvider.OpenStore(testStoreid)
+	testStore, err := testStoreProvider.OpenStore(ledgerid)
 	assert.NoError(err)
-	return &StoreEnv{t, testStoreProvider, testStore}
+	return &StoreEnv{t, testStoreProvider, testStore, ledgerid}
 }
 
 // CloseAndReopen closes and opens the store provider
@@ -38,13 +37,13 @@ func (env *StoreEnv) CloseAndReopen() {
 	var err error
 	env.TestStoreProvider.Close()
 	env.TestStoreProvider = NewProvider()
-	env.TestStore, err = env.TestStoreProvider.OpenStore(testStoreid)
+	env.TestStore, err = env.TestStoreProvider.OpenStore(env.ledgerid)
 	assert.NoError(env.t, err)
 }
 
 // Cleanup cleansup the  store env after testing
 func (env *StoreEnv) Cleanup() {
-	env.TestStoreProvider.Close()
+	//env.TestStoreProvider.Close()
 	removeStorePath(env.t)
 }
 
diff --git a/core/transientstore/store.go b/core/transientstore/store.go
index 2e1eaaa5..aa7856f5 100644
--- a/core/transientstore/store.go
+++ b/core/transientstore/store.go
@@ -16,7 +16,6 @@ import (
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
 	"github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/ledger"
-	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
 )
 
@@ -300,7 +299,7 @@ func (scanner *RwsetScanner) Next() (*EndorserPvtSimulationResults, error) {
 	if err := proto.Unmarshal(dbVal, txPvtRWSet); err != nil {
 		return nil, err
 	}
-	filteredTxPvtRWSet := pvtdatastorage.TrimPvtWSet(txPvtRWSet, scanner.filter)
+	filteredTxPvtRWSet := trimPvtWSet(txPvtRWSet, scanner.filter)
 
 	return &EndorserPvtSimulationResults{
 		ReceivedAtBlockHeight: blockHeight,
diff --git a/core/transientstore/store_helper.go b/core/transientstore/store_helper.go
index 1819f5af..867577b3 100644
--- a/core/transientstore/store_helper.go
+++ b/core/transientstore/store_helper.go
@@ -12,6 +12,8 @@ import (
 
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/core/config"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
 )
 
 var (
@@ -179,3 +181,37 @@ func GetTransientStorePath() string {
 	sysPath := config.GetPath("peer.fileSystemPath")
 	return filepath.Join(sysPath, "transientStore")
 }
+
+// trimPvtWSet returns a `TxPvtReadWriteSet` that retains only list of 'ns/collections' supplied in the filter
+// A nil filter does not filter any results and returns the original `pvtWSet` as is
+func trimPvtWSet(pvtWSet *rwset.TxPvtReadWriteSet, filter ledger.PvtNsCollFilter) *rwset.TxPvtReadWriteSet {
+	if filter == nil {
+		return pvtWSet
+	}
+
+	var filteredNsRwSet []*rwset.NsPvtReadWriteSet
+	for _, ns := range pvtWSet.NsPvtRwset {
+		var filteredCollRwSet []*rwset.CollectionPvtReadWriteSet
+		for _, coll := range ns.CollectionPvtRwset {
+			if filter.Has(ns.Namespace, coll.CollectionName) {
+				filteredCollRwSet = append(filteredCollRwSet, coll)
+			}
+		}
+		if filteredCollRwSet != nil {
+			filteredNsRwSet = append(filteredNsRwSet,
+				&rwset.NsPvtReadWriteSet{
+					Namespace:          ns.Namespace,
+					CollectionPvtRwset: filteredCollRwSet,
+				},
+			)
+		}
+	}
+	var filteredTxPvtRwSet *rwset.TxPvtReadWriteSet
+	if filteredNsRwSet != nil {
+		filteredTxPvtRwSet = &rwset.TxPvtReadWriteSet{
+			DataModel:  pvtWSet.GetDataModel(),
+			NsPvtRwset: filteredNsRwSet,
+		}
+	}
+	return filteredTxPvtRwSet
+}
-- 
2.14.3 (Apple Git-98)

