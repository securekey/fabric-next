From 5cc7e41cbb8e2fa9e7e36af1803a75a5295130d1 Mon Sep 17 00:00:00 2001
From: Aleksandar Likic <aleksandar.likic@securekey.com>
Date: Thu, 9 Aug 2018 18:21:13 -0400
Subject: [PATCH] Ledger metrics

Change-Id: I994680bb548eb9abd70c4aba33df024080bcdaae
Signed-off-by: Aleksandar Likic <aleksandar.likic@securekey.com>
---
 .../blkstorage/fsblkstorage/blockfile_mgr.go       |  45 ++
 .../ledger/blkstorage/fsblkstorage/blockindex.go   |  12 +
 .../ledger/blkstorage/fsblkstorage/blocks_itr.go   |  23 +
 common/ledger/util/leveldbhelper/leveldb_helper.go |  68 +++
 .../ledger/util/leveldbhelper/leveldb_provider.go  |  38 +-
 common/metrics/server.go                           | 251 +++++------
 common/metrics/server_test.go                      | 242 -----------
 common/metrics/tally_provider.go                   | 374 ++---------------
 common/metrics/tally_provider_test.go              | 462 ---------------------
 common/metrics/types.go                            |  45 --
 .../blocksprovider/blocksprovider.go               |   7 +
 core/endorser/endorser.go                          |  73 +++-
 .../historydb/historyleveldb/historyleveldb.go     |  12 +
 core/ledger/kvledger/kv_ledger.go                  |  50 +++
 .../privacyenabledstate/common_storage_db.go       |  14 +
 .../txmgmt/pvtstatepurgemgmt/expiry_keeper.go      |  10 +
 .../txmgmt/statedb/statecouchdb/batch_util.go      |  17 +-
 .../txmgmt/statedb/statecouchdb/commit_handling.go |  12 +
 .../txmgmt/statedb/statecouchdb/statecouchdb.go    |  12 +
 .../txmgmt/statedb/stateleveldb/stateleveldb.go    |  12 +
 .../kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go |   1 +
 .../txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go |  48 ++-
 .../kvledger/txmgmt/validator/valimpl/helper.go    |   4 +
 core/ledger/ledgerstorage/store.go                 |  50 +++
 core/ledger/util/couchdb/couchdb.go                |  29 ++
 gossip/privdata/coordinator.go                     |  19 +
 gossip/state/state.go                              |  14 +
 peer/main.go                                       |   2 +
 28 files changed, 737 insertions(+), 1209 deletions(-)
 delete mode 100644 common/metrics/server_test.go
 delete mode 100644 common/metrics/tally_provider_test.go
 delete mode 100644 common/metrics/types.go

diff --git a/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go b/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
index ab6caa8c4..ed84004c5 100644
--- a/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
+++ b/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
@@ -29,11 +29,33 @@ import (
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
 	putil "github.com/hyperledger/fabric/protos/utils"
+	"github.com/uber-go/tally"
 )
 
+var addBlockTimer tally.Timer
+var saveCurrentInfoTimer tally.Timer
+var moveToNextFileTimer tally.Timer
+var moveToNextFileCounter tally.Counter
+var appendBlockErrorCounter tally.Counter
+var blockfileSizeGauge tally.Gauge
+var updateCheckpointTimer tally.Timer
+var updateCheckpointLockTimer tally.Timer
+
+func init() {
+	addBlockTimer = metrics.RootScope.Timer("fsblkstorage_addBlock_time_seconds")
+	saveCurrentInfoTimer = metrics.RootScope.Timer("fsblkstorage_saveCurrentInfo_time_seconds")
+	moveToNextFileTimer = metrics.RootScope.Timer("fsblkstorage_moveToNextFile_time_seconds")
+	moveToNextFileCounter = metrics.RootScope.Counter("fsblkstorage_moveToNextFileCount")
+	appendBlockErrorCounter = metrics.RootScope.Counter("fsblkstorage_appendBlockErrorCount")
+	blockfileSizeGauge = metrics.RootScope.Gauge("fsblkstorage_blockfileSize")
+	updateCheckpointTimer = metrics.RootScope.Timer("fsblkstorage_updateCheckpoint_time_seconds")
+	updateCheckpointLockTimer = metrics.RootScope.Timer("fsblkstorage_updateCheckpoint_Lock_time_seconds")
+}
+
 var logger = flogging.MustGetLogger("fsblkstorage")
 
 const (
@@ -226,6 +248,11 @@ func (mgr *blockfileMgr) close() {
 }
 
 func (mgr *blockfileMgr) moveToNextFile() {
+
+	moveToNextFileCounter.Inc(1)
+	stopWatch := moveToNextFileTimer.Start()
+	defer stopWatch.Stop()
+
 	cpInfo := &checkpointInfo{
 		latestFileChunkSuffixNum: mgr.cpInfo.latestFileChunkSuffixNum + 1,
 		latestFileChunksize:      0,
@@ -247,6 +274,10 @@ func (mgr *blockfileMgr) moveToNextFile() {
 }
 
 func (mgr *blockfileMgr) addBlock(block *common.Block) error {
+
+	stopWatch := addBlockTimer.Start()
+	defer stopWatch.Stop()
+
 	if block.Header.Number != mgr.getBlockchainInfo().Height {
 		return fmt.Errorf("Block number should have been %d but was %d", mgr.getBlockchainInfo().Height, block.Header.Number)
 	}
@@ -265,6 +296,8 @@ func (mgr *blockfileMgr) addBlock(block *common.Block) error {
 	blockBytesEncodedLen := proto.EncodeVarint(uint64(blockBytesLen))
 	totalBytesToAppend := blockBytesLen + len(blockBytesEncodedLen)
 
+	blockfileSizeGauge.Update(float64(currentOffset))
+
 	//Determine if we need to start a new file since the size of this block
 	//exceeds the amount of space left in the current file
 	if currentOffset+totalBytesToAppend > mgr.conf.maxBlockfileSize {
@@ -278,6 +311,7 @@ func (mgr *blockfileMgr) addBlock(block *common.Block) error {
 		err = mgr.currentFileWriter.append(blockBytes, true)
 	}
 	if err != nil {
+		appendBlockErrorCounter.Inc(1)
 		truncateErr := mgr.currentFileWriter.truncateFile(mgr.cpInfo.latestFileChunksize)
 		if truncateErr != nil {
 			panic(fmt.Sprintf("Could not truncate current file to known size after an error during block append: %s", err))
@@ -294,6 +328,7 @@ func (mgr *blockfileMgr) addBlock(block *common.Block) error {
 		lastBlockNumber:          block.Header.Number}
 	//save the checkpoint information in the database
 	if err = mgr.saveCurrentInfo(newCPInfo, false); err != nil {
+		appendBlockErrorCounter.Inc(1)
 		truncateErr := mgr.currentFileWriter.truncateFile(currentCPInfo.latestFileChunksize)
 		if truncateErr != nil {
 			panic(fmt.Sprintf("Error in truncating current file to known size after an error in saving checkpoint info: %s", err))
@@ -428,8 +463,16 @@ func (mgr *blockfileMgr) getBlockchainInfo() *common.BlockchainInfo {
 }
 
 func (mgr *blockfileMgr) updateCheckpoint(cpInfo *checkpointInfo) {
+
+	// Measure the whole
+	stopWatch := updateCheckpointTimer.Start()
+	defer stopWatch.Stop()
+
+	// Measure acquiring the lock
+	lockStopWatch := updateCheckpointLockTimer.Start()
 	mgr.cpInfoCond.L.Lock()
 	defer mgr.cpInfoCond.L.Unlock()
+	lockStopWatch.Stop()
 	mgr.cpInfo = cpInfo
 	logger.Debugf("Broadcasting about update checkpointInfo: %s", cpInfo)
 	mgr.cpInfoCond.Broadcast()
@@ -590,6 +633,8 @@ func (mgr *blockfileMgr) loadCurrentInfo() (*checkpointInfo, error) {
 }
 
 func (mgr *blockfileMgr) saveCurrentInfo(i *checkpointInfo, sync bool) error {
+	stopWatch := saveCurrentInfoTimer.Start()
+	defer stopWatch.Stop()
 	b, err := i.marshal()
 	if err != nil {
 		return err
diff --git a/common/ledger/blkstorage/fsblkstorage/blockindex.go b/common/ledger/blkstorage/fsblkstorage/blockindex.go
index 3d21b7e45..c643e6359 100644
--- a/common/ledger/blkstorage/fsblkstorage/blockindex.go
+++ b/common/ledger/blkstorage/fsblkstorage/blockindex.go
@@ -25,11 +25,19 @@ import (
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	ledgerUtil "github.com/hyperledger/fabric/core/ledger/util"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/uber-go/tally"
 )
 
+var indexBlockTimer tally.Timer
+
+func init() {
+	indexBlockTimer = metrics.RootScope.Timer("fsblkstorage_indexBlock_time_seconds")
+}
+
 const (
 	blockNumIdxKeyPrefix           = 'n'
 	blockHashIdxKeyPrefix          = 'h'
@@ -98,6 +106,10 @@ func (index *blockIndex) getLastBlockIndexed() (uint64, error) {
 }
 
 func (index *blockIndex) indexBlock(blockIdxInfo *blockIdxInfo) error {
+
+	stopWatch := indexBlockTimer.Start()
+	defer stopWatch.Stop()
+
 	// do not index anything
 	if len(index.indexItemsMap) == 0 {
 		logger.Debug("Not indexing block... as nothing to index")
diff --git a/common/ledger/blkstorage/fsblkstorage/blocks_itr.go b/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
index 7ebe52af9..88af3b0cc 100644
--- a/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
+++ b/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
@@ -20,8 +20,20 @@ import (
 	"sync"
 
 	"github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/metrics"
+	"github.com/uber-go/tally"
 )
 
+var blockDiffGauge tally.Gauge
+var waitForBlockTimer tally.Timer
+var waitForBlockLockTimer tally.Timer
+
+func init() {
+	blockDiffGauge = metrics.RootScope.Gauge("fsblkstorage_waitForBlock_blockDiff")
+	waitForBlockTimer = metrics.RootScope.Timer("fsblkstorage_waitForBlock_time_seconds")
+	waitForBlockLockTimer = metrics.RootScope.Timer("fsblkstorage_waitForBlock_Lock_time_seconds")
+}
+
 // blocksItr - an iterator for iterating over a sequence of blocks
 type blocksItr struct {
 	mgr                  *blockfileMgr
@@ -37,8 +49,19 @@ func newBlockItr(mgr *blockfileMgr, startBlockNum uint64) *blocksItr {
 }
 
 func (itr *blocksItr) waitForBlock(blockNum uint64) uint64 {
+
+	// Measure the whole
+	stopWatch := waitForBlockTimer.Start()
+	defer stopWatch.Stop()
+
+	// Measure acquiring the lock
+	lockStopWatch := waitForBlockLockTimer.Start()
 	itr.mgr.cpInfoCond.L.Lock()
 	defer itr.mgr.cpInfoCond.L.Unlock()
+	lockStopWatch.Stop()
+
+	blockDiffGauge.Update(float64(blockNum - itr.mgr.cpInfo.lastBlockNumber))
+
 	for itr.mgr.cpInfo.lastBlockNumber < blockNum && !itr.shouldClose() {
 		logger.Debugf("Going to wait for newer blocks. maxAvailaBlockNumber=[%d], waitForBlockNum=[%d]",
 			itr.mgr.cpInfo.lastBlockNumber, blockNum)
diff --git a/common/ledger/util/leveldbhelper/leveldb_helper.go b/common/ledger/util/leveldbhelper/leveldb_helper.go
index 9e2ab17df..0c24b5b5a 100644
--- a/common/ledger/util/leveldbhelper/leveldb_helper.go
+++ b/common/ledger/util/leveldbhelper/leveldb_helper.go
@@ -20,14 +20,28 @@ import (
 	"fmt"
 	"sync"
 
+	"strings"
+
+	"bytes"
+	"time"
+
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util"
+	"github.com/hyperledger/fabric/common/metrics"
+	"github.com/spf13/viper"
 	"github.com/syndtr/goleveldb/leveldb"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
 	"github.com/syndtr/goleveldb/leveldb/opt"
 	goleveldbutil "github.com/syndtr/goleveldb/leveldb/util"
+	"github.com/uber-go/tally"
 )
 
+var writeBatch1Timer tally.Timer
+
+func init() {
+	writeBatch1Timer = metrics.RootScope.Timer("leveldbhelper_WriteBatch1_time_seconds")
+}
+
 var logger = flogging.MustGetLogger("leveldbhelper")
 
 type dbState int32
@@ -88,6 +102,7 @@ func (dbInst *DB) Open() {
 		panic(fmt.Sprintf("Error while trying to open DB: %s", err))
 	}
 	dbInst.dbState = opened
+	go dbInst.getState()
 }
 
 // Close closes the underlying db
@@ -105,6 +120,16 @@ func (dbInst *DB) Close() {
 
 // Get returns the value for the given key
 func (dbInst *DB) Get(key []byte) ([]byte, error) {
+	dbName := dbInst.conf.DBPath
+	if strings.Contains(dbName, "ledgersData/") {
+		dbName = strings.Replace(strings.Split(dbName, "ledgersData/")[1], "/", "_", -1)
+	} else {
+		dbName = strings.Replace(dbName, "/", "_", -1)
+	}
+	ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_get_%s_processing_time_seconds", dbName))
+	ccStopWatch := ccTimer.Start()
+	defer ccStopWatch.Stop()
+
 	value, err := dbInst.db.Get(key, dbInst.readOpts)
 	if err == leveldb.ErrNotFound {
 		value = nil
@@ -119,6 +144,15 @@ func (dbInst *DB) Get(key []byte) ([]byte, error) {
 
 // Put saves the key/value
 func (dbInst *DB) Put(key []byte, value []byte, sync bool) error {
+	dbName := dbInst.conf.DBPath
+	if strings.Contains(dbName, "ledgersData/") {
+		dbName = strings.Replace(strings.Split(dbName, "ledgersData/")[1], "/", "_", -1)
+	} else {
+		dbName = strings.Replace(dbName, "/", "_", -1)
+	}
+	ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_put_%s_processing_time_seconds", dbName))
+	ccStopWatch := ccTimer.Start()
+	defer ccStopWatch.Stop()
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -133,6 +167,15 @@ func (dbInst *DB) Put(key []byte, value []byte, sync bool) error {
 
 // Delete deletes the given key
 func (dbInst *DB) Delete(key []byte, sync bool) error {
+	dbName := dbInst.conf.DBPath
+	if strings.Contains(dbName, "ledgersData/") {
+		dbName = strings.Replace(strings.Split(dbName, "ledgersData/")[1], "/", "_", -1)
+	} else {
+		dbName = strings.Replace(dbName, "/", "_", -1)
+	}
+	ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_delete_%s_processing_time_seconds", dbName))
+	ccStopWatch := ccTimer.Start()
+	defer ccStopWatch.Stop()
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -154,6 +197,8 @@ func (dbInst *DB) GetIterator(startKey []byte, endKey []byte) iterator.Iterator
 
 // WriteBatch writes a batch
 func (dbInst *DB) WriteBatch(batch *leveldb.Batch, sync bool) error {
+	stopWatch := writeBatch1Timer.Start()
+	defer stopWatch.Stop()
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -163,3 +208,26 @@ func (dbInst *DB) WriteBatch(batch *leveldb.Batch, sync bool) error {
 	}
 	return nil
 }
+
+func (dbInst *DB) getState() {
+	for {
+		time.Sleep(5 * time.Second)
+		if dbInst.dbState == closed {
+			logger.Info("leveldb is closed exit the getState")
+			break
+		}
+		levelDBStats := []string{"stats", "iostats", "writedelay", "sstables", "blockpool", "cachedblock", "openedtables", "alivesnaps", "aliveiters"}
+		var b bytes.Buffer
+		for _, stats := range levelDBStats {
+			res, err := dbInst.db.GetProperty(fmt.Sprintf("leveldb.%s", stats))
+			if err != nil {
+				logger.Errorf("leveldb getState %s return error %s", stats, err)
+				continue
+			}
+			b.WriteString(res)
+		}
+		if viper.GetBool("logging.leveldbState") {
+			logger.Infof("******* leveldb getState %s", strings.Replace(b.String(), "\n", " ", -1))
+		}
+	}
+}
diff --git a/common/ledger/util/leveldbhelper/leveldb_provider.go b/common/ledger/util/leveldbhelper/leveldb_provider.go
index db41651d7..4511c3b3b 100644
--- a/common/ledger/util/leveldbhelper/leveldb_provider.go
+++ b/common/ledger/util/leveldbhelper/leveldb_provider.go
@@ -20,10 +20,21 @@ import (
 	"bytes"
 	"sync"
 
+	"fmt"
+	"strings"
+
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/syndtr/goleveldb/leveldb"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
+	"github.com/uber-go/tally"
 )
 
+var writeBatchTimer tally.Timer
+
+func init() {
+	writeBatchTimer = metrics.RootScope.Timer("leveldbhelper_WriteBatch_time_seconds")
+}
+
 var dbNameKeySep = []byte{0x00}
 var lastKeyIndicator = byte(0x01)
 
@@ -32,13 +43,14 @@ type Provider struct {
 	db        *DB
 	dbHandles map[string]*DBHandle
 	mux       sync.Mutex
+	dbPath    string
 }
 
 // NewProvider constructs a Provider
 func NewProvider(conf *Conf) *Provider {
 	db := CreateDB(conf)
 	db.Open()
-	return &Provider{db, make(map[string]*DBHandle), sync.Mutex{}}
+	return &Provider{db, make(map[string]*DBHandle), sync.Mutex{}, conf.DBPath}
 }
 
 // GetDBHandle returns a handle to a named db
@@ -47,7 +59,7 @@ func (p *Provider) GetDBHandle(dbName string) *DBHandle {
 	defer p.mux.Unlock()
 	dbHandle := p.dbHandles[dbName]
 	if dbHandle == nil {
-		dbHandle = &DBHandle{dbName, p.db}
+		dbHandle = &DBHandle{dbName, p.db, p.dbPath}
 		p.dbHandles[dbName] = dbHandle
 	}
 	return dbHandle
@@ -62,6 +74,7 @@ func (p *Provider) Close() {
 type DBHandle struct {
 	dbName string
 	db     *DB
+	dbPath string
 }
 
 // Get returns the value for the given key
@@ -81,6 +94,8 @@ func (h *DBHandle) Delete(key []byte, sync bool) error {
 
 // WriteBatch writes a batch in an atomic way
 func (h *DBHandle) WriteBatch(batch *UpdateBatch, sync bool) error {
+	stopWatch := writeBatchTimer.Start()
+	defer stopWatch.Stop()
 	levelBatch := &leveldb.Batch{}
 	for k, v := range batch.KVs {
 		key := constructLevelKey(h.dbName, []byte(k))
@@ -100,14 +115,22 @@ func (h *DBHandle) WriteBatch(batch *UpdateBatch, sync bool) error {
 // The resultset contains all the keys that are present in the db between the startKey (inclusive) and the endKey (exclusive).
 // A nil startKey represents the first available key and a nil endKey represent a logical key after the last available key
 func (h *DBHandle) GetIterator(startKey []byte, endKey []byte) *Iterator {
+	dbPath := h.dbPath
+	if strings.Contains(dbPath, "ledgersData/") {
+		dbPath = strings.Replace(strings.Split(dbPath, "ledgersData/")[1], "/", "_", -1)
+	} else {
+		dbPath = strings.Replace(dbPath, "/", "_", -1)
+	}
+	ccTimer := metrics.RootScope.Timer(strings.Replace(fmt.Sprintf("leveldb_GetIterator_%s_%s_processing_time_seconds", h.dbName, dbPath), "/", "_", -1))
+	stopwatch := ccTimer.Start()
 	sKey := constructLevelKey(h.dbName, startKey)
 	eKey := constructLevelKey(h.dbName, endKey)
 	if endKey == nil {
 		// replace the last byte 'dbNameKeySep' by 'lastKeyIndicator'
 		eKey[len(eKey)-1] = lastKeyIndicator
 	}
-	logger.Debugf("Getting iterator for range [%#v] - [%#v]", sKey, eKey)
-	return &Iterator{h.db.GetIterator(sKey, eKey)}
+	logger.Debugf("Getting iterator %s for range [%#v] - [%#v]\n", dbPath, sKey, eKey)
+	return &Iterator{h.db.GetIterator(sKey, eKey), stopwatch}
 }
 
 // UpdateBatch encloses the details of multiple `updates`
@@ -136,6 +159,7 @@ func (batch *UpdateBatch) Delete(key []byte) {
 // Iterator extends actual leveldb iterator
 type Iterator struct {
 	iterator.Iterator
+	stopwatch tally.Stopwatch
 }
 
 // Key wraps actual leveldb iterator method
@@ -143,6 +167,12 @@ func (itr *Iterator) Key() []byte {
 	return retrieveAppKey(itr.Iterator.Key())
 }
 
+// Key wraps actual leveldb iterator method
+func (itr *Iterator) Release() {
+	defer itr.stopwatch.Stop()
+	itr.Iterator.Release()
+}
+
 func constructLevelKey(dbName string, key []byte) []byte {
 	return append(append([]byte(dbName), dbNameKeySep...), key...)
 }
diff --git a/common/metrics/server.go b/common/metrics/server.go
index 677151e78..efcefa278 100644
--- a/common/metrics/server.go
+++ b/common/metrics/server.go
@@ -8,11 +8,19 @@ package metrics
 
 import (
 	"fmt"
-	"sync"
+	"io"
 	"time"
 
+	"sync"
+
+	"strings"
+
+	"runtime"
+
+	"github.com/pkg/errors"
 	"github.com/spf13/viper"
 	"github.com/uber-go/tally"
+	promreporter "github.com/uber-go/tally/prometheus"
 )
 
 const (
@@ -28,21 +36,94 @@ const (
 	defaultStatsdReporterFlushBytes    = 1432
 )
 
-var RootScope Scope
-var once sync.Once
+const (
+	peerConfigFileName = "core"
+	peerConfigPath     = "/etc/hyperledger/fabric"
+	cmdRootPrefix      = "core"
+)
+
+var peerConfig *viper.Viper
+var peerConfigPathOverride string
+
+// RootScope tally.NoopScope is a scope that does nothing
+var RootScope = tally.NoopScope
 var rootScopeMutex = &sync.Mutex{}
 var running bool
 
-// NewOpts create metrics options based config file
-func NewOpts() Opts {
+// StatsdReporterOpts ...
+type StatsdReporterOpts struct {
+	Address       string
+	Prefix        string
+	FlushInterval time.Duration
+	FlushBytes    int
+}
+
+// PromReporterOpts ...
+type PromReporterOpts struct {
+	ListenAddress string
+}
+
+// Opts ...
+type Opts struct {
+	Reporter           string
+	Interval           time.Duration
+	Enabled            bool
+	StatsdReporterOpts StatsdReporterOpts
+	PromReporterOpts   PromReporterOpts
+}
+
+func init() {
+
+	// load peer config
+	if err := initPeerConfig(); err != nil {
+		panic(fmt.Sprintf("error initPeerConfig %v", err))
+	}
+
+	if peerConfig.GetBool("peer.profile.enabled") {
+		runtime.SetMutexProfileFraction(5)
+	}
+
+	// start metric server
+	opts := NewOpts(peerConfig)
+	err := Start(opts)
+	if err != nil {
+		logger.Errorf("Failed to start metrics collection: %s", err)
+	}
+
+	logger.Info("Fabric Bootstrap filter initialized")
+}
+
+func initPeerConfig() error {
+	peerConfig = viper.New()
+	peerConfig.AddConfigPath(peerConfigPath)
+	if peerConfigPathOverride != "" {
+		peerConfig.AddConfigPath(peerConfigPathOverride)
+	}
+	peerConfig.SetConfigName(peerConfigFileName)
+	peerConfig.SetEnvPrefix(cmdRootPrefix)
+	peerConfig.AutomaticEnv()
+	peerConfig.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
+	err := peerConfig.ReadInConfig()
+	if err != nil {
+		return err
+	}
+
+	return nil
+}
+
+// NewOpts create metrics options based config file.
+// TODO: Currently this is only for peer node which uses global viper.
+// As for orderer, which uses its local viper, we are unable to get
+// metrics options with the function NewOpts()
+func NewOpts(peerConfig *viper.Viper) Opts {
 	opts := Opts{}
-	opts.Enabled = viper.GetBool("metrics.enabled")
-	if report := viper.GetString("metrics.reporter"); report != "" {
+	opts.Enabled = peerConfig.GetBool("metrics.enabled")
+	if report := peerConfig.GetString("metrics.reporter"); report != "" {
 		opts.Reporter = report
 	} else {
 		opts.Reporter = defaultReporterType
 	}
-	if interval := viper.GetDuration("metrics.interval"); interval > 0 {
+	if interval := peerConfig.GetDuration("metrics.interval"); interval > 0 {
 		opts.Interval = interval
 	} else {
 		opts.Interval = defaultInterval
@@ -50,13 +131,17 @@ func NewOpts() Opts {
 
 	if opts.Reporter == statsdReporterType {
 		statsdOpts := StatsdReporterOpts{}
-		statsdOpts.Address = viper.GetString("metrics.statsdReporter.address")
-		if flushInterval := viper.GetDuration("metrics.statsdReporter.flushInterval"); flushInterval > 0 {
+		statsdOpts.Address = peerConfig.GetString("metrics.statsdReporter.address")
+		statsdOpts.Prefix = peerConfig.GetString("metrics.statsdReporter.prefix")
+		if statsdOpts.Prefix == "" && !peerConfig.IsSet("peer.id") {
+			statsdOpts.Prefix = peerConfig.GetString("peer.id")
+		}
+		if flushInterval := peerConfig.GetDuration("metrics.statsdReporter.flushInterval"); flushInterval > 0 {
 			statsdOpts.FlushInterval = flushInterval
 		} else {
 			statsdOpts.FlushInterval = defaultStatsdReporterFlushInterval
 		}
-		if flushBytes := viper.GetInt("metrics.statsdReporter.flushBytes"); flushBytes > 0 {
+		if flushBytes := peerConfig.GetInt("metrics.statsdReporter.flushBytes"); flushBytes > 0 {
 			statsdOpts.FlushBytes = flushBytes
 		} else {
 			statsdOpts.FlushBytes = defaultStatsdReporterFlushBytes
@@ -66,45 +151,47 @@ func NewOpts() Opts {
 
 	if opts.Reporter == promReporterType {
 		promOpts := PromReporterOpts{}
-		promOpts.ListenAddress = viper.GetString("metrics.promReporter.listenAddress")
+		promOpts.ListenAddress = peerConfig.GetString("metrics.fabric.PromReporter.listenAddress")
 		opts.PromReporterOpts = promOpts
 	}
 
 	return opts
 }
 
-//Init initializes global root metrics scope instance, all callers can only use it to extend sub scope
-func Init(opts Opts) (err error) {
-	once.Do(func() {
-		RootScope, err = create(opts)
-	})
-
-	return
-}
-
-//Start starts metrics server
-func Start() error {
+// Start starts metrics server
+func Start(opts Opts) error {
+	if !opts.Enabled {
+		return errors.New("Unable to start metrics server because is disbled")
+	}
 	rootScopeMutex.Lock()
 	defer rootScopeMutex.Unlock()
-	if running {
-		return nil
+	if !running {
+		rootScope, err := create(opts)
+		if err == nil {
+			running = true
+			RootScope = rootScope
+		}
+		return err
 	}
-	running = true
-	return RootScope.Start()
+	return errors.New("metrics server was already started")
 }
 
-//Shutdown closes underlying resources used by metrics server
+// Shutdown closes underlying resources used by metrics server
 func Shutdown() error {
 	rootScopeMutex.Lock()
 	defer rootScopeMutex.Unlock()
-	if !running {
-		return nil
+	if running {
+		var err error
+		if closer, ok := RootScope.(io.Closer); ok {
+			if err = closer.Close(); err != nil {
+				return err
+			}
+		}
+		running = false
+		RootScope = tally.NoopScope
+		return err
 	}
-
-	err := RootScope.Close()
-	RootScope = nil
-	running = false
-	return err
+	return nil
 }
 
 func isRunning() bool {
@@ -113,109 +200,35 @@ func isRunning() bool {
 	return running
 }
 
-type StatsdReporterOpts struct {
-	Address       string
-	FlushInterval time.Duration
-	FlushBytes    int
-}
-
-type PromReporterOpts struct {
-	ListenAddress string
-}
-
-type Opts struct {
-	Reporter           string
-	Interval           time.Duration
-	Enabled            bool
-	StatsdReporterOpts StatsdReporterOpts
-	PromReporterOpts   PromReporterOpts
-}
-
-type noOpCounter struct {
-}
-
-func (c *noOpCounter) Inc(v int64) {
-
-}
-
-type noOpGauge struct {
-}
-
-func (g *noOpGauge) Update(v float64) {
-
-}
-
-type noOpScope struct {
-	counter *noOpCounter
-	gauge   *noOpGauge
-}
-
-func (s *noOpScope) Counter(name string) Counter {
-	return s.counter
-}
-
-func (s *noOpScope) Gauge(name string) Gauge {
-	return s.gauge
-}
-
-func (s *noOpScope) Tagged(tags map[string]string) Scope {
-	return s
-}
-
-func (s *noOpScope) SubScope(prefix string) Scope {
-	return s
-}
-
-func (s *noOpScope) Close() error {
-	return nil
-}
-
-func (s *noOpScope) Start() error {
-	return nil
-}
-
-func newNoOpScope() Scope {
-	return &noOpScope{
-		counter: &noOpCounter{},
-		gauge:   &noOpGauge{},
-	}
-}
-
-func create(opts Opts) (rootScope Scope, e error) {
+func create(opts Opts) (rootScope tally.Scope, e error) {
 	if !opts.Enabled {
-		rootScope = newNoOpScope()
-		return
+		rootScope = tally.NoopScope
 	} else {
 		if opts.Interval <= 0 {
 			e = fmt.Errorf("invalid Interval option %d", opts.Interval)
 			return
 		}
-
-		if opts.Reporter != statsdReporterType && opts.Reporter != promReporterType {
-			e = fmt.Errorf("not supported Reporter type %s", opts.Reporter)
-			return
-		}
-
 		var reporter tally.StatsReporter
 		var cachedReporter tally.CachedStatsReporter
-		if opts.Reporter == statsdReporterType {
+		switch opts.Reporter {
+		case statsdReporterType:
 			reporter, e = newStatsdReporter(opts.StatsdReporterOpts)
-		}
-
-		if opts.Reporter == promReporterType {
+		case promReporterType:
 			cachedReporter, e = newPromReporter(opts.PromReporterOpts)
+		default:
+			e = fmt.Errorf("not supported Reporter type %s", opts.Reporter)
+			return
 		}
-
 		if e != nil {
 			return
 		}
-
 		rootScope = newRootScope(
 			tally.ScopeOptions{
 				Prefix:         namespace,
 				Reporter:       reporter,
 				CachedReporter: cachedReporter,
+				Separator:      promreporter.DefaultSeparator,
 			}, opts.Interval)
-		return
 	}
+	return
 }
diff --git a/common/metrics/server_test.go b/common/metrics/server_test.go
deleted file mode 100644
index 07d010ca8..000000000
--- a/common/metrics/server_test.go
+++ /dev/null
@@ -1,242 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import (
-	"fmt"
-	"strings"
-	"testing"
-	"time"
-
-	"github.com/hyperledger/fabric/core/config/configtest"
-	. "github.com/onsi/gomega"
-	"github.com/spf13/viper"
-	"github.com/stretchr/testify/assert"
-)
-
-func TestStartSuccessStatsd(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: statsdReporterType,
-		Interval: 1 * time.Second,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		}}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartSuccessProm(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: promReporterType,
-		Interval: 1 * time.Second,
-		PromReporterOpts: PromReporterOpts{
-			ListenAddress: "127.0.0.1:0",
-		}}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartDisabled(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled: false,
-	}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartInvalidInterval(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 0,
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidAddress(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidFlushInterval(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 0,
-			FlushBytes:    512,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartPromInvalidListernAddress(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		PromReporterOpts: PromReporterOpts{
-			ListenAddress: "",
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidFlushBytes(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    0,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartInvalidReporter(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: "test",
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartAndClose(t *testing.T) {
-	t.Parallel()
-	gt := NewGomegaWithT(t)
-	defer Shutdown()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: statsdReporterType,
-		Interval: 1 * time.Second,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		}}
-	Init(opts)
-	assert.NotNil(t, RootScope)
-	go Start()
-	gt.Eventually(isRunning).Should(BeTrue())
-}
-
-func TestNoOpScopeMetrics(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled: false,
-	}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-
-	// make sure no error throws when invoke noOpScope
-	subScope := s.SubScope("test")
-	subScope.Counter("foo").Inc(2)
-	subScope.Gauge("bar").Update(1.33)
-	tagSubScope := subScope.Tagged(map[string]string{"env": "test"})
-	tagSubScope.Counter("foo").Inc(2)
-	tagSubScope.Gauge("bar").Update(1.33)
-}
-
-func TestNewOpts(t *testing.T) {
-	t.Parallel()
-	defer viper.Reset()
-	setupTestConfig()
-	opts := NewOpts()
-	assert.False(t, opts.Enabled)
-	assert.Equal(t, 1*time.Second, opts.Interval)
-	assert.Equal(t, statsdReporterType, opts.Reporter)
-	assert.Equal(t, 1432, opts.StatsdReporterOpts.FlushBytes)
-	assert.Equal(t, 2*time.Second, opts.StatsdReporterOpts.FlushInterval)
-	assert.Equal(t, "0.0.0.0:8125", opts.StatsdReporterOpts.Address)
-	viper.Reset()
-
-	setupTestConfig()
-	viper.Set("metrics.Reporter", promReporterType)
-	opts1 := NewOpts()
-	assert.False(t, opts1.Enabled)
-	assert.Equal(t, 1*time.Second, opts1.Interval)
-	assert.Equal(t, promReporterType, opts1.Reporter)
-	assert.Equal(t, "0.0.0.0:8080", opts1.PromReporterOpts.ListenAddress)
-}
-
-func TestNewOptsDefaultVar(t *testing.T) {
-	t.Parallel()
-	opts := NewOpts()
-	assert.False(t, opts.Enabled)
-	assert.Equal(t, 1*time.Second, opts.Interval)
-	assert.Equal(t, statsdReporterType, opts.Reporter)
-	assert.Equal(t, 1432, opts.StatsdReporterOpts.FlushBytes)
-	assert.Equal(t, 2*time.Second, opts.StatsdReporterOpts.FlushInterval)
-}
-
-func setupTestConfig() {
-	viper.SetConfigName("core")
-	viper.SetEnvPrefix("CORE")
-	viper.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
-	viper.AutomaticEnv()
-
-	err := configtest.AddDevConfigPath(nil)
-	if err != nil {
-		panic(fmt.Errorf("Fatal error adding dev dir: %s \n", err))
-	}
-
-	err = viper.ReadInConfig()
-	if err != nil { // Handle errors reading the config file
-		panic(fmt.Errorf("Fatal error config file: %s \n", err))
-	}
-}
diff --git a/common/metrics/tally_provider.go b/common/metrics/tally_provider.go
index cf1e3deff..aca8d54ab 100644
--- a/common/metrics/tally_provider.go
+++ b/common/metrics/tally_provider.go
@@ -7,17 +7,16 @@ SPDX-License-Identifier: Apache-2.0
 package metrics
 
 import (
-	"context"
 	"errors"
-	"fmt"
-	"io"
 	"net/http"
-	"sort"
-	"sync"
 	"time"
 
+	"net"
+
+	"sort"
+
 	"github.com/cactus/go-statsd-client/statsd"
-	"github.com/op/go-logging"
+	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/prometheus/client_golang/prometheus"
 	"github.com/prometheus/client_golang/prometheus/promhttp"
 	"github.com/uber-go/tally"
@@ -25,74 +24,11 @@ import (
 	statsdreporter "github.com/uber-go/tally/statsd"
 )
 
-var logger = logging.MustGetLogger("common/metrics/tally")
-
-var scopeRegistryKey = tally.KeyForPrefixedStringMap
-
-type counter struct {
-	tallyCounter tally.Counter
-}
-
-func newCounter(tallyCounter tally.Counter) *counter {
-	return &counter{tallyCounter: tallyCounter}
-}
-
-func (c *counter) Inc(v int64) {
-	c.tallyCounter.Inc(v)
-}
-
-type gauge struct {
-	tallyGauge tally.Gauge
-}
-
-func newGauge(tallyGauge tally.Gauge) *gauge {
-	return &gauge{tallyGauge: tallyGauge}
-}
+var logger = flogging.MustGetLogger("common/metrics/tally")
 
-func (g *gauge) Update(v float64) {
-	g.tallyGauge.Update(v)
-}
-
-type scopeRegistry struct {
-	sync.RWMutex
-	subScopes map[string]*scope
-}
-
-type scope struct {
-	separator    string
-	prefix       string
-	tags         map[string]string
-	tallyScope   tally.Scope
-	registry     *scopeRegistry
-	baseReporter tally.BaseStatsReporter
-
-	cm sync.RWMutex
-	gm sync.RWMutex
-
-	counters map[string]*counter
-	gauges   map[string]*gauge
-}
-
-func newRootScope(opts tally.ScopeOptions, interval time.Duration) Scope {
+func newRootScope(opts tally.ScopeOptions, interval time.Duration) tally.Scope {
 	s, _ := tally.NewRootScope(opts, interval)
-
-	var baseReporter tally.BaseStatsReporter
-	if opts.Reporter != nil {
-		baseReporter = opts.Reporter
-	} else if opts.CachedReporter != nil {
-		baseReporter = opts.CachedReporter
-	}
-
-	return &scope{
-		prefix:     opts.Prefix,
-		separator:  opts.Separator,
-		tallyScope: s,
-		registry: &scopeRegistry{
-			subScopes: make(map[string]*scope),
-		},
-		baseReporter: baseReporter,
-		counters:     make(map[string]*counter),
-		gauges:       make(map[string]*gauge)}
+	return s
 }
 
 func newStatsdReporter(statsdReporterOpts StatsdReporterOpts) (tally.StatsReporter, error) {
@@ -109,13 +45,13 @@ func newStatsdReporter(statsdReporterOpts StatsdReporterOpts) (tally.StatsReport
 	}
 
 	statter, err := statsd.NewBufferedClient(statsdReporterOpts.Address,
-		"", statsdReporterOpts.FlushInterval, statsdReporterOpts.FlushBytes)
+		statsdReporterOpts.Prefix, statsdReporterOpts.FlushInterval, statsdReporterOpts.FlushBytes)
 	if err != nil {
 		return nil, err
 	}
 	opts := statsdreporter.Options{}
 	reporter := statsdreporter.NewReporter(statter, opts)
-	statsdReporter := &statsdReporter{reporter: reporter, statter: statter}
+	statsdReporter := &statsdReporter{StatsReporter: reporter, statter: statter}
 	return statsdReporter, nil
 }
 
@@ -127,159 +63,52 @@ func newPromReporter(promReporterOpts PromReporterOpts) (promreporter.Reporter,
 	opts := promreporter.Options{Registerer: prometheus.NewRegistry()}
 	reporter := promreporter.NewReporter(opts)
 	mux := http.NewServeMux()
-	handler := promReporterHttpHandler(opts.Registerer.(*prometheus.Registry))
+	handler := promReporterHTTPHandler(opts.Registerer.(*prometheus.Registry))
 	mux.Handle("/metrics", handler)
-	server := &http.Server{Addr: promReporterOpts.ListenAddress, Handler: mux}
+	server := &http.Server{Handler: mux}
+	addr := promReporterOpts.ListenAddress
+	if addr == "" {
+		addr = ":http"
+	}
+	listener, err := net.Listen("tcp", addr)
+	if err != nil {
+		return nil, err
+	}
 	promReporter := &promReporter{
-		reporter: reporter,
+		Reporter: reporter,
 		server:   server,
-		registry: opts.Registerer.(*prometheus.Registry)}
+		registry: opts.Registerer.(*prometheus.Registry),
+		listener: listener}
+	go server.Serve(listener)
 	return promReporter, nil
 }
 
-func (s *scope) Counter(name string) Counter {
-	s.cm.RLock()
-	val, ok := s.counters[name]
-	s.cm.RUnlock()
-	if !ok {
-		s.cm.Lock()
-		val, ok = s.counters[name]
-		if !ok {
-			counter := s.tallyScope.Counter(name)
-			val = newCounter(counter)
-			s.counters[name] = val
-		}
-		s.cm.Unlock()
-	}
-	return val
-}
-
-func (s *scope) Gauge(name string) Gauge {
-	s.gm.RLock()
-	val, ok := s.gauges[name]
-	s.gm.RUnlock()
-	if !ok {
-		s.gm.Lock()
-		val, ok = s.gauges[name]
-		if !ok {
-			gauge := s.tallyScope.Gauge(name)
-			val = newGauge(gauge)
-			s.gauges[name] = val
-		}
-		s.gm.Unlock()
-	}
-	return val
-}
-
-func (s *scope) Tagged(tags map[string]string) Scope {
-	originTags := tags
-	tags = mergeRightTags(s.tags, tags)
-	key := scopeRegistryKey(s.prefix, tags)
-
-	s.registry.RLock()
-	existing, ok := s.registry.subScopes[key]
-	if ok {
-		s.registry.RUnlock()
-		return existing
-	}
-	s.registry.RUnlock()
-
-	s.registry.Lock()
-	defer s.registry.Unlock()
-
-	existing, ok = s.registry.subScopes[key]
-	if ok {
-		return existing
-	}
-
-	subScope := &scope{
-		separator: s.separator,
-		prefix:    s.prefix,
-		// NB(r): Take a copy of the tags on creation
-		// so that it cannot be modified after set.
-		tags:       copyStringMap(tags),
-		tallyScope: s.tallyScope.Tagged(originTags),
-		registry:   s.registry,
-
-		counters: make(map[string]*counter),
-		gauges:   make(map[string]*gauge),
-	}
-
-	s.registry.subScopes[key] = subScope
-	return subScope
-}
-
-func (s *scope) SubScope(prefix string) Scope {
-	key := scopeRegistryKey(s.fullyQualifiedName(prefix), s.tags)
-
-	s.registry.RLock()
-	existing, ok := s.registry.subScopes[key]
-	if ok {
-		s.registry.RUnlock()
-		return existing
-	}
-	s.registry.RUnlock()
-
-	s.registry.Lock()
-	defer s.registry.Unlock()
-
-	existing, ok = s.registry.subScopes[key]
-	if ok {
-		return existing
-	}
-
-	subScope := &scope{
-		separator: s.separator,
-		prefix:    s.prefix,
-		// NB(r): Take a copy of the tags on creation
-		// so that it cannot be modified after set.
-		tags:       copyStringMap(s.tags),
-		tallyScope: s.tallyScope.SubScope(prefix),
-		registry:   s.registry,
-
-		counters: make(map[string]*counter),
-		gauges:   make(map[string]*gauge),
-	}
-
-	s.registry.subScopes[key] = subScope
-	return subScope
-}
-
-func (s *scope) Close() error {
-	if closer, ok := s.tallyScope.(io.Closer); ok {
-		return closer.Close()
-	}
-	return nil
-}
-
-func (s *scope) Start() error {
-	if server, ok := s.baseReporter.(serve); ok {
-		return server.Start()
-	}
-	return nil
-}
-
 type statsdReporter struct {
-	reporter tally.StatsReporter
-	statter  statsd.Statter
+	tally.StatsReporter
+	statter statsd.Statter
 }
 
 type promReporter struct {
-	reporter promreporter.Reporter
+	promreporter.Reporter
 	server   *http.Server
+	listener net.Listener
 	registry *prometheus.Registry
 }
 
+func (r *statsdReporter) Close() error {
+	return r.statter.Close()
+}
+
 func (r *statsdReporter) ReportCounter(name string, tags map[string]string, value int64) {
-	r.reporter.ReportCounter(tagsToName(name, tags), tags, value)
+	r.StatsReporter.ReportCounter(tagsToName(name, tags), tags, value)
 }
 
 func (r *statsdReporter) ReportGauge(name string, tags map[string]string, value float64) {
-	r.reporter.ReportGauge(tagsToName(name, tags), tags, value)
+	r.StatsReporter.ReportGauge(tagsToName(name, tags), tags, value)
 }
 
 func (r *statsdReporter) ReportTimer(name string, tags map[string]string, interval time.Duration) {
-	r.reporter.ReportTimer(tagsToName(name, tags), tags, interval)
+	r.StatsReporter.ReportTimer(tagsToName(name, tags), tags, interval)
 }
 
 func (r *statsdReporter) ReportHistogramValueSamples(
@@ -290,7 +119,7 @@ func (r *statsdReporter) ReportHistogramValueSamples(
 	bucketUpperBound float64,
 	samples int64,
 ) {
-	r.reporter.ReportHistogramValueSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
+	r.StatsReporter.ReportHistogramValueSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
 }
 
 func (r *statsdReporter) ReportHistogramDurationSamples(
@@ -301,7 +130,7 @@ func (r *statsdReporter) ReportHistogramDurationSamples(
 	bucketUpperBound time.Duration,
 	samples int64,
 ) {
-	r.reporter.ReportHistogramDurationSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
+	r.StatsReporter.ReportHistogramDurationSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
 }
 
 func (r *statsdReporter) Capabilities() tally.Capabilities {
@@ -316,127 +145,20 @@ func (r *statsdReporter) Tagging() bool {
 	return true
 }
 
-func (r *statsdReporter) Flush() {
-	// no-op
-}
-
-func (r *statsdReporter) Close() error {
-	return r.statter.Close()
-}
-
-func (r *promReporter) RegisterCounter(
-	name string,
-	tagKeys []string,
-	desc string,
-) (*prometheus.CounterVec, error) {
-	return r.reporter.RegisterCounter(name, tagKeys, desc)
-}
-
-// AllocateCounter implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateCounter(name string, tags map[string]string) tally.CachedCount {
-	return r.reporter.AllocateCounter(name, tags)
-}
-
-func (r *promReporter) RegisterGauge(
-	name string,
-	tagKeys []string,
-	desc string,
-) (*prometheus.GaugeVec, error) {
-	return r.reporter.RegisterGauge(name, tagKeys, desc)
-}
-
-// AllocateGauge implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateGauge(name string, tags map[string]string) tally.CachedGauge {
-	return r.reporter.AllocateGauge(name, tags)
-}
-
-func (r *promReporter) RegisterTimer(
-	name string,
-	tagKeys []string,
-	desc string,
-	opts *promreporter.RegisterTimerOptions,
-) (promreporter.TimerUnion, error) {
-	return r.reporter.RegisterTimer(name, tagKeys, desc, opts)
-}
-
-// AllocateTimer implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateTimer(name string, tags map[string]string) tally.CachedTimer {
-	return r.reporter.AllocateTimer(name, tags)
-}
-
-func (r *promReporter) AllocateHistogram(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-) tally.CachedHistogram {
-	return r.reporter.AllocateHistogram(name, tags, buckets)
-}
-
-func (r *promReporter) Capabilities() tally.Capabilities {
-	return r
-}
-
-func (r *promReporter) Reporting() bool {
-	return true
-}
-
-func (r *promReporter) Tagging() bool {
-	return true
-}
-
-// Flush does nothing for prometheus
-func (r *promReporter) Flush() {
-
-}
-
 func (r *promReporter) Close() error {
-	//TODO: Timeout here?
-	return r.server.Shutdown(context.Background())
-}
-
-func (r *promReporter) Start() error {
-	return r.server.ListenAndServe()
+	//TODO: Shutdown server gracefully?
+	// Close() is not a graceful way since it closes server immediately
+	err := r.server.Close()
+	r.listener.Close()
+	return err
 }
 
 func (r *promReporter) HTTPHandler() http.Handler {
-	return promReporterHttpHandler(r.registry)
+	return promReporterHTTPHandler(r.registry)
 }
 
-func (s *scope) fullyQualifiedName(name string) string {
-	if len(s.prefix) == 0 {
-		return name
-	}
-	return fmt.Sprintf("%s%s%s", s.prefix, s.separator, name)
-}
-
-// mergeRightTags merges 2 sets of tags with the tags from tagsRight overriding values from tagsLeft
-func mergeRightTags(tagsLeft, tagsRight map[string]string) map[string]string {
-	if tagsLeft == nil && tagsRight == nil {
-		return nil
-	}
-	if len(tagsRight) == 0 {
-		return tagsLeft
-	}
-	if len(tagsLeft) == 0 {
-		return tagsRight
-	}
-
-	result := make(map[string]string, len(tagsLeft)+len(tagsRight))
-	for k, v := range tagsLeft {
-		result[k] = v
-	}
-	for k, v := range tagsRight {
-		result[k] = v
-	}
-	return result
-}
-
-func copyStringMap(stringMap map[string]string) map[string]string {
-	result := make(map[string]string, len(stringMap))
-	for k, v := range stringMap {
-		result[k] = v
-	}
-	return result
+func promReporterHTTPHandler(registry *prometheus.Registry) http.Handler {
+	return promhttp.HandlerFor(registry, promhttp.HandlerOpts{})
 }
 
 func tagsToName(name string, tags map[string]string) string {
@@ -447,12 +169,8 @@ func tagsToName(name string, tags map[string]string) string {
 	sort.Strings(keys)
 
 	for _, k := range keys {
-		name = name + tally.DefaultSeparator + k + "-" + tags[k]
+		name = name + promreporter.DefaultSeparator + k + "-" + tags[k]
 	}
 
 	return name
 }
-
-func promReporterHttpHandler(registry *prometheus.Registry) http.Handler {
-	return promhttp.HandlerFor(registry, promhttp.HandlerOpts{})
-}
diff --git a/common/metrics/tally_provider_test.go b/common/metrics/tally_provider_test.go
deleted file mode 100644
index 9e911c275..000000000
--- a/common/metrics/tally_provider_test.go
+++ /dev/null
@@ -1,462 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import (
-	"fmt"
-	"io"
-	"io/ioutil"
-	"net"
-	"net/http"
-	"strings"
-	"sync"
-	"sync/atomic"
-	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/uber-go/tally"
-	promreporter "github.com/uber-go/tally/prometheus"
-)
-
-const (
-	statsdAddress = "127.0.0.1:8125"
-	promAddress   = "127.0.0.1:8082"
-)
-
-type testIntValue struct {
-	val      int64
-	tags     map[string]string
-	reporter *testStatsReporter
-}
-
-func (m *testIntValue) ReportCount(value int64) {
-	m.val = value
-	m.reporter.cg.Done()
-}
-
-type testFloatValue struct {
-	val      float64
-	tags     map[string]string
-	reporter *testStatsReporter
-}
-
-func (m *testFloatValue) ReportGauge(value float64) {
-	m.val = value
-	m.reporter.gg.Done()
-}
-
-type testStatsReporter struct {
-	cg sync.WaitGroup
-	gg sync.WaitGroup
-
-	scope Scope
-
-	counters map[string]*testIntValue
-	gauges   map[string]*testFloatValue
-
-	flushes int32
-}
-
-// newTestStatsReporter returns a new TestStatsReporter
-func newTestStatsReporter() *testStatsReporter {
-	return &testStatsReporter{
-		counters: make(map[string]*testIntValue),
-		gauges:   make(map[string]*testFloatValue)}
-}
-
-func (r *testStatsReporter) WaitAll() {
-	r.cg.Wait()
-	r.gg.Wait()
-}
-
-func (r *testStatsReporter) AllocateCounter(
-	name string, tags map[string]string,
-) tally.CachedCount {
-	counter := &testIntValue{
-		val:      0,
-		tags:     tags,
-		reporter: r,
-	}
-	r.counters[name] = counter
-	return counter
-}
-
-func (r *testStatsReporter) ReportCounter(name string, tags map[string]string, value int64) {
-	r.counters[name] = &testIntValue{
-		val:  value,
-		tags: tags,
-	}
-	r.cg.Done()
-}
-
-func (r *testStatsReporter) AllocateGauge(
-	name string, tags map[string]string,
-) tally.CachedGauge {
-	gauge := &testFloatValue{
-		val:      0,
-		tags:     tags,
-		reporter: r,
-	}
-	r.gauges[name] = gauge
-	return gauge
-}
-
-func (r *testStatsReporter) ReportGauge(name string, tags map[string]string, value float64) {
-	r.gauges[name] = &testFloatValue{
-		val:  value,
-		tags: tags,
-	}
-	r.gg.Done()
-}
-
-func (r *testStatsReporter) AllocateTimer(
-	name string, tags map[string]string,
-) tally.CachedTimer {
-	return nil
-}
-
-func (r *testStatsReporter) ReportTimer(name string, tags map[string]string, interval time.Duration) {
-
-}
-
-func (r *testStatsReporter) AllocateHistogram(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-) tally.CachedHistogram {
-	return nil
-}
-
-func (r *testStatsReporter) ReportHistogramValueSamples(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-	bucketLowerBound,
-	bucketUpperBound float64,
-	samples int64,
-) {
-
-}
-
-func (r *testStatsReporter) ReportHistogramDurationSamples(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-	bucketLowerBound,
-	bucketUpperBound time.Duration,
-	samples int64,
-) {
-
-}
-
-func (r *testStatsReporter) Capabilities() tally.Capabilities {
-	return nil
-}
-
-func (r *testStatsReporter) Flush() {
-	atomic.AddInt32(&r.flushes, 1)
-}
-
-func TestCounter(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.cg.Add(1)
-	s.Counter("foo").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".foo"].val)
-
-	defer func() {
-		if r := recover(); r == nil {
-			t.Errorf("Should panic when wrong key used")
-		}
-	}()
-	assert.Equal(t, int64(1), r.counters[namespace+".foo1"].val)
-}
-
-func TestMultiCounterReport(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 2*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.cg.Add(1)
-	go s.Counter("foo").Inc(1)
-	go s.Counter("foo").Inc(3)
-	go s.Counter("foo").Inc(5)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(9), r.counters[namespace+".foo"].val)
-}
-
-func TestGauge(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.gg.Add(1)
-	s.Gauge("foo").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".foo"].val)
-}
-
-func TestMultiGaugeReport(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-
-	r.gg.Add(1)
-	s.Gauge("foo").Update(float64(1.33))
-	s.Gauge("foo").Update(float64(3.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(3.33), r.gauges[namespace+".foo"].val)
-}
-
-func TestSubScope(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("foo")
-
-	r.gg.Add(1)
-	subs.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".foo.bar"].val)
-
-	r.cg.Add(1)
-	subs.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".foo.haha"].val)
-}
-
-func TestTagged(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.Tagged(map[string]string{"env": "test"})
-
-	r.gg.Add(1)
-	subs.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".bar"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.gauges[namespace+".bar"].tags)
-
-	r.cg.Add(1)
-	subs.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".haha"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.counters[namespace+".haha"].tags)
-}
-
-func TestTaggedExistingReturnsSameScope(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-
-	for _, initialTags := range []map[string]string{
-		nil,
-		{"env": "test"},
-	} {
-		root := newRootScope(tally.ScopeOptions{Prefix: "foo", Tags: initialTags, Reporter: r}, 0)
-		go root.Start()
-		rootScope := root.(*scope)
-		fooScope := root.Tagged(map[string]string{"foo": "bar"}).(*scope)
-
-		assert.NotEqual(t, rootScope, fooScope)
-		assert.Equal(t, fooScope, fooScope.Tagged(nil))
-
-		fooBarScope := fooScope.Tagged(map[string]string{"bar": "baz"}).(*scope)
-
-		assert.NotEqual(t, fooScope, fooBarScope)
-		assert.Equal(t, fooBarScope, fooScope.Tagged(map[string]string{"bar": "baz"}).(*scope))
-		root.Close()
-	}
-}
-
-func TestSubScopeTagged(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("sub")
-	subtags := subs.Tagged(map[string]string{"env": "test"})
-
-	r.gg.Add(1)
-	subtags.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".sub.bar"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.gauges[namespace+".sub.bar"].tags)
-
-	r.cg.Add(1)
-	subtags.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".sub.haha"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.counters[namespace+".sub.haha"].tags)
-}
-
-func TestMetricsByStatsdReporter(t *testing.T) {
-	t.Parallel()
-	udpAddr, err := net.ResolveUDPAddr("udp", statsdAddress)
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	server, err := net.ListenUDP("udp", udpAddr)
-	if err != nil {
-		t.Fatal(err)
-	}
-	defer server.Close()
-
-	r, _ := newTestStatsdReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("peer").Tagged(map[string]string{"component": "committer", "env": "test"})
-	subs.Counter("success_total").Inc(1)
-	subs.Gauge("channel_total").Update(4)
-
-	buffer := make([]byte, 4096)
-	n, _ := io.ReadAtLeast(server, buffer, 1)
-	result := string(buffer[:n])
-
-	expected := []string{
-		`hyperledger_fabric.peer.success_total.component-committer.env-test:1|c`,
-		`hyperledger_fabric.peer.channel_total.component-committer.env-test:4|g`,
-	}
-
-	for i, res := range strings.Split(result, "\n") {
-		if res != expected[i] {
-			t.Errorf("Got `%s`, expected `%s`", res, expected[i])
-		}
-	}
-}
-
-func TestMetricsByPrometheusReporter(t *testing.T) {
-	t.Parallel()
-	r, _ := newTestPrometheusReporter()
-
-	opts := tally.ScopeOptions{
-		Prefix:         namespace,
-		Separator:      promreporter.DefaultSeparator,
-		CachedReporter: r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-
-	scrape := func() string {
-		resp, _ := http.Get(fmt.Sprintf("http://%s/metrics", promAddress))
-		buf, _ := ioutil.ReadAll(resp.Body)
-		return string(buf)
-	}
-	subs := s.SubScope("peer").Tagged(map[string]string{"component": "committer", "env": "test"})
-	subs.Counter("success_total").Inc(1)
-	subs.Gauge("channel_total").Update(4)
-
-	time.Sleep(2 * time.Second)
-
-	expected := []string{
-		`# HELP hyperledger_fabric_peer_channel_total hyperledger_fabric_peer_channel_total gauge`,
-		`# TYPE hyperledger_fabric_peer_channel_total gauge`,
-		`hyperledger_fabric_peer_channel_total{component="committer",env="test"} 4`,
-		`# HELP hyperledger_fabric_peer_success_total hyperledger_fabric_peer_success_total counter`,
-		`# TYPE hyperledger_fabric_peer_success_total counter`,
-		`hyperledger_fabric_peer_success_total{component="committer",env="test"} 1`,
-		``,
-	}
-
-	result := strings.Split(scrape(), "\n")
-
-	for i, res := range result {
-		if res != expected[i] {
-			t.Errorf("Got `%s`, expected `%s`", res, expected[i])
-		}
-	}
-}
-
-func newTestStatsdReporter() (tally.StatsReporter, error) {
-	opts := StatsdReporterOpts{
-		Address:       statsdAddress,
-		FlushInterval: defaultStatsdReporterFlushInterval,
-		FlushBytes:    defaultStatsdReporterFlushBytes,
-	}
-	return newStatsdReporter(opts)
-}
-
-func newTestPrometheusReporter() (promreporter.Reporter, error) {
-	opts := PromReporterOpts{
-		ListenAddress: promAddress,
-	}
-	return newPromReporter(opts)
-}
diff --git a/common/metrics/types.go b/common/metrics/types.go
deleted file mode 100644
index c70001ea1..000000000
--- a/common/metrics/types.go
+++ /dev/null
@@ -1,45 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import "io"
-
-// Counter is the interface for emitting Counter type metrics.
-type Counter interface {
-	// Inc increments the Counter by a delta.
-	Inc(delta int64)
-}
-
-// Gauge is the interface for emitting Gauge metrics.
-type Gauge interface {
-	// Update sets the gauges absolute value.
-	Update(value float64)
-}
-
-// Scope is a namespace wrapper around a stats Reporter, ensuring that
-// all emitted values have a given prefix or set of tags.
-type Scope interface {
-	serve
-	// Counter returns the Counter object corresponding to the name.
-	Counter(name string) Counter
-
-	// Gauge returns the Gauge object corresponding to the name.
-	Gauge(name string) Gauge
-
-	// Tagged returns a new child Scope with the given tags and current tags.
-	Tagged(tags map[string]string) Scope
-
-	// SubScope returns a new child Scope appending a further name prefix.
-	SubScope(name string) Scope
-}
-
-// serve is the interface represents who can provide service
-type serve interface {
-	io.Closer
-	// Start starts the server
-	Start() error
-}
diff --git a/core/deliverservice/blocksprovider/blocksprovider.go b/core/deliverservice/blocksprovider/blocksprovider.go
index c385e3ca1..9313ad391 100644
--- a/core/deliverservice/blocksprovider/blocksprovider.go
+++ b/core/deliverservice/blocksprovider/blocksprovider.go
@@ -11,8 +11,11 @@ import (
 	"sync/atomic"
 	"time"
 
+	"fmt"
+
 	"github.com/golang/protobuf/proto"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/gossip/api"
 	gossipcommon "github.com/hyperledger/fabric/gossip/common"
 	"github.com/hyperledger/fabric/gossip/discovery"
@@ -167,6 +170,8 @@ func (b *blocksProviderImpl) DeliverBlocks() {
 			statusCounter = 0
 			seqNum := t.Block.Header.Number
 
+			metrics.RootScope.Gauge(fmt.Sprintf("blocksprovider_%s_received_block_number", b.chainID)).Update(float64(seqNum))
+
 			marshaledBlock, err := proto.Marshal(t.Block)
 			if err != nil {
 				logger.Errorf("[%s] Error serializing block with sequence number %d, due to %s", b.chainID, seqNum, err)
@@ -189,6 +194,8 @@ func (b *blocksProviderImpl) DeliverBlocks() {
 				logger.Warning("Failed adding payload of", seqNum, "because:", err)
 			}
 
+			metrics.RootScope.Gauge(fmt.Sprintf("blocksprovider_%s_Gossiping_block_number", b.chainID)).Update(float64(payload.SeqNum))
+
 			// Gossip messages with other nodes
 			logger.Debugf("[%s] Gossiping block [%d], peers number [%d]", b.chainID, seqNum, numberOfPeers)
 			if !b.isDone() {
diff --git a/core/endorser/endorser.go b/core/endorser/endorser.go
index 2d29f43a4..050c1a06f 100644
--- a/core/endorser/endorser.go
+++ b/core/endorser/endorser.go
@@ -25,10 +25,19 @@ import (
 	putils "github.com/hyperledger/fabric/protos/utils"
 	"github.com/pkg/errors"
 	"golang.org/x/net/context"
+	"github.com/hyperledger/fabric/common/metrics"
+
 )
 
 var endorserLogger = flogging.MustGetLogger("endorser")
 
+
+
+
+
+
+
+
 // The Jira issue that documents Endorser flow along with its relationship to
 // the lifecycle chaincode - https://jira.hyperledger.org/browse/FAB-181
 
@@ -122,8 +131,18 @@ func NewEndorserServer(privDist privateDataDistributor, s Support) *Endorser {
 func (e *Endorser) callChaincode(ctxt context.Context, chainID string, version string, txid string, signedProp *pb.SignedProposal, prop *pb.Proposal, cis *pb.ChaincodeInvocationSpec, cid *pb.ChaincodeID, txsim ledger.TxSimulator) (*pb.Response, *pb.ChaincodeEvent, error) {
 	endorserLogger.Debugf("[%s][%s] Entry chaincode: %s version: %s", chainID, txid, cid, version)
 	defer endorserLogger.Debugf("[%s][%s] Exit", chainID, txid)
-	var err error
+
+	// Report on the CC processing time
+	ccName := cis.ChaincodeSpec.ChaincodeId.Name
+	fName := "NIL"
+	if len(cis.ChaincodeSpec.Input.Args) > 0 {
+		fName = string(cis.ChaincodeSpec.Input.Args[0])
+	}
+	ccTimer := metrics.RootScope.Timer(fmt.Sprintf("endorser_callChaincode_%s_%s_processing_time_seconds", ccName, fName))
+	ccStopWatch := ccTimer.Start()
+	defer ccStopWatch.Stop()
 	var res *pb.Response
+	var err error
 	var ccevent *pb.ChaincodeEvent
 
 	if txsim != nil {
@@ -256,6 +275,16 @@ func (e *Endorser) SimulateProposal(ctx context.Context, chainID string, txid st
 		return nil, nil, nil, nil, err
 	}
 
+	ccName := cis.ChaincodeSpec.ChaincodeId.Name
+	fName := "NIL"
+	if len(cis.ChaincodeSpec.Input.Args) > 0 {
+		fName = string(cis.ChaincodeSpec.Input.Args[0])
+	}
+	ccTimer := metrics.RootScope.Timer(fmt.Sprintf("endorser_SimulateProposal_%s_%s_processing_time_seconds", ccName, fName))
+	ccStopWatch := ccTimer.Start()
+	defer ccStopWatch.Stop()
+
+
 	// disable Java install,instantiate,upgrade for now
 	if err = e.DisableJavaCCInst(cid, cis); err != nil {
 		return nil, nil, nil, nil, err
@@ -338,6 +367,28 @@ func (e *Endorser) endorseProposal(_ context.Context, chainID string, txid strin
 	endorserLogger.Debugf("[%s][%s] Entry chaincode: %s", chainID, shorttxid(txid), ccid)
 	defer endorserLogger.Debugf("[%s][%s] Exit", chainID, shorttxid(txid))
 
+	// extract the Proposal message from signedProp
+	prop, err1 := putils.GetProposal(signedProp.ProposalBytes)
+	if err1 != nil {
+		return nil,err1
+	}
+
+	cis, err1 := putils.GetChaincodeInvocationSpec(prop)
+	if err1 != nil {
+		return nil,err1
+	}
+	// Report on the CC processing time
+	ccName := cis.ChaincodeSpec.ChaincodeId.Name
+	fName := "NIL"
+	if len(cis.ChaincodeSpec.Input.Args) > 0 {
+		fName = string(cis.ChaincodeSpec.Input.Args[0])
+	}
+	ccTimer := metrics.RootScope.Timer(fmt.Sprintf("endorser_endorseProposal_%s_%s_processing_time_seconds", ccName, fName))
+	ccStopWatch := ccTimer.Start()
+	defer ccStopWatch.Stop()
+
+
+
 	isSysCC := cd == nil
 	// 1) extract the name of the escc that is requested to endorse this chaincode
 	var escc string
@@ -461,6 +512,26 @@ func (e *Endorser) ProcessProposal(ctx context.Context, signedProp *pb.SignedPro
 		return resp, err
 	}
 
+	// extract the Proposal message from signedProp
+	prop, err := putils.GetProposal(signedProp.ProposalBytes)
+	if err != nil {
+		return nil,err
+	}
+
+	cis, err := putils.GetChaincodeInvocationSpec(prop)
+	if err != nil {
+		return nil,err
+	}
+	// Report on the CC processing time
+	ccName := cis.ChaincodeSpec.ChaincodeId.Name
+	fName := "NIL"
+	if len(cis.ChaincodeSpec.Input.Args) > 0 {
+		fName = string(cis.ChaincodeSpec.Input.Args[0])
+	}
+	ccTimer := metrics.RootScope.Timer(fmt.Sprintf("endorser_ProcessProposal_%s_%s_processing_time_seconds", ccName, fName))
+	ccStopWatch := ccTimer.Start()
+	defer ccStopWatch.Stop()
+
 	prop, hdrExt, chainID, txid := vr.prop, vr.hdrExt, vr.chainID, vr.txid
 
 	// obtaining once the tx simulator for this proposal. This will be nil
diff --git a/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go b/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
index 369787d44..6e9e0e288 100644
--- a/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
+++ b/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
@@ -10,6 +10,7 @@ import (
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/history/historydb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
@@ -18,10 +19,17 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/util"
 	"github.com/hyperledger/fabric/protos/common"
 	putils "github.com/hyperledger/fabric/protos/utils"
+	"github.com/uber-go/tally"
 )
 
 var logger = flogging.MustGetLogger("historyleveldb")
 
+var commitTimer tally.Timer
+
+func init() {
+	commitTimer = metrics.RootScope.Timer("historyleveldb_Commit_time_seconds")
+}
+
 var savePointKey = []byte{0x00}
 var emptyValue = []byte{}
 
@@ -73,6 +81,10 @@ func (historyDB *historyDB) Close() {
 // Commit implements method in HistoryDB interface
 func (historyDB *historyDB) Commit(block *common.Block) error {
 
+	// Measure the whole
+	stopWatch := commitTimer.Start()
+	defer stopWatch.Stop()
+
 	blockNo := block.Header.Number
 	//Set the starting tranNo to 0
 	var tranNo uint64
diff --git a/core/ledger/kvledger/kv_ledger.go b/core/ledger/kvledger/kv_ledger.go
index 385f90917..d9c87439b 100644
--- a/core/ledger/kvledger/kv_ledger.go
+++ b/core/ledger/kvledger/kv_ledger.go
@@ -15,6 +15,7 @@ import (
 
 	"github.com/hyperledger/fabric/common/flogging"
 	commonledger "github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
@@ -28,10 +29,27 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/ledgerstorage"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/uber-go/tally"
 )
 
 var logger = flogging.MustGetLogger("kvledger")
 
+var commitWithPvtDataTimer tally.Timer
+var commitWithPvtDataLockTimer tally.Timer
+var pvtDataAndBlockByNumTimer tally.Timer
+var pvtDataAndBlockByNumLockTimer tally.Timer
+var pvtDataByNumTimer tally.Timer
+var pvtDataByNumLockTimer tally.Timer
+
+func init() {
+	commitWithPvtDataTimer = metrics.RootScope.Timer("kvledger_CommitWithPvtData_time_seconds")
+	commitWithPvtDataLockTimer = metrics.RootScope.Timer("kvledger_CommitWithPvtData_Lock_time_seconds")
+	pvtDataAndBlockByNumTimer = metrics.RootScope.Timer("kvledger_GetPvtDataAndBlockByNum_time_seconds")
+	pvtDataAndBlockByNumLockTimer = metrics.RootScope.Timer("kvledger_GetPvtDataAndBlockByNum_Lock_time_seconds")
+	pvtDataByNumTimer = metrics.RootScope.Timer("kvledger_GetPvtDataByNum_time_seconds")
+	pvtDataByNumLockTimer = metrics.RootScope.Timer("kvledger_GetPvtDataByNum_Lock_time_seconds")
+}
+
 // KVLedger provides an implementation of `ledger.PeerLedger`.
 // This implementation provides a key-value based data model
 type kvLedger struct {
@@ -249,6 +267,11 @@ func (l *kvLedger) NewHistoryQueryExecutor() (ledger.HistoryQueryExecutor, error
 
 // CommitWithPvtData commits the block and the corresponding pvt data in an atomic operation
 func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) error {
+
+	// Measure the whole
+	stopWatch := commitWithPvtDataTimer.Start()
+	defer stopWatch.Stop()
+
 	var err error
 	block := pvtdataAndBlock.Block
 	blockNo := pvtdataAndBlock.Block.Header.Number
@@ -261,11 +284,18 @@ func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) er
 
 	logger.Debugf("Channel [%s]: Committing block [%d] to storage", l.ledgerID, blockNo)
 
+	// Measure acquiring the lock
+	lockStopWatch := commitWithPvtDataLockTimer.Start()
 	l.blockAPIsRWLock.Lock()
 	defer l.blockAPIsRWLock.Unlock()
+	lockStopWatch.Stop()
+
 	if err = l.blockStore.CommitWithPvtData(pvtdataAndBlock); err != nil {
 		return err
 	}
+
+	metrics.RootScope.Gauge(fmt.Sprintf("kvledger_%s_commited_block_number", l.ledgerID)).Update(float64(block.Header.Number))
+
 	logger.Infof("Channel [%s]: Committed block [%d] with %d transaction(s)", l.ledgerID, block.Header.Number, len(block.Data.Data))
 
 	logger.Debugf("Channel [%s]: Committing block [%d] transactions to state database", l.ledgerID, blockNo)
@@ -286,8 +316,18 @@ func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) er
 // GetPvtDataAndBlockByNum returns the block and the corresponding pvt data.
 // The pvt data is filtered by the list of 'collections' supplied
 func (l *kvLedger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsCollFilter) (*ledger.BlockAndPvtData, error) {
+
+	// Measure the whole
+	stopWatch := pvtDataAndBlockByNumTimer.Start()
+	defer stopWatch.Stop()
+
 	blockAndPvtdata, err := l.blockStore.GetPvtDataAndBlockByNum(blockNum, filter)
+
+	// Measure acquiring the lock
+	lockStopWatch := pvtDataAndBlockByNumLockTimer.Start()
 	l.blockAPIsRWLock.RLock()
+	lockStopWatch.Stop()
+
 	l.blockAPIsRWLock.RUnlock()
 	return blockAndPvtdata, err
 }
@@ -295,8 +335,18 @@ func (l *kvLedger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsC
 // GetPvtDataByNum returns only the pvt data  corresponding to the given block number
 // The pvt data is filtered by the list of 'collections' supplied
 func (l *kvLedger) GetPvtDataByNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+
+	// Measure the whole
+	stopWatch := pvtDataByNumTimer.Start()
+	defer stopWatch.Stop()
+
 	pvtdata, err := l.blockStore.GetPvtDataByNum(blockNum, filter)
+
+	// Measure acquiring the lock
+	lockStopWatch := pvtDataByNumLockTimer.Start()
 	l.blockAPIsRWLock.RLock()
+	lockStopWatch.Stop()
+
 	l.blockAPIsRWLock.RUnlock()
 	return pvtdata, err
 }
diff --git a/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go b/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
index f522ab2cc..e48aa6b0a 100644
--- a/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
+++ b/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
@@ -12,6 +12,7 @@ import (
 	"strings"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
 
@@ -20,10 +21,19 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb/stateleveldb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	"github.com/uber-go/tally"
 )
 
 var logger = flogging.MustGetLogger("privacyenabledstate")
 
+var addPvtUpdatesTimer tally.Timer
+var addHashedUpdatesTimer tally.Timer
+
+func init() {
+	addPvtUpdatesTimer = metrics.RootScope.Timer("privacyenabledstate_addPvtUpdates_time_seconds")
+	addHashedUpdatesTimer = metrics.RootScope.Timer("privacyenabledstate_addHashedUpdatesTimer_time_seconds")
+}
+
 const (
 	nsJoiner       = "$$"
 	pvtDataPrefix  = "p"
@@ -257,6 +267,8 @@ func deriveHashedDataNs(namespace, collection string) string {
 }
 
 func addPvtUpdates(pubUpdateBatch *PubUpdateBatch, pvtUpdateBatch *PvtUpdateBatch) {
+	stopWatch := addPvtUpdatesTimer.Start()
+	defer stopWatch.Stop()
 	for ns, nsBatch := range pvtUpdateBatch.UpdateMap {
 		for _, coll := range nsBatch.GetCollectionNames() {
 			for key, vv := range nsBatch.GetUpdates(coll) {
@@ -267,6 +279,8 @@ func addPvtUpdates(pubUpdateBatch *PubUpdateBatch, pvtUpdateBatch *PvtUpdateBatc
 }
 
 func addHashedUpdates(pubUpdateBatch *PubUpdateBatch, hashedUpdateBatch *HashedUpdateBatch, base64Key bool) {
+	stopWatch := addHashedUpdatesTimer.Start()
+	defer stopWatch.Stop()
 	for ns, nsBatch := range hashedUpdateBatch.UpdateMap {
 		for _, coll := range nsBatch.GetCollectionNames() {
 			for key, vv := range nsBatch.GetUpdates(coll) {
diff --git a/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go b/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
index 1d1436cb5..de2aff38d 100644
--- a/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
+++ b/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
@@ -11,9 +11,17 @@ import (
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/bookkeeping"
+	"github.com/uber-go/tally"
 )
 
+var updateBookkeepingTimer tally.Timer
+
+func init() {
+	updateBookkeepingTimer = metrics.RootScope.Timer("pvtstatepurgemgmt_updateBookkeepingTimer_time_seconds")
+}
+
 var logger = flogging.MustGetLogger("pvtstatepurgemgmt")
 
 const (
@@ -64,6 +72,8 @@ type expKeeper struct {
 // at the time of the commit of the block number 45 and the second entry was created at the time of the commit of the block number 40, however
 // both are expiring with the commit of block number 50.
 func (ek *expKeeper) updateBookkeeping(toTrack []*expiryInfo, toClear []*expiryInfoKey) error {
+	stopWatch := updateBookkeepingTimer.Start()
+	defer stopWatch.Stop()
 	updateBatch := leveldbhelper.NewUpdateBatch()
 	for _, expinfo := range toTrack {
 		k, v, err := encodeKV(expinfo)
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
index 2dd88237c..ad19868eb 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
@@ -7,8 +7,19 @@ package statecouchdb
 
 import (
 	"sync"
+
+	"github.com/hyperledger/fabric/common/metrics"
+	"github.com/uber-go/tally"
 )
 
+var executeBatchesTimer tally.Timer
+var numBatchesGauge tally.Gauge
+
+func init() {
+	executeBatchesTimer = metrics.RootScope.Timer("statecouchdb_executeBatches_time_seconds")
+	numBatchesGauge = metrics.RootScope.Gauge("statecouchdb_numBatches")
+}
+
 // batch is executed in a separate goroutine.
 type batch interface {
 	execute() error
@@ -17,8 +28,12 @@ type batch interface {
 // executeBatches executes each batch in a separate goroutine and returns error if
 // any of the batches return error during its execution
 func executeBatches(batches []batch) error {
-	logger.Debugf("Executing batches = %s", batches)
+
+	stopWatch := executeBatchesTimer.Start()
+	defer stopWatch.Stop()
+
 	numBatches := len(batches)
+	numBatchesGauge.Update(float64(numBatches))
 	if numBatches == 0 {
 		return nil
 	}
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
index 240667a73..141493742 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
@@ -9,11 +9,19 @@ import (
 	"errors"
 	"fmt"
 
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/uber-go/tally"
 )
 
+var ensureFullCommitTimer tally.Timer
+
+func init() {
+	ensureFullCommitTimer = metrics.RootScope.Timer("statecouchdb_ensureFullCommit_time_seconds")
+}
+
 // nsCommittersBuilder implements `batch` interface. Each batch operates on a specific namespace in the updates and
 // builds one or more batches of type subNsCommitter.
 type nsCommittersBuilder struct {
@@ -151,6 +159,10 @@ type nsFlusher struct {
 }
 
 func (vdb *VersionedDB) ensureFullCommit(dbs []*couchdb.CouchDatabase) error {
+
+	stopWatch := ensureFullCommitTimer.Start()
+	defer stopWatch.Stop()
+
 	var flushers []batch
 	for _, db := range dbs {
 		flushers = append(flushers, &nsFlusher{db})
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
index b10b1d229..d0db360b8 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
@@ -12,13 +12,21 @@ import (
 	"sync"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/uber-go/tally"
 )
 
+var applyUpdatesTimer tally.Timer
+
+func init() {
+	applyUpdatesTimer = metrics.RootScope.Timer("statecouchdb_ApplyUpdates_time_seconds")
+}
+
 var logger = flogging.MustGetLogger("statecouchdb")
 
 // querySkip is implemented for future use by query paging
@@ -290,6 +298,10 @@ func (vdb *VersionedDB) ExecuteQuery(namespace, query string) (statedb.ResultsIt
 
 // ApplyUpdates implements method in VersionedDB interface
 func (vdb *VersionedDB) ApplyUpdates(updates *statedb.UpdateBatch, height *version.Height) error {
+
+	stopWatch := applyUpdatesTimer.Start()
+	defer stopWatch.Stop()
+
 	// TODO a note about https://jira.hyperledger.org/browse/FAB-8622
 	// the function `Apply update can be split into three functions. Each carrying out one of the following three stages`.
 	// The write lock is needed only for the stage 2.
diff --git a/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go b/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
index ba3be0ea3..e925d38e4 100644
--- a/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
+++ b/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
@@ -11,12 +11,20 @@ import (
 
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
+	"github.com/uber-go/tally"
 )
 
+var applyUpdatesTimer tally.Timer
+
+func init() {
+	applyUpdatesTimer = metrics.RootScope.Timer("stateleveldb_ApplyUpdates_time_seconds")
+}
+
 var logger = flogging.MustGetLogger("stateleveldb")
 
 var compositeKeySep = []byte{0x00}
@@ -138,6 +146,10 @@ func (vdb *versionedDB) ExecuteQuery(namespace, query string) (statedb.ResultsIt
 
 // ApplyUpdates implements method in VersionedDB interface
 func (vdb *versionedDB) ApplyUpdates(batch *statedb.UpdateBatch, height *version.Height) error {
+
+	stopWatch := applyUpdatesTimer.Start()
+	defer stopWatch.Stop()
+
 	dbBatch := leveldbhelper.NewUpdateBatch()
 	namespaces := batch.GetUpdatedNamespaces()
 	for _, ns := range namespaces {
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
index f13febb53..073c26630 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
@@ -185,6 +185,7 @@ func (h *queryHelper) done() {
 
 	defer func() {
 		h.txmgr.commitRWLock.RUnlock()
+		h.txmgr.commitRWLockRLockStopwatch.Stop()
 		h.doneInvoked = true
 		for _, itr := range h.itrs {
 			itr.Close()
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
index d03bd0535..0e94e6813 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
@@ -11,6 +11,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/bookkeeping"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate"
@@ -20,20 +21,35 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	"github.com/uber-go/tally"
 )
 
 var logger = flogging.MustGetLogger("lockbasedtxmgr")
 
+var commitTimer tally.Timer
+var commitLockTimer tally.Timer
+var commitAfterLockTimer tally.Timer
+var updateStateListenersTimer tally.Timer
+
+func init() {
+	commitTimer = metrics.RootScope.Timer("lockbasedtxmgr_Commit_time_seconds")
+	commitLockTimer = metrics.RootScope.Timer("lockbasedtxmgr_Commit_Lock_time_seconds")
+	commitAfterLockTimer = metrics.RootScope.Timer("lockbasedtxmgr_Commit_After_Lock_time_seconds")
+	updateStateListenersTimer = metrics.RootScope.Timer("lockbasedtxmgr_updateStateListenersTimer_time_seconds")
+}
+
 // LockBasedTxMgr a simple implementation of interface `txmgmt.TxMgr`.
 // This implementation uses a read-write lock to prevent conflicts between transaction simulation and committing
 type LockBasedTxMgr struct {
-	ledgerid        string
-	db              privacyenabledstate.DB
-	pvtdataPurgeMgr *pvtdataPurgeMgr
-	validator       validator.Validator
-	stateListeners  []ledger.StateListener
-	commitRWLock    sync.RWMutex
-	current         *current
+	ledgerid                   string
+	db                         privacyenabledstate.DB
+	pvtdataPurgeMgr            *pvtdataPurgeMgr
+	validator                  validator.Validator
+	stateListeners             []ledger.StateListener
+	commitRWLock               sync.RWMutex
+	commitRWLockRLockTimer     tally.Timer
+	commitRWLockRLockStopwatch tally.Stopwatch
+	current                    *current
 }
 
 type current struct {
@@ -61,6 +77,7 @@ func NewLockBasedTxMgr(ledgerid string, db privacyenabledstate.DB, stateListener
 	}
 	txmgr.pvtdataPurgeMgr = &pvtdataPurgeMgr{pvtstatePurgeMgr, false}
 	txmgr.validator = valimpl.NewStatebasedValidator(txmgr, db)
+	txmgr.commitRWLockRLockTimer = metrics.RootScope.Timer("lockbasedtxmgr_commitRWLockRLock_time_seconds")
 	return txmgr, nil
 }
 
@@ -74,6 +91,7 @@ func (txmgr *LockBasedTxMgr) GetLastSavepoint() (*version.Height, error) {
 func (txmgr *LockBasedTxMgr) NewQueryExecutor(txid string) (ledger.QueryExecutor, error) {
 	qe := newQueryExecutor(txmgr, txid)
 	txmgr.commitRWLock.RLock()
+	txmgr.commitRWLockRLockStopwatch = txmgr.commitRWLockRLockTimer.Start()
 	return qe, nil
 }
 
@@ -85,6 +103,7 @@ func (txmgr *LockBasedTxMgr) NewTxSimulator(txid string) (ledger.TxSimulator, er
 		return nil, err
 	}
 	txmgr.commitRWLock.RLock()
+	txmgr.commitRWLockRLockStopwatch = txmgr.commitRWLockRLockTimer.Start()
 	return s, nil
 }
 
@@ -129,6 +148,11 @@ func (txmgr *LockBasedTxMgr) Shutdown() {
 
 // Commit implements method in interface `txmgmt.TxMgr`
 func (txmgr *LockBasedTxMgr) Commit() error {
+
+	// Measure the whole
+	stopWatch := commitTimer.Start()
+	defer stopWatch.Stop()
+
 	// When using the purge manager for the first block commit after peer start, the asynchronous function
 	// 'PrepareForExpiringKeys' is invoked in-line. However, for the subsequent blocks commits, this function is invoked
 	// in advance for the next block
@@ -153,8 +177,16 @@ func (txmgr *LockBasedTxMgr) Commit() error {
 		return err
 	}
 
+	// Measure acquiring the lock
+	lockStopWatch := commitLockTimer.Start()
 	txmgr.commitRWLock.Lock()
 	defer txmgr.commitRWLock.Unlock()
+	lockStopWatch.Stop()
+
+	// Measure after acquiring the lock
+	afterLockStopWatch := commitAfterLockTimer.Start()
+	defer afterLockStopWatch.Stop()
+
 	logger.Debugf("Write lock acquired for committing updates to state database")
 	commitHeight := version.NewHeight(txmgr.current.blockNum(), txmgr.current.maxTxNumber())
 	if err := txmgr.db.ApplyPrivacyAwareUpdates(txmgr.current.batch, commitHeight); err != nil {
@@ -223,6 +255,8 @@ func extractStateUpdates(batch *privacyenabledstate.UpdateBatch, namespaces []st
 }
 
 func (txmgr *LockBasedTxMgr) updateStateListeners() {
+	stopWatch := updateStateListenersTimer.Start()
+	defer stopWatch.Stop()
 	for _, l := range txmgr.current.listeners {
 		l.StateCommitDone(txmgr.ledgerid)
 	}
diff --git a/core/ledger/kvledger/txmgmt/validator/valimpl/helper.go b/core/ledger/kvledger/txmgmt/validator/valimpl/helper.go
index 73b2d70e3..43de347f5 100644
--- a/core/ledger/kvledger/txmgmt/validator/valimpl/helper.go
+++ b/core/ledger/kvledger/txmgmt/validator/valimpl/helper.go
@@ -10,6 +10,7 @@ import (
 	"bytes"
 	"fmt"
 
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/customtx"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate"
@@ -25,6 +26,8 @@ import (
 	"github.com/hyperledger/fabric/protos/utils"
 )
 
+var validatePvtdataFailedCounter = metrics.RootScope.Counter("valimpl_Validate_Pvtdata_Failed_Count")
+
 // validateAndPreparePvtBatch pulls out the private write-set for the transactions that are marked as valid
 // by the internal public data validator. Finally, it validates (if not already self-endorsed) the pvt rwset against the
 // corresponding hash present in the public rwset
@@ -43,6 +46,7 @@ func validateAndPreparePvtBatch(block *valinternal.Block, pvtdata map[uint64]*le
 		}
 		if requiresPvtdataValidation(txPvtdata) {
 			if err := validatePvtdata(tx, txPvtdata); err != nil {
+				validatePvtdataFailedCounter.Inc(1)
 				return nil, err
 			}
 		}
diff --git a/core/ledger/ledgerstorage/store.go b/core/ledger/ledgerstorage/store.go
index 5655969d5..1bb1403ea 100644
--- a/core/ledger/ledgerstorage/store.go
+++ b/core/ledger/ledgerstorage/store.go
@@ -23,13 +23,35 @@ import (
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage/fsblkstorage"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
 	"github.com/hyperledger/fabric/protos/common"
+	"github.com/uber-go/tally"
 )
 
+var commitWithPvtDataTimer tally.Timer
+var commitWithPvtDataLockTimer tally.Timer
+var pvtDataAndBlockByNumTimer tally.Timer
+var pvtDataAndBlockByNumLockTimer tally.Timer
+var pvtDataByNumTimer tally.Timer
+var pvtDataByNumLockTimer tally.Timer
+var commitWithPvtDataSkipCounter tally.Counter
+var commitWithPvtDataBlockDiff tally.Gauge
+
+func init() {
+	commitWithPvtDataTimer = metrics.RootScope.Timer("ledgerstorage_CommitWithPvtData_time_seconds")
+	commitWithPvtDataLockTimer = metrics.RootScope.Timer("ledgerstorage_CommitWithPvtData_Lock_time_seconds")
+	commitWithPvtDataSkipCounter = metrics.RootScope.Counter("ledgerstorage_CommitWithPvtData_SkipCount")
+	commitWithPvtDataBlockDiff = metrics.RootScope.Gauge("ledgerstorage_CommitWithPvtData_BlockDiff")
+	pvtDataAndBlockByNumTimer = metrics.RootScope.Timer("ledgerstorage_GetPvtDataAndBlockByNum_time_seconds")
+	pvtDataAndBlockByNumLockTimer = metrics.RootScope.Timer("ledgerstorage_GetPvtDataAndBlockByNum_Lock_time_seconds")
+	pvtDataByNumTimer = metrics.RootScope.Timer("ledgerstorage_GetPvtDataByNum_time_seconds")
+	pvtDataByNumLockTimer = metrics.RootScope.Timer("ledgerstorage_GetPvtDataByNum_Lock_time_seconds")
+}
+
 var logger = flogging.MustGetLogger("ledgerstorage")
 
 // Provider encapusaltes two providers 1) block store provider and 2) and pvt data store provider
@@ -97,14 +119,24 @@ func (s *Store) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
 
 // CommitWithPvtData commits the block and the corresponding pvt data in an atomic operation
 func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error {
+
+	// Measure the whole
+	stopWatch := commitWithPvtDataTimer.Start()
+	defer stopWatch.Stop()
+
 	blockNum := blockAndPvtdata.Block.Header.Number
+
+	// Measure acquiring the lock
+	lockStopWatch := commitWithPvtDataLockTimer.Start()
 	s.rwlock.Lock()
 	defer s.rwlock.Unlock()
+	lockStopWatch.Stop()
 
 	pvtBlkStoreHt, err := s.pvtdataStore.LastCommittedBlockHeight()
 	if err != nil {
 		return err
 	}
+	commitWithPvtDataBlockDiff.Update(float64(blockNum - pvtBlkStoreHt))
 
 	writtenToPvtStore := false
 	if pvtBlkStoreHt < blockNum+1 { // The pvt data store sanity check does not allow rewriting the pvt data.
@@ -120,6 +152,7 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 		}
 		writtenToPvtStore = true
 	} else {
+		commitWithPvtDataSkipCounter.Inc(1)
 		logger.Debugf("Skipping writing block [%d] to pvt block store as the store height is [%d]", blockNum, pvtBlkStoreHt)
 	}
 
@@ -137,8 +170,16 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 // GetPvtDataAndBlockByNum returns the block and the corresponding pvt data.
 // The pvt data is filtered by the list of 'collections' supplied
 func (s *Store) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsCollFilter) (*ledger.BlockAndPvtData, error) {
+
+	// Measure the whole
+	stopWatch := pvtDataAndBlockByNumTimer.Start()
+	defer stopWatch.Stop()
+
+	// Measure acquiring the lock
+	lockStopWatch := pvtDataAndBlockByNumLockTimer.Start()
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
+	lockStopWatch.Stop()
 
 	var block *common.Block
 	var pvtdata []*ledger.TxPvtData
@@ -156,8 +197,17 @@ func (s *Store) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsColl
 // The pvt data is filtered by the list of 'ns/collections' supplied in the filter
 // A nil filter does not filter any results
 func (s *Store) GetPvtDataByNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+
+	// Measure the whole
+	stopWatch := pvtDataByNumTimer.Start()
+	defer stopWatch.Stop()
+
+	// Measure acquiring the lock
+	lockStopWatch := pvtDataByNumLockTimer.Start()
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
+	lockStopWatch.Stop()
+
 	return s.getPvtDataByNumWithoutLock(blockNum, filter)
 }
 
diff --git a/core/ledger/util/couchdb/couchdb.go b/core/ledger/util/couchdb/couchdb.go
index 16d479802..1caea8b98 100644
--- a/core/ledger/util/couchdb/couchdb.go
+++ b/core/ledger/util/couchdb/couchdb.go
@@ -27,12 +27,26 @@ import (
 	"unicode/utf8"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	logging "github.com/op/go-logging"
+	"github.com/uber-go/tally"
 )
 
 var logger = flogging.MustGetLogger("couchdb")
 
+var invalidCouchDBReturnCounter tally.Counter
+var saveDocTimer tally.Timer
+var handleRequestTimer tally.Timer
+var ensureFullCommitTimer tally.Timer
+
+func init() {
+	invalidCouchDBReturnCounter = metrics.RootScope.Counter("couchdb_invalidCouchDBReturnCount")
+	saveDocTimer = metrics.RootScope.Timer("couchdb_saveDoc_time_seconds")
+	handleRequestTimer = metrics.RootScope.Timer("couchdb_handleRequest_time_seconds")
+	ensureFullCommitTimer = metrics.RootScope.Timer("couchdb_ensureFullCommit_time_seconds")
+}
+
 //time between retry attempts in milliseconds
 const retryWaitTime = 125
 
@@ -491,6 +505,9 @@ func (dbclient *CouchDatabase) DropDatabase() (*DBOperationResponse, error) {
 // EnsureFullCommit calls _ensure_full_commit for explicit fsync
 func (dbclient *CouchDatabase) EnsureFullCommit() (*DBOperationResponse, error) {
 
+	stopWatch := ensureFullCommitTimer.Start()
+	defer stopWatch.Stop()
+
 	logger.Debugf("Entering EnsureFullCommit()")
 
 	connectURL, err := url.Parse(dbclient.CouchInstance.conf.URL)
@@ -549,6 +566,9 @@ func (dbclient *CouchDatabase) EnsureFullCommit() (*DBOperationResponse, error)
 //SaveDoc method provides a function to save a document, id and byte array
 func (dbclient *CouchDatabase) SaveDoc(id string, rev string, couchDoc *CouchDoc) (string, error) {
 
+	stopWatch := saveDocTimer.Start()
+	defer stopWatch.Stop()
+
 	logger.Debugf("Entering SaveDoc()  id=[%s]", id)
 
 	if !utf8.ValidString(id) {
@@ -1280,6 +1300,10 @@ func (dbclient *CouchDatabase) WarmIndex(designdoc, indexname string) error {
 	//URL to execute the view function associated with the index
 	indexURL.Path = dbclient.DBName + "/_design/" + designdoc + "/_view/" + indexname
 
+	timer := metrics.RootScope.Timer("couchdb_WarmIndex_" + indexURL.Path)
+	stopWatch := timer.Start()
+	defer stopWatch.Stop()
+
 	queryParms := indexURL.Query()
 	//Query parameter that allows the execution of the URL to return immediately
 	//The update_after will cause the index update to run after the URL returns
@@ -1650,6 +1674,9 @@ func (dbclient *CouchDatabase) handleRequestWithRevisionRetry(id, method string,
 func (couchInstance *CouchInstance) handleRequest(method, connectURL string, data []byte, rev string,
 	multipartBoundary string, maxRetries int, keepConnectionOpen bool) (*http.Response, *DBReturn, error) {
 
+	stopWatch := handleRequestTimer.Start()
+	defer stopWatch.Stop()
+
 	logger.Debugf("Entering handleRequest()  method=%s  url=%v", method, connectURL)
 
 	//create the return objects for couchDB
@@ -1838,6 +1865,8 @@ func (couchInstance *CouchInstance) handleRequest(method, connectURL string, dat
 //invalidCouchDBResponse checks to make sure either a valid response or error is returned
 func invalidCouchDBReturn(resp *http.Response, errResp error) bool {
 	if resp == nil && errResp == nil {
+		logger.Errorf("invalidCouchDBReturn(%v, %v)", resp == nil, errResp == nil)
+		invalidCouchDBReturnCounter.Inc(1)
 		return true
 	}
 	return false
diff --git a/gossip/privdata/coordinator.go b/gossip/privdata/coordinator.go
index 1d1152bac..0aa4124be 100644
--- a/gossip/privdata/coordinator.go
+++ b/gossip/privdata/coordinator.go
@@ -13,6 +13,7 @@ import (
 	"time"
 
 	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/metrics"
 	util2 "github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/committer"
 	"github.com/hyperledger/fabric/core/committer/txvalidator"
@@ -31,6 +32,13 @@ import (
 	"github.com/op/go-logging"
 	"github.com/pkg/errors"
 	"github.com/spf13/viper"
+	"github.com/uber-go/tally"
+)
+
+var (
+	gossipMissingKeysGauge           tally.Gauge
+	gossipWaitingForMissingKeysTimer tally.Timer
+	gossipStoreBlockCounter          tally.Counter
 )
 
 const (
@@ -43,6 +51,9 @@ var logger *logging.Logger // package-level logger
 
 func init() {
 	logger = util.GetLogger(util.LoggingPrivModule, "")
+	gossipMissingKeysGauge = metrics.RootScope.Gauge("privdata_gossipMissingKeys")
+	gossipWaitingForMissingKeysTimer = metrics.RootScope.Timer("privdata_gossipWaitingForMissingKeys_time_seconds")
+	gossipStoreBlockCounter = metrics.RootScope.Counter("privdata_gossipStoreBlockCount")
 }
 
 // TransientStore holds private data that the corresponding blocks haven't been committed yet into the ledger
@@ -150,6 +161,9 @@ func (c *coordinator) StorePvtData(txID string, privData *transientstore2.TxPvtR
 
 // StoreBlock stores block with private data into the ledger
 func (c *coordinator) StoreBlock(block *common.Block, privateDataSets util.PvtDataCollections) error {
+
+	gossipStoreBlockCounter.Inc(1)
+
 	if block.Data == nil {
 		return errors.New("Block data is empty")
 	}
@@ -193,6 +207,10 @@ func (c *coordinator) StoreBlock(block *common.Block, privateDataSets util.PvtDa
 	}
 	start := time.Now()
 	limit := start.Add(retryThresh)
+
+	gossipMissingKeysGauge.Update(float64(len(privateInfo.missingKeys)))
+	waitingForMissingKeysStopWatch := gossipWaitingForMissingKeysTimer.Start()
+
 	for len(privateInfo.missingKeys) > 0 && time.Now().Before(limit) {
 		c.fetchFromPeers(block.Header.Number, ownedRWsets, privateInfo)
 		// If succeeded to fetch everything, no need to sleep before
@@ -202,6 +220,7 @@ func (c *coordinator) StoreBlock(block *common.Block, privateDataSets util.PvtDa
 		}
 		time.Sleep(pullRetrySleepInterval)
 	}
+	waitingForMissingKeysStopWatch.Stop()
 
 	// Only log results if we actually attempted to fetch
 	if bFetchFromPeers {
diff --git a/gossip/state/state.go b/gossip/state/state.go
index ad2703a69..9a97d7030 100644
--- a/gossip/state/state.go
+++ b/gossip/state/state.go
@@ -12,8 +12,11 @@ import (
 	"sync/atomic"
 	"time"
 
+	"fmt"
+
 	pb "github.com/golang/protobuf/proto"
 	vsccErrors "github.com/hyperledger/fabric/common/errors"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/gossip/api"
 	"github.com/hyperledger/fabric/gossip/comm"
 	common2 "github.com/hyperledger/fabric/gossip/common"
@@ -470,6 +473,9 @@ func (s *GossipStateProviderImpl) handleStateResponse(msg proto.ReceivedMessage)
 		return uint64(0), errors.New("Received state transfer response without payload")
 	}
 	for _, payload := range response.GetPayloads() {
+
+		metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_handleStateResponse_block_number", s.chainID)).Update(float64(payload.SeqNum))
+
 		logger.Debugf("Received payload with sequence number %d.", payload.SeqNum)
 		if err := s.mediator.VerifyBlock(common2.ChainID(s.chainID), payload.SeqNum, payload.Data); err != nil {
 			err = errors.WithStack(err)
@@ -514,6 +520,9 @@ func (s *GossipStateProviderImpl) queueNewMessage(msg *proto.GossipMessage) {
 
 	dataMsg := msg.GetDataMsg()
 	if dataMsg != nil {
+
+		metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_adding_payload_number", s.chainID)).Update(float64(dataMsg.Payload.SeqNum))
+
 		if err := s.addPayload(dataMsg.GetPayload(), nonBlocking); err != nil {
 			logger.Warning("Failed adding payload:", err)
 			return
@@ -531,6 +540,9 @@ func (s *GossipStateProviderImpl) deliverPayloads() {
 		select {
 		// Wait for notification that next seq has arrived
 		case <-s.payloads.Ready():
+
+			metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_received_block_number", s.chainID)).Update(float64(s.payloads.Next()))
+
 			logger.Debugf("Ready to transfer payloads to the ledger, next sequence number is = [%d]", s.payloads.Next())
 			// Collect all subsequent payloads
 			for payload := s.payloads.Pop(); payload != nil; payload = s.payloads.Pop() {
@@ -767,6 +779,8 @@ func (s *GossipStateProviderImpl) addPayload(payload *proto.Payload, blockingMod
 
 func (s *GossipStateProviderImpl) commitBlock(block *common.Block, pvtData util.PvtDataCollections) error {
 
+	metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_about_to_store_block_number", s.chainID)).Update(float64(block.Header.Number))
+
 	// Commit block with available private transactions
 	if err := s.ledger.StoreBlock(block, pvtData); err != nil {
 		logger.Errorf("Got error while committing(%+v)", errors.WithStack(err))
diff --git a/peer/main.go b/peer/main.go
index aa7d3018a..df333e808 100644
--- a/peer/main.go
+++ b/peer/main.go
@@ -23,6 +23,7 @@ import (
 	"github.com/hyperledger/fabric/peer/version"
 	"github.com/spf13/cobra"
 	"github.com/spf13/viper"
+	"github.com/hyperledger/fabric/gossip/util"
 )
 
 var logger = flogging.MustGetLogger("main")
@@ -76,6 +77,7 @@ func main() {
 		loggingSpec = viper.GetString("logging.level")
 	}
 	flogging.InitFromSpec(loggingSpec)
+	flogging.SetModuleLevel(util.LoggingPrivModule,"debug")
 
 	// Init the MSP
 	var mspMgrConfigDir = config.GetPath("peer.mspConfigPath")
-- 
2.14.3 (Apple Git-98)

