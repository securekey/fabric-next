From 3c6ef1c2ecca0d4784938e04bcff9320e07577ae Mon Sep 17 00:00:00 2001
From: Aleksandar Likic <aleksandar.likic@securekey.com>
Date: Fri, 14 Sep 2018 15:13:31 -0400
Subject: [PATCH] Ledger metrics

Change-Id: I3a8db2f1338f86af1efae625cb915a05c4481865
Signed-off-by: Aleksandar Likic <aleksandar.likic@securekey.com>
---
 bccsp/pkcs11/impl.go                          |  21 +
 bccsp/sw/impl.go                              |  25 +
 .../blkstorage/fsblkstorage/blockfile_mgr.go  |  16 +
 .../blkstorage/fsblkstorage/blockindex.go     |   7 +
 .../blkstorage/fsblkstorage/blocks_itr.go     |   8 +
 .../util/leveldbhelper/leveldb_helper.go      |  69 +++
 .../util/leveldbhelper/leveldb_provider.go    |  12 +-
 common/metrics/server.go                      | 271 +++++-----
 common/metrics/server_test.go                 | 242 ---------
 common/metrics/tally_provider.go              | 348 ++-----------
 common/metrics/tally_provider_test.go         | 462 ------------------
 common/metrics/types.go                       |  45 --
 .../blocksprovider/blocksprovider.go          |  10 +
 .../historyleveldb/historyleveldb.go          |   7 +
 core/ledger/kvledger/kv_ledger.go             |  26 +
 .../privacyenabledstate/common_storage_db.go  |   9 +
 .../txmgmt/pvtstatepurgemgmt/expiry_keeper.go |   7 +-
 .../txmgmt/statedb/statecouchdb/batch_util.go |  10 +
 .../statedb/statecouchdb/commit_handling.go   |   7 +
 .../statedb/statecouchdb/statecouchdb.go      |   7 +
 .../statedb/stateleveldb/stateleveldb.go      |   7 +
 .../txmgr/lockbasedtxmgr/lockbased_txmgr.go   |  11 +
 core/ledger/ledgerstorage/store.go            |  28 ++
 core/ledger/util/couchdb/couchdb.go           |  22 +-
 gossip/privdata/coordinator.go                |  11 +
 gossip/state/payloads_buffer.go               |   9 +-
 gossip/state/state.go                         |  27 +-
 peer/node/start.go                            |   4 +
 28 files changed, 547 insertions(+), 1181 deletions(-)
 delete mode 100644 common/metrics/server_test.go
 delete mode 100644 common/metrics/tally_provider_test.go
 delete mode 100644 common/metrics/types.go

diff --git a/bccsp/pkcs11/impl.go b/bccsp/pkcs11/impl.go
index 11f8e54c9..6594f9daa 100644
--- a/bccsp/pkcs11/impl.go
+++ b/bccsp/pkcs11/impl.go
@@ -15,6 +15,7 @@ import (
 	"github.com/hyperledger/fabric/bccsp"
 	"github.com/hyperledger/fabric/bccsp/sw"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/miekg/pkcs11"
 	"github.com/pkg/errors"
 )
@@ -154,6 +155,10 @@ func (csp *impl) KeyImport(raw interface{}, opts bccsp.KeyImportOpts) (k bccsp.K
 // GetKey returns the key this CSP associates to
 // the Subject Key Identifier ski.
 func (csp *impl) GetKey(ski []byte) (bccsp.Key, error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_getkey_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	pubKey, isPriv, err := csp.getECKey(ski)
 	if err == nil {
 		if isPriv {
@@ -171,6 +176,10 @@ func (csp *impl) GetKey(ski []byte) (bccsp.Key, error) {
 // the caller is responsible for hashing the larger message and passing
 // the hash (as digest).
 func (csp *impl) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) ([]byte, error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_sign_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil")
@@ -190,6 +199,10 @@ func (csp *impl) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) ([]byte
 
 // Verify verifies signature against key k and digest
 func (csp *impl) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.SignerOpts) (bool, error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_verify_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return false, errors.New("Invalid Key. It must not be nil")
@@ -215,6 +228,10 @@ func (csp *impl) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.Signer
 // Encrypt encrypts plaintext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *impl) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts) ([]byte, error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_encrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// TODO: Add PKCS11 support for encryption, when fabric starts requiring it
 	return csp.BCCSP.Encrypt(k, plaintext, opts)
 }
@@ -222,6 +239,10 @@ func (csp *impl) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts
 // Decrypt decrypts ciphertext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *impl) Decrypt(k bccsp.Key, ciphertext []byte, opts bccsp.DecrypterOpts) ([]byte, error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_decrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	return csp.BCCSP.Decrypt(k, ciphertext, opts)
 }
 
diff --git a/bccsp/sw/impl.go b/bccsp/sw/impl.go
index a7089476e..078c7b9f3 100644
--- a/bccsp/sw/impl.go
+++ b/bccsp/sw/impl.go
@@ -21,6 +21,7 @@ import (
 
 	"github.com/hyperledger/fabric/bccsp"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/pkg/errors"
 )
 
@@ -165,6 +166,10 @@ func (csp *CSP) KeyImport(raw interface{}, opts bccsp.KeyImportOpts) (k bccsp.Ke
 // GetKey returns the key this CSP associates to
 // the Subject Key Identifier ski.
 func (csp *CSP) GetKey(ski []byte) (k bccsp.Key, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_getkey_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	k, err = csp.ks.GetKey(ski)
 	if err != nil {
 		return nil, errors.Wrapf(err, "Failed getting key for SKI [%v]", ski)
@@ -175,6 +180,10 @@ func (csp *CSP) GetKey(ski []byte) (k bccsp.Key, err error) {
 
 // Hash hashes messages msg using options opts.
 func (csp *CSP) Hash(msg []byte, opts bccsp.HashOpts) (digest []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_hash_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if opts == nil {
 		return nil, errors.New("Invalid opts. It must not be nil.")
@@ -221,6 +230,10 @@ func (csp *CSP) GetHash(opts bccsp.HashOpts) (h hash.Hash, err error) {
 // the caller is responsible for hashing the larger message and passing
 // the hash (as digest).
 func (csp *CSP) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) (signature []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_sign_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil.")
@@ -245,6 +258,10 @@ func (csp *CSP) Sign(k bccsp.Key, digest []byte, opts bccsp.SignerOpts) (signatu
 
 // Verify verifies signature against key k and digest
 func (csp *CSP) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.SignerOpts) (valid bool, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_verify_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return false, errors.New("Invalid Key. It must not be nil.")
@@ -272,6 +289,10 @@ func (csp *CSP) Verify(k bccsp.Key, signature, digest []byte, opts bccsp.SignerO
 // Encrypt encrypts plaintext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *CSP) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts) ([]byte, error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_encrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil.")
@@ -288,6 +309,10 @@ func (csp *CSP) Encrypt(k bccsp.Key, plaintext []byte, opts bccsp.EncrypterOpts)
 // Decrypt decrypts ciphertext using key k.
 // The opts argument should be appropriate for the primitive used.
 func (csp *CSP) Decrypt(k bccsp.Key, ciphertext []byte, opts bccsp.DecrypterOpts) (plaintext []byte, err error) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("crypto_decrypt_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	// Validate arguments
 	if k == nil {
 		return nil, errors.New("Invalid Key. It must not be nil.")
diff --git a/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go b/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
index 761db0161..86cf7bf9d 100644
--- a/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
+++ b/common/ledger/blkstorage/fsblkstorage/blockfile_mgr.go
@@ -19,6 +19,7 @@ import (
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
 	putil "github.com/hyperledger/fabric/protos/utils"
@@ -238,6 +239,10 @@ func (mgr *blockfileMgr) moveToNextFile() {
 }
 
 func (mgr *blockfileMgr) addBlock(block *common.Block) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_addBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	bcInfo := mgr.getBlockchainInfo()
 	if block.Header.Number != bcInfo.Height {
 		return errors.Errorf(
@@ -432,6 +437,13 @@ func (mgr *blockfileMgr) getBlockchainInfo() *common.BlockchainInfo {
 }
 
 func (mgr *blockfileMgr) updateCheckpoint(cpInfo *checkpointInfo) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_updateCheckpoint_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	mgr.cpInfoCond.L.Lock()
 	defer mgr.cpInfoCond.L.Unlock()
 	mgr.cpInfo = cpInfo
@@ -594,6 +606,10 @@ func (mgr *blockfileMgr) loadCurrentInfo() (*checkpointInfo, error) {
 }
 
 func (mgr *blockfileMgr) saveCurrentInfo(i *checkpointInfo, sync bool) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_saveCurrentInfo_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	b, err := i.marshal()
 	if err != nil {
 		return err
diff --git a/common/ledger/blkstorage/fsblkstorage/blockindex.go b/common/ledger/blkstorage/fsblkstorage/blockindex.go
index 11a504638..d2fa19d70 100644
--- a/common/ledger/blkstorage/fsblkstorage/blockindex.go
+++ b/common/ledger/blkstorage/fsblkstorage/blockindex.go
@@ -14,6 +14,7 @@ import (
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	ledgerUtil "github.com/hyperledger/fabric/core/ledger/util"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
@@ -88,6 +89,12 @@ func (index *blockIndex) getLastBlockIndexed() (uint64, error) {
 }
 
 func (index *blockIndex) indexBlock(blockIdxInfo *blockIdxInfo) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_indexBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// do not index anything
 	if len(index.indexItemsMap) == 0 {
 		logger.Debug("Not indexing block... as nothing to index")
diff --git a/common/ledger/blkstorage/fsblkstorage/blocks_itr.go b/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
index 3e06887b6..28d00d944 100644
--- a/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
+++ b/common/ledger/blkstorage/fsblkstorage/blocks_itr.go
@@ -20,6 +20,7 @@ import (
 	"sync"
 
 	"github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/metrics"
 )
 
 // blocksItr - an iterator for iterating over a sequence of blocks
@@ -37,6 +38,13 @@ func newBlockItr(mgr *blockfileMgr, startBlockNum uint64) *blocksItr {
 }
 
 func (itr *blocksItr) waitForBlock(blockNum uint64) uint64 {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("fsblkstorage_waitForBlock_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	itr.mgr.cpInfoCond.L.Lock()
 	defer itr.mgr.cpInfoCond.L.Unlock()
 	for itr.mgr.cpInfo.lastBlockNumber < blockNum && !itr.shouldClose() {
diff --git a/common/ledger/util/leveldbhelper/leveldb_helper.go b/common/ledger/util/leveldbhelper/leveldb_helper.go
index 4dba0ba40..ac08a308c 100644
--- a/common/ledger/util/leveldbhelper/leveldb_helper.go
+++ b/common/ledger/util/leveldbhelper/leveldb_helper.go
@@ -10,9 +10,16 @@ import (
 	"fmt"
 	"sync"
 
+	"strings"
+
+	"bytes"
+	"time"
+
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/pkg/errors"
+	"github.com/hyperledger/fabric/common/metrics"
+	"github.com/spf13/viper"
 	"github.com/syndtr/goleveldb/leveldb"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
 	"github.com/syndtr/goleveldb/leveldb/opt"
@@ -79,6 +86,9 @@ func (dbInst *DB) Open() {
 		panic(fmt.Sprintf("Error opening leveldb: %s", err))
 	}
 	dbInst.dbState = opened
+	if viper.GetBool("logging.leveldbState") {
+		go dbInst.getState()
+	}
 }
 
 // Close closes the underlying db
@@ -96,6 +106,18 @@ func (dbInst *DB) Close() {
 
 // Get returns the value for the given key
 func (dbInst *DB) Get(key []byte) ([]byte, error) {
+	if metrics.IsDebug() {
+		dbName := dbInst.conf.DBPath
+		if strings.Contains(dbName, "ledgersData/") {
+			dbName = metrics.FilterMetricName(strings.Split(dbName, "ledgersData/")[1])
+		} else {
+			dbName = metrics.FilterMetricName(dbName)
+		}
+		ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_get_%s_processing_time_seconds", dbName))
+		ccStopWatch := ccTimer.Start()
+		defer ccStopWatch.Stop()
+	}
+
 	value, err := dbInst.db.Get(key, dbInst.readOpts)
 	if err == leveldb.ErrNotFound {
 		value = nil
@@ -110,6 +132,17 @@ func (dbInst *DB) Get(key []byte) ([]byte, error) {
 
 // Put saves the key/value
 func (dbInst *DB) Put(key []byte, value []byte, sync bool) error {
+	if metrics.IsDebug() {
+		dbName := dbInst.conf.DBPath
+		if strings.Contains(dbName, "ledgersData/") {
+			dbName = metrics.FilterMetricName(strings.Split(dbName, "ledgersData/")[1])
+		} else {
+			dbName = metrics.FilterMetricName(dbName)
+		}
+		ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_put_%s_processing_time_seconds", dbName))
+		ccStopWatch := ccTimer.Start()
+		defer ccStopWatch.Stop()
+	}
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -124,6 +157,17 @@ func (dbInst *DB) Put(key []byte, value []byte, sync bool) error {
 
 // Delete deletes the given key
 func (dbInst *DB) Delete(key []byte, sync bool) error {
+	if metrics.IsDebug() {
+		dbName := dbInst.conf.DBPath
+		if strings.Contains(dbName, "ledgersData/") {
+			dbName = metrics.FilterMetricName(strings.Split(dbName, "ledgersData/")[1])
+		} else {
+			dbName = metrics.FilterMetricName(dbName)
+		}
+		ccTimer := metrics.RootScope.Timer(fmt.Sprintf("leveldb_delete_%s_processing_time_seconds", dbName))
+		ccStopWatch := ccTimer.Start()
+		defer ccStopWatch.Stop()
+	}
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -145,6 +189,10 @@ func (dbInst *DB) GetIterator(startKey []byte, endKey []byte) iterator.Iterator
 
 // WriteBatch writes a batch
 func (dbInst *DB) WriteBatch(batch *leveldb.Batch, sync bool) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("leveldbhelper_db_WriteBatch_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	wo := dbInst.writeOptsNoSync
 	if sync {
 		wo = dbInst.writeOptsSync
@@ -154,3 +202,24 @@ func (dbInst *DB) WriteBatch(batch *leveldb.Batch, sync bool) error {
 	}
 	return nil
 }
+
+func (dbInst *DB) getState() {
+	for {
+		time.Sleep(5 * time.Second)
+		if dbInst.dbState == closed {
+			logger.Info("leveldb is closed exit the getState")
+			break
+		}
+		levelDBStats := []string{"stats", "iostats", "writedelay", "sstables", "blockpool", "cachedblock", "openedtables", "alivesnaps", "aliveiters"}
+		var b bytes.Buffer
+		for _, stats := range levelDBStats {
+			res, err := dbInst.db.GetProperty(fmt.Sprintf("leveldb.%s", stats))
+			if err != nil {
+				logger.Errorf("leveldb getState %s return error %s", stats, err)
+				continue
+			}
+			b.WriteString(res)
+		}
+		logger.Infof("******* leveldb getState %s", strings.Replace(b.String(), "\n", " ", -1))
+	}
+}
diff --git a/common/ledger/util/leveldbhelper/leveldb_provider.go b/common/ledger/util/leveldbhelper/leveldb_provider.go
index 388a18e3d..d82664cf3 100644
--- a/common/ledger/util/leveldbhelper/leveldb_provider.go
+++ b/common/ledger/util/leveldbhelper/leveldb_provider.go
@@ -22,6 +22,8 @@ import (
 
 	"github.com/syndtr/goleveldb/leveldb"
 	"github.com/syndtr/goleveldb/leveldb/iterator"
+	"github.com/hyperledger/fabric/common/metrics"
+
 )
 
 var dbNameKeySep = []byte{0x00}
@@ -32,13 +34,14 @@ type Provider struct {
 	db        *DB
 	dbHandles map[string]*DBHandle
 	mux       sync.Mutex
+	dbPath    string
 }
 
 // NewProvider constructs a Provider
 func NewProvider(conf *Conf) *Provider {
 	db := CreateDB(conf)
 	db.Open()
-	return &Provider{db, make(map[string]*DBHandle), sync.Mutex{}}
+	return &Provider{db, make(map[string]*DBHandle), sync.Mutex{}, conf.DBPath}
 }
 
 // GetDBHandle returns a handle to a named db
@@ -47,7 +50,7 @@ func (p *Provider) GetDBHandle(dbName string) *DBHandle {
 	defer p.mux.Unlock()
 	dbHandle := p.dbHandles[dbName]
 	if dbHandle == nil {
-		dbHandle = &DBHandle{dbName, p.db}
+		dbHandle = &DBHandle{dbName, p.db, p.dbPath}
 		p.dbHandles[dbName] = dbHandle
 	}
 	return dbHandle
@@ -62,6 +65,7 @@ func (p *Provider) Close() {
 type DBHandle struct {
 	dbName string
 	db     *DB
+	dbPath string
 }
 
 // Get returns the value for the given key
@@ -81,6 +85,10 @@ func (h *DBHandle) Delete(key []byte, sync bool) error {
 
 // WriteBatch writes a batch in an atomic way
 func (h *DBHandle) WriteBatch(batch *UpdateBatch, sync bool) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("leveldbhelper_dbhandle_WriteBatch_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	if len(batch.KVs) == 0 {
 		return nil
 	}
diff --git a/common/metrics/server.go b/common/metrics/server.go
index 677151e78..fc42eac09 100644
--- a/common/metrics/server.go
+++ b/common/metrics/server.go
@@ -8,11 +8,21 @@ package metrics
 
 import (
 	"fmt"
-	"sync"
+	"io"
 	"time"
 
+	"sync"
+
+	"strings"
+
+	"runtime"
+
+	"regexp"
+
+	"github.com/pkg/errors"
 	"github.com/spf13/viper"
 	"github.com/uber-go/tally"
+	promreporter "github.com/uber-go/tally/prometheus"
 )
 
 const (
@@ -28,21 +38,112 @@ const (
 	defaultStatsdReporterFlushBytes    = 1432
 )
 
-var RootScope Scope
-var once sync.Once
+const (
+	peerConfigFileName = "core"
+	peerConfigPath     = "/etc/hyperledger/fabric"
+	cmdRootPrefix      = "core"
+)
+
+var peerConfig *viper.Viper
+var peerConfigPathOverride string
+
+// RootScope tally.NoopScope is a scope that does nothing
+var RootScope = tally.NoopScope
 var rootScopeMutex = &sync.Mutex{}
 var running bool
+var debugOn bool
+
+// StatsdReporterOpts ...
+type StatsdReporterOpts struct {
+	Address       string
+	Prefix        string
+	FlushInterval time.Duration
+	FlushBytes    int
+}
+
+// PromReporterOpts ...
+type PromReporterOpts struct {
+	ListenAddress string
+}
+
+// Opts ...
+type Opts struct {
+	Reporter           string
+	Interval           time.Duration
+	Enabled            bool
+	StatsdReporterOpts StatsdReporterOpts
+	PromReporterOpts   PromReporterOpts
+}
+
+// IsDebug ...
+func IsDebug() bool {
+	return debugOn
+}
+
+var reg *regexp.Regexp
 
-// NewOpts create metrics options based config file
-func NewOpts() Opts {
+// Initialize ...
+func Initialize() {
+
+	// load peer config
+	if err := initPeerConfig(); err != nil {
+		panic(fmt.Sprintf("error initPeerConfig %v", err))
+	}
+
+	if peerConfig.GetBool("peer.profile.enabled") {
+		runtime.SetMutexProfileFraction(5)
+	}
+	if peerConfig.GetBool("metrics.enabled") {
+		debugOn = peerConfig.GetBool("metrics.debug.enabled")
+	}
+
+	// start metric server
+	opts := NewOpts(peerConfig)
+	err := Start(opts)
+	if err != nil {
+		logger.Errorf("Failed to start metrics collection: %s", err)
+	}
+
+	reg = regexp.MustCompile("[^a-zA-Z0-9_]+")
+
+	logger.Info("Fabric Bootstrap filter initialized")
+}
+
+func FilterMetricName(name string) string {
+	return reg.ReplaceAllString(name, "_")
+}
+
+func initPeerConfig() error {
+	peerConfig = viper.New()
+	peerConfig.AddConfigPath(peerConfigPath)
+	if peerConfigPathOverride != "" {
+		peerConfig.AddConfigPath(peerConfigPathOverride)
+	}
+	peerConfig.SetConfigName(peerConfigFileName)
+	peerConfig.SetEnvPrefix(cmdRootPrefix)
+	peerConfig.AutomaticEnv()
+	peerConfig.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
+	err := peerConfig.ReadInConfig()
+	if err != nil {
+		return err
+	}
+
+	return nil
+}
+
+// NewOpts create metrics options based config file.
+// TODO: Currently this is only for peer node which uses global viper.
+// As for orderer, which uses its local viper, we are unable to get
+// metrics options with the function NewOpts()
+func NewOpts(peerConfig *viper.Viper) Opts {
 	opts := Opts{}
-	opts.Enabled = viper.GetBool("metrics.enabled")
-	if report := viper.GetString("metrics.reporter"); report != "" {
+	opts.Enabled = peerConfig.GetBool("metrics.enabled")
+	if report := peerConfig.GetString("metrics.reporter"); report != "" {
 		opts.Reporter = report
 	} else {
 		opts.Reporter = defaultReporterType
 	}
-	if interval := viper.GetDuration("metrics.interval"); interval > 0 {
+	if interval := peerConfig.GetDuration("metrics.interval"); interval > 0 {
 		opts.Interval = interval
 	} else {
 		opts.Interval = defaultInterval
@@ -50,13 +151,17 @@ func NewOpts() Opts {
 
 	if opts.Reporter == statsdReporterType {
 		statsdOpts := StatsdReporterOpts{}
-		statsdOpts.Address = viper.GetString("metrics.statsdReporter.address")
-		if flushInterval := viper.GetDuration("metrics.statsdReporter.flushInterval"); flushInterval > 0 {
+		statsdOpts.Address = peerConfig.GetString("metrics.statsdReporter.address")
+		statsdOpts.Prefix = peerConfig.GetString("metrics.statsdReporter.prefix")
+		if statsdOpts.Prefix == "" && !peerConfig.IsSet("peer.id") {
+			statsdOpts.Prefix = peerConfig.GetString("peer.id")
+		}
+		if flushInterval := peerConfig.GetDuration("metrics.statsdReporter.flushInterval"); flushInterval > 0 {
 			statsdOpts.FlushInterval = flushInterval
 		} else {
 			statsdOpts.FlushInterval = defaultStatsdReporterFlushInterval
 		}
-		if flushBytes := viper.GetInt("metrics.statsdReporter.flushBytes"); flushBytes > 0 {
+		if flushBytes := peerConfig.GetInt("metrics.statsdReporter.flushBytes"); flushBytes > 0 {
 			statsdOpts.FlushBytes = flushBytes
 		} else {
 			statsdOpts.FlushBytes = defaultStatsdReporterFlushBytes
@@ -66,45 +171,47 @@ func NewOpts() Opts {
 
 	if opts.Reporter == promReporterType {
 		promOpts := PromReporterOpts{}
-		promOpts.ListenAddress = viper.GetString("metrics.promReporter.listenAddress")
+		promOpts.ListenAddress = peerConfig.GetString("metrics.fabric.PromReporter.listenAddress")
 		opts.PromReporterOpts = promOpts
 	}
 
 	return opts
 }
 
-//Init initializes global root metrics scope instance, all callers can only use it to extend sub scope
-func Init(opts Opts) (err error) {
-	once.Do(func() {
-		RootScope, err = create(opts)
-	})
-
-	return
-}
-
-//Start starts metrics server
-func Start() error {
+// Start starts metrics server
+func Start(opts Opts) error {
+	if !opts.Enabled {
+		return errors.New("Unable to start metrics server because is disbled")
+	}
 	rootScopeMutex.Lock()
 	defer rootScopeMutex.Unlock()
-	if running {
-		return nil
+	if !running {
+		rootScope, err := create(opts)
+		if err == nil {
+			running = true
+			RootScope = rootScope
+		}
+		return err
 	}
-	running = true
-	return RootScope.Start()
+	return errors.New("metrics server was already started")
 }
 
-//Shutdown closes underlying resources used by metrics server
+// Shutdown closes underlying resources used by metrics server
 func Shutdown() error {
 	rootScopeMutex.Lock()
 	defer rootScopeMutex.Unlock()
-	if !running {
-		return nil
+	if running {
+		var err error
+		if closer, ok := RootScope.(io.Closer); ok {
+			if err = closer.Close(); err != nil {
+				return err
+			}
+		}
+		running = false
+		RootScope = tally.NoopScope
+		return err
 	}
-
-	err := RootScope.Close()
-	RootScope = nil
-	running = false
-	return err
+	return nil
 }
 
 func isRunning() bool {
@@ -113,109 +220,35 @@ func isRunning() bool {
 	return running
 }
 
-type StatsdReporterOpts struct {
-	Address       string
-	FlushInterval time.Duration
-	FlushBytes    int
-}
-
-type PromReporterOpts struct {
-	ListenAddress string
-}
-
-type Opts struct {
-	Reporter           string
-	Interval           time.Duration
-	Enabled            bool
-	StatsdReporterOpts StatsdReporterOpts
-	PromReporterOpts   PromReporterOpts
-}
-
-type noOpCounter struct {
-}
-
-func (c *noOpCounter) Inc(v int64) {
-
-}
-
-type noOpGauge struct {
-}
-
-func (g *noOpGauge) Update(v float64) {
-
-}
-
-type noOpScope struct {
-	counter *noOpCounter
-	gauge   *noOpGauge
-}
-
-func (s *noOpScope) Counter(name string) Counter {
-	return s.counter
-}
-
-func (s *noOpScope) Gauge(name string) Gauge {
-	return s.gauge
-}
-
-func (s *noOpScope) Tagged(tags map[string]string) Scope {
-	return s
-}
-
-func (s *noOpScope) SubScope(prefix string) Scope {
-	return s
-}
-
-func (s *noOpScope) Close() error {
-	return nil
-}
-
-func (s *noOpScope) Start() error {
-	return nil
-}
-
-func newNoOpScope() Scope {
-	return &noOpScope{
-		counter: &noOpCounter{},
-		gauge:   &noOpGauge{},
-	}
-}
-
-func create(opts Opts) (rootScope Scope, e error) {
+func create(opts Opts) (rootScope tally.Scope, e error) {
 	if !opts.Enabled {
-		rootScope = newNoOpScope()
-		return
+		rootScope = tally.NoopScope
 	} else {
 		if opts.Interval <= 0 {
 			e = fmt.Errorf("invalid Interval option %d", opts.Interval)
 			return
 		}
-
-		if opts.Reporter != statsdReporterType && opts.Reporter != promReporterType {
-			e = fmt.Errorf("not supported Reporter type %s", opts.Reporter)
-			return
-		}
-
 		var reporter tally.StatsReporter
 		var cachedReporter tally.CachedStatsReporter
-		if opts.Reporter == statsdReporterType {
+		switch opts.Reporter {
+		case statsdReporterType:
 			reporter, e = newStatsdReporter(opts.StatsdReporterOpts)
-		}
-
-		if opts.Reporter == promReporterType {
+		case promReporterType:
 			cachedReporter, e = newPromReporter(opts.PromReporterOpts)
+		default:
+			e = fmt.Errorf("not supported Reporter type %s", opts.Reporter)
+			return
 		}
-
 		if e != nil {
 			return
 		}
-
 		rootScope = newRootScope(
 			tally.ScopeOptions{
 				Prefix:         namespace,
 				Reporter:       reporter,
 				CachedReporter: cachedReporter,
+				Separator:      promreporter.DefaultSeparator,
 			}, opts.Interval)
-		return
 	}
+	return
 }
diff --git a/common/metrics/server_test.go b/common/metrics/server_test.go
deleted file mode 100644
index 07d010ca8..000000000
--- a/common/metrics/server_test.go
+++ /dev/null
@@ -1,242 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import (
-	"fmt"
-	"strings"
-	"testing"
-	"time"
-
-	"github.com/hyperledger/fabric/core/config/configtest"
-	. "github.com/onsi/gomega"
-	"github.com/spf13/viper"
-	"github.com/stretchr/testify/assert"
-)
-
-func TestStartSuccessStatsd(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: statsdReporterType,
-		Interval: 1 * time.Second,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		}}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartSuccessProm(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: promReporterType,
-		Interval: 1 * time.Second,
-		PromReporterOpts: PromReporterOpts{
-			ListenAddress: "127.0.0.1:0",
-		}}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartDisabled(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled: false,
-	}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-}
-
-func TestStartInvalidInterval(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 0,
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidAddress(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidFlushInterval(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 0,
-			FlushBytes:    512,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartPromInvalidListernAddress(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		PromReporterOpts: PromReporterOpts{
-			ListenAddress: "",
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartStatsdInvalidFlushBytes(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: statsdReporterType,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    0,
-		},
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartInvalidReporter(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled:  true,
-		Interval: 1 * time.Second,
-		Reporter: "test",
-	}
-	s, err := create(opts)
-	assert.Nil(t, s)
-	assert.Error(t, err)
-}
-
-func TestStartAndClose(t *testing.T) {
-	t.Parallel()
-	gt := NewGomegaWithT(t)
-	defer Shutdown()
-	opts := Opts{
-		Enabled:  true,
-		Reporter: statsdReporterType,
-		Interval: 1 * time.Second,
-		StatsdReporterOpts: StatsdReporterOpts{
-			Address:       "127.0.0.1:0",
-			FlushInterval: 2 * time.Second,
-			FlushBytes:    512,
-		}}
-	Init(opts)
-	assert.NotNil(t, RootScope)
-	go Start()
-	gt.Eventually(isRunning).Should(BeTrue())
-}
-
-func TestNoOpScopeMetrics(t *testing.T) {
-	t.Parallel()
-	opts := Opts{
-		Enabled: false,
-	}
-	s, err := create(opts)
-	go s.Start()
-	defer s.Close()
-	assert.NotNil(t, s)
-	assert.NoError(t, err)
-
-	// make sure no error throws when invoke noOpScope
-	subScope := s.SubScope("test")
-	subScope.Counter("foo").Inc(2)
-	subScope.Gauge("bar").Update(1.33)
-	tagSubScope := subScope.Tagged(map[string]string{"env": "test"})
-	tagSubScope.Counter("foo").Inc(2)
-	tagSubScope.Gauge("bar").Update(1.33)
-}
-
-func TestNewOpts(t *testing.T) {
-	t.Parallel()
-	defer viper.Reset()
-	setupTestConfig()
-	opts := NewOpts()
-	assert.False(t, opts.Enabled)
-	assert.Equal(t, 1*time.Second, opts.Interval)
-	assert.Equal(t, statsdReporterType, opts.Reporter)
-	assert.Equal(t, 1432, opts.StatsdReporterOpts.FlushBytes)
-	assert.Equal(t, 2*time.Second, opts.StatsdReporterOpts.FlushInterval)
-	assert.Equal(t, "0.0.0.0:8125", opts.StatsdReporterOpts.Address)
-	viper.Reset()
-
-	setupTestConfig()
-	viper.Set("metrics.Reporter", promReporterType)
-	opts1 := NewOpts()
-	assert.False(t, opts1.Enabled)
-	assert.Equal(t, 1*time.Second, opts1.Interval)
-	assert.Equal(t, promReporterType, opts1.Reporter)
-	assert.Equal(t, "0.0.0.0:8080", opts1.PromReporterOpts.ListenAddress)
-}
-
-func TestNewOptsDefaultVar(t *testing.T) {
-	t.Parallel()
-	opts := NewOpts()
-	assert.False(t, opts.Enabled)
-	assert.Equal(t, 1*time.Second, opts.Interval)
-	assert.Equal(t, statsdReporterType, opts.Reporter)
-	assert.Equal(t, 1432, opts.StatsdReporterOpts.FlushBytes)
-	assert.Equal(t, 2*time.Second, opts.StatsdReporterOpts.FlushInterval)
-}
-
-func setupTestConfig() {
-	viper.SetConfigName("core")
-	viper.SetEnvPrefix("CORE")
-	viper.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
-	viper.AutomaticEnv()
-
-	err := configtest.AddDevConfigPath(nil)
-	if err != nil {
-		panic(fmt.Errorf("Fatal error adding dev dir: %s \n", err))
-	}
-
-	err = viper.ReadInConfig()
-	if err != nil { // Handle errors reading the config file
-		panic(fmt.Errorf("Fatal error config file: %s \n", err))
-	}
-}
diff --git a/common/metrics/tally_provider.go b/common/metrics/tally_provider.go
index b83eba88c..83a775a7d 100644
--- a/common/metrics/tally_provider.go
+++ b/common/metrics/tally_provider.go
@@ -7,15 +7,14 @@ SPDX-License-Identifier: Apache-2.0
 package metrics
 
 import (
-	"context"
 	"errors"
-	"fmt"
-	"io"
 	"net/http"
-	"sort"
-	"sync"
 	"time"
 
+	"net"
+
+	"sort"
+
 	"github.com/cactus/go-statsd-client/statsd"
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/prometheus/client_golang/prometheus"
@@ -49,50 +48,9 @@ func newGauge(tallyGauge tally.Gauge) *gauge {
 	return &gauge{tallyGauge: tallyGauge}
 }
 
-func (g *gauge) Update(v float64) {
-	g.tallyGauge.Update(v)
-}
-
-type scopeRegistry struct {
-	sync.RWMutex
-	subScopes map[string]*scope
-}
-
-type scope struct {
-	separator    string
-	prefix       string
-	tags         map[string]string
-	tallyScope   tally.Scope
-	registry     *scopeRegistry
-	baseReporter tally.BaseStatsReporter
-
-	cm sync.RWMutex
-	gm sync.RWMutex
-
-	counters map[string]*counter
-	gauges   map[string]*gauge
-}
-
-func newRootScope(opts tally.ScopeOptions, interval time.Duration) Scope {
+func newRootScope(opts tally.ScopeOptions, interval time.Duration) tally.Scope {
 	s, _ := tally.NewRootScope(opts, interval)
-
-	var baseReporter tally.BaseStatsReporter
-	if opts.Reporter != nil {
-		baseReporter = opts.Reporter
-	} else if opts.CachedReporter != nil {
-		baseReporter = opts.CachedReporter
-	}
-
-	return &scope{
-		prefix:     opts.Prefix,
-		separator:  opts.Separator,
-		tallyScope: s,
-		registry: &scopeRegistry{
-			subScopes: make(map[string]*scope),
-		},
-		baseReporter: baseReporter,
-		counters:     make(map[string]*counter),
-		gauges:       make(map[string]*gauge)}
+	return s
 }
 
 func newStatsdReporter(statsdReporterOpts StatsdReporterOpts) (tally.StatsReporter, error) {
@@ -109,13 +67,13 @@ func newStatsdReporter(statsdReporterOpts StatsdReporterOpts) (tally.StatsReport
 	}
 
 	statter, err := statsd.NewBufferedClient(statsdReporterOpts.Address,
-		"", statsdReporterOpts.FlushInterval, statsdReporterOpts.FlushBytes)
+		statsdReporterOpts.Prefix, statsdReporterOpts.FlushInterval, statsdReporterOpts.FlushBytes)
 	if err != nil {
 		return nil, err
 	}
 	opts := statsdreporter.Options{}
 	reporter := statsdreporter.NewReporter(statter, opts)
-	statsdReporter := &statsdReporter{reporter: reporter, statter: statter}
+	statsdReporter := &statsdReporter{StatsReporter: reporter, statter: statter}
 	return statsdReporter, nil
 }
 
@@ -127,159 +85,52 @@ func newPromReporter(promReporterOpts PromReporterOpts) (promreporter.Reporter,
 	opts := promreporter.Options{Registerer: prometheus.NewRegistry()}
 	reporter := promreporter.NewReporter(opts)
 	mux := http.NewServeMux()
-	handler := promReporterHttpHandler(opts.Registerer.(*prometheus.Registry))
+	handler := promReporterHTTPHandler(opts.Registerer.(*prometheus.Registry))
 	mux.Handle("/metrics", handler)
-	server := &http.Server{Addr: promReporterOpts.ListenAddress, Handler: mux}
+	server := &http.Server{Handler: mux}
+	addr := promReporterOpts.ListenAddress
+	if addr == "" {
+		addr = ":http"
+	}
+	listener, err := net.Listen("tcp", addr)
+	if err != nil {
+		return nil, err
+	}
 	promReporter := &promReporter{
-		reporter: reporter,
+		Reporter: reporter,
 		server:   server,
-		registry: opts.Registerer.(*prometheus.Registry)}
+		registry: opts.Registerer.(*prometheus.Registry),
+		listener: listener}
+	go server.Serve(listener)
 	return promReporter, nil
 }
 
-func (s *scope) Counter(name string) Counter {
-	s.cm.RLock()
-	val, ok := s.counters[name]
-	s.cm.RUnlock()
-	if !ok {
-		s.cm.Lock()
-		val, ok = s.counters[name]
-		if !ok {
-			counter := s.tallyScope.Counter(name)
-			val = newCounter(counter)
-			s.counters[name] = val
-		}
-		s.cm.Unlock()
-	}
-	return val
-}
-
-func (s *scope) Gauge(name string) Gauge {
-	s.gm.RLock()
-	val, ok := s.gauges[name]
-	s.gm.RUnlock()
-	if !ok {
-		s.gm.Lock()
-		val, ok = s.gauges[name]
-		if !ok {
-			gauge := s.tallyScope.Gauge(name)
-			val = newGauge(gauge)
-			s.gauges[name] = val
-		}
-		s.gm.Unlock()
-	}
-	return val
-}
-
-func (s *scope) Tagged(tags map[string]string) Scope {
-	originTags := tags
-	tags = mergeRightTags(s.tags, tags)
-	key := scopeRegistryKey(s.prefix, tags)
-
-	s.registry.RLock()
-	existing, ok := s.registry.subScopes[key]
-	if ok {
-		s.registry.RUnlock()
-		return existing
-	}
-	s.registry.RUnlock()
-
-	s.registry.Lock()
-	defer s.registry.Unlock()
-
-	existing, ok = s.registry.subScopes[key]
-	if ok {
-		return existing
-	}
-
-	subScope := &scope{
-		separator: s.separator,
-		prefix:    s.prefix,
-		// NB(r): Take a copy of the tags on creation
-		// so that it cannot be modified after set.
-		tags:       copyStringMap(tags),
-		tallyScope: s.tallyScope.Tagged(originTags),
-		registry:   s.registry,
-
-		counters: make(map[string]*counter),
-		gauges:   make(map[string]*gauge),
-	}
-
-	s.registry.subScopes[key] = subScope
-	return subScope
-}
-
-func (s *scope) SubScope(prefix string) Scope {
-	key := scopeRegistryKey(s.fullyQualifiedName(prefix), s.tags)
-
-	s.registry.RLock()
-	existing, ok := s.registry.subScopes[key]
-	if ok {
-		s.registry.RUnlock()
-		return existing
-	}
-	s.registry.RUnlock()
-
-	s.registry.Lock()
-	defer s.registry.Unlock()
-
-	existing, ok = s.registry.subScopes[key]
-	if ok {
-		return existing
-	}
-
-	subScope := &scope{
-		separator: s.separator,
-		prefix:    s.prefix,
-		// NB(r): Take a copy of the tags on creation
-		// so that it cannot be modified after set.
-		tags:       copyStringMap(s.tags),
-		tallyScope: s.tallyScope.SubScope(prefix),
-		registry:   s.registry,
-
-		counters: make(map[string]*counter),
-		gauges:   make(map[string]*gauge),
-	}
-
-	s.registry.subScopes[key] = subScope
-	return subScope
-}
-
-func (s *scope) Close() error {
-	if closer, ok := s.tallyScope.(io.Closer); ok {
-		return closer.Close()
-	}
-	return nil
-}
-
-func (s *scope) Start() error {
-	if server, ok := s.baseReporter.(serve); ok {
-		return server.Start()
-	}
-	return nil
-}
-
 type statsdReporter struct {
-	reporter tally.StatsReporter
-	statter  statsd.Statter
+	tally.StatsReporter
+	statter statsd.Statter
 }
 
 type promReporter struct {
-	reporter promreporter.Reporter
+	promreporter.Reporter
 	server   *http.Server
+	listener net.Listener
 	registry *prometheus.Registry
 }
 
+func (r *statsdReporter) Close() error {
+	return r.statter.Close()
+}
+
 func (r *statsdReporter) ReportCounter(name string, tags map[string]string, value int64) {
-	r.reporter.ReportCounter(tagsToName(name, tags), tags, value)
+	r.StatsReporter.ReportCounter(tagsToName(name, tags), tags, value)
 }
 
 func (r *statsdReporter) ReportGauge(name string, tags map[string]string, value float64) {
-	r.reporter.ReportGauge(tagsToName(name, tags), tags, value)
+	r.StatsReporter.ReportGauge(tagsToName(name, tags), tags, value)
 }
 
 func (r *statsdReporter) ReportTimer(name string, tags map[string]string, interval time.Duration) {
-	r.reporter.ReportTimer(tagsToName(name, tags), tags, interval)
+	r.StatsReporter.ReportTimer(tagsToName(name, tags), tags, interval)
 }
 
 func (r *statsdReporter) ReportHistogramValueSamples(
@@ -290,7 +141,7 @@ func (r *statsdReporter) ReportHistogramValueSamples(
 	bucketUpperBound float64,
 	samples int64,
 ) {
-	r.reporter.ReportHistogramValueSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
+	r.StatsReporter.ReportHistogramValueSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
 }
 
 func (r *statsdReporter) ReportHistogramDurationSamples(
@@ -301,7 +152,7 @@ func (r *statsdReporter) ReportHistogramDurationSamples(
 	bucketUpperBound time.Duration,
 	samples int64,
 ) {
-	r.reporter.ReportHistogramDurationSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
+	r.StatsReporter.ReportHistogramDurationSamples(tagsToName(name, tags), tags, buckets, bucketLowerBound, bucketUpperBound, samples)
 }
 
 func (r *statsdReporter) Capabilities() tally.Capabilities {
@@ -316,127 +167,20 @@ func (r *statsdReporter) Tagging() bool {
 	return true
 }
 
-func (r *statsdReporter) Flush() {
-	// no-op
-}
-
-func (r *statsdReporter) Close() error {
-	return r.statter.Close()
-}
-
-func (r *promReporter) RegisterCounter(
-	name string,
-	tagKeys []string,
-	desc string,
-) (*prometheus.CounterVec, error) {
-	return r.reporter.RegisterCounter(name, tagKeys, desc)
-}
-
-// AllocateCounter implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateCounter(name string, tags map[string]string) tally.CachedCount {
-	return r.reporter.AllocateCounter(name, tags)
-}
-
-func (r *promReporter) RegisterGauge(
-	name string,
-	tagKeys []string,
-	desc string,
-) (*prometheus.GaugeVec, error) {
-	return r.reporter.RegisterGauge(name, tagKeys, desc)
-}
-
-// AllocateGauge implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateGauge(name string, tags map[string]string) tally.CachedGauge {
-	return r.reporter.AllocateGauge(name, tags)
-}
-
-func (r *promReporter) RegisterTimer(
-	name string,
-	tagKeys []string,
-	desc string,
-	opts *promreporter.RegisterTimerOptions,
-) (promreporter.TimerUnion, error) {
-	return r.reporter.RegisterTimer(name, tagKeys, desc, opts)
-}
-
-// AllocateTimer implements tally.CachedStatsReporter.
-func (r *promReporter) AllocateTimer(name string, tags map[string]string) tally.CachedTimer {
-	return r.reporter.AllocateTimer(name, tags)
-}
-
-func (r *promReporter) AllocateHistogram(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-) tally.CachedHistogram {
-	return r.reporter.AllocateHistogram(name, tags, buckets)
-}
-
-func (r *promReporter) Capabilities() tally.Capabilities {
-	return r
-}
-
-func (r *promReporter) Reporting() bool {
-	return true
-}
-
-func (r *promReporter) Tagging() bool {
-	return true
-}
-
-// Flush does nothing for prometheus
-func (r *promReporter) Flush() {
-
-}
-
 func (r *promReporter) Close() error {
-	//TODO: Timeout here?
-	return r.server.Shutdown(context.Background())
-}
-
-func (r *promReporter) Start() error {
-	return r.server.ListenAndServe()
+	//TODO: Shutdown server gracefully?
+	// Close() is not a graceful way since it closes server immediately
+	err := r.server.Close()
+	r.listener.Close()
+	return err
 }
 
 func (r *promReporter) HTTPHandler() http.Handler {
-	return promReporterHttpHandler(r.registry)
-}
-
-func (s *scope) fullyQualifiedName(name string) string {
-	if len(s.prefix) == 0 {
-		return name
-	}
-	return fmt.Sprintf("%s%s%s", s.prefix, s.separator, name)
+	return promReporterHTTPHandler(r.registry)
 }
 
-// mergeRightTags merges 2 sets of tags with the tags from tagsRight overriding values from tagsLeft
-func mergeRightTags(tagsLeft, tagsRight map[string]string) map[string]string {
-	if tagsLeft == nil && tagsRight == nil {
-		return nil
-	}
-	if len(tagsRight) == 0 {
-		return tagsLeft
-	}
-	if len(tagsLeft) == 0 {
-		return tagsRight
-	}
-
-	result := make(map[string]string, len(tagsLeft)+len(tagsRight))
-	for k, v := range tagsLeft {
-		result[k] = v
-	}
-	for k, v := range tagsRight {
-		result[k] = v
-	}
-	return result
-}
-
-func copyStringMap(stringMap map[string]string) map[string]string {
-	result := make(map[string]string, len(stringMap))
-	for k, v := range stringMap {
-		result[k] = v
-	}
-	return result
+func promReporterHTTPHandler(registry *prometheus.Registry) http.Handler {
+	return promhttp.HandlerFor(registry, promhttp.HandlerOpts{})
 }
 
 func tagsToName(name string, tags map[string]string) string {
@@ -447,12 +191,8 @@ func tagsToName(name string, tags map[string]string) string {
 	sort.Strings(keys)
 
 	for _, k := range keys {
-		name = name + tally.DefaultSeparator + k + "-" + tags[k]
+		name = name + promreporter.DefaultSeparator + k + "-" + tags[k]
 	}
 
 	return name
 }
-
-func promReporterHttpHandler(registry *prometheus.Registry) http.Handler {
-	return promhttp.HandlerFor(registry, promhttp.HandlerOpts{})
-}
diff --git a/common/metrics/tally_provider_test.go b/common/metrics/tally_provider_test.go
deleted file mode 100644
index 9e911c275..000000000
--- a/common/metrics/tally_provider_test.go
+++ /dev/null
@@ -1,462 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import (
-	"fmt"
-	"io"
-	"io/ioutil"
-	"net"
-	"net/http"
-	"strings"
-	"sync"
-	"sync/atomic"
-	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/uber-go/tally"
-	promreporter "github.com/uber-go/tally/prometheus"
-)
-
-const (
-	statsdAddress = "127.0.0.1:8125"
-	promAddress   = "127.0.0.1:8082"
-)
-
-type testIntValue struct {
-	val      int64
-	tags     map[string]string
-	reporter *testStatsReporter
-}
-
-func (m *testIntValue) ReportCount(value int64) {
-	m.val = value
-	m.reporter.cg.Done()
-}
-
-type testFloatValue struct {
-	val      float64
-	tags     map[string]string
-	reporter *testStatsReporter
-}
-
-func (m *testFloatValue) ReportGauge(value float64) {
-	m.val = value
-	m.reporter.gg.Done()
-}
-
-type testStatsReporter struct {
-	cg sync.WaitGroup
-	gg sync.WaitGroup
-
-	scope Scope
-
-	counters map[string]*testIntValue
-	gauges   map[string]*testFloatValue
-
-	flushes int32
-}
-
-// newTestStatsReporter returns a new TestStatsReporter
-func newTestStatsReporter() *testStatsReporter {
-	return &testStatsReporter{
-		counters: make(map[string]*testIntValue),
-		gauges:   make(map[string]*testFloatValue)}
-}
-
-func (r *testStatsReporter) WaitAll() {
-	r.cg.Wait()
-	r.gg.Wait()
-}
-
-func (r *testStatsReporter) AllocateCounter(
-	name string, tags map[string]string,
-) tally.CachedCount {
-	counter := &testIntValue{
-		val:      0,
-		tags:     tags,
-		reporter: r,
-	}
-	r.counters[name] = counter
-	return counter
-}
-
-func (r *testStatsReporter) ReportCounter(name string, tags map[string]string, value int64) {
-	r.counters[name] = &testIntValue{
-		val:  value,
-		tags: tags,
-	}
-	r.cg.Done()
-}
-
-func (r *testStatsReporter) AllocateGauge(
-	name string, tags map[string]string,
-) tally.CachedGauge {
-	gauge := &testFloatValue{
-		val:      0,
-		tags:     tags,
-		reporter: r,
-	}
-	r.gauges[name] = gauge
-	return gauge
-}
-
-func (r *testStatsReporter) ReportGauge(name string, tags map[string]string, value float64) {
-	r.gauges[name] = &testFloatValue{
-		val:  value,
-		tags: tags,
-	}
-	r.gg.Done()
-}
-
-func (r *testStatsReporter) AllocateTimer(
-	name string, tags map[string]string,
-) tally.CachedTimer {
-	return nil
-}
-
-func (r *testStatsReporter) ReportTimer(name string, tags map[string]string, interval time.Duration) {
-
-}
-
-func (r *testStatsReporter) AllocateHistogram(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-) tally.CachedHistogram {
-	return nil
-}
-
-func (r *testStatsReporter) ReportHistogramValueSamples(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-	bucketLowerBound,
-	bucketUpperBound float64,
-	samples int64,
-) {
-
-}
-
-func (r *testStatsReporter) ReportHistogramDurationSamples(
-	name string,
-	tags map[string]string,
-	buckets tally.Buckets,
-	bucketLowerBound,
-	bucketUpperBound time.Duration,
-	samples int64,
-) {
-
-}
-
-func (r *testStatsReporter) Capabilities() tally.Capabilities {
-	return nil
-}
-
-func (r *testStatsReporter) Flush() {
-	atomic.AddInt32(&r.flushes, 1)
-}
-
-func TestCounter(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.cg.Add(1)
-	s.Counter("foo").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".foo"].val)
-
-	defer func() {
-		if r := recover(); r == nil {
-			t.Errorf("Should panic when wrong key used")
-		}
-	}()
-	assert.Equal(t, int64(1), r.counters[namespace+".foo1"].val)
-}
-
-func TestMultiCounterReport(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 2*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.cg.Add(1)
-	go s.Counter("foo").Inc(1)
-	go s.Counter("foo").Inc(3)
-	go s.Counter("foo").Inc(5)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(9), r.counters[namespace+".foo"].val)
-}
-
-func TestGauge(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	r.gg.Add(1)
-	s.Gauge("foo").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".foo"].val)
-}
-
-func TestMultiGaugeReport(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-
-	r.gg.Add(1)
-	s.Gauge("foo").Update(float64(1.33))
-	s.Gauge("foo").Update(float64(3.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(3.33), r.gauges[namespace+".foo"].val)
-}
-
-func TestSubScope(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("foo")
-
-	r.gg.Add(1)
-	subs.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".foo.bar"].val)
-
-	r.cg.Add(1)
-	subs.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".foo.haha"].val)
-}
-
-func TestTagged(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.Tagged(map[string]string{"env": "test"})
-
-	r.gg.Add(1)
-	subs.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".bar"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.gauges[namespace+".bar"].tags)
-
-	r.cg.Add(1)
-	subs.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".haha"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.counters[namespace+".haha"].tags)
-}
-
-func TestTaggedExistingReturnsSameScope(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-
-	for _, initialTags := range []map[string]string{
-		nil,
-		{"env": "test"},
-	} {
-		root := newRootScope(tally.ScopeOptions{Prefix: "foo", Tags: initialTags, Reporter: r}, 0)
-		go root.Start()
-		rootScope := root.(*scope)
-		fooScope := root.Tagged(map[string]string{"foo": "bar"}).(*scope)
-
-		assert.NotEqual(t, rootScope, fooScope)
-		assert.Equal(t, fooScope, fooScope.Tagged(nil))
-
-		fooBarScope := fooScope.Tagged(map[string]string{"bar": "baz"}).(*scope)
-
-		assert.NotEqual(t, fooScope, fooBarScope)
-		assert.Equal(t, fooBarScope, fooScope.Tagged(map[string]string{"bar": "baz"}).(*scope))
-		root.Close()
-	}
-}
-
-func TestSubScopeTagged(t *testing.T) {
-	t.Parallel()
-	r := newTestStatsReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("sub")
-	subtags := subs.Tagged(map[string]string{"env": "test"})
-
-	r.gg.Add(1)
-	subtags.Gauge("bar").Update(float64(1.33))
-	r.gg.Wait()
-
-	assert.Equal(t, float64(1.33), r.gauges[namespace+".sub.bar"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.gauges[namespace+".sub.bar"].tags)
-
-	r.cg.Add(1)
-	subtags.Counter("haha").Inc(1)
-	r.cg.Wait()
-
-	assert.Equal(t, int64(1), r.counters[namespace+".sub.haha"].val)
-	assert.EqualValues(t, map[string]string{
-		"env": "test",
-	}, r.counters[namespace+".sub.haha"].tags)
-}
-
-func TestMetricsByStatsdReporter(t *testing.T) {
-	t.Parallel()
-	udpAddr, err := net.ResolveUDPAddr("udp", statsdAddress)
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	server, err := net.ListenUDP("udp", udpAddr)
-	if err != nil {
-		t.Fatal(err)
-	}
-	defer server.Close()
-
-	r, _ := newTestStatsdReporter()
-	opts := tally.ScopeOptions{
-		Prefix:    namespace,
-		Separator: tally.DefaultSeparator,
-		Reporter:  r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-	subs := s.SubScope("peer").Tagged(map[string]string{"component": "committer", "env": "test"})
-	subs.Counter("success_total").Inc(1)
-	subs.Gauge("channel_total").Update(4)
-
-	buffer := make([]byte, 4096)
-	n, _ := io.ReadAtLeast(server, buffer, 1)
-	result := string(buffer[:n])
-
-	expected := []string{
-		`hyperledger_fabric.peer.success_total.component-committer.env-test:1|c`,
-		`hyperledger_fabric.peer.channel_total.component-committer.env-test:4|g`,
-	}
-
-	for i, res := range strings.Split(result, "\n") {
-		if res != expected[i] {
-			t.Errorf("Got `%s`, expected `%s`", res, expected[i])
-		}
-	}
-}
-
-func TestMetricsByPrometheusReporter(t *testing.T) {
-	t.Parallel()
-	r, _ := newTestPrometheusReporter()
-
-	opts := tally.ScopeOptions{
-		Prefix:         namespace,
-		Separator:      promreporter.DefaultSeparator,
-		CachedReporter: r}
-
-	s := newRootScope(opts, 1*time.Second)
-	go s.Start()
-	defer s.Close()
-
-	scrape := func() string {
-		resp, _ := http.Get(fmt.Sprintf("http://%s/metrics", promAddress))
-		buf, _ := ioutil.ReadAll(resp.Body)
-		return string(buf)
-	}
-	subs := s.SubScope("peer").Tagged(map[string]string{"component": "committer", "env": "test"})
-	subs.Counter("success_total").Inc(1)
-	subs.Gauge("channel_total").Update(4)
-
-	time.Sleep(2 * time.Second)
-
-	expected := []string{
-		`# HELP hyperledger_fabric_peer_channel_total hyperledger_fabric_peer_channel_total gauge`,
-		`# TYPE hyperledger_fabric_peer_channel_total gauge`,
-		`hyperledger_fabric_peer_channel_total{component="committer",env="test"} 4`,
-		`# HELP hyperledger_fabric_peer_success_total hyperledger_fabric_peer_success_total counter`,
-		`# TYPE hyperledger_fabric_peer_success_total counter`,
-		`hyperledger_fabric_peer_success_total{component="committer",env="test"} 1`,
-		``,
-	}
-
-	result := strings.Split(scrape(), "\n")
-
-	for i, res := range result {
-		if res != expected[i] {
-			t.Errorf("Got `%s`, expected `%s`", res, expected[i])
-		}
-	}
-}
-
-func newTestStatsdReporter() (tally.StatsReporter, error) {
-	opts := StatsdReporterOpts{
-		Address:       statsdAddress,
-		FlushInterval: defaultStatsdReporterFlushInterval,
-		FlushBytes:    defaultStatsdReporterFlushBytes,
-	}
-	return newStatsdReporter(opts)
-}
-
-func newTestPrometheusReporter() (promreporter.Reporter, error) {
-	opts := PromReporterOpts{
-		ListenAddress: promAddress,
-	}
-	return newPromReporter(opts)
-}
diff --git a/common/metrics/types.go b/common/metrics/types.go
deleted file mode 100644
index c70001ea1..000000000
--- a/common/metrics/types.go
+++ /dev/null
@@ -1,45 +0,0 @@
-/*
-Copyright IBM Corp. All Rights Reserved.
-
-SPDX-License-Identifier: Apache-2.0
-*/
-
-package metrics
-
-import "io"
-
-// Counter is the interface for emitting Counter type metrics.
-type Counter interface {
-	// Inc increments the Counter by a delta.
-	Inc(delta int64)
-}
-
-// Gauge is the interface for emitting Gauge metrics.
-type Gauge interface {
-	// Update sets the gauges absolute value.
-	Update(value float64)
-}
-
-// Scope is a namespace wrapper around a stats Reporter, ensuring that
-// all emitted values have a given prefix or set of tags.
-type Scope interface {
-	serve
-	// Counter returns the Counter object corresponding to the name.
-	Counter(name string) Counter
-
-	// Gauge returns the Gauge object corresponding to the name.
-	Gauge(name string) Gauge
-
-	// Tagged returns a new child Scope with the given tags and current tags.
-	Tagged(tags map[string]string) Scope
-
-	// SubScope returns a new child Scope appending a further name prefix.
-	SubScope(name string) Scope
-}
-
-// serve is the interface represents who can provide service
-type serve interface {
-	io.Closer
-	// Start starts the server
-	Start() error
-}
diff --git a/core/deliverservice/blocksprovider/blocksprovider.go b/core/deliverservice/blocksprovider/blocksprovider.go
index 8f34c099b..e5ebbee27 100644
--- a/core/deliverservice/blocksprovider/blocksprovider.go
+++ b/core/deliverservice/blocksprovider/blocksprovider.go
@@ -11,8 +11,11 @@ import (
 	"sync/atomic"
 	"time"
 
+	"fmt"
+
 	"github.com/golang/protobuf/proto"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/gossip/api"
 	gossipcommon "github.com/hyperledger/fabric/gossip/common"
 	"github.com/hyperledger/fabric/gossip/discovery"
@@ -161,6 +164,10 @@ func (b *blocksProviderImpl) DeliverBlocks() {
 			statusCounter = 0
 			blockNum := t.Block.Header.Number
 
+			//if metrics.IsDebug() {
+			metrics.RootScope.Gauge(fmt.Sprintf("blocksprovider_%s_received_block_number", metrics.FilterMetricName(b.chainID))).Update(float64(blockNum))
+			//}
+
 			marshaledBlock, err := proto.Marshal(t.Block)
 			if err != nil {
 				logger.Errorf("[%s] Error serializing block with sequence number %d, due to %s", b.chainID, blockNum, err)
@@ -183,6 +190,9 @@ func (b *blocksProviderImpl) DeliverBlocks() {
 				logger.Warningf("Block [%d] received from ordering service wasn't added to payload buffer: %v", blockNum, err)
 			}
 
+			if metrics.IsDebug() {
+				metrics.RootScope.Gauge(fmt.Sprintf("blocksprovider_%s_gossiping_block_number", metrics.FilterMetricName(b.chainID))).Update(float64(payload.SeqNum))
+			}
 			// Gossip messages with other nodes
 			logger.Debugf("[%s] Gossiping block [%d], peers number [%d]", b.chainID, blockNum, numberOfPeers)
 			if !b.isDone() {
diff --git a/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go b/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
index f3703e7b0..c4e02c9bb 100644
--- a/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
+++ b/core/ledger/kvledger/history/historydb/historyleveldb/historyleveldb.go
@@ -10,6 +10,7 @@ import (
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/history/historydb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
@@ -73,6 +74,12 @@ func (historyDB *historyDB) Close() {
 // Commit implements method in HistoryDB interface
 func (historyDB *historyDB) Commit(block *common.Block) error {
 
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("historyleveldb_Commit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	blockNo := block.Header.Number
 	//Set the starting tranNo to 0
 	var tranNo uint64
diff --git a/core/ledger/kvledger/kv_ledger.go b/core/ledger/kvledger/kv_ledger.go
index df8607d67..a101d1921 100644
--- a/core/ledger/kvledger/kv_ledger.go
+++ b/core/ledger/kvledger/kv_ledger.go
@@ -13,6 +13,7 @@ import (
 
 	"github.com/hyperledger/fabric/common/flogging"
 	commonledger "github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
@@ -252,6 +253,13 @@ func (l *kvLedger) NewHistoryQueryExecutor() (ledger.HistoryQueryExecutor, error
 
 // CommitWithPvtData commits the block and the corresponding pvt data in an atomic operation
 func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) error {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("kvledger_CommitWithPvtData_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	var err error
 	block := pvtdataAndBlock.Block
 	blockNo := pvtdataAndBlock.Block.Header.Number
@@ -273,6 +281,10 @@ func (l *kvLedger) CommitWithPvtData(pvtdataAndBlock *ledger.BlockAndPvtData) er
 	}
 	elapsedCommitBlockStorage := time.Since(startCommitBlockStorage) / time.Millisecond // duration in ms
 
+	// KEEP EVEN WHEN metrics.debug IS OFF
+	metrics.RootScope.Gauge(fmt.Sprintf("kvledger_%s_commited_block_number", metrics.FilterMetricName(l.ledgerID))).Update(float64(block.Header.Number))
+	logger.Infof("Channel [%s]: Committed block [%d] with %d transaction(s)", metrics.FilterMetricName(l.ledgerID), block.Header.Number, len(block.Data.Data))
+
 	startCommitState := time.Now()
 	logger.Debugf("[%s] Committing block [%d] transactions to state database", l.ledgerID, blockNo)
 	if err = l.txtmgmt.Commit(); err != nil {
@@ -307,6 +319,13 @@ func (l *kvLedger) GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int) (ledge
 // GetPvtDataAndBlockByNum returns the block and the corresponding pvt data.
 // The pvt data is filtered by the list of 'collections' supplied
 func (l *kvLedger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsCollFilter) (*ledger.BlockAndPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("kvledger_GetPvtDataAndBlockByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	blockAndPvtdata, err := l.blockStore.GetPvtDataAndBlockByNum(blockNum, filter)
 	l.blockAPIsRWLock.RLock()
 	l.blockAPIsRWLock.RUnlock()
@@ -316,6 +335,13 @@ func (l *kvLedger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsC
 // GetPvtDataByNum returns only the pvt data  corresponding to the given block number
 // The pvt data is filtered by the list of 'collections' supplied
 func (l *kvLedger) GetPvtDataByNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("kvledger_GetPvtDataByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	pvtdata, err := l.blockStore.GetPvtDataByNum(blockNum, filter)
 	l.blockAPIsRWLock.RLock()
 	l.blockAPIsRWLock.RUnlock()
diff --git a/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go b/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
index c8eaa945f..255243267 100644
--- a/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
+++ b/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go
@@ -12,6 +12,7 @@ import (
 
 	"github.com/golang/protobuf/proto"
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/common/privdata"
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
@@ -352,6 +353,10 @@ func deriveHashedDataNs(namespace, collection string) string {
 }
 
 func addPvtUpdates(pubUpdateBatch *PubUpdateBatch, pvtUpdateBatch *PvtUpdateBatch) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("privacyenabledstate_addPvtUpdates_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	for ns, nsBatch := range pvtUpdateBatch.UpdateMap {
 		for _, coll := range nsBatch.GetCollectionNames() {
 			for key, vv := range nsBatch.GetUpdates(coll) {
@@ -362,6 +367,10 @@ func addPvtUpdates(pubUpdateBatch *PubUpdateBatch, pvtUpdateBatch *PvtUpdateBatc
 }
 
 func addHashedUpdates(pubUpdateBatch *PubUpdateBatch, hashedUpdateBatch *HashedUpdateBatch, base64Key bool) {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("privacyenabledstate_addHashedUpdatesTimer_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	for ns, nsBatch := range hashedUpdateBatch.UpdateMap {
 		for _, coll := range nsBatch.GetCollectionNames() {
 			for key, vv := range nsBatch.GetUpdates(coll) {
diff --git a/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go b/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
index 1d1436cb5..0f94a7ec3 100644
--- a/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
+++ b/core/ledger/kvledger/txmgmt/pvtstatepurgemgmt/expiry_keeper.go
@@ -7,10 +7,11 @@ SPDX-License-Identifier: Apache-2.0
 package pvtstatepurgemgmt
 
 import (
-	proto "github.com/golang/protobuf/proto"
+	"github.com/golang/protobuf/proto"
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/bookkeeping"
 )
 
@@ -64,6 +65,10 @@ type expKeeper struct {
 // at the time of the commit of the block number 45 and the second entry was created at the time of the commit of the block number 40, however
 // both are expiring with the commit of block number 50.
 func (ek *expKeeper) updateBookkeeping(toTrack []*expiryInfo, toClear []*expiryInfoKey) error {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("pvtstatepurgemgmt_updateBookkeepingTimer_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	updateBatch := leveldbhelper.NewUpdateBatch()
 	for _, expinfo := range toTrack {
 		k, v, err := encodeKV(expinfo)
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
index 2dd88237c..a80f2f3a3 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/batch_util.go
@@ -7,6 +7,8 @@ package statecouchdb
 
 import (
 	"sync"
+
+	"github.com/hyperledger/fabric/common/metrics"
 )
 
 // batch is executed in a separate goroutine.
@@ -18,7 +20,15 @@ type batch interface {
 // any of the batches return error during its execution
 func executeBatches(batches []batch) error {
 	logger.Debugf("Executing batches = %s", batches)
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("statecouchdb_executeBatches_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	numBatches := len(batches)
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge("statecouchdb_numBatches").Update(float64(numBatches))
+	}
 	if numBatches == 0 {
 		return nil
 	}
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
index a62769d4f..39e92247e 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/commit_handling.go
@@ -8,6 +8,7 @@ package statecouchdb
 import (
 	"fmt"
 
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
@@ -151,6 +152,12 @@ type nsFlusher struct {
 }
 
 func (vdb *VersionedDB) ensureFullCommit(dbs []*couchdb.CouchDatabase) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("statecouchdb_ensureFullCommit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	var flushers []batch
 	for _, db := range dbs {
 		flushers = append(flushers, &nsFlusher{db})
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
index 02fe4d0ce..00efa961f 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
@@ -12,6 +12,7 @@ import (
 	"sync"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
@@ -415,6 +416,12 @@ func validateQueryMetadata(metadata map[string]interface{}) error {
 
 // ApplyUpdates implements method in VersionedDB interface
 func (vdb *VersionedDB) ApplyUpdates(updates *statedb.UpdateBatch, height *version.Height) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("statecouchdb_ApplyUpdates_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// TODO a note about https://jira.hyperledger.org/browse/FAB-8622
 	// the function `Apply update can be split into three functions. Each carrying out one of the following three stages`.
 	// The write lock is needed only for the stage 2.
diff --git a/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go b/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
index a88290277..4a294efa5 100644
--- a/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
+++ b/core/ledger/kvledger/txmgmt/statedb/stateleveldb/stateleveldb.go
@@ -10,6 +10,7 @@ import (
 
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
@@ -166,6 +167,12 @@ func (vdb *versionedDB) ExecuteQueryWithMetadata(namespace, query string, metada
 
 // ApplyUpdates implements method in VersionedDB interface
 func (vdb *versionedDB) ApplyUpdates(batch *statedb.UpdateBatch, height *version.Height) error {
+
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("stateleveldb_ApplyUpdates_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	dbBatch := leveldbhelper.NewUpdateBatch()
 	namespaces := batch.GetUpdatedNamespaces()
 	for _, ns := range namespaces {
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
index 556e9fd08..81b762b74 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
@@ -9,6 +9,7 @@ import (
 	"sync"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/bookkeeping"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate"
@@ -150,6 +151,12 @@ func (txmgr *LockBasedTxMgr) Shutdown() {
 
 // Commit implements method in interface `txmgmt.TxMgr`
 func (txmgr *LockBasedTxMgr) Commit() error {
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("lockbasedtxmgr_Commit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	// When using the purge manager for the first block commit after peer start, the asynchronous function
 	// 'PrepareForExpiringKeys' is invoked in-line. However, for the subsequent blocks commits, this function is invoked
 	// in advance for the next block
@@ -251,6 +258,10 @@ func extractStateUpdates(batch *privacyenabledstate.UpdateBatch, namespaces []st
 }
 
 func (txmgr *LockBasedTxMgr) updateStateListeners() {
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("lockbasedtxmgr_updateStateListenersTimer_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 	for _, l := range txmgr.current.listeners {
 		l.StateCommitDone(txmgr.ledgerid)
 	}
diff --git a/core/ledger/ledgerstorage/store.go b/core/ledger/ledgerstorage/store.go
index 9a8243865..898595267 100644
--- a/core/ledger/ledgerstorage/store.go
+++ b/core/ledger/ledgerstorage/store.go
@@ -12,6 +12,7 @@ import (
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage"
 	"github.com/hyperledger/fabric/common/ledger/blkstorage/fsblkstorage"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
@@ -88,6 +89,13 @@ func (s *Store) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
 
 // CommitWithPvtData commits the block and the corresponding pvt data in an atomic operation
 func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("ledgerstorage_CommitWithPvtData_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	blockNum := blockAndPvtdata.Block.Header.Number
 	missingDataList := blockAndPvtdata.Missing
 
@@ -103,6 +111,9 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 	if err != nil {
 		return err
 	}
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge("ledgerstorage_CommitWithPvtData_BlockDiff").Update(float64(blockNum - pvtBlkStoreHt))
+	}
 
 	writtenToPvtStore := false
 	if pvtBlkStoreHt < blockNum+1 { // The pvt data store sanity check does not allow rewriting the pvt data.
@@ -118,6 +129,9 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 		}
 		writtenToPvtStore = true
 	} else {
+		if metrics.IsDebug() {
+			metrics.RootScope.Counter("ledgerstorage_CommitWithPvtData_SkipCount").Inc(1)
+		}
 		logger.Debugf("Skipping writing block [%d] to pvt block store as the store height is [%d]", blockNum, pvtBlkStoreHt)
 	}
 
@@ -135,6 +149,13 @@ func (s *Store) CommitWithPvtData(blockAndPvtdata *ledger.BlockAndPvtData) error
 // GetPvtDataAndBlockByNum returns the block and the corresponding pvt data.
 // The pvt data is filtered by the list of 'collections' supplied
 func (s *Store) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsCollFilter) (*ledger.BlockAndPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("ledgerstorage_GetPvtDataAndBlockByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
 
@@ -154,6 +175,13 @@ func (s *Store) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger.PvtNsColl
 // The pvt data is filtered by the list of 'ns/collections' supplied in the filter
 // A nil filter does not filter any results
 func (s *Store) GetPvtDataByNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+
+	if metrics.IsDebug() {
+		// Measure the whole
+		stopWatch := metrics.RootScope.Timer("ledgerstorage_GetPvtDataByNum_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	s.rwlock.RLock()
 	defer s.rwlock.RUnlock()
 	return s.getPvtDataByNumWithoutLock(blockNum, filter)
diff --git a/core/ledger/util/couchdb/couchdb.go b/core/ledger/util/couchdb/couchdb.go
index 0cd1a6c7b..40f3b6443 100644
--- a/core/ledger/util/couchdb/couchdb.go
+++ b/core/ledger/util/couchdb/couchdb.go
@@ -27,6 +27,7 @@ import (
 	"unicode/utf8"
 
 	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/pkg/errors"
 	"go.uber.org/zap/zapcore"
@@ -484,8 +485,11 @@ func (dbclient *CouchDatabase) DropDatabase() (*DBOperationResponse, error) {
 
 // EnsureFullCommit calls _ensure_full_commit for explicit fsync
 func (dbclient *CouchDatabase) EnsureFullCommit() (*DBOperationResponse, error) {
-
 	logger.Debugf("[%s] Entering EnsureFullCommit()", dbclient.DBName)
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("couchdb_ensureFullCommit_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 
 	connectURL, err := url.Parse(dbclient.CouchInstance.conf.URL)
 	if err != nil {
@@ -538,8 +542,11 @@ func (dbclient *CouchDatabase) EnsureFullCommit() (*DBOperationResponse, error)
 
 //SaveDoc method provides a function to save a document, id and byte array
 func (dbclient *CouchDatabase) SaveDoc(id string, rev string, couchDoc *CouchDoc) (string, error) {
-
 	logger.Debugf("[%s] Entering SaveDoc() id=[%s]", dbclient.DBName, id)
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("couchdb_saveDoc_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
 
 	if !utf8.ValidString(id) {
 		return "", errors.Errorf("doc id [%x] not a valid utf8 string", id)
@@ -1278,6 +1285,12 @@ func (dbclient *CouchDatabase) WarmIndex(designdoc, indexname string) error {
 	//URL to execute the view function associated with the index
 	indexURL = constructCouchDBUrl(indexURL, dbclient.DBName, "_design", designdoc, "_view", indexname)
 
+	if metrics.IsDebug() {
+		timer := metrics.RootScope.Timer("couchdb_WarmIndex")
+		stopWatch := timer.Start()
+		defer stopWatch.Stop()
+	}
+
 	queryParms := indexURL.Query()
 	//Query parameter that allows the execution of the URL to return immediately
 	//The update_after will cause the index update to run after the URL returns
@@ -1646,6 +1659,11 @@ func (dbclient *CouchDatabase) handleRequestWithRevisionRetry(id, method, connec
 func (couchInstance *CouchInstance) handleRequest(method, connectURL string, data []byte, rev string,
 	multipartBoundary string, maxRetries int, keepConnectionOpen bool) (*http.Response, *DBReturn, error) {
 
+	if metrics.IsDebug() {
+		stopWatch := metrics.RootScope.Timer("couchdb_handleRequest_time_seconds").Start()
+		defer stopWatch.Stop()
+	}
+
 	logger.Debugf("Entering handleRequest()  method=%s  url=%v", method, connectURL)
 
 	//create the return objects for couchDB
diff --git a/gossip/privdata/coordinator.go b/gossip/privdata/coordinator.go
index 7e13b52fd..3e53162b4 100644
--- a/gossip/privdata/coordinator.go
+++ b/gossip/privdata/coordinator.go
@@ -13,6 +13,7 @@ import (
 	"time"
 
 	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/metrics"
 	util2 "github.com/hyperledger/fabric/common/util"
 	"github.com/hyperledger/fabric/core/committer"
 	"github.com/hyperledger/fabric/core/committer/txvalidator"
@@ -30,6 +31,7 @@ import (
 	"github.com/hyperledger/fabric/protos/utils"
 	"github.com/pkg/errors"
 	"github.com/spf13/viper"
+	"github.com/uber-go/tally"
 )
 
 const (
@@ -183,6 +185,12 @@ func (c *coordinator) StoreBlock(block *common.Block, privateDataSets util.PvtDa
 	}
 	startPull := time.Now()
 	limit := startPull.Add(retryThresh)
+
+	var waitingForMissingKeysStopWatch tally.Stopwatch
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge("privdata_gossipMissingKeys").Update(float64(len(privateInfo.missingKeys)))
+		waitingForMissingKeysStopWatch = metrics.RootScope.Timer("privdata_gossipWaitingForMissingKeys_time_seconds").Start()
+	}
 	for len(privateInfo.missingKeys) > 0 && time.Now().Before(limit) {
 		c.fetchFromPeers(block.Header.Number, ownedRWsets, privateInfo)
 		// If succeeded to fetch everything, no need to sleep before
@@ -194,6 +202,9 @@ func (c *coordinator) StoreBlock(block *common.Block, privateDataSets util.PvtDa
 	}
 	elapsedPull := int64(time.Since(startPull) / time.Millisecond) // duration in ms
 
+	if metrics.IsDebug() {
+		waitingForMissingKeysStopWatch.Stop()
+	}
 	// Only log results if we actually attempted to fetch
 	if bFetchFromPeers {
 		if len(privateInfo.missingKeys) == 0 {
diff --git a/gossip/state/payloads_buffer.go b/gossip/state/payloads_buffer.go
index 6ad96b75f..a740474df 100644
--- a/gossip/state/payloads_buffer.go
+++ b/gossip/state/payloads_buffer.go
@@ -20,7 +20,7 @@ import (
 // to signal whenever expected block has arrived.
 type PayloadsBuffer interface {
 	// Adds new block into the buffer
-	Push(payload *proto.Payload)
+	Push(payload *proto.Payload) bool
 
 	// Returns next expected sequence number
 	Next() uint64
@@ -73,7 +73,7 @@ func (b *PayloadsBufferImpl) Ready() chan struct{} {
 // sequence number is below the expected next block number payload will be
 // thrown away.
 // TODO return bool to indicate if payload was added or not, so that caller can log result.
-func (b *PayloadsBufferImpl) Push(payload *proto.Payload) {
+func (b *PayloadsBufferImpl) Push(payload *proto.Payload) bool {
 	b.mutex.Lock()
 	defer b.mutex.Unlock()
 
@@ -81,15 +81,18 @@ func (b *PayloadsBufferImpl) Push(payload *proto.Payload) {
 
 	if seqNum < b.next || b.buf[seqNum] != nil {
 		logger.Debugf("Payload with sequence number = %d has been already processed", payload.SeqNum)
-		return
+		return false
 	}
 
 	b.buf[seqNum] = payload
 
 	// Send notification that next sequence has arrived
+
 	if seqNum == b.next && len(b.readyChan) == 0 {
 		b.readyChan <- struct{}{}
 	}
+
+	return true
 }
 
 // Next function provides the number of the next expected block
diff --git a/gossip/state/state.go b/gossip/state/state.go
index 4850613ef..c000b9bf0 100644
--- a/gossip/state/state.go
+++ b/gossip/state/state.go
@@ -12,8 +12,11 @@ import (
 	"sync/atomic"
 	"time"
 
+	"fmt"
+
 	pb "github.com/golang/protobuf/proto"
 	vsccErrors "github.com/hyperledger/fabric/common/errors"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/gossip/api"
 	"github.com/hyperledger/fabric/gossip/comm"
 	common2 "github.com/hyperledger/fabric/gossip/common"
@@ -470,6 +473,10 @@ func (s *GossipStateProviderImpl) handleStateResponse(msg proto.ReceivedMessage)
 		return uint64(0), errors.New("Received state transfer response without payload")
 	}
 	for _, payload := range response.GetPayloads() {
+
+		if metrics.IsDebug() {
+			metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_handleStateResponse_block_number", metrics.FilterMetricName(s.chainID))).Update(float64(payload.SeqNum))
+		}
 		logger.Debugf("Received payload with sequence number %d.", payload.SeqNum)
 		if err := s.mediator.VerifyBlock(common2.ChainID(s.chainID), payload.SeqNum, payload.Data); err != nil {
 			err = errors.WithStack(err)
@@ -514,6 +521,11 @@ func (s *GossipStateProviderImpl) queueNewMessage(msg *proto.GossipMessage) {
 
 	dataMsg := msg.GetDataMsg()
 	if dataMsg != nil {
+
+		if metrics.IsDebug() {
+			metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_adding_payload_number", metrics.FilterMetricName(s.chainID))).Update(float64(dataMsg.Payload.SeqNum))
+		}
+
 		if err := s.addPayload(dataMsg.GetPayload(), nonBlocking); err != nil {
 			logger.Warningf("Block [%d] received from gossip wasn't added to payload buffer: %v", dataMsg.Payload.SeqNum, err)
 			return
@@ -531,6 +543,9 @@ func (s *GossipStateProviderImpl) deliverPayloads() {
 		select {
 		// Wait for notification that next seq has arrived
 		case <-s.payloads.Ready():
+			// KEEP EVEN WHEN metrics.debug IS OFF
+			metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_next_sequence_arrived", metrics.FilterMetricName(s.chainID))).Update(float64(s.payloads.Next()))
+
 			logger.Debugf("[%s] Ready to transfer payloads (blocks) to the ledger, next block number is = [%d]", s.chainID, s.payloads.Next())
 			// Collect all subsequent payloads
 			for payload := s.payloads.Pop(); payload != nil; payload = s.payloads.Pop() {
@@ -747,6 +762,9 @@ func (s *GossipStateProviderImpl) addPayload(payload *proto.Payload, blockingMod
 	if payload == nil {
 		return errors.New("Given payload is nil")
 	}
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_addPayload", metrics.FilterMetricName(s.chainID))).Update(float64(payload.SeqNum))
+	}
 	logger.Debugf("[%s] Adding payload to local buffer, blockNum = [%d]", s.chainID, payload.SeqNum)
 	height, err := s.ledger.LedgerHeight()
 	if err != nil {
@@ -761,12 +779,19 @@ func (s *GossipStateProviderImpl) addPayload(payload *proto.Payload, blockingMod
 		time.Sleep(enqueueRetryInterval)
 	}
 
-	s.payloads.Push(payload)
+	if s.payloads.Push(payload) {
+		metrics.RootScope.Gauge(fmt.Sprintf("payloadbuffer_%s_push_block_number", metrics.FilterMetricName(s.chainID))).Update(float64(payload.SeqNum))
+		logger.Debugf("[%s] Adding Payload to local buffer done, blockNum = [%d]", s.chainID, payload.SeqNum)
+	}
 	return nil
 }
 
 func (s *GossipStateProviderImpl) commitBlock(block *common.Block, pvtData util.PvtDataCollections) error {
 
+	if metrics.IsDebug() {
+		metrics.RootScope.Gauge(fmt.Sprintf("gossip_state_%s_about_to_store_block_number", metrics.FilterMetricName(s.chainID))).Update(float64(block.Header.Number))
+	}
+
 	// Commit block with available private transactions
 	if err := s.ledger.StoreBlock(block, pvtData); err != nil {
 		logger.Errorf("Got error while committing(%+v)", errors.WithStack(err))
diff --git a/peer/node/start.go b/peer/node/start.go
index eca20c33d..057380038 100644
--- a/peer/node/start.go
+++ b/peer/node/start.go
@@ -22,6 +22,7 @@ import (
 	"github.com/hyperledger/fabric/common/deliver"
 	"github.com/hyperledger/fabric/common/flogging"
 	"github.com/hyperledger/fabric/common/localmsp"
+	"github.com/hyperledger/fabric/common/metrics"
 	"github.com/hyperledger/fabric/common/policies"
 	"github.com/hyperledger/fabric/common/viperutil"
 	"github.com/hyperledger/fabric/core/aclmgmt"
@@ -114,6 +115,9 @@ var nodeStartCmd = &cobra.Command{
 }
 
 func serve(args []string) error {
+
+	metrics.Initialize()
+
 	// currently the peer only works with the standard MSP
 	// because in certain scenarios the MSP has to make sure
 	// that from a single credential you only have a single 'identity'.
-- 
2.17.1 (Apple Git-112)

