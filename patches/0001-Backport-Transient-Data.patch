From af03f622f008c39c23a173ebfe34b92a165b1b99 Mon Sep 17 00:00:00 2001
From: Firas Qutishat <firas.qutishat@securekey.com>
Date: Tue, 17 Dec 2019 11:08:29 -0500
Subject: [PATCH] Backport Transient Data

Signed-off-by: Firas Qutishat <firas.qutishat@securekey.com>
---
 Gopkg.lock                                    | 114 ++-
 Gopkg.toml                                    |  12 +-
 core/chaincode/exectransaction_test.go        |   2 +-
 core/endorser/endorser.go                     |  32 +-
 .../historydb/historyleveldb/pkg_test.go      |   2 +-
 core/ledger/kvledger/idstore/idstore.go       |  25 +
 core/ledger/kvledger/idstore/readme.md        |   2 +
 core/ledger/kvledger/kv_ledger.go             |   9 +-
 core/ledger/kvledger/kv_ledger_provider.go    |   6 +-
 .../txmgmt/rwsetutil/rwset_builder.go         |   2 +-
 .../txmgmt/rwsetutil/rwset_proto_util.go      |   5 +-
 .../txmgmt/rwsetutil/rwset_proto_util_test.go |   2 +-
 .../statedb/statecouchdb/statecouchdb.go      |   2 +-
 .../txmgr/lockbasedtxmgr/collection_val.go    |  13 +-
 .../lockbasedtxmgr/collection_val_ext.go      |  35 +
 .../lockbasedtxmgr/collection_val_ext_test.go |  66 ++
 .../txmgmt/txmgr/lockbasedtxmgr/helper.go     |   9 +
 .../lockbased_query_executer.go               |   4 +-
 .../txmgr/lockbasedtxmgr/lockbased_txmgr.go   |  14 +-
 .../txmgmt/txmgr/lockbasedtxmgr/pkg_test.go   |   4 +-
 .../txmgr/lockbasedtxmgr/pvtdatahandler.go    |  65 ++
 core/ledger/ledger_interface.go               |   2 +
 core/ledger/ledgermgmt/ledger_mgmt.go         |   3 +
 core/ledger/pvtdatastorage/store.go           |  10 +
 core/ledger/util/couchdb/couchdb.go           |  34 +
 core/ledger/util/couchdb/couchdb_ext.go       |  12 +
 core/ledger/util/couchdb/couchdb_test.go      |  48 +-
 core/ledger/util/couchdb/couchdbutil.go       |   9 +-
 core/ledger/util/couchdb/couchdbutil_test.go  |   6 +-
 core/ledger/util/couchdb/metrics_test.go      |   2 +-
 core/peer/configtx_test.go                    |   5 +
 core/peer/mock_helpers.go                     |   2 +
 core/peer/peer.go                             |  51 +-
 core/peer/peer_impl.go                        |  10 +-
 core/peer/peer_test.go                        |   5 +-
 extensions/blkstorage/store.go                |  27 +
 extensions/blkstorage/store_test.go           |  22 +
 .../api/dissemination/dissemination.go        |  18 +
 extensions/collections/api/store/key.go       |  57 ++
 extensions/collections/api/store/provider.go  |  78 ++
 extensions/collections/api/support/support.go |  23 +
 .../dissemination/disseminationplan.go        |  72 ++
 .../dissemination/disseminationplan_test.go   | 101 ++
 extensions/collections/policy/validator.go    |  89 ++
 .../collections/policy/validator_test.go      | 271 ++++++
 .../pvtdatahandler/pvtdatahandler.go          |  17 +
 .../pvtdatahandler/pvtdatahandler_test.go     |  40 +
 extensions/collections/retriever/retriever.go |  25 +
 .../collections/retriever/retriever_test.go   |  33 +
 .../storeprovider/mocks/mocktdstore.go        | 119 +++
 .../storeprovider/storeprovider.go            |  16 +
 .../storeprovider/storeprovider_test.go       | 104 ++
 extensions/endorser/api/endorser.go           |  27 +
 extensions/endorser/endorser.go               |  24 +
 extensions/endorser/endorser_test.go          |  45 +
 extensions/gossip/api/gossipapi.go            |  44 +
 .../gossip/blockpublisher/blockpublisher.go   |  16 +
 .../blockpublisher/blockpublisher_test.go     |  67 ++
 extensions/gossip/coordinator/coordinator.go  |  24 +
 .../gossip/coordinator/coordinator_test.go    |  50 +
 extensions/gossip/dispatcher/dispatcher.go    |  37 +
 .../gossip/dispatcher/dispatcher_test.go      |  38 +
 extensions/gossip/mocks/blockpublisher.go     |  51 +
 extensions/idstore/store.go                   |  17 +
 extensions/idstore/store_test.go              |  20 +
 extensions/mocks/mockdatastore.go             | 101 ++
 extensions/mocks/mockprovider.go              |  53 ++
 extensions/pvtdatastorage/store.go            |  17 +
 extensions/pvtdatastorage/store_test.go       |  33 +
 extensions/roles/ledger_roles_config.go       |  30 +
 extensions/roles/ledger_roles_config_test.go  |  29 +
 extensions/statedb/store.go                   |  16 +
 extensions/statedb/store_test.go              |  34 +
 extensions/testutil/ext_test_env.go           |  25 +
 extensions/transientstore/store.go            |  17 +
 extensions/transientstore/store_test.go       |  17 +
 gossip/privdata/coordinator.go                |  16 +-
 gossip/privdata/coordinator_test.go           |  16 +
 gossip/privdata/dissemination.go              |  33 +
 gossip/privdata/distributor.go                |  22 +-
 gossip/privdata/distributor_test.go           |   4 +
 gossip/protoext/extensions.go                 |   7 +
 gossip/service/gossip_service.go              |  11 +-
 gossip/service/gossip_service_test.go         |  16 +-
 gossip/service/integration_test.go            |   6 +-
 gossip/state/state.go                         |  23 +-
 gossip/state/state_test.go                    |   9 +-
 peer/chaincode/common.go                      |  58 +-
 peer/chaincode/transientcollconfig_test.go    |  64 ++
 peer/node/start.go                            |  15 +-
 protos/common/collection.pb.go                | 147 ++-
 protos/common/collection.proto                |  19 +-
 protos/gossip/message.pb.go                   | 738 ++++++++++----
 protos/gossip/message.proto                   |  45 +
 protoutil/blockutils.go                       | 124 +++
 protoutil/blockutils_test.go                  | 206 ++++
 protoutil/chaincodeutils.go                   |  25 +
 protoutil/commonutils.go                      | 339 +++++++
 protoutil/commonutils_test.go                 | 429 +++++++++
 protoutil/proputils.go                        | 642 +++++++++++++
 protoutil/proputils_test.go                   | 665 +++++++++++++
 protoutil/readme.md                           |   3 +
 protoutil/txutils.go                          | 453 +++++++++
 protoutil/txutils_test.go                     | 522 ++++++++++
 vendor/github.com/bluele/gcache/LICENSE       |  21 +
 vendor/github.com/bluele/gcache/arc.go        | 452 +++++++++
 vendor/github.com/bluele/gcache/cache.go      | 205 ++++
 vendor/github.com/bluele/gcache/clock.go      |  53 ++
 vendor/github.com/bluele/gcache/lfu.go        | 335 +++++++
 vendor/github.com/bluele/gcache/lru.go        | 301 ++++++
 vendor/github.com/bluele/gcache/simple.go     | 289 ++++++
 .../github.com/bluele/gcache/singleflight.go  |  82 ++
 vendor/github.com/bluele/gcache/stats.go      |  53 ++
 vendor/github.com/bluele/gcache/utils.go      |  15 +
 vendor/github.com/btcsuite/btcutil/LICENSE    |  16 +
 .../btcsuite/btcutil/base58/alphabet.go       |  49 +
 .../btcsuite/btcutil/base58/base58.go         |  75 ++
 .../btcsuite/btcutil/base58/base58check.go    |  52 +
 .../github.com/btcsuite/btcutil/base58/doc.go |  29 +
 .../btcsuite/btcutil/base58/genalphabet.go    |  79 ++
 .../magiconair/properties/assert/assert.go    |  90 ++
 vendor/github.com/pkg/errors/errors.go        |  43 +-
 vendor/github.com/pkg/errors/stack.go         |  51 +-
 .../trustbloc/fabric-peer-ext/LICENSE         | 201 ++++
 .../cdbblkstorage/block_serialization.go      |  67 ++
 .../blkstorage/cdbblkstorage/blocks_itr.go    |  73 ++
 .../cdbblkstorage/cdb_blkstorage.go           | 368 +++++++
 .../cdbblkstorage/cdb_blkstorage_provider.go  | 159 ++++
 .../cdbblkstorage/cdb_checkpoint.go           | 121 +++
 .../blkstorage/cdbblkstorage/couchdoc_conv.go | 377 ++++++++
 .../collections/offledger/api/offledger.go    |  56 ++
 .../pkg/collections/offledger/dcas/cas.go     |  42 +
 .../pkg/collections/offledger/dcas/dcas.go    |  67 ++
 .../dissemination/disseminationplan.go        |  96 ++
 .../offledger/dissemination/disseminator.go   | 126 +++
 .../offledger/mocks/mockprovider.go           |  40 +
 .../collections/offledger/policy/validator.go |  42 +
 .../offledger/retriever/olretriever.go        | 470 +++++++++
 .../offledger/storeprovider/olstore.go        | 341 +++++++
 .../storeprovider/olstoreprovider.go          | 119 +++
 .../offledger/storeprovider/store/api/api.go  |  61 ++
 .../storeprovider/store/cache/cache.go        | 170 ++++
 .../store/couchdbstore/dbstore.go             | 232 +++++
 .../store/couchdbstore/dbstore_provider.go    | 207 ++++
 .../pvtdatahandler/pvtdatahandler.go          | 156 +++
 .../collections/pvtdatastore/pvtdatastore.go  | 190 ++++
 .../pkg/collections/retriever/retriever.go    | 124 +++
 .../pkg/collections/retriever/test_exports.go |  25 +
 .../pkg/collections/storeprovider/store.go    |  86 ++
 .../storeprovider/storeprovider.go            |  94 ++
 .../collections/storeprovider/test_exports.go |  18 +
 .../transientdata/api/transientdata.go        |  52 +
 .../dissemination/disseminationplan.go        |  92 ++
 .../dissemination/disseminator.go             | 135 +++
 .../transientdata/mocks/mockprovider.go       |  40 +
 .../transientdata/policy/validator.go         |  39 +
 .../retriever/transientdataretriever.go       | 384 ++++++++
 .../storeprovider/store/api/api.go            |  30 +
 .../storeprovider/store/cache/cache.go        | 150 +++
 .../storeprovider/store/dbstore/dbstore.go    | 130 +++
 .../store/dbstore/dbstore_provider.go         |  41 +
 .../transientdata/storeprovider/tdstore.go    | 166 ++++
 .../storeprovider/tdstoreprovider.go          |  67 ++
 .../fabric-peer-ext/pkg/common/common.go      |  88 ++
 .../pkg/common/discovery/discovery.go         | 123 +++
 .../pkg/common/discovery/member.go            |  38 +
 .../pkg/common/discovery/peergroup.go         | 136 +++
 .../pkg/common/multirequest/multirequest.go   | 105 ++
 .../pkg/common/requestmgr/requestmgr.go       | 207 ++++
 .../pkg/common/support/collconfigretriever.go | 206 ++++
 .../pkg/common/support/support.go             |  66 ++
 .../fabric-peer-ext/pkg/config/config.go      | 145 +++
 .../fabric-peer-ext/pkg/endorser/endorser.go  | 116 +++
 .../gossip/blockpublisher/blockpublisher.go   | 514 ++++++++++
 .../pkg/gossip/dispatcher/dispatcher.go       | 268 ++++++
 .../pkg/idstore/couchdoc_conv.go              | 142 +++
 .../fabric-peer-ext/pkg/idstore/store_impl.go | 269 ++++++
 .../pkg/idstore/test_exports.go               |  59 ++
 .../pkg/mocks/mockaccesspolicy.go             |  61 ++
 .../pkg/mocks/mockblockbuilder.go             | 367 +++++++
 .../pkg/mocks/mockblockhandler.go             |  91 ++
 .../pkg/mocks/mockblockpublisher.go           |  56 ++
 .../pkg/mocks/mockdataprovider.go             |  91 ++
 .../pkg/mocks/mockdatastore.go                | 101 ++
 .../pkg/mocks/mockgossipadapter.go            |  96 ++
 .../pkg/mocks/mockgossipmsg.go                | 130 +++
 .../fabric-peer-ext/pkg/mocks/mockledger.go   | 107 +++
 .../pkg/mocks/mockqueryexecutor.go            | 124 +++
 .../pkg/mocks/mockrwsetbuilder.go             | 345 +++++++
 .../fabric-peer-ext/pkg/mocks/mocksupport.go  |  61 ++
 .../fabric-peer-ext/pkg/mocks/mocktxsim.go    |  80 ++
 .../cachedpvtdatastore/store_impl.go          | 285 ++++++
 .../cachedpvtdatastore/test_exports.go        |  46 +
 .../cdbpvtdatastore/couchdb_conv.go           | 277 ++++++
 .../cdbpvtdatastore/store_impl.go             | 851 +++++++++++++++++
 .../cdbpvtdatastore/test_exports.go           |  79 ++
 .../pkg/pvtdatastorage/common/collelgproc.go  | 142 +++
 .../pkg/pvtdatastorage/common/helper.go       | 235 +++++
 .../pkg/pvtdatastorage/common/kv_encoding.go  | 192 ++++
 .../pkg/pvtdatastorage/common/store.go        | 404 ++++++++
 .../pkg/pvtdatastorage/common/v11.go          |  78 ++
 .../pkg/pvtdatastorage/store_impl.go          | 206 ++++
 .../pkg/pvtdatastorage/test_exports.go        |  84 ++
 .../fabric-peer-ext/pkg/roles/roles.go        | 123 +++
 .../fabric-peer-ext/pkg/roles/test_exports.go |  14 +
 .../pkg/testutil/ext_test_env.go              | 159 ++++
 .../common/common_store_helper.go             | 159 ++++
 .../pkg/transientstore/store.go               | 471 +++++++++
 .../pkg/transientstore/store_helper.go        | 190 ++++
 .../pkg/transientstore/storeprovider.go       |  33 +
 vendor/google.golang.org/grpc/backoff.go      |   2 +-
 vendor/google.golang.org/grpc/balancer.go     |   2 +-
 .../grpc/balancer/balancer.go                 |  44 +-
 .../grpc/balancer/base/balancer.go            |  55 +-
 .../grpc/balancer/base/base.go                |  12 +
 .../grpc/balancer/roundrobin/roundrobin.go    |  16 +-
 .../grpc/balancer_conn_wrappers.go            |  34 +-
 .../grpc/balancer_v1_wrapper.go               |  69 +-
 .../grpc_binarylog_v1/binarylog.pb.go         | 900 ++++++++++++++++++
 vendor/google.golang.org/grpc/call.go         |   4 +-
 vendor/google.golang.org/grpc/clientconn.go   | 832 +++++++++-------
 .../grpc/connectivity/connectivity.go         |   5 +-
 .../grpc/credentials/credentials.go           |  55 +-
 .../grpc/credentials/go16.go                  |  57 --
 .../grpc/credentials/go17.go                  |  59 --
 .../grpc/credentials/go18.go                  |  46 -
 .../grpc/credentials/internal/syscallconn.go  |  61 ++
 .../internal/syscallconn_appengine.go}        |  18 +-
 .../grpc/credentials/{go19.go => tls13.go}    |  21 +-
 vendor/google.golang.org/grpc/dialoptions.go  |  71 +-
 vendor/google.golang.org/grpc/go16.go         |  71 --
 vendor/google.golang.org/grpc/go17.go         |  72 --
 vendor/google.golang.org/grpc/interceptor.go  |   2 +-
 .../grpc/internal/binarylog/binarylog.go      | 167 ++++
 .../internal/binarylog/binarylog_testutil.go  |  42 +
 .../grpc/internal/binarylog/env_config.go     | 210 ++++
 .../grpc/internal/binarylog/method_logger.go  | 423 ++++++++
 .../grpc/internal/binarylog/sink.go           | 162 ++++
 .../go16.go => internal/binarylog/util.go}    |  35 +-
 .../grpc/internal/channelz/funcs.go           | 202 +++-
 .../grpc/internal/channelz/types.go           | 311 +++++-
 .../grpc/internal/channelz/types_linux.go     |   2 +-
 .../grpc/internal/channelz/types_nonlinux.go  |  18 +-
 .../{util_linux_go19.go => util_linux.go}     |   2 +-
 ..._nonlinux_pre_go19.go => util_nonlinux.go} |   2 +-
 .../grpc/internal/envconfig/envconfig.go      |  40 +-
 .../grpc/internal/grpcsync/event.go           |  61 ++
 .../grpc/internal/internal.go                 |  32 +-
 .../grpc/internal/syscall/syscall_linux.go    | 114 +++
 .../grpc/internal/syscall/syscall_nonlinux.go |  63 ++
 .../grpc/internal/transport/bdp_estimator.go  |   7 +-
 .../grpc/internal/transport/go16.go           |  52 -
 .../grpc/internal/transport/go17.go           |  53 --
 .../grpc/internal/transport/handler_server.go |   4 +-
 .../grpc/internal/transport/http2_client.go   | 109 ++-
 .../grpc/internal/transport/http2_server.go   |  85 +-
 .../grpc/internal/transport/http_util.go      |  10 +
 .../grpc/internal/transport/transport.go      | 104 +-
 .../grpc/keepalive/keepalive.go               |  64 +-
 .../grpc/metadata/metadata.go                 |   3 +-
 .../grpc/naming/dns_resolver.go               |  11 +-
 vendor/google.golang.org/grpc/naming/go17.go  |  34 -
 .../google.golang.org/grpc/naming/naming.go   |   2 +-
 vendor/google.golang.org/grpc/peer/peer.go    |   2 +-
 .../google.golang.org/grpc/picker_wrapper.go  |  20 +-
 vendor/google.golang.org/grpc/pickfirst.go    |   4 +-
 vendor/google.golang.org/grpc/proxy.go        |  52 +-
 .../grpc/resolver/dns/dns_resolver.go         |  81 +-
 .../grpc/resolver/dns/go17.go                 |  35 -
 .../grpc/resolver/dns/go18.go                 |  29 -
 .../grpc/resolver_conn_wrapper.go             |  93 +-
 vendor/google.golang.org/grpc/rpc_util.go     | 147 ++-
 vendor/google.golang.org/grpc/server.go       | 238 +++--
 .../google.golang.org/grpc/service_config.go  |  20 +-
 .../google.golang.org/grpc/stats/handlers.go  |   3 +-
 vendor/google.golang.org/grpc/stats/stats.go  |   3 +-
 vendor/google.golang.org/grpc/status/go17.go  |  44 -
 .../google.golang.org/grpc/status/status.go   |  25 +-
 vendor/google.golang.org/grpc/stream.go       | 518 +++++++++-
 vendor/google.golang.org/grpc/tap/tap.go      |   2 +-
 vendor/google.golang.org/grpc/version.go      |   2 +-
 281 files changed, 27550 insertions(+), 1889 deletions(-)
 create mode 100644 core/ledger/kvledger/idstore/idstore.go
 create mode 100644 core/ledger/kvledger/idstore/readme.md
 create mode 100644 core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext.go
 create mode 100644 core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext_test.go
 create mode 100644 core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pvtdatahandler.go
 create mode 100644 core/ledger/util/couchdb/couchdb_ext.go
 create mode 100644 extensions/blkstorage/store.go
 create mode 100644 extensions/blkstorage/store_test.go
 create mode 100644 extensions/collections/api/dissemination/dissemination.go
 create mode 100644 extensions/collections/api/store/key.go
 create mode 100644 extensions/collections/api/store/provider.go
 create mode 100644 extensions/collections/api/support/support.go
 create mode 100644 extensions/collections/dissemination/disseminationplan.go
 create mode 100644 extensions/collections/dissemination/disseminationplan_test.go
 create mode 100644 extensions/collections/policy/validator.go
 create mode 100644 extensions/collections/policy/validator_test.go
 create mode 100644 extensions/collections/pvtdatahandler/pvtdatahandler.go
 create mode 100644 extensions/collections/pvtdatahandler/pvtdatahandler_test.go
 create mode 100644 extensions/collections/retriever/retriever.go
 create mode 100644 extensions/collections/retriever/retriever_test.go
 create mode 100644 extensions/collections/storeprovider/mocks/mocktdstore.go
 create mode 100644 extensions/collections/storeprovider/storeprovider.go
 create mode 100644 extensions/collections/storeprovider/storeprovider_test.go
 create mode 100644 extensions/endorser/api/endorser.go
 create mode 100644 extensions/endorser/endorser.go
 create mode 100644 extensions/endorser/endorser_test.go
 create mode 100644 extensions/gossip/api/gossipapi.go
 create mode 100644 extensions/gossip/blockpublisher/blockpublisher.go
 create mode 100644 extensions/gossip/blockpublisher/blockpublisher_test.go
 create mode 100644 extensions/gossip/coordinator/coordinator.go
 create mode 100644 extensions/gossip/coordinator/coordinator_test.go
 create mode 100644 extensions/gossip/dispatcher/dispatcher.go
 create mode 100644 extensions/gossip/dispatcher/dispatcher_test.go
 create mode 100644 extensions/gossip/mocks/blockpublisher.go
 create mode 100644 extensions/idstore/store.go
 create mode 100644 extensions/idstore/store_test.go
 create mode 100644 extensions/mocks/mockdatastore.go
 create mode 100644 extensions/mocks/mockprovider.go
 create mode 100644 extensions/pvtdatastorage/store.go
 create mode 100644 extensions/pvtdatastorage/store_test.go
 create mode 100644 extensions/roles/ledger_roles_config.go
 create mode 100644 extensions/roles/ledger_roles_config_test.go
 create mode 100644 extensions/statedb/store.go
 create mode 100644 extensions/statedb/store_test.go
 create mode 100644 extensions/testutil/ext_test_env.go
 create mode 100644 extensions/transientstore/store.go
 create mode 100644 extensions/transientstore/store_test.go
 create mode 100644 gossip/privdata/dissemination.go
 create mode 100644 peer/chaincode/transientcollconfig_test.go
 create mode 100644 protoutil/blockutils.go
 create mode 100644 protoutil/blockutils_test.go
 create mode 100644 protoutil/chaincodeutils.go
 create mode 100644 protoutil/commonutils.go
 create mode 100644 protoutil/commonutils_test.go
 create mode 100644 protoutil/proputils.go
 create mode 100644 protoutil/proputils_test.go
 create mode 100644 protoutil/readme.md
 create mode 100644 protoutil/txutils.go
 create mode 100644 protoutil/txutils_test.go
 create mode 100644 vendor/github.com/bluele/gcache/LICENSE
 create mode 100644 vendor/github.com/bluele/gcache/arc.go
 create mode 100644 vendor/github.com/bluele/gcache/cache.go
 create mode 100644 vendor/github.com/bluele/gcache/clock.go
 create mode 100644 vendor/github.com/bluele/gcache/lfu.go
 create mode 100644 vendor/github.com/bluele/gcache/lru.go
 create mode 100644 vendor/github.com/bluele/gcache/simple.go
 create mode 100644 vendor/github.com/bluele/gcache/singleflight.go
 create mode 100644 vendor/github.com/bluele/gcache/stats.go
 create mode 100644 vendor/github.com/bluele/gcache/utils.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/LICENSE
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/alphabet.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/base58.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/base58check.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/doc.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/genalphabet.go
 create mode 100644 vendor/github.com/magiconair/properties/assert/assert.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/LICENSE
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/block_serialization.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/blocks_itr.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage_provider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_checkpoint.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/couchdoc_conv.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api/offledger.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/cas.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/dcas.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminationplan.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminator.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/mocks/mockprovider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/policy/validator.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/retriever/olretriever.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstore.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstoreprovider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api/api.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/cache/cache.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatahandler/pvtdatahandler.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatastore/pvtdatastore.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/retriever.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/test_exports.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/store.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/storeprovider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/test_exports.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api/transientdata.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminationplan.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminator.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/mocks/mockprovider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/policy/validator.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/retriever/transientdataretriever.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/api/api.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/cache/cache.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstore.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstoreprovider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/common.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/discovery.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/member.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/peergroup.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/multirequest/multirequest.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/requestmgr/requestmgr.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/collconfigretriever.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/support.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/config/config.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/endorser/endorser.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/blockpublisher/blockpublisher.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/dispatcher/dispatcher.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/couchdoc_conv.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/store_impl.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/test_exports.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockaccesspolicy.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockbuilder.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockhandler.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockpublisher.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdataprovider.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdatastore.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipadapter.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipmsg.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockledger.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockqueryexecutor.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockrwsetbuilder.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocksupport.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocktxsim.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/store_impl.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/test_exports.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/store_impl.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/test_exports.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/collelgproc.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/helper.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/kv_encoding.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/store.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/v11.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/store_impl.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/test_exports.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/roles.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/test_exports.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/testutil/ext_test_env.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/common/common_store_helper.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store_helper.go
 create mode 100644 vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/storeprovider.go
 create mode 100644 vendor/google.golang.org/grpc/binarylog/grpc_binarylog_v1/binarylog.pb.go
 delete mode 100644 vendor/google.golang.org/grpc/credentials/go16.go
 delete mode 100644 vendor/google.golang.org/grpc/credentials/go17.go
 delete mode 100644 vendor/google.golang.org/grpc/credentials/go18.go
 create mode 100644 vendor/google.golang.org/grpc/credentials/internal/syscallconn.go
 rename vendor/google.golang.org/grpc/{naming/go18.go => credentials/internal/syscallconn_appengine.go} (73%)
 rename vendor/google.golang.org/grpc/credentials/{go19.go => tls13.go} (60%)
 delete mode 100644 vendor/google.golang.org/grpc/go16.go
 delete mode 100644 vendor/google.golang.org/grpc/go17.go
 create mode 100644 vendor/google.golang.org/grpc/internal/binarylog/binarylog.go
 create mode 100644 vendor/google.golang.org/grpc/internal/binarylog/binarylog_testutil.go
 create mode 100644 vendor/google.golang.org/grpc/internal/binarylog/env_config.go
 create mode 100644 vendor/google.golang.org/grpc/internal/binarylog/method_logger.go
 create mode 100644 vendor/google.golang.org/grpc/internal/binarylog/sink.go
 rename vendor/google.golang.org/grpc/{status/go16.go => internal/binarylog/util.go} (51%)
 rename vendor/google.golang.org/grpc/internal/channelz/{util_linux_go19.go => util_linux.go} (96%)
 rename vendor/google.golang.org/grpc/internal/channelz/{util_nonlinux_pre_go19.go => util_nonlinux.go} (95%)
 create mode 100644 vendor/google.golang.org/grpc/internal/grpcsync/event.go
 create mode 100644 vendor/google.golang.org/grpc/internal/syscall/syscall_linux.go
 create mode 100644 vendor/google.golang.org/grpc/internal/syscall/syscall_nonlinux.go
 delete mode 100644 vendor/google.golang.org/grpc/internal/transport/go16.go
 delete mode 100644 vendor/google.golang.org/grpc/internal/transport/go17.go
 delete mode 100644 vendor/google.golang.org/grpc/naming/go17.go
 delete mode 100644 vendor/google.golang.org/grpc/resolver/dns/go17.go
 delete mode 100644 vendor/google.golang.org/grpc/resolver/dns/go18.go
 delete mode 100644 vendor/google.golang.org/grpc/status/go17.go

diff --git a/Gopkg.lock b/Gopkg.lock
index 2733e38ce..f52db3264 100644
--- a/Gopkg.lock
+++ b/Gopkg.lock
@@ -93,6 +93,21 @@
   pruneopts = "NUT"
   revision = "3a771d992973f24aa725d07868b467d1ddfceafb"

+[[projects]]
+  digest = "1:b870f4fc8ac5a04b3dbb6540e81a4c16da874e9f922dbe8762a90d9bcc294910"
+  name = "github.com/bluele/gcache"
+  packages = ["."]
+  pruneopts = "NUT"
+  revision = "79ae3b2d8680cbc7ad3dba9db66b8a648575221c"
+
+[[projects]]
+  branch = "master"
+  digest = "1:9af5b27cadd7fefde7d20dfc72470d39679337e972460a251c129062a70f8518"
+  name = "github.com/btcsuite/btcutil"
+  packages = ["base58"]
+  pruneopts = "NUT"
+  revision = "9e5f4b9a998d263e3ce9c56664a7816001ac8000"
+
 [[projects]]
   branch = "master"
   digest = "1:4a029051269e04c040c092eb4ddd92732f8f3a3921a8b43b82b30804e00f3357"
@@ -398,9 +413,12 @@
   version = "v0.1.0"

 [[projects]]
-  digest = "1:d244f8666a838fe6ad70ec8fe77f50ebc29fdc3331a2729ba5886bef8435d10d"
+  digest = "1:4244266b65ea535b8ebd109a327720821707b59f9a37bda738946d52ec69442d"
   name = "github.com/magiconair/properties"
-  packages = ["."]
+  packages = [
+    ".",
+    "assert",
+  ]
   pruneopts = "NUT"
   revision = "c2353362d570a7bfa228149c62842019201cfb71"
   version = "v1.8.0"
@@ -534,12 +552,12 @@
   version = "v2.0.6"

 [[projects]]
-  digest = "1:5cf3f025cbee5951a4ee961de067c8a89fc95a5adabead774f82822efabab121"
+  digest = "1:14715f705ff5dfe0ffd6571d7d201dd8e921030f8070321a79380d8ca4ec1a24"
   name = "github.com/pkg/errors"
   packages = ["."]
   pruneopts = "NUT"
-  revision = "645ef00459ed84a119197bfb8d8205042c6df63d"
-  version = "v0.8.0"
+  revision = "ba968bfe8b2f7e042a574c888954fccecfa385b4"
+  version = "v0.8.1"

 [[projects]]
   digest = "1:0028cb19b2e4c3112225cd871870f2d9cf49b9b4276531f03438a88e94be86fe"
@@ -710,6 +728,57 @@
   pruneopts = "NUT"
   revision = "bea94bb476ccecfbd31b12ed493a971bdb8c904b"

+[[projects]]
+  digest = "1:2af3aeb5d098d72b09e268dfb269f99383d724344676b74c0f5f072b99850cee"
+  name = "github.com/trustbloc/fabric-peer-ext"
+  packages = [
+    "pkg/blkstorage/cdbblkstorage",
+    "pkg/collections/offledger/api",
+    "pkg/collections/offledger/dcas",
+    "pkg/collections/offledger/dissemination",
+    "pkg/collections/offledger/mocks",
+    "pkg/collections/offledger/policy",
+    "pkg/collections/offledger/retriever",
+    "pkg/collections/offledger/storeprovider",
+    "pkg/collections/offledger/storeprovider/store/api",
+    "pkg/collections/offledger/storeprovider/store/cache",
+    "pkg/collections/offledger/storeprovider/store/couchdbstore",
+    "pkg/collections/pvtdatahandler",
+    "pkg/collections/pvtdatastore",
+    "pkg/collections/retriever",
+    "pkg/collections/storeprovider",
+    "pkg/collections/transientdata/api",
+    "pkg/collections/transientdata/dissemination",
+    "pkg/collections/transientdata/mocks",
+    "pkg/collections/transientdata/policy",
+    "pkg/collections/transientdata/retriever",
+    "pkg/collections/transientdata/storeprovider",
+    "pkg/collections/transientdata/storeprovider/store/api",
+    "pkg/collections/transientdata/storeprovider/store/cache",
+    "pkg/collections/transientdata/storeprovider/store/dbstore",
+    "pkg/common",
+    "pkg/common/discovery",
+    "pkg/common/multirequest",
+    "pkg/common/requestmgr",
+    "pkg/common/support",
+    "pkg/config",
+    "pkg/endorser",
+    "pkg/gossip/blockpublisher",
+    "pkg/gossip/dispatcher",
+    "pkg/idstore",
+    "pkg/mocks",
+    "pkg/pvtdatastorage",
+    "pkg/pvtdatastorage/cachedpvtdatastore",
+    "pkg/pvtdatastorage/cdbpvtdatastore",
+    "pkg/pvtdatastorage/common",
+    "pkg/roles",
+    "pkg/testutil",
+    "pkg/transientstore",
+    "pkg/transientstore/common",
+  ]
+  pruneopts = "NUT"
+  revision = "9b20d84ee0a4b89039086a3a2da103584e7ed1d8"
+
 [[projects]]
   digest = "1:3f3f2b36f76d1187ccf6640dd5bdbce43fd3c1a2cc0d747abc1e0de374d13e63"
   name = "github.com/willf/bitset"
@@ -891,24 +960,29 @@
   revision = "c7e5094acea1ca1b899e2259d80a6b0f882f81f8"

 [[projects]]
-  digest = "1:5b805b8e03b29399b344655cac16873f026e54dc0a7c17b381f6f4d4c7b6d741"
+  digest = "1:f2df7088cd31687fdce0f2eee853682416bf1e70baa487094913e3881f1f12f8"
   name = "google.golang.org/grpc"
   packages = [
     ".",
     "balancer",
     "balancer/base",
     "balancer/roundrobin",
+    "binarylog/grpc_binarylog_v1",
     "codes",
     "connectivity",
     "credentials",
+    "credentials/internal",
     "encoding",
     "encoding/proto",
     "grpclog",
     "internal",
     "internal/backoff",
+    "internal/binarylog",
     "internal/channelz",
     "internal/envconfig",
     "internal/grpcrand",
+    "internal/grpcsync",
+    "internal/syscall",
     "internal/transport",
     "keepalive",
     "metadata",
@@ -922,8 +996,8 @@
     "tap",
   ]
   pruneopts = "NUT"
-  revision = "8dea3dc473e90c8179e519d91302d0597c0ca1d1"
-  version = "v1.15.0"
+  revision = "2fdaae294f38ed9a121193c51ec99fecd3b13eb7"
+  version = "v1.19.0"

 [[projects]]
   digest = "1:22b2dee6f30bc8601f087449a2a819df8388e54e9547349c658f14d8f8c590f2"
@@ -978,6 +1052,7 @@
     "github.com/hyperledger/fabric-amcl/amcl/FP256BN",
     "github.com/hyperledger/fabric-lib-go/healthz",
     "github.com/kr/pretty",
+    "github.com/magiconair/properties/assert",
     "github.com/miekg/pkcs11",
     "github.com/mitchellh/mapstructure",
     "github.com/onsi/ginkgo",
@@ -1007,6 +1082,29 @@
     "github.com/tedsuo/ifrit",
     "github.com/tedsuo/ifrit/ginkgomon",
     "github.com/tedsuo/ifrit/grouper",
+    "github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/mocks",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/policy",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatahandler",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatastore",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/mocks",
+    "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/policy",
+    "github.com/trustbloc/fabric-peer-ext/pkg/common",
+    "github.com/trustbloc/fabric-peer-ext/pkg/endorser",
+    "github.com/trustbloc/fabric-peer-ext/pkg/gossip/blockpublisher",
+    "github.com/trustbloc/fabric-peer-ext/pkg/gossip/dispatcher",
+    "github.com/trustbloc/fabric-peer-ext/pkg/idstore",
+    "github.com/trustbloc/fabric-peer-ext/pkg/mocks",
+    "github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage",
+    "github.com/trustbloc/fabric-peer-ext/pkg/roles",
+    "github.com/trustbloc/fabric-peer-ext/pkg/testutil",
+    "github.com/trustbloc/fabric-peer-ext/pkg/transientstore",
     "github.com/willf/bitset",
     "go.etcd.io/etcd/etcdserver/api/snap",
     "go.etcd.io/etcd/pkg/fileutil",
diff --git a/Gopkg.toml b/Gopkg.toml
index 481e92d0c..fb1c7c3ec 100644
--- a/Gopkg.toml
+++ b/Gopkg.toml
@@ -15,6 +15,14 @@ noverify = [
     "github.com/grpc-ecosystem/go-grpc-middleware"
 ]

+[[constraint]]
+  name = "github.com/trustbloc/fabric-peer-ext"
+  revision = "9b20d84ee0a4b89039086a3a2da103584e7ed1d8"
+
+[[override]]
+  name = "github.com/bluele/gcache"
+  revision = "79ae3b2d8680cbc7ad3dba9db66b8a648575221c"
+
 [[constraint]]
   name = "github.com/Knetic/govaluate"
   version = "3.0.0"
@@ -79,7 +87,7 @@ noverify = [

 [[constraint]]
   name = "github.com/pkg/errors"
-  version = "0.8.0"
+  version = "0.8.1"

 [[constraint]]
   name = "github.com/spf13/cobra"
@@ -123,7 +131,7 @@ noverify = [

 [[constraint]]
   name = "google.golang.org/grpc"
-  version = "=1.15.0"
+  version = "=1.19.0"

 [[constraint]]
   name = "gopkg.in/alecthomas/kingpin.v2"
diff --git a/core/chaincode/exectransaction_test.go b/core/chaincode/exectransaction_test.go
index 81b3fd1c9..5ae11dc6e 100644
--- a/core/chaincode/exectransaction_test.go
+++ b/core/chaincode/exectransaction_test.go
@@ -198,7 +198,7 @@ func finitPeer(lis net.Listener, chainIDs ...string) {
 		requestTimeout := viper.GetDuration("ledger.state.couchDBConfig.requestTimeout")
 		createGlobalChangesDB := viper.GetBool("ledger.state.couchDBConfig.createGlobalChangesDB")

-		couchInstance, _ := couchdb.CreateCouchInstance(connectURL, username, password, maxRetries, maxRetriesOnStartup, requestTimeout, createGlobalChangesDB, &disabled.Provider{})
+		couchInstance, _ := couchdb.CreateCouchInstance1_4_1(connectURL, username, password, maxRetries, maxRetriesOnStartup, requestTimeout, createGlobalChangesDB, &disabled.Provider{})
 		db := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: chainID}
 		//drop the test database
 		db.DropDatabase()
diff --git a/core/endorser/endorser.go b/core/endorser/endorser.go
index fc9e03cc7..40ae566bb 100644
--- a/core/endorser/endorser.go
+++ b/core/endorser/endorser.go
@@ -23,7 +23,11 @@ import (
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/common/validation"
 	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/peer"
+	xendorser "github.com/hyperledger/fabric/extensions/endorser"
+	xendorserapi "github.com/hyperledger/fabric/extensions/endorser/api"
 	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
 	pb "github.com/hyperledger/fabric/protos/peer"
 	"github.com/hyperledger/fabric/protos/transientstore"
 	putils "github.com/hyperledger/fabric/protos/utils"
@@ -99,13 +103,18 @@ type Support interface {
 	GetLedgerHeight(channelID string) (uint64, error)
 }

+type rwSetFilter interface {
+	Filter(channelID string, pubSimulationResults *rwset.TxReadWriteSet) (*rwset.TxReadWriteSet, error)
+}
+
 // Endorser provides the Endorser service ProcessProposal
 type Endorser struct {
 	distributePrivateData privateDataDistributor
 	s                     Support
 	PlatformRegistry      *platforms.Registry
 	PvtRWSetAssembler
-	Metrics *EndorserMetrics
+	Metrics     *EndorserMetrics
+	rwSetFilter rwSetFilter
 }

 // validateResult provides the result of endorseProposal verification
@@ -117,6 +126,16 @@ type validateResult struct {
 	resp    *pb.ProposalResponse
 }

+type ledgerProvider func(cid string) ledger.PeerLedger
+
+type qeProviderFactory struct {
+	getLedger ledgerProvider
+}
+
+func (q *qeProviderFactory) GetQueryExecutorProvider(channelID string) xendorserapi.QueryExecutorProvider {
+	return q.getLedger(channelID)
+}
+
 // NewEndorserServer creates and returns a new Endorser server instance.
 func NewEndorserServer(privDist privateDataDistributor, s Support, pr *platforms.Registry, metricsProv metrics.Provider) *Endorser {
 	e := &Endorser{
@@ -125,6 +144,11 @@ func NewEndorserServer(privDist privateDataDistributor, s Support, pr *platforms
 		PlatformRegistry:      pr,
 		PvtRWSetAssembler:     &rwSetAssembler{},
 		Metrics:               NewEndorserMetrics(metricsProv),
+		rwSetFilter: xendorser.NewCollRWSetFilter(
+			&qeProviderFactory{
+				getLedger: peer.GetLedger,
+			},
+			peer.BlockPublisher),
 	}
 	return e
 }
@@ -286,7 +310,11 @@ func (e *Endorser) SimulateProposal(txParams *ccprovider.TransactionParams, cid
 		}

 		txParams.TXSimulator.Done()
-		if pubSimResBytes, err = simResult.GetPubSimulationBytes(); err != nil {
+		pubSimRes, err := e.rwSetFilter.Filter(txParams.ChannelID, simResult.PubSimulationResults)
+		if err != nil {
+			return nil, nil, nil, nil, err
+		}
+		if pubSimResBytes, err = proto.Marshal(pubSimRes); err != nil {
 			return nil, nil, nil, nil, err
 		}
 	}
diff --git a/core/ledger/kvledger/history/historydb/historyleveldb/pkg_test.go b/core/ledger/kvledger/history/historydb/historyleveldb/pkg_test.go
index beede3736..4b5a5c5b4 100644
--- a/core/ledger/kvledger/history/historydb/historyleveldb/pkg_test.go
+++ b/core/ledger/kvledger/history/historydb/historyleveldb/pkg_test.go
@@ -61,7 +61,7 @@ func newTestHistoryEnv(t *testing.T) *levelDBLockBasedHistoryEnv {
 	testDB := testDBEnv.GetDBHandle(testLedgerID)
 	testBookkeepingEnv := bookkeeping.NewTestEnv(t)

-	txMgr, err := lockbasedtxmgr.NewLockBasedTxMgr(testLedgerID, testDB, nil, nil, testBookkeepingEnv.TestProvider, &mock.DeployedChaincodeInfoProvider{})
+	txMgr, err := lockbasedtxmgr.NewLockBasedTxMgr(testLedgerID, testDB, nil, nil, testBookkeepingEnv.TestProvider, &mock.DeployedChaincodeInfoProvider{}, nil)
 	assert.NoError(t, err)
 	testHistoryDBProvider := NewHistoryDBProvider()
 	testHistoryDB, err := testHistoryDBProvider.GetDBHandle("TestHistoryDB")
diff --git a/core/ledger/kvledger/idstore/idstore.go b/core/ledger/kvledger/idstore/idstore.go
new file mode 100644
index 000000000..9e22b45f7
--- /dev/null
+++ b/core/ledger/kvledger/idstore/idstore.go
@@ -0,0 +1,25 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package idstore
+
+import "github.com/hyperledger/fabric/protos/common"
+
+type IDStore interface {
+	SetUnderConstructionFlag(string) error
+	UnsetUnderConstructionFlag() error
+	GetUnderConstructionFlag() (string, error)
+	CreateLedgerID(ledgerID string, gb *common.Block) error
+	LedgerIDExists(ledgerID string) (bool, error)
+	GetAllLedgerIds() ([]string, error)
+	GetLedgeIDValue(ledgerID string) ([]byte, error)
+	Close()
+}
+
+// not implemented for Fabric 1.4.1
+func OpenIDStore(path string) IDStore {
+	return nil
+}
diff --git a/core/ledger/kvledger/idstore/readme.md b/core/ledger/kvledger/idstore/readme.md
new file mode 100644
index 000000000..8f58e92a2
--- /dev/null
+++ b/core/ledger/kvledger/idstore/readme.md
@@ -0,0 +1,2 @@
+idstore was added with an empty OpenStore call
+it has been created for trustbloc/fabric-peer-ext dependency
\ No newline at end of file
diff --git a/core/ledger/kvledger/kv_ledger.go b/core/ledger/kvledger/kv_ledger.go
index 5f8804392..1d753abfc 100644
--- a/core/ledger/kvledger/kv_ledger.go
+++ b/core/ledger/kvledger/kv_ledger.go
@@ -27,6 +27,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/ledgerstorage"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
 	lutil "github.com/hyperledger/fabric/core/ledger/util"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/peer"
 	"github.com/hyperledger/fabric/protos/utils"
@@ -59,6 +60,7 @@ func newKVLedger(
 	bookkeeperProvider bookkeeping.Provider,
 	ccInfoProvider ledger.DeployedChaincodeInfoProvider,
 	stats *ledgerStats,
+	collDataProvider storeapi.Provider,
 ) (*kvLedger, error) {
 	logger.Debugf("Creating KVLedger ledgerID=%s: ", ledgerID)
 	// Create a kvLedger for this chain/ledger, which encasulates the underlying
@@ -81,7 +83,7 @@ func newKVLedger(
 		cceventmgmt.GetMgr().Register(ledgerID, ccEventListener)
 	}
 	btlPolicy := pvtdatapolicy.ConstructBTLPolicy(&collectionInfoRetriever{l, ccInfoProvider})
-	if err := l.initTxMgr(versionedDB, stateListeners, btlPolicy, bookkeeperProvider, ccInfoProvider); err != nil {
+	if err := l.initTxMgr(versionedDB, stateListeners, btlPolicy, bookkeeperProvider, ccInfoProvider, collDataProvider); err != nil {
 		return nil, err
 	}
 	l.initBlockStore(btlPolicy)
@@ -96,9 +98,10 @@ func newKVLedger(
 }

 func (l *kvLedger) initTxMgr(versionedDB privacyenabledstate.DB, stateListeners []ledger.StateListener,
-	btlPolicy pvtdatapolicy.BTLPolicy, bookkeeperProvider bookkeeping.Provider, ccInfoProvider ledger.DeployedChaincodeInfoProvider) error {
+	btlPolicy pvtdatapolicy.BTLPolicy, bookkeeperProvider bookkeeping.Provider, ccInfoProvider ledger.DeployedChaincodeInfoProvider,
+	collDataProvider storeapi.Provider) error {
 	var err error
-	l.txtmgmt, err = lockbasedtxmgr.NewLockBasedTxMgr(l.ledgerID, versionedDB, stateListeners, btlPolicy, bookkeeperProvider, ccInfoProvider)
+	l.txtmgmt, err = lockbasedtxmgr.NewLockBasedTxMgr(l.ledgerID, versionedDB, stateListeners, btlPolicy, bookkeeperProvider, ccInfoProvider, collDataProvider)
 	return err
 }

diff --git a/core/ledger/kvledger/kv_ledger_provider.go b/core/ledger/kvledger/kv_ledger_provider.go
index f33fbe5a1..09417215c 100644
--- a/core/ledger/kvledger/kv_ledger_provider.go
+++ b/core/ledger/kvledger/kv_ledger_provider.go
@@ -20,6 +20,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/ledgerstorage"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/utils"
 	"github.com/pkg/errors"
@@ -51,6 +52,7 @@ type Provider struct {
 	collElgNotifier     *collElgNotifier
 	stats               *stats
 	fileLock            *leveldbhelper.FileLock
+	collDataProvider    storeapi.Provider
 }

 // NewProvider instantiates a new Provider.
@@ -70,7 +72,7 @@ func NewProvider() (ledger.PeerLedgerProvider, error) {

 	logger.Info("ledger provider Initialized")
 	provider := &Provider{idStore, nil,
-		nil, historydbProvider, nil, nil, nil, nil, nil, nil, fileLock}
+		nil, historydbProvider, nil, nil, nil, nil, nil, nil, fileLock, nil}
 	return provider, nil
 }

@@ -99,6 +101,7 @@ func (provider *Provider) Initialize(initializer *ledger.Initializer) error {
 	}
 	provider.stats = newStats(initializer.MetricsProvider)
 	provider.recoverUnderConstructionLedger()
+	provider.collDataProvider = initializer.CollDataProvider
 	return nil
 }

@@ -178,6 +181,7 @@ func (provider *Provider) openInternal(ledgerID string) (ledger.PeerLedger, erro
 		provider.stateListeners, provider.bookkeepingProvider,
 		provider.initializer.DeployedChaincodeInfoProvider,
 		provider.stats.ledgerStats(ledgerID),
+		provider.collDataProvider,
 	)
 	if err != nil {
 		return nil, err
diff --git a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go
index d0b1b4e30..6da679fc2 100644
--- a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go
+++ b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go
@@ -138,7 +138,7 @@ func (b *RWSetBuilder) GetTxSimulationResults() (*ledger.TxSimulationResults, er

 	// Populate the collection-level hashes into pub rwset and compute the proto bytes for pvt rwset
 	if pvtData != nil {
-		if pvtDataProto, err = pvtData.toProtoMsg(); err != nil {
+		if pvtDataProto, err = pvtData.ToProtoMsg(); err != nil {
 			return nil, err
 		}
 		for _, ns := range pvtDataProto.NsPvtRwset {
diff --git a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go
index 0851b1ab4..c48fb86bd 100644
--- a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go
+++ b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go
@@ -128,7 +128,7 @@ func (txRwSet *TxRwSet) FromProtoBytes(protoBytes []byte) error {
 func (txPvtRwSet *TxPvtRwSet) ToProtoBytes() ([]byte, error) {
 	var protoMsg *rwset.TxPvtReadWriteSet
 	var err error
-	if protoMsg, err = txPvtRwSet.toProtoMsg(); err != nil {
+	if protoMsg, err = txPvtRwSet.ToProtoMsg(); err != nil {
 		return nil, err
 	}
 	return proto.Marshal(protoMsg)
@@ -249,7 +249,8 @@ func (txRwSet *TxRwSet) NumCollections() int {
 // functions for private read-write set
 ///////////////////////////////////////////////////////////////////////////////

-func (txPvtRwSet *TxPvtRwSet) toProtoMsg() (*rwset.TxPvtReadWriteSet, error) {
+// ToProtoMsg returns a TxPvtReadWriteSet from the current TxPvtRwSet
+func (txPvtRwSet *TxPvtRwSet) ToProtoMsg() (*rwset.TxPvtReadWriteSet, error) {
 	protoMsg := &rwset.TxPvtReadWriteSet{DataModel: rwset.TxReadWriteSet_KV}
 	var nsProtoMsg *rwset.NsPvtReadWriteSet
 	var err error
diff --git a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util_test.go b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util_test.go
index c7baf57da..2a5922ac1 100644
--- a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util_test.go
+++ b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util_test.go
@@ -193,7 +193,7 @@ func sampleCollHashedRwSet(collectionName string) *CollHashedRwSet {

 func TestTxPvtRwSetConversion(t *testing.T) {
 	txPvtRwSet := sampleTxPvtRwSet()
-	protoMsg, err := txPvtRwSet.toProtoMsg()
+	protoMsg, err := txPvtRwSet.ToProtoMsg()
 	assert.NoError(t, err)
 	txPvtRwSet1, err := TxPvtRwSetFromProtoMsg(protoMsg)
 	assert.NoError(t, err)
diff --git a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
index f335a213f..4a103b7cc 100644
--- a/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
+++ b/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go
@@ -43,7 +43,7 @@ type VersionedDBProvider struct {
 func NewVersionedDBProvider(metricsProvider metrics.Provider) (*VersionedDBProvider, error) {
 	logger.Debugf("constructing CouchDB VersionedDBProvider")
 	couchDBDef := couchdb.GetCouchDBDefinition()
-	couchInstance, err := couchdb.CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := couchdb.CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, metricsProvider)
 	if err != nil {
 		return nil, err
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val.go
index cc1d0acf9..95a60fad7 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val.go
@@ -53,7 +53,7 @@ func (v *collNameValidator) retrieveCollConfigFromStateDB(ns string) (*common.Co
 	return confPkg, nil
 }

-type collConfigCache map[collConfigkey]bool
+type collConfigCache map[collConfigkey]*common.CollectionConfig

 type collConfigkey struct {
 	ns, coll string
@@ -62,20 +62,23 @@ type collConfigkey struct {
 func (c collConfigCache) populate(ns string, pkg *common.CollectionConfigPackage) {
 	// an entry with an empty collection name to indicate that the cache is populated for the namespace 'ns'
 	// see function 'isPopulatedFor'
-	c[collConfigkey{ns, ""}] = true
+	c[collConfigkey{ns, ""}] = nil
 	for _, config := range pkg.Config {
 		sConfig := config.GetStaticCollectionConfig()
 		if sConfig == nil {
+			logger.Warningf("Error getting collection name in namespace [%s]", ns)
 			continue
 		}
-		c[collConfigkey{ns, sConfig.Name}] = true
+		c[collConfigkey{ns, sConfig.Name}] = config
 	}
 }

 func (c collConfigCache) isPopulatedFor(ns string) bool {
-	return c[collConfigkey{ns, ""}]
+	_, ok := c[collConfigkey{ns, ""}]
+	return ok
 }

 func (c collConfigCache) containsCollName(ns, coll string) bool {
-	return c[collConfigkey{ns, coll}]
+	_, ok := c[collConfigkey{ns, coll}]
+	return ok
 }
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext.go
new file mode 100644
index 000000000..89022e523
--- /dev/null
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext.go
@@ -0,0 +1,35 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package lockbasedtxmgr
+
+import (
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/protos/common"
+)
+
+func (v *collNameValidator) getCollConfig(ns, coll string) (*common.CollectionConfig, error) {
+	if !v.cache.isPopulatedFor(ns) {
+		conf, err := v.retrieveCollConfigFromStateDB(ns)
+		if err != nil {
+			return nil, err
+		}
+		v.cache.populate(ns, conf)
+	}
+	config, ok := v.cache.getCollConfig(ns, coll)
+	if !ok {
+		return nil, &ledger.InvalidCollNameError{
+			Ns:   ns,
+			Coll: coll,
+		}
+	}
+	return config, nil
+}
+
+func (c collConfigCache) getCollConfig(ns, coll string) (*common.CollectionConfig, bool) {
+	config, ok := c[collConfigkey{ns, coll}]
+	return config, ok
+}
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext_test.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext_test.go
new file mode 100644
index 000000000..754f6d00f
--- /dev/null
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/collection_val_ext_test.go
@@ -0,0 +1,66 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package lockbasedtxmgr
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
+	"github.com/hyperledger/fabric/core/ledger/mock"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestUnknownCollectionValidation(t *testing.T) {
+	testEnv := testEnvsMap[levelDBtestEnvName]
+	testEnv.init(t, "testLedger", nil)
+	txMgr := testEnv.getTxMgr()
+
+	populateUnknownCollConfigForTest(t, txMgr.(*LockBasedTxMgr),
+		[]collConfigkey{
+			{"ns1", "coll1"},
+		},
+		version.NewHeight(1, 1),
+	)
+
+	sim, err := txMgr.NewTxSimulator("tx-id1")
+	assert.NoError(t, err)
+
+	const key1 = "key1"
+	const key2 = "key1"
+
+	_, err = sim.GetPrivateData("ns1", "coll1", key1)
+	assert.NoError(t, err)
+
+	_, err = sim.GetPrivateDataMultipleKeys("ns1", "coll1", []string{key1, key2})
+	assert.NoError(t, err)
+}
+
+func populateUnknownCollConfigForTest(t *testing.T, txMgr *LockBasedTxMgr, nsColls []collConfigkey, ht *version.Height) {
+	m := map[string]*common.CollectionConfigPackage{}
+	for _, nsColl := range nsColls {
+		ns, coll := nsColl.ns, nsColl.coll
+		pkg, ok := m[ns]
+		if !ok {
+			pkg = &common.CollectionConfigPackage{}
+			m[ns] = pkg
+		}
+		tCollConfig := &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name: coll,
+				Type: common.CollectionType_COL_UNKNOWN,
+			},
+		}
+		pkg.Config = append(pkg.Config, &common.CollectionConfig{Payload: tCollConfig})
+	}
+	ccInfoProvider := &mock.DeployedChaincodeInfoProvider{}
+	ccInfoProvider.ChaincodeInfoStub = func(ccName string, qe ledger.SimpleQueryExecutor) (*ledger.DeployedChaincodeInfo, error) {
+		return &ledger.DeployedChaincodeInfo{Name: ccName, CollectionConfigPkg: m[ccName]}, nil
+	}
+	txMgr.ccInfoProvider = ccInfoProvider
+}
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
index 4533e4c33..2fe7b8842 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go
@@ -17,11 +17,18 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
 	"github.com/hyperledger/fabric/core/ledger/util"
+	"github.com/hyperledger/fabric/extensions/collections/pvtdatahandler"
+	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/ledger/queryresult"
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
 	"github.com/pkg/errors"
 )

+type pvtDataHandler interface {
+	HandleGetPrivateData(txID, ns string, config *common.StaticCollectionConfig, key string) ([]byte, bool, error)
+	HandleGetPrivateDataMultipleKeys(txID, ns string, config *common.StaticCollectionConfig, keys []string) ([][]byte, bool, error)
+}
+
 type queryHelper struct {
 	txmgr             *LockBasedTxMgr
 	collNameValidator *collNameValidator
@@ -29,12 +36,14 @@ type queryHelper struct {
 	itrs              []*resultsItr
 	err               error
 	doneInvoked       bool
+	pvtDataHandler    pvtDataHandler
 }

 func newQueryHelper(txmgr *LockBasedTxMgr, rwsetBuilder *rwsetutil.RWSetBuilder) *queryHelper {
 	helper := &queryHelper{txmgr: txmgr, rwsetBuilder: rwsetBuilder}
 	validator := newCollNameValidator(txmgr.ccInfoProvider, &lockBasedQueryExecutor{helper: helper})
 	helper.collNameValidator = validator
+	helper.pvtDataHandler = pvtdatahandler.New(txmgr.ledgerid, txmgr.collDataProvider)
 	return helper
 }

diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_query_executer.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_query_executer.go
index b684c23e8..29b333558 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_query_executer.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_query_executer.go
@@ -67,7 +67,7 @@ func (q *lockBasedQueryExecutor) ExecuteQueryWithMetadata(namespace, query strin

 // GetPrivateData implements method in interface `ledger.QueryExecutor`
 func (q *lockBasedQueryExecutor) GetPrivateData(namespace, collection, key string) ([]byte, error) {
-	return q.helper.getPrivateData(namespace, collection, key)
+	return q.helper.handleGetPrivateData(q.txid, namespace, collection, key)
 }

 func (q *lockBasedQueryExecutor) GetPrivateDataHash(namespace, collection, key string) ([]byte, error) {
@@ -87,7 +87,7 @@ func (q *lockBasedQueryExecutor) GetPrivateDataMetadataByHash(namespace, collect

 // GetPrivateDataMultipleKeys implements method in interface `ledger.QueryExecutor`
 func (q *lockBasedQueryExecutor) GetPrivateDataMultipleKeys(namespace, collection string, keys []string) ([][]byte, error) {
-	return q.helper.getPrivateDataMultipleKeys(namespace, collection, keys)
+	return q.helper.handleGetPrivateDataMultipleKeys(q.txid, namespace, collection, keys)
 }

 // GetPrivateDataRangeScanIterator implements method in interface `ledger.QueryExecutor`
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
index c4cb4aa5e..844a59011 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_txmgr.go
@@ -22,6 +22,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
 	"github.com/hyperledger/fabric/core/ledger/util"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/ledger/rwset"
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
@@ -41,6 +42,8 @@ type LockBasedTxMgr struct {
 	commitRWLock    sync.RWMutex
 	oldBlockCommit  sync.Mutex
 	current         *current
+
+	collDataProvider storeapi.Provider
 }

 type current struct {
@@ -59,7 +62,8 @@ func (c *current) maxTxNumber() uint64 {

 // NewLockBasedTxMgr constructs a new instance of NewLockBasedTxMgr
 func NewLockBasedTxMgr(ledgerid string, db privacyenabledstate.DB, stateListeners []ledger.StateListener,
-	btlPolicy pvtdatapolicy.BTLPolicy, bookkeepingProvider bookkeeping.Provider, ccInfoProvider ledger.DeployedChaincodeInfoProvider) (*LockBasedTxMgr, error) {
+	btlPolicy pvtdatapolicy.BTLPolicy, bookkeepingProvider bookkeeping.Provider, ccInfoProvider ledger.DeployedChaincodeInfoProvider,
+	collDataProvider storeapi.Provider) (*LockBasedTxMgr, error) {
 	db.Open()
 	txmgr := &LockBasedTxMgr{
 		ledgerid:       ledgerid,
@@ -67,6 +71,8 @@ func NewLockBasedTxMgr(ledgerid string, db privacyenabledstate.DB, stateListener
 		stateListeners: stateListeners,
 		ccInfoProvider: ccInfoProvider,
 	}
+	txmgr.collDataProvider = collDataProvider
+
 	pvtstatePurgeMgr, err := pvtstatepurgemgmt.InstantiatePurgeMgr(ledgerid, db, btlPolicy, bookkeepingProvider)
 	if err != nil {
 		return nil, err
@@ -113,7 +119,7 @@ func (txmgr *LockBasedTxMgr) ValidateAndPrepare(blockAndPvtdata *ledger.BlockAnd
 	// interleave and nullify the optimization provided by the bulk read API.
 	// Once the ledger cache (FAB-103) is introduced and existing
 	// LoadCommittedVersions() is refactored to return a map, we can allow
-	// these three functions to execute parallely.
+	// these three functions to execute parallelly.
 	logger.Debugf("Waiting for purge mgr to finish the background job of computing expirying keys for the block")
 	txmgr.pvtdataPurgeMgr.WaitForPrepareToFinish()
 	txmgr.oldBlockCommit.Lock()
@@ -159,7 +165,7 @@ func (txmgr *LockBasedTxMgr) RemoveStaleAndCommitPvtDataOfOldBlocks(blocksPvtDat
 	// interleave and nullify the optimization provided by the bulk read API.
 	// Once the ledger cache (FAB-103) is introduced and existing
 	// LoadCommittedVersions() is refactored to return a map, we can allow
-	// these three functions to execute parallely. However, we cannot remove
+	// these three functions to execute parallelly. However, we cannot remove
 	// the lock on oldBlockCommit as it is also used to avoid interleaving
 	// between Commit() and execution of this function for the correctness.
 	logger.Debug("Waiting for purge mgr to finish the background job of computing expirying keys for the block")
@@ -486,7 +492,7 @@ func (txmgr *LockBasedTxMgr) Commit() error {
 	if err := txmgr.pvtdataPurgeMgr.BlockCommitDone(); err != nil {
 		return err
 	}
-	// In the case of error state listeners will not recieve this call - instead a peer panic is caused by the ledger upon receiveing
+	// In the case of error state listeners will not receive this call - instead a peer panic is caused by the ledger upon receiveing
 	// an error from this function
 	txmgr.updateStateListeners()
 	return nil
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pkg_test.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pkg_test.go
index 24f6a3d83..57f85d11f 100644
--- a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pkg_test.go
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pkg_test.go
@@ -30,6 +30,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/mock"
 	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
 	"github.com/hyperledger/fabric/core/ledger/util"
+	"github.com/hyperledger/fabric/extensions/mocks"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/ledger/queryresult"
 	"github.com/hyperledger/fabric/protos/ledger/rwset"
@@ -90,7 +91,8 @@ func (env *lockBasedEnv) init(t *testing.T, testLedgerID string, btlPolicy pvtda
 	env.txmgr, err = NewLockBasedTxMgr(
 		testLedgerID, env.testDB, nil,
 		btlPolicy, env.testBookkeepingEnv.TestProvider,
-		&mock.DeployedChaincodeInfoProvider{})
+		&mock.DeployedChaincodeInfoProvider{},
+		&mocks.DataProvider{})
 	assert.NoError(t, err)

 }
diff --git a/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pvtdatahandler.go b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pvtdatahandler.go
new file mode 100644
index 000000000..987225ad7
--- /dev/null
+++ b/core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/pvtdatahandler.go
@@ -0,0 +1,65 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package lockbasedtxmgr
+
+import (
+	"github.com/pkg/errors"
+)
+
+func (h *queryHelper) handleGetPrivateData(txID, ns, coll, key string) ([]byte, error) {
+	if err := h.checkDone(); err != nil {
+		return nil, err
+	}
+
+	config, err := h.collNameValidator.getCollConfig(ns, coll)
+	if err != nil {
+		return nil, err
+	}
+
+	staticConfig := config.GetStaticCollectionConfig()
+	if staticConfig == nil {
+		return nil, errors.New("invalid collection config")
+	}
+
+	value, handled, err := h.pvtDataHandler.HandleGetPrivateData(txID, ns, staticConfig, key)
+	if err != nil {
+		return nil, err
+	}
+
+	if handled {
+		return value, nil
+	}
+
+	return h.getPrivateData(ns, coll, key)
+}
+
+func (h *queryHelper) handleGetPrivateDataMultipleKeys(txID, ns, coll string, keys []string) ([][]byte, error) {
+	if err := h.checkDone(); err != nil {
+		return nil, err
+	}
+
+	config, err := h.collNameValidator.getCollConfig(ns, coll)
+	if err != nil {
+		return nil, err
+	}
+
+	staticConfig := config.GetStaticCollectionConfig()
+	if staticConfig == nil {
+		return nil, errors.New("invalid collection config")
+	}
+
+	value, handled, err := h.pvtDataHandler.HandleGetPrivateDataMultipleKeys(txID, ns, staticConfig, keys)
+	if err != nil {
+		return nil, err
+	}
+
+	if handled {
+		return value, nil
+	}
+
+	return h.getPrivateDataMultipleKeys(ns, coll, keys)
+}
diff --git a/core/ledger/ledger_interface.go b/core/ledger/ledger_interface.go
index 36bb776d0..5bf878de9 100644
--- a/core/ledger/ledger_interface.go
+++ b/core/ledger/ledger_interface.go
@@ -13,6 +13,7 @@ import (
 	"github.com/hyperledger/fabric-lib-go/healthz"
 	commonledger "github.com/hyperledger/fabric/common/ledger"
 	"github.com/hyperledger/fabric/common/metrics"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/ledger/rwset"
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
@@ -26,6 +27,7 @@ type Initializer struct {
 	MembershipInfoProvider        MembershipInfoProvider
 	MetricsProvider               metrics.Provider
 	HealthCheckRegistry           HealthCheckRegistry
+	CollDataProvider              storeapi.Provider
 }

 // PeerLedgerProvider provides handle to ledger instances
diff --git a/core/ledger/ledgermgmt/ledger_mgmt.go b/core/ledger/ledgermgmt/ledger_mgmt.go
index eb92047ff..bfb0ac6e3 100644
--- a/core/ledger/ledgermgmt/ledger_mgmt.go
+++ b/core/ledger/ledgermgmt/ledger_mgmt.go
@@ -18,6 +18,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
 	"github.com/hyperledger/fabric/core/ledger/customtx"
 	"github.com/hyperledger/fabric/core/ledger/kvledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/utils"
 	"github.com/pkg/errors"
@@ -45,6 +46,7 @@ type Initializer struct {
 	MembershipInfoProvider        ledger.MembershipInfoProvider
 	MetricsProvider               metrics.Provider
 	HealthCheckRegistry           ledger.HealthCheckRegistry
+	CollDataProvider              storeapi.Provider
 }

 // Initialize initializes ledgermgmt
@@ -76,6 +78,7 @@ func initialize(initializer *Initializer) {
 		MembershipInfoProvider:        initializer.MembershipInfoProvider,
 		MetricsProvider:               initializer.MetricsProvider,
 		HealthCheckRegistry:           initializer.HealthCheckRegistry,
+		CollDataProvider:              initializer.CollDataProvider,
 	})
 	if err != nil {
 		panic(errors.WithMessage(err, "Error initializing ledger provider"))
diff --git a/core/ledger/pvtdatastorage/store.go b/core/ledger/pvtdatastorage/store.go
index afd476352..b9a36089d 100644
--- a/core/ledger/pvtdatastorage/store.go
+++ b/core/ledger/pvtdatastorage/store.go
@@ -79,6 +79,11 @@ type Store interface {
 	Shutdown()
 }

+// NewErrIllegalCall creates an illegal call error
+func NewErrIllegalCall(msg string) *ErrIllegalCall {
+	return &ErrIllegalCall{msg: msg}
+}
+
 // ErrIllegalCall is to be thrown by a store impl if the store does not expect a call to Prepare/Commit/InitLastCommittedBlock
 type ErrIllegalCall struct {
 	msg string
@@ -97,6 +102,11 @@ func (err *ErrIllegalArgs) Error() string {
 	return err.msg
 }

+// NewErrOutOfRange creates an out of range error
+func NewErrOutOfRange(msg string) *ErrOutOfRange {
+	return &ErrOutOfRange{msg: msg}
+}
+
 // ErrOutOfRange is to be thrown for the request for the data that is not yet committed
 type ErrOutOfRange struct {
 	msg string
diff --git a/core/ledger/util/couchdb/couchdb.go b/core/ledger/util/couchdb/couchdb.go
index e12239e21..7a896258f 100644
--- a/core/ledger/util/couchdb/couchdb.go
+++ b/core/ledger/util/couchdb/couchdb.go
@@ -116,6 +116,40 @@ type QueryResult struct {
 	Attachments []*AttachmentInfo
 }

+// Config is a structure used to configure a CouchInstance.
+// NOTE: This struct is added in order to satisfy the compilation of fabric-peer-ext.
+type Config struct {
+	// Address is the hostname:port of the CouchDB database instance.
+	Address string
+	// Username is the username used to authenticate with CouchDB.  This username
+	// must have read and write access permissions.
+	Username string
+	// Password is the password for Username.
+	Password string
+	// MaxRetries is the maximum number of times to retry CouchDB operations on
+	// failure.
+	MaxRetries int
+	// MaxRetriesOnStartup is the maximum number of times to retry CouchDB operations on
+	// failure when initializing the ledger.
+	MaxRetriesOnStartup int
+	// RequestTimeout is the timeout used for CouchDB operations.
+	RequestTimeout time.Duration
+	// InternalQueryLimit is the maximum number of records to return internally
+	// when querying CouchDB.
+	InternalQueryLimit int
+	// MaxBatchUpdateSize is the maximum number of records to included in CouchDB
+	// bulk update operations.
+	MaxBatchUpdateSize int
+	// WarmIndexesAfterNBlocks is the number of blocks after which to warm any
+	// CouchDB indexes.
+	WarmIndexesAfterNBlocks int
+	// CreateGlobalChangesDB determines whether or not to create the "_global_changes"
+	// system database.
+	CreateGlobalChangesDB bool
+	// RedoLogPath is the directory where the CouchDB redo log files are stored.
+	RedoLogPath string
+}
+
 //CouchConnectionDef contains parameters
 type CouchConnectionDef struct {
 	URL                   string
diff --git a/core/ledger/util/couchdb/couchdb_ext.go b/core/ledger/util/couchdb/couchdb_ext.go
new file mode 100644
index 000000000..ceee7809c
--- /dev/null
+++ b/core/ledger/util/couchdb/couchdb_ext.go
@@ -0,0 +1,12 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdb
+
+// CreateNewIndexWithRetry added to match Fabric 2.0 but not used in 1.4.1
+func (dbclient *CouchDatabase) CreateNewIndexWithRetry(indexdefinition string, designDoc string) error {
+	return nil
+}
diff --git a/core/ledger/util/couchdb/couchdb_test.go b/core/ledger/util/couchdb/couchdb_test.go
index 52edac4be..4deb08ef4 100644
--- a/core/ledger/util/couchdb/couchdb_test.go
+++ b/core/ledger/util/couchdb/couchdb_test.go
@@ -35,7 +35,7 @@ var couchDBDef *CouchDBDef

 func cleanup(database string) error {
 	//create a new connection
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	if err != nil {
 		fmt.Println("Unexpected error", err)
@@ -258,7 +258,7 @@ func TestDBCreateSaveWithoutRevision(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -281,7 +281,7 @@ func TestDBCreateEnsureFullCommit(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -302,28 +302,28 @@ func TestDBCreateEnsureFullCommit(t *testing.T) {
 func TestDBBadDatabaseName(t *testing.T) {

 	//create a new instance and database object using a valid database name mixed case
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	_, dberr := CreateCouchDatabase(couchInstance, "testDB")
 	assert.Error(t, dberr, "Error should have been thrown for an invalid db name")

 	//create a new instance and database object using a valid database name letters and numbers
-	couchInstance, err = CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err = CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	_, dberr = CreateCouchDatabase(couchInstance, "test132")
 	assert.NoError(t, dberr, "Error when testing a valid database name")

 	//create a new instance and database object using a valid database name - special characters
-	couchInstance, err = CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err = CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	_, dberr = CreateCouchDatabase(couchInstance, "test1234~!@#$%^&*()[]{}.")
 	assert.Error(t, dberr, "Error should have been thrown for an invalid db name")

 	//create a new instance and database object using a invalid database name - too long	/*
-	couchInstance, err = CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err = CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	_, dberr = CreateCouchDatabase(couchInstance, "a12345678901234567890123456789012345678901234"+
@@ -338,7 +338,7 @@ func TestDBBadConnection(t *testing.T) {

 	//create a new instance and database object
 	//Limit the maxRetriesOnStartup to 3 in order to reduce time for the failure
-	_, err := CreateCouchInstance(badConnectURL, couchDBDef.Username, couchDBDef.Password,
+	_, err := CreateCouchInstance1_4_1(badConnectURL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, 3, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.Error(t, err, "Error should have been thrown for a bad connection")
 }
@@ -351,7 +351,7 @@ func TestBadDBCredentials(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	_, err = CreateCouchInstance(couchDBDef.URL, "fred", "fred",
+	_, err = CreateCouchInstance1_4_1(couchDBDef.URL, "fred", "fred",
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.Error(t, err, "Error should have been thrown for bad credentials")

@@ -381,7 +381,7 @@ func testDBCreateDatabaseAndPersist(t *testing.T, maxRetries int) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		maxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -600,12 +600,12 @@ func TestDBRequestTimeout(t *testing.T) {

 	//create a new instance and database object with a timeout that will fail
 	//Also use a maxRetriesOnStartup=3 to reduce the number of retries
-	_, err = CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	_, err = CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, 3, impossibleTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.Error(t, err, "Error should have been thown while trying to create a couchdb instance with a connection timeout")

 	//create a new instance and database object
-	_, err = CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	_, err = CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		-1, 3, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.Error(t, err, "Error should have been thrown while attempting to create a database")

@@ -619,7 +619,7 @@ func TestDBTimeoutConflictRetry(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, 3, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -659,7 +659,7 @@ func TestDBBadNumberOfRetries(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	_, err = CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	_, err = CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		-1, 3, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.Error(t, err, "Error should have been thrown while attempting to create a database")

@@ -673,7 +673,7 @@ func TestDBBadJSON(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -703,7 +703,7 @@ func TestPrefixScan(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -773,7 +773,7 @@ func TestDBSaveAttachment(t *testing.T) {
 	attachments = append(attachments, attachment)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -803,7 +803,7 @@ func TestDBDeleteDocument(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -838,7 +838,7 @@ func TestDBDeleteNonExistingDocument(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -887,7 +887,7 @@ func TestIndexOperations(t *testing.T) {
 	byteJSON10 := []byte(`{"_id":"10", "asset_name":"marble10","color":"white","size":10,"owner":"tom"}`)

 	//create a new instance and database object   --------------------------------------------------------
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -1146,7 +1146,7 @@ func TestRichQuery(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object   --------------------------------------------------------
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -1387,7 +1387,7 @@ func testBatchBatchOperations(t *testing.T, maxRetries int) {
 	defer cleanup(database)

 	//create a new instance and database object   --------------------------------------------------------
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -1588,7 +1588,7 @@ func TestDatabaseSecuritySettings(t *testing.T) {
 	defer cleanup(database)

 	//create a new instance and database object   --------------------------------------------------------
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
@@ -1651,7 +1651,7 @@ func TestURLWithSpecialCharacters(t *testing.T) {
 	assert.Equal(t, "http://127.0.0.1:5984/testdb%2Bwith%2Bplus_sign/_index/designdoc/json/indexname", couchdbURL.String())

 	//create a new instance and database object   --------------------------------------------------------
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to create couch instance")
 	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
diff --git a/core/ledger/util/couchdb/couchdbutil.go b/core/ledger/util/couchdb/couchdbutil.go
index 9fef90fb6..2d42b06a5 100644
--- a/core/ledger/util/couchdb/couchdbutil.go
+++ b/core/ledger/util/couchdb/couchdbutil.go
@@ -32,8 +32,13 @@ var chainNameAllowedLength = 50
 var namespaceNameAllowedLength = 50
 var collectionNameAllowedLength = 50

-//CreateCouchInstance creates a CouchDB instance
-func CreateCouchInstance(couchDBConnectURL, id, pw string, maxRetries,
+//CreateCouchInstance should never be called by Fabric 1.4.1 code. It's added here so that fabric-peer-ext compiles
+func CreateCouchInstance(config *Config, metricsProvider metrics.Provider) (*CouchInstance, error) {
+	panic("not implemented")
+}
+
+//CreateCouchInstance1_4_1 creates a CouchDB instance using Fabric 1.4.1.
+func CreateCouchInstance1_4_1(couchDBConnectURL, id, pw string, maxRetries,
 	maxRetriesOnStartup int, connectionTimeout time.Duration, createGlobalChangesDB bool, metricsProvider metrics.Provider) (*CouchInstance, error) {

 	couchConf, err := CreateConnectionDefinition(couchDBConnectURL,
diff --git a/core/ledger/util/couchdb/couchdbutil_test.go b/core/ledger/util/couchdb/couchdbutil_test.go
index 5d1a97540..c1ccf0077 100644
--- a/core/ledger/util/couchdb/couchdbutil_test.go
+++ b/core/ledger/util/couchdb/couchdbutil_test.go
@@ -21,7 +21,7 @@ func TestCreateCouchDBConnectionAndDB(t *testing.T) {
 	cleanup(database)
 	defer cleanup(database)
 	//create a new connection
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to CreateCouchInstance")

@@ -40,7 +40,7 @@ func TestNotCreateCouchGlobalChangesDB(t *testing.T) {
 	defer cleanup(database)

 	//create a new connection
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
 	assert.NoError(t, err, "Error when trying to CreateCouchInstance")

@@ -63,7 +63,7 @@ func TestCreateCouchDBSystemDBs(t *testing.T) {
 	defer cleanup(database)

 	//create a new connection
-	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+	couchInstance, err := CreateCouchInstance1_4_1(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
 		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})

 	assert.NoError(t, err, "Error when trying to CreateCouchInstance")
diff --git a/core/ledger/util/couchdb/metrics_test.go b/core/ledger/util/couchdb/metrics_test.go
index 59c8c25d8..d5962e78f 100644
--- a/core/ledger/util/couchdb/metrics_test.go
+++ b/core/ledger/util/couchdb/metrics_test.go
@@ -22,7 +22,7 @@ func TestAPIProcessTimeMetric(t *testing.T) {
 	fakeHistogram.WithReturns(fakeHistogram)

 	// create a new couch instance
-	couchInstance, err := CreateCouchInstance(
+	couchInstance, err := CreateCouchInstance1_4_1(
 		couchDBDef.URL,
 		couchDBDef.Username,
 		couchDBDef.Password,
diff --git a/core/peer/configtx_test.go b/core/peer/configtx_test.go
index c36a596ca..928da35dc 100644
--- a/core/peer/configtx_test.go
+++ b/core/peer/configtx_test.go
@@ -26,6 +26,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/customtx"
 	"github.com/hyperledger/fabric/core/ledger/ledgermgmt"
+	"github.com/hyperledger/fabric/extensions/mocks"
 	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
 	ordererconfig "github.com/hyperledger/fabric/orderer/common/localconfig"
 	"github.com/hyperledger/fabric/protos/common"
@@ -52,6 +53,7 @@ func TestConfigTxCreateLedger(t *testing.T) {
 	ledgermgmt.InitializeTestEnvWithInitializer(
 		&ledgermgmt.Initializer{
 			CustomTxProcessors: ConfigTxProcessors,
+			CollDataProvider:   &mocks.DataProvider{},
 		},
 	)

@@ -76,6 +78,7 @@ func TestConfigTxUpdateChanConfig(t *testing.T) {
 	ledgermgmt.InitializeTestEnvWithInitializer(
 		&ledgermgmt.Initializer{
 			CustomTxProcessors: ConfigTxProcessors,
+			CollDataProvider:   &mocks.DataProvider{},
 		},
 	)

@@ -118,6 +121,7 @@ func TestGenesisBlockCreateLedger(t *testing.T) {
 	ledgermgmt.InitializeTestEnvWithInitializer(
 		&ledgermgmt.Initializer{
 			CustomTxProcessors: ConfigTxProcessors,
+			CollDataProvider:   &mocks.DataProvider{},
 		},
 	)

@@ -137,6 +141,7 @@ func TestCustomTxProcessors(t *testing.T) {

 	ledgermgmt.InitializeExistingTestEnvWithInitializer(&ledgermgmt.Initializer{
 		CustomTxProcessors: ConfigTxProcessors,
+		CollDataProvider:   &mocks.DataProvider{},
 	})
 	defer ledgermgmt.CleanupTestEnv()

diff --git a/core/peer/mock_helpers.go b/core/peer/mock_helpers.go
index dfd5695ce..5c44e9f32 100644
--- a/core/peer/mock_helpers.go
+++ b/core/peer/mock_helpers.go
@@ -13,6 +13,7 @@ import (
 	mockpolicies "github.com/hyperledger/fabric/common/mocks/policies"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/ledgermgmt"
+	"github.com/hyperledger/fabric/extensions/mocks"
 )

 //MockInitialize resets chains for test env
@@ -20,6 +21,7 @@ func MockInitialize() {
 	ledgermgmt.InitializeTestEnvWithInitializer(
 		&ledgermgmt.Initializer{
 			CustomTxProcessors: ConfigTxProcessors,
+			CollDataProvider:   &mocks.DataProvider{},
 		},
 	)
 	chains.list = make(map[string]*chain)
diff --git a/core/peer/peer.go b/core/peer/peer.go
index 4d8659147..147843313 100644
--- a/core/peer/peer.go
+++ b/core/peer/peer.go
@@ -33,6 +33,9 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/customtx"
 	"github.com/hyperledger/fabric/core/ledger/ledgermgmt"
 	"github.com/hyperledger/fabric/core/transientstore"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/collections/storeprovider"
+	"github.com/hyperledger/fabric/extensions/gossip/blockpublisher"
 	"github.com/hyperledger/fabric/gossip/api"
 	"github.com/hyperledger/fabric/gossip/service"
 	"github.com/hyperledger/fabric/msp"
@@ -112,6 +115,26 @@ type chainSupport struct {

 var TransientStoreFactory = &storeProvider{stores: make(map[string]transientstore.Store)}

+var collectionDataStoreFactory CollStoreProvider
+var initCollDataStoreFactoryOnce sync.Once
+
+// CollStoreProvider manages the collection stores for multiple channels
+type CollStoreProvider interface {
+	StoreForChannel(channelID string) storeapi.Store
+	OpenStore(channelID string) (storeapi.Store, error)
+}
+
+// CollectionDataStoreFactory returns transient data stores by channel ID
+func CollectionDataStoreFactory() CollStoreProvider {
+	initCollDataStoreFactoryOnce.Do(func() {
+		collectionDataStoreFactory = storeprovider.NewProviderFactory()
+	})
+	return collectionDataStoreFactory
+}
+
+// publisher manages the block publishers for all channels
+var BlockPublisher = blockpublisher.NewProvider()
+
 type storeProvider struct {
 	stores map[string]transientstore.Store
 	transientstore.StoreProvider
@@ -240,7 +263,8 @@ var validationWorkersSemaphore *semaphore.Weighted
 // ready
 func Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider,
 	pm txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider,
-	membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider) {
+	membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider,
+	collDataProvider storeapi.Provider) {
 	nWorkers := viper.GetInt("peer.validatorPoolSize")
 	if nWorkers <= 0 {
 		nWorkers = runtime.NumCPU()
@@ -258,6 +282,7 @@ func Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccp
 		DeployedChaincodeInfoProvider: deployedCCInfoProvider,
 		MembershipInfoProvider:        membershipProvider,
 		MetricsProvider:               metricsProvider,
+		CollDataProvider:              collDataProvider,
 	})
 	ledgerIds, err := ledgermgmt.GetLedgerIDs()
 	if err != nil {
@@ -438,12 +463,19 @@ func createChain(
 		*semaphore.Weighted
 	}{cs, validationWorkersSemaphore}
 	validator := txvalidator.NewTxValidator(cid, vcs, sccp, pm)
+	blockPublisher := BlockPublisher.ForChannel(cid)
 	c := committer.NewLedgerCommitterReactive(ledger, func(block *common.Block) error {
-		chainID, err := utils.GetChainIDFromBlock(block)
-		if err != nil {
-			return err
+		// Updating CSCC with new configuration block
+		if utils.IsConfigBlock(block) {
+			logger.Debug("Received configuration update, calling CSCC ConfigUpdate")
+			err := SetCurrConfigBlock(block, cid)
+			if err != nil {
+				return err
+			}
 		}
-		return SetCurrConfigBlock(block, chainID)
+		// Inform applicable registered handlers of the new block
+		blockPublisher.Publish(block)
+		return nil
 	})

 	oc, ok := bundle.OrdererConfig()
@@ -477,6 +509,11 @@ func createChain(
 		return errors.Wrapf(err, "[channel %s] failed opening transient store", bundle.ConfigtxValidator().ChainID())
 	}

+	collDataStore, err := CollectionDataStoreFactory().OpenStore(bundle.ConfigtxValidator().ChainID())
+	if err != nil {
+		return errors.Wrapf(err, "[channel %s] failed opening transient data store", bundle.ConfigtxValidator().ChainID())
+	}
+
 	csStoreSupport := &CollectionSupport{
 		PeerLedger: ledger,
 	}
@@ -492,9 +529,13 @@ func createChain(
 		Validator:            validator,
 		Committer:            c,
 		Store:                store,
+		CollDataStore:        collDataStore,
 		Cs:                   simpleCollectionStore,
 		IdDeserializeFactory: csStoreSupport,
 		CapabilityProvider:   cp,
+		Capabilities:         cs.Application.Capabilities(),
+		Ledger:               ledger,
+		BlockPublisher:       blockPublisher,
 	})

 	chains.Lock()
diff --git a/core/peer/peer_impl.go b/core/peer/peer_impl.go
index 7e33563df..8665c0407 100644
--- a/core/peer/peer_impl.go
+++ b/core/peer/peer_impl.go
@@ -15,6 +15,7 @@ import (
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/common/sysccprovider"
 	"github.com/hyperledger/fabric/core/ledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	pb "github.com/hyperledger/fabric/protos/peer"
 )
@@ -31,7 +32,7 @@ type Operations interface {
 	GetMSPIDs(cid string) []string
 	GetPolicyManager(cid string) policies.Manager
 	InitChain(cid string)
-	Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, pm txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider)
+	Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, pm txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider, collDataProvider storeapi.Provider)
 }

 type peerImpl struct {
@@ -43,7 +44,8 @@ type peerImpl struct {
 	getMSPIDs            func(cid string) []string
 	getPolicyManager     func(cid string) policies.Manager
 	initChain            func(cid string)
-	initialize           func(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider)
+	initialize           func(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider,
+		collDataProvider storeapi.Provider)
 }

 // Default provides in implementation of the Peer interface that provides
@@ -74,6 +76,6 @@ func (p *peerImpl) GetLedger(cid string) ledger.PeerLedger       { return p.getL
 func (p *peerImpl) GetMSPIDs(cid string) []string                { return p.getMSPIDs(cid) }
 func (p *peerImpl) GetPolicyManager(cid string) policies.Manager { return p.getPolicyManager(cid) }
 func (p *peerImpl) InitChain(cid string)                         { p.initChain(cid) }
-func (p *peerImpl) Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider) {
-	p.initialize(init, ccp, sccp, mapper, pr, deployedCCInfoProvider, membershipProvider, metricsProvider)
+func (p *peerImpl) Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider, tdp storeapi.Provider) {
+	p.initialize(init, ccp, sccp, mapper, pr, deployedCCInfoProvider, membershipProvider, metricsProvider, tdp)
 }
diff --git a/core/peer/peer_test.go b/core/peer/peer_test.go
index a2b7cd7b8..bce36c0b6 100644
--- a/core/peer/peer_test.go
+++ b/core/peer/peer_test.go
@@ -27,6 +27,7 @@ import (
 	ledgermocks "github.com/hyperledger/fabric/core/ledger/mock"
 	"github.com/hyperledger/fabric/core/mocks/ccprovider"
 	fakeconfig "github.com/hyperledger/fabric/core/peer/mocks"
+	storemocks "github.com/hyperledger/fabric/extensions/mocks"
 	"github.com/hyperledger/fabric/gossip/api"
 	"github.com/hyperledger/fabric/gossip/service"
 	"github.com/hyperledger/fabric/msp/mgmt"
@@ -90,14 +91,14 @@ func TestInitialize(t *testing.T) {
 	cleanup := setupPeerFS(t)
 	defer cleanup()

-	Initialize(nil, &ccprovider.MockCcProviderImpl{}, (&mscc.MocksccProviderFactory{}).NewSystemChaincodeProvider(), txvalidator.MapBasedPluginMapper(map[string]validation.PluginFactory{}), nil, &ledgermocks.DeployedChaincodeInfoProvider{}, nil, &disabled.Provider{})
+	Initialize(nil, &ccprovider.MockCcProviderImpl{}, (&mscc.MocksccProviderFactory{}).NewSystemChaincodeProvider(), txvalidator.MapBasedPluginMapper(map[string]validation.PluginFactory{}), nil, &ledgermocks.DeployedChaincodeInfoProvider{}, nil, &disabled.Provider{}, &storemocks.DataProvider{})
 }

 func TestCreateChainFromBlock(t *testing.T) {
 	cleanup := setupPeerFS(t)
 	defer cleanup()

-	Initialize(nil, &ccprovider.MockCcProviderImpl{}, (&mscc.MocksccProviderFactory{}).NewSystemChaincodeProvider(), txvalidator.MapBasedPluginMapper(map[string]validation.PluginFactory{}), &platforms.Registry{}, &ledgermocks.DeployedChaincodeInfoProvider{}, nil, &disabled.Provider{})
+	Initialize(nil, &ccprovider.MockCcProviderImpl{}, (&mscc.MocksccProviderFactory{}).NewSystemChaincodeProvider(), txvalidator.MapBasedPluginMapper(map[string]validation.PluginFactory{}), &platforms.Registry{}, &ledgermocks.DeployedChaincodeInfoProvider{}, nil, &disabled.Provider{}, &storemocks.DataProvider{})
 	testChainID := fmt.Sprintf("mytestchainid-%d", rand.Int())
 	block, err := configtxtest.MakeGenesisBlock(testChainID)
 	if err != nil {
diff --git a/extensions/blkstorage/store.go b/extensions/blkstorage/store.go
new file mode 100644
index 000000000..e0b602ac4
--- /dev/null
+++ b/extensions/blkstorage/store.go
@@ -0,0 +1,27 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package blkstorage
+
+import (
+	"github.com/hyperledger/fabric/common/ledger/blkstorage"
+	"github.com/hyperledger/fabric/common/ledger/blkstorage/fsblkstorage"
+	"github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage"
+)
+
+//NewProvider returns couchdb blockstorage provider
+func NewProvider(conf *fsblkstorage.Conf, indexConfig *blkstorage.IndexConfig) blkstorage.BlockStoreProvider {
+	pvdr, err := cdbblkstorage.NewProvider(indexConfig)
+	if err != nil {
+		panic(err)
+	}
+	return pvdr
+}
+
+//NewConf is returns file system based blockstorage conf
+func NewConf(blockStorageDir string, maxBlockfileSize int) *fsblkstorage.Conf {
+	return fsblkstorage.NewConf(blockStorageDir, maxBlockfileSize)
+}
diff --git a/extensions/blkstorage/store_test.go b/extensions/blkstorage/store_test.go
new file mode 100644
index 000000000..eb29b2176
--- /dev/null
+++ b/extensions/blkstorage/store_test.go
@@ -0,0 +1,22 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package blkstorage
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/common/ledger/blkstorage"
+	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	"github.com/hyperledger/fabric/extensions/testutil"
+	"github.com/stretchr/testify/require"
+)
+
+func TestNewProvider(t *testing.T) {
+	_, _, destroy := testutil.SetupExtTestEnv()
+	defer destroy()
+	require.NotEmpty(t, NewProvider(NewConf(ledgerconfig.GetBlockStorePath(), ledgerconfig.GetMaxBlockfileSize()), &blkstorage.IndexConfig{}))
+}
diff --git a/extensions/collections/api/dissemination/dissemination.go b/extensions/collections/api/dissemination/dissemination.go
new file mode 100644
index 000000000..6fe5b4c79
--- /dev/null
+++ b/extensions/collections/api/dissemination/dissemination.go
@@ -0,0 +1,18 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"github.com/hyperledger/fabric/gossip/gossip"
+	"github.com/hyperledger/fabric/gossip/protoext"
+)
+
+// Plan contains the dissemination plan for extensions private data types
+type Plan struct {
+	Msg      *protoext.SignedGossipMessage
+	Criteria gossip.SendCriteria
+}
diff --git a/extensions/collections/api/store/key.go b/extensions/collections/api/store/key.go
new file mode 100644
index 000000000..68b4b17f7
--- /dev/null
+++ b/extensions/collections/api/store/key.go
@@ -0,0 +1,57 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package store
+
+import (
+	"fmt"
+)
+
+// Key is a key for retrieving collection data
+type Key struct {
+	EndorsedAtTxID string
+	Namespace      string
+	Collection     string
+	Key            string
+}
+
+// NewKey returns a new collection key
+func NewKey(endorsedAtTxID string, ns string, coll string, key string) *Key {
+	return &Key{
+		EndorsedAtTxID: endorsedAtTxID,
+		Namespace:      ns,
+		Collection:     coll,
+		Key:            key,
+	}
+}
+
+// String returns the string representation of the key
+func (k *Key) String() string {
+	return fmt.Sprintf("%s:%s:%s-%s", k.Namespace, k.Collection, k.Key, k.EndorsedAtTxID)
+}
+
+// MultiKey is a key for retrieving collection data for multiple keys
+type MultiKey struct {
+	EndorsedAtTxID string
+	Namespace      string
+	Collection     string
+	Keys           []string
+}
+
+// NewMultiKey returns a new collection data multi-key
+func NewMultiKey(endorsedAtTxID string, ns string, coll string, keys ...string) *MultiKey {
+	return &MultiKey{
+		EndorsedAtTxID: endorsedAtTxID,
+		Namespace:      ns,
+		Collection:     coll,
+		Keys:           keys,
+	}
+}
+
+// String returns the string representation of the key
+func (k *MultiKey) String() string {
+	return fmt.Sprintf("%s:%s:%s-%s", k.Namespace, k.Collection, k.Keys, k.EndorsedAtTxID)
+}
diff --git a/extensions/collections/api/store/provider.go b/extensions/collections/api/store/provider.go
new file mode 100644
index 000000000..376356480
--- /dev/null
+++ b/extensions/collections/api/store/provider.go
@@ -0,0 +1,78 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package store
+
+import (
+	"context"
+	"time"
+
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common"
+)
+
+// ExpiringValue is holds the value and expiration time.
+type ExpiringValue struct {
+	Value  []byte
+	Expiry time.Time
+}
+
+// ExpiringValues expiring values
+type ExpiringValues []*ExpiringValue
+
+// Store manages the storage of private data collections.
+type Store interface {
+	// Persist stores the private write set of a transaction.
+	Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error
+
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(key *Key) (*ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(key *MultiKey) (ExpiringValues, error)
+
+	// GetData gets the value for the given item
+	GetData(key *Key) (*ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple items in a single call
+	GetDataMultipleKeys(key *MultiKey) (ExpiringValues, error)
+
+	// PutData stores the key/value.
+	PutData(config *cb.StaticCollectionConfig, key *Key, value *ExpiringValue) error
+
+	// Close closes the store
+	Close()
+}
+
+// Retriever retrieves private data
+type Retriever interface {
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(ctxt context.Context, key *Key) (*ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(ctxt context.Context, key *MultiKey) (ExpiringValues, error)
+
+	// GetData gets the value for the given data item
+	GetData(ctxt context.Context, key *Key) (*ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple data items in a single call
+	GetDataMultipleKeys(ctxt context.Context, key *MultiKey) (ExpiringValues, error)
+}
+
+// Provider provides private data retrievers
+type Provider interface {
+	RetrieverForChannel(channel string) Retriever
+}
+
+// Values returns the ExpiringValues as Values
+func (ev ExpiringValues) Values() common.Values {
+	vals := make(common.Values, len(ev))
+	for i, v := range ev {
+		vals[i] = v
+	}
+	return vals
+}
diff --git a/extensions/collections/api/support/support.go b/extensions/collections/api/support/support.go
new file mode 100644
index 000000000..267ecf5c4
--- /dev/null
+++ b/extensions/collections/api/support/support.go
@@ -0,0 +1,23 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+)
+
+// GossipAdapter defines the Gossip functions that are required for collection data processing
+type GossipAdapter interface {
+	PeersOfChannel(gcommon.ChainID) []discovery.NetworkMember
+	SelfMembershipInfo() discovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+	Send(msg *gproto.GossipMessage, peers ...*comm.RemotePeer)
+}
diff --git a/extensions/collections/dissemination/disseminationplan.go b/extensions/collections/dissemination/disseminationplan.go
new file mode 100644
index 000000000..bcfc1e414
--- /dev/null
+++ b/extensions/collections/dissemination/disseminationplan.go
@@ -0,0 +1,72 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/pkg/errors"
+	oldissemination "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination"
+	tdissemination "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination"
+)
+
+type gossipAdapter interface {
+	PeersOfChannel(common.ChainID) []discovery.NetworkMember
+	SelfMembershipInfo() discovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+var computeTransientDataDisseminationPlan = func(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *protoext.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	return tdissemination.ComputeDisseminationPlan(channelID, ns, rwSet, colAP, pvtDataMsg, gossipAdapter)
+}
+
+var computeOffLedgerDisseminationPlan = func(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	collConfig *cb.StaticCollectionConfig,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *protoext.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	return oldissemination.ComputeDisseminationPlan(channelID, ns, rwSet, collConfig, colAP, pvtDataMsg, gossipAdapter)
+}
+
+// ComputeDisseminationPlan returns the dissemination plan for various collection types
+func ComputeDisseminationPlan(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	colCP *cb.CollectionConfig,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *protoext.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+
+	collConfig := colCP.GetStaticCollectionConfig()
+	if collConfig == nil {
+		return nil, false, errors.New("static collection config not defined")
+	}
+
+	switch collConfig.Type {
+	case cb.CollectionType_COL_TRANSIENT:
+		return computeTransientDataDisseminationPlan(channelID, ns, rwSet, colAP, pvtDataMsg, gossipAdapter)
+	case cb.CollectionType_COL_DCAS:
+		fallthrough
+	case cb.CollectionType_COL_OFFLEDGER:
+		return computeOffLedgerDisseminationPlan(channelID, ns, rwSet, collConfig, colAP, pvtDataMsg, gossipAdapter)
+	default:
+		return nil, false, errors.Errorf("unsupported collection type: [%s]", collConfig.Type)
+	}
+}
diff --git a/extensions/collections/dissemination/disseminationplan_test.go b/extensions/collections/dissemination/disseminationplan_test.go
new file mode 100644
index 000000000..fecdf7573
--- /dev/null
+++ b/extensions/collections/dissemination/disseminationplan_test.go
@@ -0,0 +1,101 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestDisseminationPlan(t *testing.T) {
+	const (
+		channelID = "testchannel"
+		ns        = "ns1"
+	)
+
+	computeTransientDataDisseminationPlan = func(
+		channelID, ns string,
+		rwSet *rwset.CollectionPvtReadWriteSet,
+		colAP privdata.CollectionAccessPolicy,
+		pvtDataMsg *protoext.SignedGossipMessage,
+		gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+		return nil, false, nil
+	}
+
+	computeOffLedgerDisseminationPlan = func(
+		channelID, ns string,
+		rwSet *rwset.CollectionPvtReadWriteSet,
+		colCP *common.StaticCollectionConfig,
+		colAP privdata.CollectionAccessPolicy,
+		pvtDataMsg *protoext.SignedGossipMessage,
+		gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+		return nil, false, nil
+	}
+
+	t.Run("Empty config", func(t *testing.T) {
+		colConfig1 := &common.CollectionConfig{}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, colConfig1, nil, nil, nil)
+		assert.EqualError(t, err, "static collection config not defined")
+	})
+
+	t.Run("Unknown config", func(t *testing.T) {
+		colConfig2 := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, colConfig2, nil, nil, nil)
+		assert.EqualError(t, err, "unsupported collection type: [COL_UNKNOWN]")
+	})
+
+	t.Run("Transient Data config", func(t *testing.T) {
+		transientConfig := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{
+					Type: common.CollectionType_COL_TRANSIENT,
+				},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, transientConfig, nil, nil, nil)
+		assert.NoError(t, err)
+	})
+
+	t.Run("Off-Ledger config", func(t *testing.T) {
+		olConfig := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{
+					Type: common.CollectionType_COL_OFFLEDGER,
+				},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, olConfig, nil, nil, nil)
+		assert.NoError(t, err)
+	})
+
+	t.Run("DCAS config", func(t *testing.T) {
+		dcasConfig := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{
+					Type: common.CollectionType_COL_DCAS,
+				},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, dcasConfig, nil, nil, nil)
+		assert.NoError(t, err)
+	})
+}
diff --git a/extensions/collections/policy/validator.go b/extensions/collections/policy/validator.go
new file mode 100644
index 000000000..96b629c01
--- /dev/null
+++ b/extensions/collections/policy/validator.go
@@ -0,0 +1,89 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package policy
+
+import (
+	"fmt"
+
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+	olpolicy "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/policy"
+	tdatapolicy "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/policy"
+)
+
+// Validator is a collection policy validator
+type Validator struct {
+}
+
+// NewValidator returns a new collection policy validator
+func NewValidator() *Validator {
+	return &Validator{}
+}
+
+// Validate validates various collection config types
+func (v *Validator) Validate(collConfig *common.CollectionConfig) error {
+	config := collConfig.GetStaticCollectionConfig()
+	if config == nil {
+		return errors.New("unknown collection configuration type")
+	}
+
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		return tdatapolicy.ValidateConfig(config)
+	case common.CollectionType_COL_DCAS:
+		fallthrough
+	case common.CollectionType_COL_OFFLEDGER:
+		return olpolicy.ValidateConfig(config)
+	default:
+		// Nothing to do
+		return nil
+	}
+}
+
+// ValidateNewCollectionConfigsAgainstOld validates updated collection configs
+func (v *Validator) ValidateNewCollectionConfigsAgainstOld(newCollectionConfigs []*common.CollectionConfig, oldCollectionConfigs []*common.CollectionConfig,
+) error {
+	newCollectionsMap := make(map[string]*common.StaticCollectionConfig, len(newCollectionConfigs))
+	for _, newCollectionConfig := range newCollectionConfigs {
+		newCollection := newCollectionConfig.GetStaticCollectionConfig()
+		newCollectionsMap[newCollection.GetName()] = newCollection
+	}
+
+	for _, oldCollConfig := range oldCollectionConfigs {
+		oldColl := oldCollConfig.GetStaticCollectionConfig()
+		if oldColl == nil {
+			// This shouldn't happen since we've already gone through the basic validation
+			return errors.New("invalid collection")
+		}
+		newColl, ok := newCollectionsMap[oldColl.Name]
+		if !ok {
+			continue
+		}
+
+		newCollType := getCollType(newColl)
+		oldCollType := getCollType(oldColl)
+		if newCollType != oldCollType {
+			return fmt.Errorf("collection-name: %s -- attempt to change collection type from [%s] to [%s]", oldColl.Name, oldCollType, newCollType)
+		}
+	}
+	return nil
+}
+
+func getCollType(config *common.StaticCollectionConfig) common.CollectionType {
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		return common.CollectionType_COL_TRANSIENT
+	case common.CollectionType_COL_OFFLEDGER:
+		return common.CollectionType_COL_OFFLEDGER
+	case common.CollectionType_COL_DCAS:
+		return common.CollectionType_COL_DCAS
+	case common.CollectionType_COL_PRIVATE:
+		fallthrough
+	default:
+		return common.CollectionType_COL_PRIVATE
+	}
+}
diff --git a/extensions/collections/policy/validator_test.go b/extensions/collections/policy/validator_test.go
new file mode 100644
index 000000000..6b1e4af8b
--- /dev/null
+++ b/extensions/collections/policy/validator_test.go
@@ -0,0 +1,271 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package policy
+
+import (
+	"strings"
+	"testing"
+
+	"github.com/hyperledger/fabric/common/cauthdsl"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestValidateTransientDataCollectionConfig(t *testing.T) {
+	coll1 := "mycollection"
+
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+
+	v := NewValidator()
+
+	t.Run("No collection type -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		collConfig := createStaticCollectionConfig(coll1, policyEnvelope, 1, 2, 1000)
+		err := v.Validate(collConfig)
+		assert.NoError(t, err)
+	})
+
+	t.Run("Private collection type -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		collConfig := createPrivateCollectionConfig(coll1, policyEnvelope, 1, 2, 1000)
+		err := v.Validate(collConfig)
+		assert.NoError(t, err)
+	})
+
+	t.Run("Transient collection -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "1m"))
+		assert.NoError(t, err)
+	})
+}
+
+func TestValidateOffLedgerCollectionConfig(t *testing.T) {
+	coll1 := "mycollection"
+
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+
+	v := NewValidator()
+
+	t.Run("Off-Ledger collection -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createOffLedgerCollectionConfig(coll1, policyEnvelope, 1, 2, "1m"))
+		assert.NoError(t, err)
+	})
+
+	t.Run("Off-Ledger req == 0 -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createOffLedgerCollectionConfig(coll1, policyEnvelope, 0, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "required peer count must be greater than 0"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("transient collection req > max -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 3, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "maximum peer count (2) must be greater than or equal to required peer count (3)"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("Off-Ledger no time-to-live -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createOffLedgerCollectionConfig(coll1, policyEnvelope, 1, 2, ""))
+		require.NoError(t, err)
+	})
+
+	t.Run("Off-Ledger invalid time-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createOffLedgerCollectionConfig(coll1, policyEnvelope, 1, 2, "1k"))
+		require.Error(t, err)
+		expectedErr := "invalid time format for time to live"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("Off-Ledger with blocks-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		config := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "1m")
+		config.GetStaticCollectionConfig().BlockToLive = 100
+		err := v.Validate(config)
+		require.Error(t, err)
+		expectedErr := "block-to-live not supported"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+}
+
+func TestValidateDCASCollectionConfig(t *testing.T) {
+	coll1 := "mycollection"
+
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+
+	v := NewValidator()
+
+	t.Run("DCAS collection -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 1, 2, "1m"))
+		assert.NoError(t, err)
+	})
+
+	t.Run("DCAS req == 0 -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 0, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "required peer count must be greater than 0"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("transient collection req > max -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 3, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "maximum peer count (2) must be greater than or equal to required peer count (3)"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("DCAS no time-to-live -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 1, 2, ""))
+		require.NoError(t, err)
+	})
+
+	t.Run("DCAS invalid time-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 1, 2, "1k"))
+		require.Error(t, err)
+		expectedErr := "invalid time format for time to live"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("DCAS with blocks-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		config := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "1m")
+		config.GetStaticCollectionConfig().BlockToLive = 100
+		err := v.Validate(config)
+		require.Error(t, err)
+		expectedErr := "block-to-live not supported"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+}
+
+func TestValidateNewCollectionConfigAgainstOld(t *testing.T) {
+	coll1 := "mycollection"
+
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+
+	v := NewValidator()
+
+	t.Run("updated -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		oldCollConfig := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "10m")
+		newCollConfig := createTransientCollectionConfig(coll1, policyEnvelope, 2, 3, "20m")
+		err := v.ValidateNewCollectionConfigsAgainstOld([]*common.CollectionConfig{newCollConfig}, []*common.CollectionConfig{oldCollConfig})
+		assert.NoError(t, err)
+	})
+
+	t.Run("private collection updated to transient -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		oldCollConfig := createStaticCollectionConfig(coll1, policyEnvelope, 1, 2, 1000)
+		newCollConfig := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "10m")
+		err := v.ValidateNewCollectionConfigsAgainstOld([]*common.CollectionConfig{newCollConfig}, []*common.CollectionConfig{oldCollConfig})
+		assert.EqualError(t, err, "collection-name: mycollection -- attempt to change collection type from [COL_PRIVATE] to [COL_TRANSIENT]")
+	})
+
+	t.Run("private collection updated to off-ledger -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		oldCollConfig := createStaticCollectionConfig(coll1, policyEnvelope, 1, 2, 1000)
+		newCollConfig := createOffLedgerCollectionConfig(coll1, policyEnvelope, 1, 2, "10m")
+		err := v.ValidateNewCollectionConfigsAgainstOld([]*common.CollectionConfig{newCollConfig}, []*common.CollectionConfig{oldCollConfig})
+		assert.EqualError(t, err, "collection-name: mycollection -- attempt to change collection type from [COL_PRIVATE] to [COL_OFFLEDGER]")
+	})
+}
+
+func createTransientCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, ttl string) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				Type:              common.CollectionType_COL_TRANSIENT,
+				MemberOrgsPolicy:  &common.CollectionPolicyConfig{Payload: signaturePolicy},
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+				TimeToLive:        ttl,
+			},
+		},
+	}
+}
+
+func createPrivateCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, blockToLive uint64) *common.CollectionConfig {
+	config := createStaticCollectionConfig(collectionName, signaturePolicyEnvelope, requiredPeerCount, maximumPeerCount, blockToLive)
+	config.GetStaticCollectionConfig().Type = common.CollectionType_COL_PRIVATE
+	return config
+}
+
+func createStaticCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, blockToLive uint64) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				MemberOrgsPolicy:  &common.CollectionPolicyConfig{Payload: signaturePolicy},
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+				BlockToLive:       blockToLive,
+			},
+		},
+	}
+}
+
+func createOffLedgerCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, ttl string) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				Type:              common.CollectionType_COL_OFFLEDGER,
+				MemberOrgsPolicy:  &common.CollectionPolicyConfig{Payload: signaturePolicy},
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+				TimeToLive:        ttl,
+			},
+		},
+	}
+}
+
+func createDCASCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, ttl string) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				Type:              common.CollectionType_COL_DCAS,
+				MemberOrgsPolicy:  &common.CollectionPolicyConfig{Payload: signaturePolicy},
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+				TimeToLive:        ttl,
+			},
+		},
+	}
+}
diff --git a/extensions/collections/pvtdatahandler/pvtdatahandler.go b/extensions/collections/pvtdatahandler/pvtdatahandler.go
new file mode 100644
index 000000000..012f68836
--- /dev/null
+++ b/extensions/collections/pvtdatahandler/pvtdatahandler.go
@@ -0,0 +1,17 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatahandler
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	extpvtdatahandler "github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatahandler"
+)
+
+// New returns a new Handler
+func New(channelID string, collDataProvider storeapi.Provider) *extpvtdatahandler.Handler {
+	return extpvtdatahandler.New(channelID, collDataProvider)
+}
diff --git a/extensions/collections/pvtdatahandler/pvtdatahandler_test.go b/extensions/collections/pvtdatahandler/pvtdatahandler_test.go
new file mode 100644
index 000000000..3d033f161
--- /dev/null
+++ b/extensions/collections/pvtdatahandler/pvtdatahandler_test.go
@@ -0,0 +1,40 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatahandler
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestHandler_HandleGetPrivateData(t *testing.T) {
+	h := New("testchannel", nil)
+
+	config := &common.StaticCollectionConfig{
+		Name: "coll1",
+	}
+
+	value, handled, err := h.HandleGetPrivateData("tx1", "ns1", config, "key1")
+	assert.NoError(t, err)
+	assert.False(t, handled)
+	assert.Nil(t, value)
+}
+
+func TestHandler_HandleGetPrivateDataMultipleKeys(t *testing.T) {
+	h := New("testchannel", nil)
+
+	config := &common.StaticCollectionConfig{
+		Name: "coll1",
+	}
+
+	value, handled, err := h.HandleGetPrivateDataMultipleKeys("tx1", "ns1", config, []string{"key1", "key2"})
+	assert.NoError(t, err)
+	assert.False(t, handled)
+	assert.Nil(t, value)
+}
diff --git a/extensions/collections/retriever/retriever.go b/extensions/collections/retriever/retriever.go
new file mode 100644
index 000000000..7697e0847
--- /dev/null
+++ b/extensions/collections/retriever/retriever.go
@@ -0,0 +1,25 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	"github.com/hyperledger/fabric/core/ledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	extretriever "github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever"
+)
+
+// NewProvider returns a new private data Retriever provider
+func NewProvider(
+	storeProvider func(channelID string) storeapi.Store,
+	ledgerProvider func(channelID string) ledger.PeerLedger,
+	gossipProvider func() supportapi.GossipAdapter,
+	blockPublisherProvider func(channelID string) gossipapi.BlockPublisher) storeapi.Provider {
+
+	return extretriever.NewProvider(storeProvider, ledgerProvider, gossipProvider, blockPublisherProvider)
+}
diff --git a/extensions/collections/retriever/retriever_test.go b/extensions/collections/retriever/retriever_test.go
new file mode 100644
index 000000000..928883f5d
--- /dev/null
+++ b/extensions/collections/retriever/retriever_test.go
@@ -0,0 +1,33 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	"testing"
+
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	"github.com/stretchr/testify/require"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+	olmocks "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/mocks"
+	extretriever "github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever"
+	tdataapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+	tdatamocks "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/mocks"
+)
+
+func TestNewProvider(t *testing.T) {
+	extretriever.SetTransientDataProvider(func(storeProvider func(channelID string) tdataapi.Store, support extretriever.Support, gossipProvider func() supportapi.GossipAdapter) tdataapi.Provider {
+		return &tdatamocks.TransientDataProvider{}
+	})
+
+	extretriever.SetOffLedgerProvider(func(storeProvider func(channelID string) olapi.Store, support extretriever.Support, gossipProvider func() supportapi.GossipAdapter) olapi.Provider {
+		return &olmocks.Provider{}
+	})
+
+	p := NewProvider(nil, nil, nil, nil)
+	require.NotNil(t, p)
+	require.NotNil(t, p.RetrieverForChannel("testchannel"))
+}
diff --git a/extensions/collections/storeprovider/mocks/mocktdstore.go b/extensions/collections/storeprovider/mocks/mocktdstore.go
new file mode 100644
index 000000000..a2f0d9c39
--- /dev/null
+++ b/extensions/collections/storeprovider/mocks/mocktdstore.go
@@ -0,0 +1,119 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+	tdapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+)
+
+// TransientDataStoreProvider implements a mock transient data store provider
+type TransientDataStoreProvider struct {
+	store *TransientDataStore
+	err   error
+}
+
+// NewTransientDataStoreProvider creates a new provider
+func NewTransientDataStoreProvider() *TransientDataStoreProvider {
+	return &TransientDataStoreProvider{
+		store: NewTransientDataStore(),
+	}
+}
+
+// Data stores key value
+func (p *TransientDataStoreProvider) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *TransientDataStoreProvider {
+	p.store.Data(key, value)
+	return p
+}
+
+// Error stores the error
+func (p *TransientDataStoreProvider) Error(err error) *TransientDataStoreProvider {
+	p.err = err
+	return p
+}
+
+// StoreError stores the StoreError
+func (p *TransientDataStoreProvider) StoreError(err error) *TransientDataStoreProvider {
+	p.store.Error(err)
+	return p
+}
+
+// StoreForChannel returns the transient data store for the given channel
+func (p *TransientDataStoreProvider) StoreForChannel(channelID string) tdapi.Store {
+	return p.store
+}
+
+// OpenStore opens the transient data store for the given channel
+func (p *TransientDataStoreProvider) OpenStore(channelID string) (tdapi.Store, error) {
+	return p.store, p.err
+}
+
+// Close closes the transient data store for the given channel
+func (p *TransientDataStoreProvider) Close() {
+	p.store.Close()
+}
+
+// IsStoreClosed indicates whether the transient data store is closed
+func (p *TransientDataStoreProvider) IsStoreClosed() bool {
+	return p.store.closed
+}
+
+// TransientDataStore implements a mock transient data store
+type TransientDataStore struct {
+	transientData map[storeapi.Key]*storeapi.ExpiringValue
+	err           error
+	closed        bool
+}
+
+// NewTransientDataStore returns a mock transient data store
+func NewTransientDataStore() *TransientDataStore {
+	return &TransientDataStore{
+		transientData: make(map[storeapi.Key]*storeapi.ExpiringValue),
+	}
+}
+
+// Data sets the transient data for the given key
+func (m *TransientDataStore) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *TransientDataStore {
+	m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return m
+}
+
+// Error sets an err
+func (m *TransientDataStore) Error(err error) *TransientDataStore {
+	m.err = err
+	return m
+}
+
+// Persist stores the private write set of a transaction along with the collection config
+// in the transient store based on txid and the block height the private data was received at
+func (m *TransientDataStore) Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error {
+	return m.err
+}
+
+// GetTransientData gets the value for the given transient data item
+func (m *TransientDataStore) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}], m.err
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (m *TransientDataStore) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	var values storeapi.ExpiringValues
+	for _, k := range key.Keys {
+		value, err := m.GetTransientData(&storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: k})
+		if err != nil {
+			return nil, err
+		}
+		values = append(values, value)
+	}
+	return values, m.err
+}
+
+// Close closes the store
+func (m *TransientDataStore) Close() {
+	m.closed = true
+}
diff --git a/extensions/collections/storeprovider/storeprovider.go b/extensions/collections/storeprovider/storeprovider.go
new file mode 100644
index 000000000..faac4e9cd
--- /dev/null
+++ b/extensions/collections/storeprovider/storeprovider.go
@@ -0,0 +1,16 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	extstoreprovider "github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider"
+)
+
+// NewProviderFactory returns a new store provider factory
+func NewProviderFactory() *extstoreprovider.StoreProvider {
+	return extstoreprovider.New()
+}
diff --git a/extensions/collections/storeprovider/storeprovider_test.go b/extensions/collections/storeprovider/storeprovider_test.go
new file mode 100644
index 000000000..444e188d4
--- /dev/null
+++ b/extensions/collections/storeprovider/storeprovider_test.go
@@ -0,0 +1,104 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"testing"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	spmocks "github.com/hyperledger/fabric/extensions/collections/storeprovider/mocks"
+	"github.com/pkg/errors"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+	extstoreprovider "github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider"
+	tdapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/mocks"
+)
+
+func TestStoreProvider(t *testing.T) {
+	tdataProvider := spmocks.NewTransientDataStoreProvider()
+
+	extstoreprovider.SetNewTransientDataProvider(func() tdapi.StoreProvider {
+		return tdataProvider
+	})
+
+	t.Run("OpenStore - success", func(t *testing.T) {
+		p := NewProviderFactory()
+		require.NotNil(t, p)
+
+		s, err := p.OpenStore("testchannel")
+		require.NoError(t, err)
+		require.NotNil(t, s)
+
+		s2 := p.StoreForChannel("testchannel")
+		require.Equal(t, s, s2)
+	})
+
+	t.Run("OpenStore - transient data error", func(t *testing.T) {
+		p := NewProviderFactory()
+		require.NotNil(t, p)
+
+		expectedErr := errors.New("transientdata error")
+		tdataProvider.Error(expectedErr)
+		defer tdataProvider.Error(nil)
+
+		s, err := p.OpenStore("testchannel")
+		assert.EqualError(t, err, expectedErr.Error())
+		require.Nil(t, s)
+	})
+}
+
+func TestStore_PutAndGetData(t *testing.T) {
+	const (
+		tx1   = "tx1"
+		ns1   = "ns1"
+		coll1 = "coll1"
+		key1  = "key1"
+		key2  = "key2"
+	)
+
+	k1 := storeapi.NewKey(tx1, ns1, coll1, key1)
+	k2 := storeapi.NewKey(tx1, ns1, coll1, key2)
+
+	v1 := &storeapi.ExpiringValue{Value: []byte("value1")}
+	v2 := &storeapi.ExpiringValue{Value: []byte("value1")}
+
+	tdataProvider := spmocks.NewTransientDataStoreProvider()
+
+	extstoreprovider.SetNewTransientDataProvider(func() tdapi.StoreProvider {
+		return tdataProvider.Data(k1, v1).Data(k2, v2)
+	})
+
+	p := NewProviderFactory()
+	require.NotNil(t, p)
+
+	s, err := p.OpenStore("testchannel")
+	require.NoError(t, err)
+	require.NotNil(t, s)
+
+	t.Run("GetTransientData", func(t *testing.T) {
+		value, err := s.GetTransientData(k1)
+		require.NoError(t, err)
+		require.NotNil(t, value)
+
+		values, err := s.GetTransientDataMultipleKeys(storeapi.NewMultiKey(tx1, ns1, coll1, key1, key2))
+		require.NoError(t, err)
+		assert.Equal(t, 2, len(values))
+	})
+
+	t.Run("Persist", func(t *testing.T) {
+		err := s.Persist(tx1, mocks.NewPvtReadWriteSetBuilder().Build())
+		assert.NoError(t, err)
+
+		expectedErr := errors.New("transient data error")
+		tdataProvider.StoreError(expectedErr)
+		err = s.Persist(tx1, mocks.NewPvtReadWriteSetBuilder().Build())
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		tdataProvider.StoreError(nil)
+	})
+}
diff --git a/extensions/endorser/api/endorser.go b/extensions/endorser/api/endorser.go
new file mode 100644
index 000000000..9a9199ec0
--- /dev/null
+++ b/extensions/endorser/api/endorser.go
@@ -0,0 +1,27 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	"github.com/hyperledger/fabric/core/ledger"
+	xgossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+)
+
+// QueryExecutorProvider returns a query executor
+type QueryExecutorProvider interface {
+	NewQueryExecutor() (ledger.QueryExecutor, error)
+}
+
+// QueryExecutorProviderFactory returns a query executor provider for a given channel
+type QueryExecutorProviderFactory interface {
+	GetQueryExecutorProvider(channelID string) QueryExecutorProvider
+}
+
+// BlockPublisherProvider returns a block publisher for a given channel
+type BlockPublisherProvider interface {
+	ForChannel(channelID string) xgossipapi.BlockPublisher
+}
diff --git a/extensions/endorser/endorser.go b/extensions/endorser/endorser.go
new file mode 100644
index 000000000..0e034474b
--- /dev/null
+++ b/extensions/endorser/endorser.go
@@ -0,0 +1,24 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package endorser
+
+import (
+	"github.com/hyperledger/fabric/extensions/endorser/api"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	extendorser "github.com/trustbloc/fabric-peer-ext/pkg/endorser"
+)
+
+// CollRWSetFilter filters out all off-ledger (including transient data) read-write sets from the simulation results
+// so that they won't be included in the block.
+type CollRWSetFilter interface {
+	Filter(channelID string, pubSimulationResults *rwset.TxReadWriteSet) (*rwset.TxReadWriteSet, error)
+}
+
+// NewCollRWSetFilter returns a new collection RW set filter
+func NewCollRWSetFilter(qepf api.QueryExecutorProviderFactory, bpp api.BlockPublisherProvider) CollRWSetFilter {
+	return extendorser.NewCollRWSetFilter(qepf, bpp)
+}
diff --git a/extensions/endorser/endorser_test.go b/extensions/endorser/endorser_test.go
new file mode 100644
index 000000000..172be735a
--- /dev/null
+++ b/extensions/endorser/endorser_test.go
@@ -0,0 +1,45 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package endorser
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/endorser/api"
+	xgossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+)
+
+func TestFilterPubSimulationResults(t *testing.T) {
+	f := NewCollRWSetFilter(&mockQueryExecutorProviderFactory{}, &mockBlockPublisherProvider{})
+	require.NotNil(t, f)
+
+	pubSimulationResults := &rwset.TxReadWriteSet{}
+	p, err := f.Filter(channelID, pubSimulationResults)
+	assert.NoError(t, err)
+	assert.Equal(t, pubSimulationResults, p)
+}
+
+type mockQueryExecutorProviderFactory struct {
+}
+
+func (m *mockQueryExecutorProviderFactory) GetQueryExecutorProvider(channelID string) api.QueryExecutorProvider {
+	return nil
+}
+
+type mockBlockPublisherProvider struct {
+}
+
+func (m *mockBlockPublisherProvider) ForChannel(channelID string) xgossipapi.BlockPublisher {
+	return nil
+}
diff --git a/extensions/gossip/api/gossipapi.go b/extensions/gossip/api/gossipapi.go
new file mode 100644
index 000000000..a53afea57
--- /dev/null
+++ b/extensions/gossip/api/gossipapi.go
@@ -0,0 +1,44 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+)
+
+// ConfigUpdateHandler handles a config update
+type ConfigUpdateHandler func(blockNum uint64, configUpdate *cb.ConfigUpdate) error
+
+// WriteHandler handles a KV write
+type WriteHandler func(blockNum uint64, channelID, txID, namespace string, kvWrite *kvrwset.KVWrite) error
+
+// ReadHandler handles a KV read
+type ReadHandler func(blockNum uint64, channelID, txID, namespace string, kvRead *kvrwset.KVRead) error
+
+// ChaincodeEventHandler handles a chaincode event
+type ChaincodeEventHandler func(blockNum uint64, channelID, txID string, event *pb.ChaincodeEvent) error
+
+// ChaincodeUpgradeHandler handles chaincode upgrade events
+type ChaincodeUpgradeHandler func(blockNum uint64, txID string, chaincodeName string) error
+
+// BlockPublisher allows clients to add handlers for various block events
+type BlockPublisher interface {
+	// AddCCUpgradeHandler adds a handler for chaincode upgrades
+	AddCCUpgradeHandler(handler ChaincodeUpgradeHandler)
+	// AddConfigUpdateHandler adds a handler for config updates
+	AddConfigUpdateHandler(handler ConfigUpdateHandler)
+	// AddWriteHandler adds a handler for KV writes
+	AddWriteHandler(handler WriteHandler)
+	// AddReadHandler adds a handler for KV reads
+	AddReadHandler(handler ReadHandler)
+	// AddCCEventHandler adds a handler for chaincode events
+	AddCCEventHandler(handler ChaincodeEventHandler)
+	// Publish traverses the block and invokes all applicable handlers
+	Publish(block *cb.Block)
+}
diff --git a/extensions/gossip/blockpublisher/blockpublisher.go b/extensions/gossip/blockpublisher/blockpublisher.go
new file mode 100644
index 000000000..bd9315b1a
--- /dev/null
+++ b/extensions/gossip/blockpublisher/blockpublisher.go
@@ -0,0 +1,16 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package blockpublisher
+
+import (
+	extblockpublisher "github.com/trustbloc/fabric-peer-ext/pkg/gossip/blockpublisher"
+)
+
+// NewProvider returns a new block publisher provider
+func NewProvider() *extblockpublisher.Provider {
+	return extblockpublisher.NewProvider()
+}
diff --git a/extensions/gossip/blockpublisher/blockpublisher_test.go b/extensions/gossip/blockpublisher/blockpublisher_test.go
new file mode 100644
index 000000000..775451f45
--- /dev/null
+++ b/extensions/gossip/blockpublisher/blockpublisher_test.go
@@ -0,0 +1,67 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package blockpublisher
+
+import (
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/magiconair/properties/assert"
+	"github.com/stretchr/testify/require"
+	"github.com/trustbloc/fabric-peer-ext/pkg/mocks"
+)
+
+const (
+	channelID = "testchannel"
+	txID1     = "tx1"
+	ccID1     = "cc1"
+	key1      = "key1"
+	ccEvent1  = "ccevent1"
+)
+
+func TestProvider(t *testing.T) {
+	var (
+		value1 = []byte("value1")
+
+		v1 = &kvrwset.Version{
+			BlockNum: 1000,
+			TxNum:    0,
+		}
+	)
+
+	p := NewProvider()
+	require.NotNil(t, p)
+
+	publisher := p.ForChannel(channelID)
+	require.NotNil(t, publisher)
+
+	handler := mocks.NewMockBlockHandler()
+
+	publisher.AddWriteHandler(handler.HandleWrite)
+	publisher.AddReadHandler(handler.HandleRead)
+	publisher.AddCCEventHandler(handler.HandleChaincodeEvent)
+
+	b := mocks.NewBlockBuilder(channelID, 1100)
+	defer p.Close()
+
+	b.Transaction(txID1, pb.TxValidationCode_VALID).
+		ChaincodeAction(ccID1).
+		Write(key1, value1).
+		Read(key1, v1).
+		ChaincodeEvent(ccEvent1, []byte("ccpayload"))
+
+	publisher.Publish(b.Build())
+
+	// Wait a bit for the events to be published
+	time.Sleep(500 * time.Millisecond)
+
+	assert.Equal(t, handler.NumReads(), 1)
+	assert.Equal(t, handler.NumWrites(), 1)
+	assert.Equal(t, handler.NumCCEvents(), 1)
+}
diff --git a/extensions/gossip/coordinator/coordinator.go b/extensions/gossip/coordinator/coordinator.go
new file mode 100644
index 000000000..23573b2db
--- /dev/null
+++ b/extensions/gossip/coordinator/coordinator.go
@@ -0,0 +1,24 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package coordinator
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatastore"
+)
+
+type transientStore interface {
+	// PersistWithConfig stores the private write set of a transaction along with the collection config
+	// in the transient store based on txid and the block height the private data was received at
+	PersistWithConfig(txid string, blockHeight uint64, privateSimulationResultsWithConfig *transientstore.TxPvtReadWriteSetWithConfigInfo) error
+}
+
+// New returns a new PvtDataStore
+func New(channelID string, transientStore transientStore, collDataStore storeapi.Store) *pvtdatastore.Store {
+	return pvtdatastore.New(channelID, transientStore, collDataStore)
+}
diff --git a/extensions/gossip/coordinator/coordinator_test.go b/extensions/gossip/coordinator/coordinator_test.go
new file mode 100644
index 000000000..1730f21e4
--- /dev/null
+++ b/extensions/gossip/coordinator/coordinator_test.go
@@ -0,0 +1,50 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package coordinator
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+	"github.com/trustbloc/fabric-peer-ext/pkg/mocks"
+)
+
+const (
+	channelID = "testchannel"
+
+	ns1 = "ns1"
+
+	coll1 = "coll1"
+	coll2 = "coll2"
+
+	policy1 = "OR('Org1MSP.member','Org2MSP.member')"
+)
+
+func TestCoordinator_StorePvtData(t *testing.T) {
+	tStore := &mockTransientStore{}
+	collStore := mocks.NewDataStore()
+
+	c := New(channelID, tStore, collStore)
+	require.NotNil(t, c)
+
+	b := mocks.NewPvtReadWriteSetBuilder()
+	nsBuilder := b.Namespace(ns1)
+	nsBuilder.Collection(coll1).StaticConfig(policy1, 2, 5, 1000)
+	nsBuilder.Collection(coll2).TransientConfig(policy1, 2, 5, "1m")
+
+	err := c.StorePvtData("tx1", b.Build(), 1000)
+	assert.NoError(t, err)
+}
+
+type mockTransientStore struct {
+}
+
+func (m *mockTransientStore) PersistWithConfig(txid string, blockHeight uint64, privateSimulationResultsWithConfig *transientstore.TxPvtReadWriteSetWithConfigInfo) error {
+	return nil
+}
diff --git a/extensions/gossip/dispatcher/dispatcher.go b/extensions/gossip/dispatcher/dispatcher.go
new file mode 100644
index 000000000..9ddb4ed51
--- /dev/null
+++ b/extensions/gossip/dispatcher/dispatcher.go
@@ -0,0 +1,37 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dispatcher
+
+import (
+	"github.com/hyperledger/fabric/core/ledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	gossip "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	extdispatcher "github.com/trustbloc/fabric-peer-ext/pkg/gossip/dispatcher"
+)
+
+type gossipAdapter interface {
+	PeersOfChannel(common.ChainID) []discovery.NetworkMember
+	SelfMembershipInfo() discovery.NetworkMember
+	IdentityInfo() gossip.PeerIdentitySet
+}
+
+type blockPublisher interface {
+	AddCCUpgradeHandler(handler gossipapi.ChaincodeUpgradeHandler)
+}
+
+// New returns a new Gossip message dispatcher
+func New(
+	channelID string,
+	dataStore storeapi.Store,
+	gossipAdapter gossipAdapter,
+	ledger ledger.PeerLedger,
+	blockPublisher blockPublisher) *extdispatcher.Dispatcher {
+	return extdispatcher.New(channelID, dataStore, gossipAdapter, ledger, blockPublisher)
+}
diff --git a/extensions/gossip/dispatcher/dispatcher_test.go b/extensions/gossip/dispatcher/dispatcher_test.go
new file mode 100644
index 000000000..e93f73723
--- /dev/null
+++ b/extensions/gossip/dispatcher/dispatcher_test.go
@@ -0,0 +1,38 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dispatcher
+
+import (
+	"testing"
+
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+	"github.com/trustbloc/fabric-peer-ext/pkg/mocks"
+)
+
+func TestProvider(t *testing.T) {
+	const channelID = "testchannel"
+
+	dispatcher := New(
+		channelID,
+		&mocks.DataStore{},
+		mocks.NewMockGossipAdapter(),
+		&mocks.Ledger{QueryExecutor: mocks.NewQueryExecutor(nil)},
+		mocks.NewBlockPublisher(),
+	)
+
+	var response *gproto.GossipMessage
+	msg := &mocks.MockReceivedMessage{
+		Message: mocks.NewDataMsg(channelID),
+		RespondTo: func(msg *gproto.GossipMessage) {
+			response = msg
+		},
+	}
+	assert.False(t, dispatcher.Dispatch(msg))
+	require.Nil(t, response)
+}
diff --git a/extensions/gossip/mocks/blockpublisher.go b/extensions/gossip/mocks/blockpublisher.go
new file mode 100644
index 000000000..688bcdaa1
--- /dev/null
+++ b/extensions/gossip/mocks/blockpublisher.go
@@ -0,0 +1,51 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"github.com/hyperledger/fabric/extensions/gossip/api"
+	cb "github.com/hyperledger/fabric/protos/common"
+)
+
+// BlockPublisher is a mock block publisher
+type BlockPublisher struct {
+}
+
+// NewBlockPublisher returns a new mock block publisher
+func NewBlockPublisher() *BlockPublisher {
+	return &BlockPublisher{}
+}
+
+// AddCCUpgradeHandler adds a handler for chaincode upgrades
+func (m *BlockPublisher) AddCCUpgradeHandler(handler api.ChaincodeUpgradeHandler) {
+	// Not implemented
+}
+
+// AddConfigUpdateHandler adds a handler for config updates
+func (m *BlockPublisher) AddConfigUpdateHandler(handler api.ConfigUpdateHandler) {
+	// Not implemented
+}
+
+// AddWriteHandler adds a handler for KV writes
+func (m *BlockPublisher) AddWriteHandler(handler api.WriteHandler) {
+	// Not implemented
+}
+
+// AddReadHandler adds a handler for KV reads
+func (m *BlockPublisher) AddReadHandler(handler api.ReadHandler) {
+	// Not implemented
+}
+
+// AddCCEventHandler adds a handler for chaincode events
+func (m *BlockPublisher) AddCCEventHandler(handler api.ChaincodeEventHandler) {
+	// Not implemented
+}
+
+// Publish traverses the block and invokes all applicable handlers
+func (m *BlockPublisher) Publish(block *cb.Block) {
+	// Not implemented
+}
diff --git a/extensions/idstore/store.go b/extensions/idstore/store.go
new file mode 100644
index 000000000..a9d5861d0
--- /dev/null
+++ b/extensions/idstore/store.go
@@ -0,0 +1,17 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package idstore
+
+import (
+	"github.com/hyperledger/fabric/core/ledger/kvledger/idstore"
+	s "github.com/trustbloc/fabric-peer-ext/pkg/idstore"
+)
+
+// OpenIDStore open idstore
+func OpenIDStore(path string) idstore.IDStore {
+	return s.OpenIDStore(path)
+}
diff --git a/extensions/idstore/store_test.go b/extensions/idstore/store_test.go
new file mode 100644
index 000000000..a1c92945b
--- /dev/null
+++ b/extensions/idstore/store_test.go
@@ -0,0 +1,20 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package idstore
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/testutil"
+	"github.com/stretchr/testify/require"
+)
+
+func TestOpenIDStore(t *testing.T) {
+	_, _, destroy := testutil.SetupExtTestEnv()
+	defer destroy()
+	require.NotEmpty(t, OpenIDStore(""))
+}
diff --git a/extensions/mocks/mockdatastore.go b/extensions/mocks/mockdatastore.go
new file mode 100644
index 000000000..7410c46d0
--- /dev/null
+++ b/extensions/mocks/mockdatastore.go
@@ -0,0 +1,101 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// DataStore implements a mock data store
+type DataStore struct {
+	transientData map[storeapi.Key]*storeapi.ExpiringValue
+	olData        map[storeapi.Key]*storeapi.ExpiringValue
+	err           error
+}
+
+// NewDataStore returns a mock transient data store
+func NewDataStore() *DataStore {
+	return &DataStore{
+		transientData: make(map[storeapi.Key]*storeapi.ExpiringValue),
+		olData:        make(map[storeapi.Key]*storeapi.ExpiringValue),
+	}
+}
+
+// TransientData sets the transient data for the given key
+func (m *DataStore) TransientData(key *storeapi.Key, value *storeapi.ExpiringValue) *DataStore {
+	m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return m
+}
+
+// Data sets the data for the given key
+func (m *DataStore) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *DataStore {
+	m.olData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return m
+}
+
+// Error sets an err
+func (m *DataStore) Error(err error) *DataStore {
+	m.err = err
+	return m
+}
+
+// Persist stores the private write set of a transaction along with the collection config
+// in the transient store based on txid and the block height the private data was received at
+func (m *DataStore) Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error {
+	return m.err
+}
+
+// GetTransientData gets the value for the given transient data item
+func (m *DataStore) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}], m.err
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (m *DataStore) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	var values storeapi.ExpiringValues
+	for _, k := range key.Keys {
+		value, err := m.GetTransientData(&storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: k})
+		if err != nil {
+			return nil, err
+		}
+		values = append(values, value)
+	}
+	return values, m.err
+}
+
+// PutData stores the key/value
+func (m *DataStore) PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error {
+	if m.err != nil {
+		return m.err
+	}
+	m.olData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return nil
+}
+
+// GetData gets the value for the given DCAS item
+func (m *DataStore) GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return m.olData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}], m.err
+}
+
+// GetDataMultipleKeys gets the values for the multiple DCAS items in a single call
+func (m *DataStore) GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	var values storeapi.ExpiringValues
+	for _, k := range key.Keys {
+		value, err := m.GetData(&storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: k})
+		if err != nil {
+			return nil, err
+		}
+		values = append(values, value)
+	}
+	return values, m.err
+}
+
+// Close closes the store
+func (m *DataStore) Close() {
+}
diff --git a/extensions/mocks/mockprovider.go b/extensions/mocks/mockprovider.go
new file mode 100644
index 000000000..b33774bcb
--- /dev/null
+++ b/extensions/mocks/mockprovider.go
@@ -0,0 +1,53 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+)
+
+// DataProvider is a mock data provider
+type DataProvider struct {
+}
+
+// RetrieverForChannel retrieve data for channel
+func (p *DataProvider) RetrieverForChannel(channel string) storeapi.Retriever {
+	return &dataRetriever{}
+}
+
+type dataRetriever struct {
+}
+
+// GetTransientData returns the transient data for the given context and key
+func (m *dataRetriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return &storeapi.ExpiringValue{Value: []byte(key.Key)}, nil
+}
+
+// GetTransientDataMultipleKeys returns the transient data with multiple keys for the given context and key
+func (m *dataRetriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = &storeapi.ExpiringValue{Value: []byte(k)}
+	}
+	return values, nil
+}
+
+// GetData returns the data for the given context and key
+func (m *dataRetriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return &storeapi.ExpiringValue{Value: []byte(key.Key)}, nil
+}
+
+// GetDataMultipleKeys returns the  data with multiple keys for the given context and key
+func (m *dataRetriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = &storeapi.ExpiringValue{Value: []byte(k)}
+	}
+	return values, nil
+}
diff --git a/extensions/pvtdatastorage/store.go b/extensions/pvtdatastorage/store.go
new file mode 100644
index 000000000..cb98cf71b
--- /dev/null
+++ b/extensions/pvtdatastorage/store.go
@@ -0,0 +1,17 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	s "github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage"
+)
+
+// NewProvider instantiates a StoreProvider
+func NewProvider() pvtdatastorage.Provider {
+	return s.NewProvider()
+}
diff --git a/extensions/pvtdatastorage/store_test.go b/extensions/pvtdatastorage/store_test.go
new file mode 100644
index 000000000..9cc0411ee
--- /dev/null
+++ b/extensions/pvtdatastorage/store_test.go
@@ -0,0 +1,33 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"io/ioutil"
+	"os"
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/testutil"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+func TestNewProvider(t *testing.T) {
+	cleanup := setupPath(t)
+	defer cleanup()
+	_, _, destroy := testutil.SetupExtTestEnv()
+	defer destroy()
+	require.NotEmpty(t, NewProvider())
+}
+
+func setupPath(t *testing.T) (cleanup func()) {
+	tempDir, err := ioutil.TempDir("", "pvtdatastorage")
+	require.NoError(t, err)
+
+	viper.Set("peer.fileSystemPath", tempDir)
+	return func() { os.RemoveAll(tempDir) }
+}
diff --git a/extensions/roles/ledger_roles_config.go b/extensions/roles/ledger_roles_config.go
new file mode 100644
index 000000000..1acf7c6e6
--- /dev/null
+++ b/extensions/roles/ledger_roles_config.go
@@ -0,0 +1,30 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package roles
+
+import "github.com/trustbloc/fabric-peer-ext/pkg/roles"
+
+// IsCommitter returns true if the peer is a committer, otherwise the peer does not commit to the DB
+func IsCommitter() bool {
+	return roles.IsCommitter()
+}
+
+// IsEndorser returns true if the peer is an endorser
+func IsEndorser() bool {
+	return roles.IsEndorser()
+}
+
+// IsValidator returns true if the peer is a validator
+func IsValidator() bool {
+	return roles.IsValidator()
+}
+
+// RolesAsString returns the roles for the peer
+// nolint - this is an exported function (Renaming function name will break in other projects)
+func RolesAsString() []string {
+	return roles.AsString()
+}
diff --git a/extensions/roles/ledger_roles_config_test.go b/extensions/roles/ledger_roles_config_test.go
new file mode 100644
index 000000000..7d11d02c1
--- /dev/null
+++ b/extensions/roles/ledger_roles_config_test.go
@@ -0,0 +1,29 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package roles
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/require"
+)
+
+func TestIsCommitter(t *testing.T) {
+	require.True(t, IsCommitter())
+}
+
+func TestIsEndorser(t *testing.T) {
+	require.True(t, IsEndorser())
+}
+
+func TestIsValidator(t *testing.T) {
+	require.True(t, IsValidator())
+}
+
+func TestRolesAsString(t *testing.T) {
+	require.Empty(t, RolesAsString())
+}
diff --git a/extensions/statedb/store.go b/extensions/statedb/store.go
new file mode 100644
index 000000000..c70363075
--- /dev/null
+++ b/extensions/statedb/store.go
@@ -0,0 +1,16 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package privacyenabledstate
+
+import (
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
+)
+
+// NewVersionedDBProvider instantiates VersionedDBProvider
+func NewVersionedDBProvider(vdbProvider statedb.VersionedDBProvider) statedb.VersionedDBProvider {
+	return vdbProvider
+}
diff --git a/extensions/statedb/store_test.go b/extensions/statedb/store_test.go
new file mode 100644
index 000000000..d5c1be605
--- /dev/null
+++ b/extensions/statedb/store_test.go
@@ -0,0 +1,34 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package privacyenabledstate
+
+import (
+	"io/ioutil"
+	"os"
+	"testing"
+
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb/stateleveldb"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+func TestNewProvider(t *testing.T) {
+	cleanup := setupPath(t)
+	defer cleanup()
+
+	require.NotEmpty(t, NewVersionedDBProvider(stateleveldb.NewVersionedDBProvider()))
+
+	require.Empty(t, NewVersionedDBProvider(nil))
+}
+
+func setupPath(t *testing.T) (cleanup func()) {
+	tempDir, err := ioutil.TempDir("", "statedb")
+	require.NoError(t, err)
+
+	viper.Set("peer.fileSystemPath", tempDir)
+	return func() { os.RemoveAll(tempDir) }
+}
diff --git a/extensions/testutil/ext_test_env.go b/extensions/testutil/ext_test_env.go
new file mode 100644
index 000000000..deec5a557
--- /dev/null
+++ b/extensions/testutil/ext_test_env.go
@@ -0,0 +1,25 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package testutil
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb"
+	"github.com/trustbloc/fabric-peer-ext/pkg/testutil"
+)
+
+//SetupExtTestEnv creates new couchdb instance for test
+//returns couchdbd address, cleanup and stop function handle.
+func SetupExtTestEnv() (addr string, cleanup func(string), stop func()) {
+	return testutil.SetupExtTestEnv()
+}
+
+// GetExtStateDBProvider returns the implementation of the versionedDBProvider
+func GetExtStateDBProvider(t testing.TB, dbProvider statedb.VersionedDBProvider) statedb.VersionedDBProvider {
+	return nil
+}
diff --git a/extensions/transientstore/store.go b/extensions/transientstore/store.go
new file mode 100644
index 000000000..7a91bcb5a
--- /dev/null
+++ b/extensions/transientstore/store.go
@@ -0,0 +1,17 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"github.com/hyperledger/fabric/core/transientstore"
+	ts "github.com/trustbloc/fabric-peer-ext/pkg/transientstore"
+)
+
+// NewStoreProvider return new store provider
+func NewStoreProvider() transientstore.StoreProvider {
+	return ts.NewStoreProvider()
+}
diff --git a/extensions/transientstore/store_test.go b/extensions/transientstore/store_test.go
new file mode 100644
index 000000000..be4683fcc
--- /dev/null
+++ b/extensions/transientstore/store_test.go
@@ -0,0 +1,17 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/require"
+)
+
+func TestNewStoreProvider(t *testing.T) {
+	require.Empty(t, NewStoreProvider())
+}
diff --git a/gossip/privdata/coordinator.go b/gossip/privdata/coordinator.go
index fe840a2a8..0eeacefa2 100644
--- a/gossip/privdata/coordinator.go
+++ b/gossip/privdata/coordinator.go
@@ -22,6 +22,8 @@ import (
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
 	"github.com/hyperledger/fabric/core/transientstore"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	extcoord "github.com/hyperledger/fabric/extensions/gossip/coordinator"
 	"github.com/hyperledger/fabric/gossip/metrics"
 	privdatacommon "github.com/hyperledger/fabric/gossip/privdata/common"
 	"github.com/hyperledger/fabric/gossip/util"
@@ -122,10 +124,15 @@ type Support struct {
 	txvalidator.Validator
 	committer.Committer
 	TransientStore
+	CollDataStore storeapi.Store
 	Fetcher
 	CapabilityProvider
 }

+type pvtDataStore interface {
+	StorePvtData(txID string, privData *transientstore2.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error
+}
+
 type coordinator struct {
 	mspID          string
 	selfSignedData common.SignedData
@@ -133,6 +140,7 @@ type coordinator struct {
 	transientBlockRetention        uint64
 	metrics                        *metrics.PrivdataMetrics
 	pullRetryThreshold             time.Duration
+	pvtDataStore                   pvtDataStore
 	skipPullingInvalidTransactions bool
 }

@@ -142,6 +150,11 @@ type CoordinatorConfig struct {
 	SkipPullingInvalidTransactions bool
 }

+// getPvtDataStore may be overridden by unit tests
+var getPvtDataStore = func(channelID string, transientStore TransientStore, collDataStore storeapi.Store) pvtDataStore {
+	return extcoord.New(channelID, transientStore, collDataStore)
+}
+
 // NewCoordinator creates a new instance of coordinator
 func NewCoordinator(mspID string, support Support, selfSignedData common.SignedData, metrics *metrics.PrivdataMetrics,
 	config CoordinatorConfig) Coordinator {
@@ -151,12 +164,13 @@ func NewCoordinator(mspID string, support Support, selfSignedData common.SignedD
 		transientBlockRetention:        config.TransientBlockRetention,
 		metrics:                        metrics,
 		pullRetryThreshold:             config.PullRetryThreshold,
+		pvtDataStore:                   getPvtDataStore(support.ChainID, support.TransientStore, support.CollDataStore),
 		skipPullingInvalidTransactions: config.SkipPullingInvalidTransactions}
 }

 // StorePvtData used to persist private date into transient store
 func (c *coordinator) StorePvtData(txID string, privData *transientstore2.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error {
-	return c.TransientStore.PersistWithConfig(txID, blkHeight, privData)
+	return c.pvtDataStore.StorePvtData(txID, privData, blkHeight)
 }

 // StoreBlock stores block with private data into the ledger
diff --git a/gossip/privdata/coordinator_test.go b/gossip/privdata/coordinator_test.go
index abe141dc8..069bee247 100644
--- a/gossip/privdata/coordinator_test.go
+++ b/gossip/privdata/coordinator_test.go
@@ -23,6 +23,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
 	"github.com/hyperledger/fabric/core/transientstore"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/gossip/metrics"
 	gmetricsmocks "github.com/hyperledger/fabric/gossip/metrics/mocks"
 	privdatacommon "github.com/hyperledger/fabric/gossip/privdata/common"
@@ -35,6 +36,7 @@ import (
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
 	"github.com/hyperledger/fabric/protos/msp"
 	"github.com/hyperledger/fabric/protos/peer"
+	tp "github.com/hyperledger/fabric/protos/transientstore"
 	transientstore2 "github.com/hyperledger/fabric/protos/transientstore"
 	"github.com/stretchr/testify/assert"
 	"github.com/stretchr/testify/mock"
@@ -418,6 +420,15 @@ func (cap *collectionAccessPolicy) AccessFilter() privdata.Filter {
 	}
 }

+type mockPvtDataStore struct {
+	transientStore TransientStore
+}
+
+// StorePvtData redirects the call to the transient store
+func (m *mockPvtDataStore) StorePvtData(txID string, privData *tp.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error {
+	return m.transientStore.PersistWithConfig(txID, blkHeight, privData)
+}
+
 func TestPvtDataCollections_FailOnEmptyPayload(t *testing.T) {
 	collection := &util.PvtDataCollections{
 		&ledger.TxPvtData{
@@ -1632,6 +1643,9 @@ func TestPurgeByHeight(t *testing.T) {
 }

 func TestCoordinatorStorePvtData(t *testing.T) {
+	getPvtDataStore = func(channelID string, transientStore TransientStore, collDataStore storeapi.Store) pvtDataStore {
+		return &mockPvtDataStore{transientStore: transientStore}
+	}
 	mspID := "Org1MSP"
 	metrics := metrics.NewGossipMetrics(&disabled.Provider{}).PrivdataMetrics
 	cs := createcollectionStore(common.SignedData{}).thatAcceptsAll()
@@ -1639,6 +1653,7 @@ func TestCoordinatorStorePvtData(t *testing.T) {
 	store := &mockTransientStore{t: t}
 	store.On("PersistWithConfig", mock.Anything, uint64(5), mock.Anything).
 		expectRWSet("ns1", "c1", []byte("rws-pre-image")).Return(nil)
+	tdStore := extmocks.NewDataStore()
 	fetcher := &fetcherMock{t: t}
 	committer.On("DoesPvtDataInfoExistInLedger", mock.Anything).Return(false, nil)

@@ -1651,6 +1666,7 @@ func TestCoordinatorStorePvtData(t *testing.T) {
 		Committer:          committer,
 		Fetcher:            fetcher,
 		TransientStore:     store,
+		CollDataStore:      tdStore,
 		Validator:          &validatorMock{},
 		CapabilityProvider: capabilityProvider,
 	}, common.SignedData{}, metrics, testConfig)
diff --git a/gossip/privdata/dissemination.go b/gossip/privdata/dissemination.go
new file mode 100644
index 000000000..f64665513
--- /dev/null
+++ b/gossip/privdata/dissemination.go
@@ -0,0 +1,33 @@
+/*
+	Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+	SPDX-License-Identifier: Apache-2.0
+*/
+
+package privdata
+
+import (
+	"github.com/hyperledger/fabric/core/common/privdata"
+	extdissemination "github.com/hyperledger/fabric/extensions/collections/dissemination"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+)
+
+func (d *distributorImpl) disseminationPlanForExt(ns string, rwSet *rwset.CollectionPvtReadWriteSet, colCP *common.CollectionConfig, colAP privdata.CollectionAccessPolicy, pvtDataMsg *protoext.SignedGossipMessage) ([]*dissemination, error) {
+	dissPlan, handled, err := extdissemination.ComputeDisseminationPlan(d.chainID, ns, rwSet, colCP, colAP, pvtDataMsg, d.gossipAdapter)
+	if err != nil {
+		return nil, err
+	}
+
+	if !handled {
+		// Use default dissemination plan
+		return d.disseminationPlanForMsg(colAP, colAP.AccessFilter(), pvtDataMsg)
+	}
+
+	dPlan := make([]*dissemination, len(dissPlan))
+	for i, dp := range dissPlan {
+		dPlan[i] = &dissemination{msg: dp.Msg, criteria: dp.Criteria}
+	}
+	return dPlan, nil
+}
diff --git a/gossip/privdata/distributor.go b/gossip/privdata/distributor.go
index 6a7cc1878..9341472a4 100644
--- a/gossip/privdata/distributor.go
+++ b/gossip/privdata/distributor.go
@@ -47,6 +47,9 @@ type gossipAdapter interface {
 	// PeersOfChannel returns the NetworkMembers considered alive
 	// and also subscribed to the channel given
 	PeersOfChannel(gossipCommon.ChainID) []discovery.NetworkMember
+
+	// SelfMembershipInfo returns the peer's membership information
+	SelfMembershipInfo() discovery.NetworkMember
 }

 // PvtDataDistributor interface to defines API of distributing private data
@@ -91,10 +94,10 @@ func (p *policyAccessFactory) AccessPolicy(config *common.CollectionConfig, chai
 		if err != nil {
 			return nil, errors.WithMessage(err, fmt.Sprintf("error setting up collection  %#v", cconf.StaticCollectionConfig.Name))
 		}
+		return colAP, nil
 	default:
 		return nil, errors.New("unexpected collection type")
 	}
-	return colAP, nil
 }

 // NewCollectionAccessFactory
@@ -170,11 +173,22 @@ func (d *distributorImpl) computeDisseminationPlan(txID string,
 			}

 			logger.Debugf("Computing dissemination plan for collection [%s]", collectionName)
-			dPlan, err := d.disseminationPlanForMsg(colAP, colFilter, pvtDataMsg)
-			if err != nil {
-				return nil, errors.WithStack(err)
+			collType := colCP.GetStaticCollectionConfig().Type
+			var dPlan []*dissemination
+			switch {
+			case collType == common.CollectionType_COL_PRIVATE || collType == common.CollectionType_COL_UNKNOWN:
+				dPlan, err = d.disseminationPlanForMsg(colAP, colFilter, pvtDataMsg)
+				if err != nil {
+					return nil, errors.WithStack(err)
+				}
+			default:
+				dPlan, err = d.disseminationPlanForExt(namespace, collection, colCP, colAP, pvtDataMsg)
+				if err != nil {
+					return nil, errors.WithStack(err)
+				}
 			}
 			disseminationPlan = append(disseminationPlan, dPlan...)
+
 		}
 	}
 	return disseminationPlan, nil
diff --git a/gossip/privdata/distributor_test.go b/gossip/privdata/distributor_test.go
index 121f4f71d..16886a510 100644
--- a/gossip/privdata/distributor_test.go
+++ b/gossip/privdata/distributor_test.go
@@ -105,6 +105,10 @@ func (g *gossipMock) PeerFilter(channel gcommon.ChainID, messagePredicate api.Su
 	}, nil
 }

+func (g *gossipMock) SelfMembershipInfo() discovery.NetworkMember {
+	panic("not implemented")
+}
+
 func TestDistributor(t *testing.T) {
 	channelID := "test"

diff --git a/gossip/protoext/extensions.go b/gossip/protoext/extensions.go
index d1dd3e225..c56ff4b92 100644
--- a/gossip/protoext/extensions.go
+++ b/gossip/protoext/extensions.go
@@ -298,6 +298,13 @@ func IsTagLegal(m *gossip.GossipMessage) error {
 		return nil
 	}

+	if m.GetCollDataReq() != nil || m.GetCollDataRes() != nil {
+		if m.Tag != gossip.GossipMessage_CHAN_ONLY {
+			return fmt.Errorf("Tag should be %s", gossip.GossipMessage_Tag_name[int32(gossip.GossipMessage_CHAN_ONLY)])
+		}
+		return nil
+	}
+
 	return fmt.Errorf("Unknown message type: %v", m)
 }

diff --git a/gossip/service/gossip_service.go b/gossip/service/gossip_service.go
index de68c7d8b..c6311c81c 100644
--- a/gossip/service/gossip_service.go
+++ b/gossip/service/gossip_service.go
@@ -30,6 +30,10 @@ import (
 	"github.com/hyperledger/fabric/protos/transientstore"
 	"github.com/pkg/errors"
 	"github.com/spf13/viper"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/core/ledger"
+	extgossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/extensions/gossip/dispatcher"
 	"google.golang.org/grpc"
 )

@@ -255,6 +259,10 @@ type Support struct {
 	Cs                   privdata.CollectionStore
 	IdDeserializeFactory privdata2.IdentityDeserializerFactory
 	CapabilityProvider   privdata2.CapabilityProvider
+	Capabilities         privdata2.AppCapabilities
+	CollDataStore        storeapi.Store
+	Ledger               ledger.PeerLedger
+	BlockPublisher       extgossipapi.BlockPublisher
 }

 // DataStoreSupport aggregates interfaces capable
@@ -296,6 +304,7 @@ func (g *gossipServiceImpl) InitializeChannel(chainID string, oac OrdererAddress
 		CollectionStore:    support.Cs,
 		Validator:          support.Validator,
 		TransientStore:     support.Store,
+		CollDataStore:      support.CollDataStore,
 		Committer:          support.Committer,
 		Fetcher:            fetcher,
 		CapabilityProvider: support.CapabilityProvider,
@@ -321,7 +330,7 @@ func (g *gossipServiceImpl) InitializeChannel(chainID string, oac OrdererAddress
 	g.privateHandlers[chainID].reconciler.Start()

 	g.chains[chainID] = state.NewGossipStateProvider(chainID, servicesAdapter, coordinator,
-		g.metrics.StateMetrics, getStateConfiguration())
+		g.metrics.StateMetrics, getStateConfiguration(), dispatcher.New(chainID, support.CollDataStore, servicesAdapter, support.Ledger, support.BlockPublisher))
 	if g.deliveryService[chainID] == nil {
 		var err error
 		g.deliveryService[chainID], err = g.deliveryFactory.Service(g, oac, g.mcs)
diff --git a/gossip/service/gossip_service_test.go b/gossip/service/gossip_service_test.go
index 115ad759e..245c0fd1e 100644
--- a/gossip/service/gossip_service_test.go
+++ b/gossip/service/gossip_service_test.go
@@ -22,6 +22,7 @@ import (
 	"github.com/hyperledger/fabric/core/deliverservice/blocksprovider"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/transientstore"
+	kmocks "github.com/hyperledger/fabric/extensions/gossip/mocks"
 	"github.com/hyperledger/fabric/gossip/api"
 	gcomm "github.com/hyperledger/fabric/gossip/comm"
 	gossipCommon "github.com/hyperledger/fabric/gossip/common"
@@ -152,8 +153,9 @@ func TestLeaderElectionWithDeliverClient(t *testing.T) {
 		deliverServiceFactory.service.running[channelName] = false

 		gossips[i].InitializeChannel(channelName, endpointConfig, Support{
-			Store:     &mockTransientStore{},
-			Committer: &mockLedgerInfo{1},
+			Store:          &mockTransientStore{},
+			Committer:      &mockLedgerInfo{1},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 		service, exist := gossips[i].(*gossipGRPC).gossipServiceImpl.leaderElection[channelName]
 		assert.True(t, exist, "Leader election service should be created for peer %d and channel %s", i, channelName)
@@ -210,8 +212,9 @@ func TestWithStaticDeliverClientLeader(t *testing.T) {
 		gossips[i].(*gossipGRPC).gossipServiceImpl.deliveryFactory = deliverServiceFactory
 		deliverServiceFactory.service.running[channelName] = false
 		gossips[i].InitializeChannel(channelName, endpointConfig, Support{
-			Committer: &mockLedgerInfo{1},
-			Store:     &mockTransientStore{},
+			Committer:      &mockLedgerInfo{1},
+			Store:          &mockTransientStore{},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 	}

@@ -224,8 +227,9 @@ func TestWithStaticDeliverClientLeader(t *testing.T) {
 	for i := 0; i < n; i++ {
 		deliverServiceFactory.service.running[channelName] = false
 		gossips[i].InitializeChannel(channelName, endpointConfig, Support{
-			Committer: &mockLedgerInfo{1},
-			Store:     &mockTransientStore{},
+			Committer:      &mockLedgerInfo{1},
+			Store:          &mockTransientStore{},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 	}

diff --git a/gossip/service/integration_test.go b/gossip/service/integration_test.go
index fde1db86b..03091f335 100644
--- a/gossip/service/integration_test.go
+++ b/gossip/service/integration_test.go
@@ -15,6 +15,7 @@ import (
 	"github.com/hyperledger/fabric/core/deliverservice/blocksprovider"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/transientstore"
+	kmocks "github.com/hyperledger/fabric/extensions/gossip/mocks"
 	"github.com/hyperledger/fabric/gossip/api"
 	"github.com/hyperledger/fabric/gossip/util"
 	"github.com/hyperledger/fabric/protos/ledger/rwset"
@@ -137,8 +138,9 @@ func TestLeaderYield(t *testing.T) {
 		gs.deliveryFactory = &embeddingDeliveryServiceFactory{&deliveryFactoryImpl{}}
 		gossipServiceInstance = gs
 		gs.InitializeChannel(channelName, OrdererAddressConfig{Addresses: []string{endpoint}}, Support{
-			Committer: &mockLedgerInfo{1},
-			Store:     &transientStoreMock{},
+			Committer:      &mockLedgerInfo{1},
+			Store:          &transientStoreMock{},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 		return gs
 	}
diff --git a/gossip/state/state.go b/gossip/state/state.go
index 97d2094c0..2edcce2ab 100644
--- a/gossip/state/state.go
+++ b/gossip/state/state.go
@@ -84,6 +84,12 @@ type GossipAdapter interface {
 	// PeersOfChannel returns the NetworkMembers considered alive
 	// and also subscribed to the channel given
 	PeersOfChannel(common2.ChainID) []discovery.NetworkMember
+
+	// SelfMembershipInfo returns the peer's membership information
+	SelfMembershipInfo() discovery.NetworkMember
+
+	// IdentityInfo returns information known peer identities
+	IdentityInfo() api.PeerIdentitySet
 }

 // MCSAdapter adapter of message crypto service interface to bound
@@ -130,6 +136,10 @@ type ServicesMediator struct {
 	MCSAdapter
 }

+type messageDispatcher interface {
+	Dispatch(msg protoext.ReceivedMessage) bool
+}
+
 // GossipStateProviderImpl the implementation of the GossipStateProvider interface
 // the struct to handle in memory sliding window of
 // new ledger block to be acquired by hyper ledger
@@ -166,6 +176,8 @@ type GossipStateProviderImpl struct {
 	config *Configuration

 	stateMetrics *metrics.StateMetrics
+
+	msgDispatcher messageDispatcher
 }

 var logger = util.GetLogger(util.StateLogger, "")
@@ -189,7 +201,7 @@ func (v *stateRequestValidator) validate(request *proto.RemoteStateRequest, batc

 // NewGossipStateProvider creates state provider with coordinator instance
 // to orchestrate arrival of private rwsets and blocks before committing them into the ledger.
-func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger ledgerResources, stateMetrics *metrics.StateMetrics, config *Configuration) GossipStateProvider {
+func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger ledgerResources, stateMetrics *metrics.StateMetrics, config *Configuration, msgDispatcher messageDispatcher) GossipStateProvider {

 	gossipChan, _ := services.Accept(func(message interface{}) bool {
 		// Get only data messages
@@ -200,7 +212,8 @@ func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger l
 	remoteStateMsgFilter := func(message interface{}) bool {
 		receivedMsg := message.(protoext.ReceivedMessage)
 		msg := receivedMsg.GetGossipMessage()
-		if !(protoext.IsRemoteStateMessage(msg.GossipMessage) || msg.GetPrivateData() != nil) {
+		if !(protoext.IsRemoteStateMessage(msg.GossipMessage) || msg.GetPrivateData() != nil ||
+			msg.GetCollDataReq() != nil || msg.GetCollDataRes() != nil) {
 			return false
 		}
 		// Ensure we deal only with messages that belong to this channel
@@ -270,6 +283,8 @@ func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger l
 		config: config,

 		stateMetrics: stateMetrics,
+
+		msgDispatcher: msgDispatcher,
 	}

 	logger.Infof("Updating metadata information for channel %s, "+
@@ -321,6 +336,10 @@ func (s *GossipStateProviderImpl) dispatch(msg protoext.ReceivedMessage) {
 		logger.Debug("Handling private data collection message")
 		// Handling private data replication message
 		s.privateDataMessage(msg)
+	} else if s.msgDispatcher != nil {
+		if dispatched := s.msgDispatcher.Dispatch(msg); dispatched {
+			logger.Debug("Handled Extensions message")
+		}
 	}

 }
diff --git a/gossip/state/state_test.go b/gossip/state/state_test.go
index c9b561d75..b168a2523 100644
--- a/gossip/state/state_test.go
+++ b/gossip/state/state_test.go
@@ -434,8 +434,7 @@ func newPeerNodeWithGossipWithValidatorWithMetrics(id int, committer committer.C
 		Committer:          committer,
 		CapabilityProvider: capabilityProvider,
 	}, pcomm.SignedData{}, gossipMetrics.PrivdataMetrics, coordConfig)
-	sp := NewGossipStateProvider(util.GetTestChainID(), servicesAdapater, coord, gossipMetrics.StateMetrics, config)
-
+	sp := NewGossipStateProvider(util.GetTestChainID(), servicesAdapater, coord, gossipMetrics.StateMetrics, config, nil)
 	if sp == nil {
 		gRPCServer.Stop()
 		return nil, port
@@ -1487,7 +1486,7 @@ func TestTransferOfPrivateRWSet(t *testing.T) {

 	servicesAdapater := &ServicesMediator{GossipAdapter: g, MCSAdapter: &cryptoServiceMock{acceptor: noopPeerIdentityAcceptor}}
 	stateMetrics := metrics.NewGossipMetrics(&disabled.Provider{}).StateMetrics
-	st := NewGossipStateProvider(chainID, servicesAdapater, coord1, stateMetrics, config)
+	st := NewGossipStateProvider(chainID, servicesAdapater, coord1, stateMetrics, config, nil)
 	defer st.Stop()

 	// Mocked state request message
@@ -1724,11 +1723,11 @@ func TestTransferOfPvtDataBetweenPeers(t *testing.T) {
 	stateMetrics := metrics.NewGossipMetrics(&disabled.Provider{}).StateMetrics

 	mediator := &ServicesMediator{GossipAdapter: peers["peer1"], MCSAdapter: cryptoService}
-	peer1State := NewGossipStateProvider(chainID, mediator, peers["peer1"].coord, stateMetrics, config)
+	peer1State := NewGossipStateProvider(chainID, mediator, peers["peer1"].coord, stateMetrics, config, nil)
 	defer peer1State.Stop()

 	mediator = &ServicesMediator{GossipAdapter: peers["peer2"], MCSAdapter: cryptoService}
-	peer2State := NewGossipStateProvider(chainID, mediator, peers["peer2"].coord, stateMetrics, config)
+	peer2State := NewGossipStateProvider(chainID, mediator, peers["peer2"].coord, stateMetrics, config, nil)
 	defer peer2State.Stop()

 	// Make sure state was replicated
diff --git a/peer/chaincode/common.go b/peer/chaincode/common.go
index d259a2259..b6c26d259 100644
--- a/peer/chaincode/common.go
+++ b/peer/chaincode/common.go
@@ -152,6 +152,14 @@ func chaincodeInvokeOrQuery(cmd *cobra.Command, invoke bool, cf *ChaincodeCmdFac
 	return nil
 }

+// CollectionType enumerates the various types of private data collections.
+type CollectionType string
+
+const (
+	CollectionType_PRIVATE   CollectionType = "PRIVATE"
+	CollectionType_TRANSIENT CollectionType = "TRANSIENT"
+)
+
 type collectionConfigJson struct {
 	Name           string `json:"name"`
 	Policy         string `json:"policy"`
@@ -159,6 +167,9 @@ type collectionConfigJson struct {
 	MaxPeerCount   int32  `json:"maxPeerCount"`
 	BlockToLive    uint64 `json:"blockToLive"`
 	MemberOnlyRead bool   `json:"memberOnlyRead"`
+
+	Type       CollectionType `json:"type"`
+	TimeToLive string         `json:"timeToLive"`
 }

 // getCollectionConfig retrieves the collection configuration
@@ -196,17 +207,44 @@ func getCollectionConfigFromBytes(cconfBytes []byte) ([]byte, error) {
 			},
 		}

-		cc := &pcommon.CollectionConfig{
-			Payload: &pcommon.CollectionConfig_StaticCollectionConfig{
-				StaticCollectionConfig: &pcommon.StaticCollectionConfig{
-					Name:              cconfitem.Name,
-					MemberOrgsPolicy:  cpc,
-					RequiredPeerCount: cconfitem.RequiredCount,
-					MaximumPeerCount:  cconfitem.MaxPeerCount,
-					BlockToLive:       cconfitem.BlockToLive,
-					MemberOnlyRead:    cconfitem.MemberOnlyRead,
+		var cc *pcommon.CollectionConfig
+		switch cconfitem.Type {
+		case CollectionType_TRANSIENT:
+			cc = &pcommon.CollectionConfig{
+				Payload: &pcommon.CollectionConfig_StaticCollectionConfig{
+					StaticCollectionConfig: &pcommon.StaticCollectionConfig{
+						Name:              cconfitem.Name,
+						MemberOrgsPolicy:  cpc,
+						RequiredPeerCount: cconfitem.RequiredCount,
+						MaximumPeerCount:  cconfitem.MaxPeerCount,
+						MemberOnlyRead:    cconfitem.MemberOnlyRead,
+						//MemberOnlyWrite is added post Fabric 1.4.1
+						//MemberOnlyWrite:   cconfitem.MemberOnlyWrite,
+
+						Type:       pcommon.CollectionType_COL_TRANSIENT,
+						TimeToLive: cconfitem.TimeToLive,
+					},
 				},
-			},
+			}
+		case CollectionType_PRIVATE:
+			fallthrough
+		case "":
+			cc = &pcommon.CollectionConfig{
+				Payload: &pcommon.CollectionConfig_StaticCollectionConfig{
+					StaticCollectionConfig: &pcommon.StaticCollectionConfig{
+						Name:              cconfitem.Name,
+						MemberOrgsPolicy:  cpc,
+						RequiredPeerCount: cconfitem.RequiredCount,
+						MaximumPeerCount:  cconfitem.MaxPeerCount,
+						BlockToLive:       cconfitem.BlockToLive,
+						MemberOnlyRead:    cconfitem.MemberOnlyRead,
+						//MemberOnlyWrite is added post Fabric 1.4.1
+						//MemberOnlyWrite:   cconfitem.MemberOnlyWrite,
+					},
+				},
+			}
+		default:
+			return nil, errors.Errorf("unsupported collection configuration type [%s]", cconfitem.Type)
 		}

 		ccarray = append(ccarray, cc)
diff --git a/peer/chaincode/transientcollconfig_test.go b/peer/chaincode/transientcollconfig_test.go
new file mode 100644
index 000000000..c16087d13
--- /dev/null
+++ b/peer/chaincode/transientcollconfig_test.go
@@ -0,0 +1,64 @@
+/*
+	Copyright Digital Asset Holdings, LLC. All Rights Reserved.
+	Copyright IBM Corp. All Rights Reserved.
+
+	SPDX-License-Identifier: Apache-2.0
+*/
+
+package chaincode
+
+import (
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/cauthdsl"
+	pcommon "github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const sampleCollectionConfigTransient = `[
+		{
+			"name": "foo",
+			"policy": "OR('A.member', 'B.member')",
+			"requiredPeerCount": 3,
+			"maxPeerCount": 5,
+			"type": "TRANSIENT",
+			"timeToLive": "2m"
+		}
+	]`
+
+const sampleCollectionConfigInvalidType = `[
+		{
+			"name": "foo",
+			"policy": "OR('A.member', 'B.member')",
+			"requiredPeerCount": 3,
+			"maxPeerCount": 5,
+			"type": "INVALID"
+		}
+	]`
+
+func TestCollectionTypeParsing(t *testing.T) {
+	pol, _ := cauthdsl.FromString("OR('A.member', 'B.member')")
+
+	t.Run("Invalid Collection Config Type", func(t *testing.T) {
+		_, err := getCollectionConfigFromBytes([]byte(sampleCollectionConfigInvalidType))
+		assert.Error(t, err)
+	})
+
+	t.Run("Transient Collection Config", func(t *testing.T) {
+		cc, err := getCollectionConfigFromBytes([]byte(sampleCollectionConfigTransient))
+		assert.NoError(t, err)
+		assert.NotNil(t, cc)
+		ccp := &pcommon.CollectionConfigPackage{Config: []*pcommon.CollectionConfig{}}
+		err = proto.Unmarshal(cc, ccp)
+		assert.NoError(t, err)
+		conf := ccp.Config[0].GetStaticCollectionConfig()
+		require.NotNil(t, conf)
+		assert.Equal(t, "foo", conf.Name)
+		assert.Equal(t, int32(3), conf.RequiredPeerCount)
+		assert.Equal(t, int32(5), conf.MaximumPeerCount)
+		assert.True(t, proto.Equal(pol, conf.MemberOrgsPolicy.GetSignaturePolicy()))
+		assert.Equal(t, "2m", conf.TimeToLive)
+	})
+}
diff --git a/peer/node/start.go b/peer/node/start.go
index 49a486a53..58e9ad49e 100644
--- a/peer/node/start.go
+++ b/peer/node/start.go
@@ -75,6 +75,8 @@ import (
 	ccsupport "github.com/hyperledger/fabric/discovery/support/chaincode"
 	"github.com/hyperledger/fabric/discovery/support/config"
 	"github.com/hyperledger/fabric/discovery/support/gossip"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	collretriever "github.com/hyperledger/fabric/extensions/collections/retriever"
 	gossipcommon "github.com/hyperledger/fabric/gossip/common"
 	"github.com/hyperledger/fabric/gossip/service"
 	"github.com/hyperledger/fabric/msp"
@@ -179,6 +181,16 @@ func serve(args []string) error {
 	flogging.Global.SetObserver(logObserver)

 	membershipInfoProvider := privdata.NewMembershipInfoProvider(mspID, createSelfSignedData(), identityDeserializerFactory)
+
+	transientDataProvider := collretriever.NewProvider(
+		peer.CollectionDataStoreFactory().StoreForChannel,
+		peer.GetLedger,
+		func() supportapi.GossipAdapter {
+			return service.GetGossipService()
+		},
+		peer.BlockPublisher.ForChannel,
+	)
+
 	//initialize resource management exit
 	ledgermgmt.Initialize(
 		&ledgermgmt.Initializer{
@@ -188,6 +200,7 @@ func serve(args []string) error {
 			MembershipInfoProvider:        membershipInfoProvider,
 			MetricsProvider:               metricsProvider,
 			HealthCheckRegistry:           opsSystem,
+			CollDataProvider:              transientDataProvider,
 		},
 	)

@@ -373,7 +386,7 @@ func serve(args []string) error {
 		}
 		cceventmgmt.GetMgr().Register(cid, sub)
 	}, ccp, sccp, txvalidator.MapBasedPluginMapper(validationPluginsByName),
-		pr, deployedCCInfoProvider, membershipInfoProvider, metricsProvider)
+		pr, deployedCCInfoProvider, membershipInfoProvider, metricsProvider, transientDataProvider)

 	if viper.GetBool("peer.discovery.enabled") {
 		registerDiscoveryService(peerServer, policyMgr, lifecycle)
diff --git a/protos/common/collection.pb.go b/protos/common/collection.pb.go
index e4d936342..237f41418 100644
--- a/protos/common/collection.pb.go
+++ b/protos/common/collection.pb.go
@@ -18,6 +18,39 @@ var _ = math.Inf
 // proto package needs to be updated.
 const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

+// CollectionType enumerates the various types of private data collections.
+type CollectionType int32
+
+const (
+	CollectionType_COL_UNKNOWN   CollectionType = 0
+	CollectionType_COL_PRIVATE   CollectionType = 1
+	CollectionType_COL_TRANSIENT CollectionType = 2
+	CollectionType_COL_OFFLEDGER CollectionType = 3
+	CollectionType_COL_DCAS      CollectionType = 4
+)
+
+var CollectionType_name = map[int32]string{
+	0: "COL_UNKNOWN",
+	1: "COL_PRIVATE",
+	2: "COL_TRANSIENT",
+	3: "COL_OFFLEDGER",
+	4: "COL_DCAS",
+}
+var CollectionType_value = map[string]int32{
+	"COL_UNKNOWN":   0,
+	"COL_PRIVATE":   1,
+	"COL_TRANSIENT": 2,
+	"COL_OFFLEDGER": 3,
+	"COL_DCAS":      4,
+}
+
+func (x CollectionType) String() string {
+	return proto.EnumName(CollectionType_name, int32(x))
+}
+func (CollectionType) EnumDescriptor() ([]byte, []int) {
+	return fileDescriptor_collection_59b2e02e8b8b23b5, []int{0}
+}
+
 // CollectionConfigPackage represents an array of CollectionConfig
 // messages; the extra struct is required because repeated oneof is
 // forbidden by the protobuf syntax
@@ -32,7 +65,7 @@ func (m *CollectionConfigPackage) Reset()         { *m = CollectionConfigPackage
 func (m *CollectionConfigPackage) String() string { return proto.CompactTextString(m) }
 func (*CollectionConfigPackage) ProtoMessage()    {}
 func (*CollectionConfigPackage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{0}
+	return fileDescriptor_collection_59b2e02e8b8b23b5, []int{0}
 }
 func (m *CollectionConfigPackage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionConfigPackage.Unmarshal(m, b)
@@ -75,7 +108,7 @@ func (m *CollectionConfig) Reset()         { *m = CollectionConfig{} }
 func (m *CollectionConfig) String() string { return proto.CompactTextString(m) }
 func (*CollectionConfig) ProtoMessage()    {}
 func (*CollectionConfig) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{1}
+	return fileDescriptor_collection_59b2e02e8b8b23b5, []int{1}
 }
 func (m *CollectionConfig) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionConfig.Unmarshal(m, b)
@@ -199,7 +232,15 @@ type StaticCollectionConfig struct {
 	// can read the private data (if set to true), or even non members can
 	// read the data (if set to false, for example if you want to implement more granular
 	// access logic in the chaincode)
-	MemberOnlyRead       bool     `protobuf:"varint,6,opt,name=member_only_read,json=memberOnlyRead,proto3" json:"member_only_read,omitempty"`
+	MemberOnlyRead bool `protobuf:"varint,6,opt,name=member_only_read,json=memberOnlyRead,proto3" json:"member_only_read,omitempty"`
+	// The type of collection.
+	Type CollectionType `protobuf:"varint,9900,opt,name=type,proto3,enum=common.CollectionType" json:"type,omitempty"`
+	// The time after which the collection data expires. For example,
+	// if the value is set to "10m" then the data will be purged
+	// 10 minutes after it was stored. An empty value indicates that
+	// the data should never be purged.
+	// The format of this string must be parseable by time.ParseDuration
+	TimeToLive           string   `protobuf:"bytes,9901,opt,name=time_to_live,json=timeToLive,proto3" json:"time_to_live,omitempty"`
 	XXX_NoUnkeyedLiteral struct{} `json:"-"`
 	XXX_unrecognized     []byte   `json:"-"`
 	XXX_sizecache        int32    `json:"-"`
@@ -209,7 +250,7 @@ func (m *StaticCollectionConfig) Reset()         { *m = StaticCollectionConfig{}
 func (m *StaticCollectionConfig) String() string { return proto.CompactTextString(m) }
 func (*StaticCollectionConfig) ProtoMessage()    {}
 func (*StaticCollectionConfig) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{2}
+	return fileDescriptor_collection_59b2e02e8b8b23b5, []int{2}
 }
 func (m *StaticCollectionConfig) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StaticCollectionConfig.Unmarshal(m, b)
@@ -271,6 +312,20 @@ func (m *StaticCollectionConfig) GetMemberOnlyRead() bool {
 	return false
 }

+func (m *StaticCollectionConfig) GetType() CollectionType {
+	if m != nil {
+		return m.Type
+	}
+	return CollectionType_COL_UNKNOWN
+}
+
+func (m *StaticCollectionConfig) GetTimeToLive() string {
+	if m != nil {
+		return m.TimeToLive
+	}
+	return ""
+}
+
 // Collection policy configuration. Initially, the configuration can only
 // contain a SignaturePolicy. In the future, the SignaturePolicy may be a
 // more general Policy. Instead of containing the actual policy, the
@@ -288,7 +343,7 @@ func (m *CollectionPolicyConfig) Reset()         { *m = CollectionPolicyConfig{}
 func (m *CollectionPolicyConfig) String() string { return proto.CompactTextString(m) }
 func (*CollectionPolicyConfig) ProtoMessage()    {}
 func (*CollectionPolicyConfig) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{3}
+	return fileDescriptor_collection_59b2e02e8b8b23b5, []int{3}
 }
 func (m *CollectionPolicyConfig) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionPolicyConfig.Unmarshal(m, b)
@@ -403,7 +458,7 @@ func (m *CollectionCriteria) Reset()         { *m = CollectionCriteria{} }
 func (m *CollectionCriteria) String() string { return proto.CompactTextString(m) }
 func (*CollectionCriteria) ProtoMessage()    {}
 func (*CollectionCriteria) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{4}
+	return fileDescriptor_collection_59b2e02e8b8b23b5, []int{4}
 }
 func (m *CollectionCriteria) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionCriteria.Unmarshal(m, b)
@@ -457,40 +512,48 @@ func init() {
 	proto.RegisterType((*StaticCollectionConfig)(nil), "common.StaticCollectionConfig")
 	proto.RegisterType((*CollectionPolicyConfig)(nil), "common.CollectionPolicyConfig")
 	proto.RegisterType((*CollectionCriteria)(nil), "common.CollectionCriteria")
-}
-
-func init() { proto.RegisterFile("common/collection.proto", fileDescriptor_collection_12a2cf6632dc7d83) }
-
-var fileDescriptor_collection_12a2cf6632dc7d83 = []byte{
-	// 480 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x6c, 0x93, 0x51, 0x6b, 0xdb, 0x30,
-	0x10, 0xc7, 0xeb, 0x36, 0x4d, 0xe7, 0x0b, 0xdb, 0x32, 0x95, 0xa5, 0x66, 0x8c, 0x2e, 0x84, 0x3d,
-	0x18, 0x36, 0x9c, 0xd1, 0x7d, 0x83, 0x86, 0x41, 0xc7, 0x02, 0x0b, 0xea, 0x9e, 0xfa, 0x62, 0x14,
-	0xf9, 0xea, 0x88, 0xca, 0x92, 0x2b, 0x2b, 0x21, 0x7e, 0xdc, 0x97, 0xd9, 0xe7, 0x1c, 0x91, 0xec,
-	0x24, 0x0d, 0x79, 0xf3, 0xdd, 0xff, 0x77, 0xe7, 0xbb, 0xfb, 0xdb, 0x70, 0xc5, 0x75, 0x51, 0x68,
-	0x35, 0xe6, 0x5a, 0x4a, 0xe4, 0x56, 0x68, 0x95, 0x94, 0x46, 0x5b, 0x4d, 0xba, 0x5e, 0xf8, 0xf0,
-	0xbe, 0x01, 0x4a, 0x2d, 0x05, 0x17, 0x58, 0x79, 0x79, 0xf4, 0x0b, 0xae, 0x26, 0xdb, 0x92, 0x89,
-	0x56, 0x8f, 0x22, 0x9f, 0x31, 0xfe, 0xc4, 0x72, 0x24, 0xdf, 0xa0, 0xcb, 0x5d, 0x22, 0x0a, 0x86,
-	0x67, 0x71, 0xef, 0x26, 0x4a, 0x7c, 0x8b, 0xe4, 0xb0, 0x80, 0x36, 0xdc, 0xa8, 0x86, 0xfe, 0xa1,
-	0x46, 0x1e, 0x20, 0xaa, 0x2c, 0xb3, 0x82, 0xa7, 0xbb, 0xd1, 0xd2, 0x6d, 0xdf, 0x20, 0xee, 0xdd,
-	0x5c, 0xb7, 0x7d, 0xef, 0x1d, 0x77, 0xd8, 0xe1, 0xee, 0x84, 0x0e, 0xaa, 0xa3, 0xca, 0x6d, 0x08,
-	0x17, 0x25, 0xab, 0xa5, 0x66, 0xd9, 0xe8, 0xdf, 0x29, 0x0c, 0x8e, 0xd7, 0x13, 0x02, 0x1d, 0xc5,
-	0x0a, 0x74, 0x6f, 0x0b, 0xa9, 0x7b, 0x26, 0x53, 0x20, 0x05, 0x16, 0x73, 0x34, 0xa9, 0x36, 0x79,
-	0x95, 0xba, 0xa3, 0xd4, 0xd1, 0xe9, 0xcb, 0x79, 0x76, 0x9d, 0x66, 0x4e, 0x6f, 0xb6, 0xed, 0xfb,
-	0xca, 0xdf, 0x26, 0xaf, 0x7c, 0x9e, 0x24, 0x70, 0x69, 0xf0, 0x79, 0x29, 0x0c, 0x66, 0x69, 0x89,
-	0x68, 0x52, 0xae, 0x97, 0xca, 0x46, 0x67, 0xc3, 0x20, 0x3e, 0xa7, 0xef, 0x5a, 0x69, 0x86, 0x68,
-	0x26, 0x1b, 0x81, 0x7c, 0x05, 0x52, 0xb0, 0xb5, 0x28, 0x96, 0xc5, 0x3e, 0xde, 0x71, 0x78, 0xbf,
-	0x51, 0x76, 0xf4, 0x08, 0x5e, 0xcf, 0xa5, 0xe6, 0x4f, 0xa9, 0xd5, 0xa9, 0x14, 0x2b, 0x8c, 0xce,
-	0x87, 0x41, 0xdc, 0xa1, 0x3d, 0x97, 0xfc, 0xa3, 0xa7, 0x62, 0x85, 0x24, 0x86, 0x7e, 0xbb, 0x8f,
-	0x92, 0x75, 0x6a, 0x90, 0x65, 0x51, 0x77, 0x18, 0xc4, 0xaf, 0xe8, 0x9b, 0x66, 0x5a, 0x25, 0x6b,
-	0x8a, 0x2c, 0x1b, 0x3d, 0xc3, 0xe0, 0xf8, 0x5e, 0x64, 0x0a, 0xfd, 0x4a, 0xe4, 0x8a, 0xd9, 0xa5,
-	0xc1, 0xf6, 0x22, 0xde, 0xa1, 0x4f, 0x5b, 0x87, 0x5a, 0xdd, 0x17, 0xfe, 0x50, 0x2b, 0x94, 0xba,
-	0xc4, 0xbb, 0x13, 0xfa, 0xb6, 0x7a, 0x29, 0xed, 0x7b, 0xf3, 0x37, 0x00, 0xb2, 0xe7, 0x8a, 0x11,
-	0x16, 0x8d, 0x60, 0x24, 0x82, 0x0b, 0xbe, 0x60, 0x4a, 0xa1, 0x6c, 0xac, 0x69, 0x43, 0x72, 0x09,
-	0xe7, 0x76, 0x9d, 0x8a, 0xcc, 0x19, 0x12, 0xd2, 0x8e, 0x5d, 0xff, 0xcc, 0xc8, 0x35, 0xc0, 0xee,
-	0x0b, 0x72, 0xb7, 0x0d, 0xe9, 0x5e, 0x86, 0x7c, 0x84, 0x70, 0x63, 0x6d, 0x55, 0x32, 0x8e, 0xee,
-	0x96, 0x21, 0xdd, 0x25, 0x6e, 0xef, 0xe1, 0xb3, 0x36, 0x79, 0xb2, 0xa8, 0x4b, 0x34, 0x12, 0xb3,
-	0x1c, 0x4d, 0xf2, 0xc8, 0xe6, 0x46, 0x70, 0xff, 0x1f, 0x54, 0xcd, 0x86, 0x0f, 0x5f, 0x72, 0x61,
-	0x17, 0xcb, 0xf9, 0x26, 0x1c, 0xef, 0xc1, 0x63, 0x0f, 0x8f, 0x3d, 0x3c, 0xf6, 0xf0, 0xbc, 0xeb,
-	0xc2, 0xef, 0xff, 0x03, 0x00, 0x00, 0xff, 0xff, 0xf0, 0x3b, 0x7c, 0x15, 0x7d, 0x03, 0x00, 0x00,
+	proto.RegisterEnum("common.CollectionType", CollectionType_name, CollectionType_value)
+}
+
+func init() { proto.RegisterFile("common/collection.proto", fileDescriptor_collection_59b2e02e8b8b23b5) }
+
+var fileDescriptor_collection_59b2e02e8b8b23b5 = []byte{
+	// 591 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x6c, 0x53, 0x5d, 0x4f, 0xdb, 0x30,
+	0x14, 0x25, 0x50, 0x0a, 0xbd, 0xe5, 0x23, 0x18, 0xad, 0x44, 0xd3, 0xc4, 0xba, 0x6a, 0x0f, 0xd1,
+	0x98, 0xda, 0x89, 0xfd, 0x02, 0x28, 0x65, 0x20, 0x4a, 0x5b, 0xb9, 0xdd, 0x26, 0xf1, 0x12, 0xb9,
+	0xc9, 0x25, 0x58, 0x24, 0x71, 0x70, 0x5c, 0x44, 0x1e, 0xf7, 0x7f, 0xb6, 0xbf, 0xb7, 0xe7, 0x29,
+	0x4e, 0xd2, 0x16, 0xc6, 0x5b, 0x7c, 0xce, 0xb9, 0xd7, 0xf7, 0x9e, 0xe3, 0xc0, 0x81, 0x2b, 0xc2,
+	0x50, 0x44, 0x1d, 0x57, 0x04, 0x01, 0xba, 0x8a, 0x8b, 0xa8, 0x1d, 0x4b, 0xa1, 0x04, 0xa9, 0xe6,
+	0xc4, 0xdb, 0x37, 0x85, 0x20, 0x16, 0x01, 0x77, 0x39, 0x26, 0x39, 0xdd, 0xba, 0x82, 0x83, 0xee,
+	0xbc, 0xa4, 0x2b, 0xa2, 0x5b, 0xee, 0x8f, 0x98, 0x7b, 0xcf, 0x7c, 0x24, 0x5f, 0xa0, 0xea, 0x6a,
+	0xc0, 0x32, 0x9a, 0x6b, 0x76, 0xfd, 0xd8, 0x6a, 0xe7, 0x2d, 0xda, 0x2f, 0x0b, 0x68, 0xa1, 0x6b,
+	0xa5, 0x60, 0xbe, 0xe4, 0xc8, 0x0d, 0x58, 0x89, 0x62, 0x8a, 0xbb, 0xce, 0x62, 0x34, 0x67, 0xde,
+	0xd7, 0xb0, 0xeb, 0xc7, 0x87, 0x65, 0xdf, 0xb1, 0xd6, 0xbd, 0xec, 0x70, 0xb1, 0x42, 0x1b, 0xc9,
+	0xab, 0xcc, 0x69, 0x0d, 0x36, 0x62, 0x96, 0x06, 0x82, 0x79, 0xad, 0xbf, 0xab, 0xd0, 0x78, 0xbd,
+	0x9e, 0x10, 0xa8, 0x44, 0x2c, 0x44, 0x7d, 0x5b, 0x8d, 0xea, 0x6f, 0xd2, 0x07, 0x12, 0x62, 0x38,
+	0x45, 0xe9, 0x08, 0xe9, 0x27, 0x8e, 0x36, 0x25, 0xb5, 0x56, 0x9f, 0xcf, 0xb3, 0xe8, 0x34, 0xd2,
+	0x7c, 0xb1, 0xad, 0x99, 0x57, 0x0e, 0xa5, 0x9f, 0xe4, 0x38, 0x69, 0xc3, 0xbe, 0xc4, 0x87, 0x19,
+	0x97, 0xe8, 0x39, 0x31, 0xa2, 0x74, 0x5c, 0x31, 0x8b, 0x94, 0xb5, 0xd6, 0x34, 0xec, 0x75, 0xba,
+	0x57, 0x52, 0x23, 0x44, 0xd9, 0xcd, 0x08, 0xf2, 0x19, 0x48, 0xc8, 0x9e, 0x78, 0x38, 0x0b, 0x97,
+	0xe5, 0x15, 0x2d, 0x37, 0x0b, 0x66, 0xa1, 0x6e, 0xc1, 0xf6, 0x34, 0x10, 0xee, 0xbd, 0xa3, 0x84,
+	0x13, 0xf0, 0x47, 0xb4, 0xd6, 0x9b, 0x86, 0x5d, 0xa1, 0x75, 0x0d, 0x4e, 0x44, 0x9f, 0x3f, 0x22,
+	0xb1, 0xc1, 0x2c, 0xf7, 0x89, 0x82, 0xd4, 0x91, 0xc8, 0x3c, 0xab, 0xda, 0x34, 0xec, 0x4d, 0xba,
+	0x53, 0x4c, 0x1b, 0x05, 0x29, 0x45, 0xe6, 0x91, 0x23, 0xa8, 0xa8, 0x34, 0x46, 0xeb, 0xf7, 0x75,
+	0xd3, 0xb0, 0x77, 0x8e, 0x1b, 0xff, 0x2f, 0x3b, 0x49, 0x63, 0xa4, 0x5a, 0x44, 0x3e, 0xc0, 0x96,
+	0xe2, 0x21, 0xce, 0x6f, 0xfe, 0x73, 0xad, 0x3d, 0x84, 0x0c, 0xcc, 0x6f, 0x6e, 0x3d, 0x40, 0xe3,
+	0x75, 0x9f, 0x48, 0x1f, 0xcc, 0x84, 0xfb, 0x11, 0x53, 0x33, 0x89, 0xa5, 0xc3, 0x79, 0xe2, 0xef,
+	0xe7, 0x89, 0x97, 0x7c, 0x5e, 0xd8, 0x8b, 0x1e, 0x31, 0x10, 0x31, 0x5e, 0xac, 0xd0, 0xdd, 0xe4,
+	0x39, 0xb5, 0x9c, 0xf5, 0x2f, 0x03, 0xc8, 0x52, 0xca, 0x92, 0x2b, 0x94, 0x9c, 0x11, 0x0b, 0x36,
+	0xdc, 0x3b, 0x16, 0x45, 0x18, 0x14, 0x51, 0x97, 0x47, 0xb2, 0x0f, 0xeb, 0xea, 0xc9, 0xe1, 0x9e,
+	0x0e, 0xb8, 0x46, 0x2b, 0xea, 0xe9, 0xd2, 0x23, 0x87, 0x00, 0x8b, 0x17, 0xa9, 0xb3, 0xaa, 0xd1,
+	0x25, 0x84, 0xbc, 0x83, 0x5a, 0xf6, 0x54, 0x92, 0x98, 0xb9, 0xa8, 0xb3, 0xa9, 0xd1, 0x05, 0xf0,
+	0xe9, 0x16, 0x76, 0x9e, 0x3b, 0x46, 0x76, 0xa1, 0xde, 0x1d, 0xf6, 0x9d, 0xef, 0x83, 0xab, 0xc1,
+	0xf0, 0xe7, 0xc0, 0x5c, 0x29, 0x81, 0x11, 0xbd, 0xfc, 0x71, 0x32, 0xe9, 0x99, 0x06, 0xd9, 0x83,
+	0xed, 0x0c, 0x98, 0xd0, 0x93, 0xc1, 0xf8, 0xb2, 0x37, 0x98, 0x98, 0xab, 0x25, 0x34, 0x3c, 0x3f,
+	0xef, 0xf7, 0xce, 0xbe, 0xf5, 0xa8, 0xb9, 0x46, 0xb6, 0x60, 0x33, 0x83, 0xce, 0xba, 0x27, 0x63,
+	0xb3, 0x72, 0x3a, 0x86, 0x8f, 0x42, 0xfa, 0xed, 0xbb, 0x34, 0x46, 0x19, 0xa0, 0xe7, 0xa3, 0x6c,
+	0xdf, 0xb2, 0xa9, 0xe4, 0x6e, 0xfe, 0xff, 0x26, 0x85, 0x93, 0x37, 0x47, 0x3e, 0x57, 0x77, 0xb3,
+	0x69, 0x76, 0xec, 0x2c, 0x89, 0x3b, 0xb9, 0xb8, 0x93, 0x8b, 0x3b, 0xb9, 0x78, 0x5a, 0xd5, 0xc7,
+	0xaf, 0xff, 0x02, 0x00, 0x00, 0xff, 0xff, 0xf7, 0x1a, 0x4f, 0xba, 0x35, 0x04, 0x00, 0x00,
 }
diff --git a/protos/common/collection.proto b/protos/common/collection.proto
index 321bfd72c..dbd575c98 100644
--- a/protos/common/collection.proto
+++ b/protos/common/collection.proto
@@ -13,6 +13,15 @@ option java_package = "org.hyperledger.fabric.protos.common";

 package common;

+// CollectionType enumerates the various types of private data collections.
+enum CollectionType {
+    COL_UNKNOWN = 0;    // Unspecified type - will use PRIVATE as default.
+    COL_PRIVATE = 1;    // Persisted private data collection.
+    COL_TRANSIENT = 2;  // Transient private data collection.
+    COL_OFFLEDGER = 3;  // Off-ledger private data collection. NOT USED By Fabric 1.4.1 release, added here for trustbloc/fabric-peer-ext dependency
+    COL_DCAS = 4;       // Distributed Content Addressable Store (CAS) private data collection. NOT USED By Fabric 1.4.1, same as above.
+}
+
 // CollectionConfigPackage represents an array of CollectionConfig
 // messages; the extra struct is required because repeated oneof is
 // forbidden by the protobuf syntax
@@ -56,6 +65,14 @@ message StaticCollectionConfig {
     // read the data (if set to false, for example if you want to implement more granular
     // access logic in the chaincode)
     bool member_only_read = 6;
+    // The type of collection.
+    CollectionType type = 9900;
+    // The time after which the collection data expires. For example,
+    // if the value is set to "10m" then the data will be purged
+    // 10 minutes after it was stored. An empty value indicates that
+    // the data should never be purged.
+    // The format of this string must be parseable by time.ParseDuration
+    string time_to_live = 9901;
 }


@@ -70,7 +87,7 @@ message CollectionPolicyConfig {
         // Later, the SignaturePolicy will be replaced by a Policy.
         //        Policy policy = 1;
         // A reference to a Policy is planned to be added later.
-//        string reference = 2;
+        //        string reference = 2;
     }
 }

diff --git a/protos/gossip/message.pb.go b/protos/gossip/message.pb.go
index 3d2c7a1b2..4db4f85c7 100644
--- a/protos/gossip/message.pb.go
+++ b/protos/gossip/message.pb.go
@@ -6,6 +6,7 @@ package gossip // import "github.com/hyperledger/fabric/protos/gossip"
 import proto "github.com/golang/protobuf/proto"
 import fmt "fmt"
 import math "math"
+import timestamp "github.com/golang/protobuf/ptypes/timestamp"
 import common "github.com/hyperledger/fabric/protos/common"

 import (
@@ -47,7 +48,7 @@ func (x PullMsgType) String() string {
 	return proto.EnumName(PullMsgType_name, int32(x))
 }
 func (PullMsgType) EnumDescriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{0}
+	return fileDescriptor_message_563b77f4f682605d, []int{0}
 }

 type GossipMessage_Tag int32
@@ -82,7 +83,7 @@ func (x GossipMessage_Tag) String() string {
 	return proto.EnumName(GossipMessage_Tag_name, int32(x))
 }
 func (GossipMessage_Tag) EnumDescriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{3, 0}
+	return fileDescriptor_message_563b77f4f682605d, []int{3, 0}
 }

 // Envelope contains a marshalled
@@ -102,7 +103,7 @@ func (m *Envelope) Reset()         { *m = Envelope{} }
 func (m *Envelope) String() string { return proto.CompactTextString(m) }
 func (*Envelope) ProtoMessage()    {}
 func (*Envelope) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{0}
+	return fileDescriptor_message_563b77f4f682605d, []int{0}
 }
 func (m *Envelope) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Envelope.Unmarshal(m, b)
@@ -160,7 +161,7 @@ func (m *SecretEnvelope) Reset()         { *m = SecretEnvelope{} }
 func (m *SecretEnvelope) String() string { return proto.CompactTextString(m) }
 func (*SecretEnvelope) ProtoMessage()    {}
 func (*SecretEnvelope) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{1}
+	return fileDescriptor_message_563b77f4f682605d, []int{1}
 }
 func (m *SecretEnvelope) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_SecretEnvelope.Unmarshal(m, b)
@@ -210,7 +211,7 @@ func (m *Secret) Reset()         { *m = Secret{} }
 func (m *Secret) String() string { return proto.CompactTextString(m) }
 func (*Secret) ProtoMessage()    {}
 func (*Secret) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{2}
+	return fileDescriptor_message_563b77f4f682605d, []int{2}
 }
 func (m *Secret) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Secret.Unmarshal(m, b)
@@ -339,7 +340,9 @@ type GossipMessage struct {
 	//	*GossipMessage_PrivateReq
 	//	*GossipMessage_PrivateRes
 	//	*GossipMessage_PrivateData
-	Content              IsGossipMessage_Content `protobuf_oneof:"content"`
+	//	*GossipMessage_CollDataReq
+	//	*GossipMessage_CollDataRes
+	Content              isGossipMessage_Content `protobuf_oneof:"content"`
 	XXX_NoUnkeyedLiteral struct{}                `json:"-"`
 	XXX_unrecognized     []byte                  `json:"-"`
 	XXX_sizecache        int32                   `json:"-"`
@@ -349,7 +352,7 @@ func (m *GossipMessage) Reset()         { *m = GossipMessage{} }
 func (m *GossipMessage) String() string { return proto.CompactTextString(m) }
 func (*GossipMessage) ProtoMessage()    {}
 func (*GossipMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{3}
+	return fileDescriptor_message_563b77f4f682605d, []int{3}
 }
 func (m *GossipMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_GossipMessage.Unmarshal(m, b)
@@ -390,8 +393,8 @@ func (m *GossipMessage) GetTag() GossipMessage_Tag {
 	return GossipMessage_UNDEFINED
 }

-type IsGossipMessage_Content interface {
-	IsGossipMessage_Content()
+type isGossipMessage_Content interface {
+	isGossipMessage_Content()
 }

 type GossipMessage_AliveMsg struct {
@@ -478,49 +481,61 @@ type GossipMessage_PrivateData struct {
 	PrivateData *PrivateDataMessage `protobuf:"bytes,25,opt,name=private_data,json=privateData,proto3,oneof"`
 }

-func (*GossipMessage_AliveMsg) IsGossipMessage_Content() {}
+type GossipMessage_CollDataReq struct {
+	CollDataReq *RemoteCollDataRequest `protobuf:"bytes,90,opt,name=collDataReq,proto3,oneof"`
+}
+
+type GossipMessage_CollDataRes struct {
+	CollDataRes *RemoteCollDataResponse `protobuf:"bytes,91,opt,name=collDataRes,proto3,oneof"`
+}
+
+func (*GossipMessage_AliveMsg) isGossipMessage_Content() {}
+
+func (*GossipMessage_MemReq) isGossipMessage_Content() {}
+
+func (*GossipMessage_MemRes) isGossipMessage_Content() {}

-func (*GossipMessage_MemReq) IsGossipMessage_Content() {}
+func (*GossipMessage_DataMsg) isGossipMessage_Content() {}

-func (*GossipMessage_MemRes) IsGossipMessage_Content() {}
+func (*GossipMessage_Hello) isGossipMessage_Content() {}

-func (*GossipMessage_DataMsg) IsGossipMessage_Content() {}
+func (*GossipMessage_DataDig) isGossipMessage_Content() {}

-func (*GossipMessage_Hello) IsGossipMessage_Content() {}
+func (*GossipMessage_DataReq) isGossipMessage_Content() {}

-func (*GossipMessage_DataDig) IsGossipMessage_Content() {}
+func (*GossipMessage_DataUpdate) isGossipMessage_Content() {}

-func (*GossipMessage_DataReq) IsGossipMessage_Content() {}
+func (*GossipMessage_Empty) isGossipMessage_Content() {}

-func (*GossipMessage_DataUpdate) IsGossipMessage_Content() {}
+func (*GossipMessage_Conn) isGossipMessage_Content() {}

-func (*GossipMessage_Empty) IsGossipMessage_Content() {}
+func (*GossipMessage_StateInfo) isGossipMessage_Content() {}

-func (*GossipMessage_Conn) IsGossipMessage_Content() {}
+func (*GossipMessage_StateSnapshot) isGossipMessage_Content() {}

-func (*GossipMessage_StateInfo) IsGossipMessage_Content() {}
+func (*GossipMessage_StateInfoPullReq) isGossipMessage_Content() {}

-func (*GossipMessage_StateSnapshot) IsGossipMessage_Content() {}
+func (*GossipMessage_StateRequest) isGossipMessage_Content() {}

-func (*GossipMessage_StateInfoPullReq) IsGossipMessage_Content() {}
+func (*GossipMessage_StateResponse) isGossipMessage_Content() {}

-func (*GossipMessage_StateRequest) IsGossipMessage_Content() {}
+func (*GossipMessage_LeadershipMsg) isGossipMessage_Content() {}

-func (*GossipMessage_StateResponse) IsGossipMessage_Content() {}
+func (*GossipMessage_PeerIdentity) isGossipMessage_Content() {}

-func (*GossipMessage_LeadershipMsg) IsGossipMessage_Content() {}
+func (*GossipMessage_Ack) isGossipMessage_Content() {}

-func (*GossipMessage_PeerIdentity) IsGossipMessage_Content() {}
+func (*GossipMessage_PrivateReq) isGossipMessage_Content() {}

-func (*GossipMessage_Ack) IsGossipMessage_Content() {}
+func (*GossipMessage_PrivateRes) isGossipMessage_Content() {}

-func (*GossipMessage_PrivateReq) IsGossipMessage_Content() {}
+func (*GossipMessage_PrivateData) isGossipMessage_Content() {}

-func (*GossipMessage_PrivateRes) IsGossipMessage_Content() {}
+func (*GossipMessage_CollDataReq) isGossipMessage_Content() {}

-func (*GossipMessage_PrivateData) IsGossipMessage_Content() {}
+func (*GossipMessage_CollDataRes) isGossipMessage_Content() {}

-func (m *GossipMessage) GetContent() IsGossipMessage_Content {
+func (m *GossipMessage) GetContent() isGossipMessage_Content {
 	if m != nil {
 		return m.Content
 	}
@@ -674,6 +689,20 @@ func (m *GossipMessage) GetPrivateData() *PrivateDataMessage {
 	return nil
 }

+func (m *GossipMessage) GetCollDataReq() *RemoteCollDataRequest {
+	if x, ok := m.GetContent().(*GossipMessage_CollDataReq); ok {
+		return x.CollDataReq
+	}
+	return nil
+}
+
+func (m *GossipMessage) GetCollDataRes() *RemoteCollDataResponse {
+	if x, ok := m.GetContent().(*GossipMessage_CollDataRes); ok {
+		return x.CollDataRes
+	}
+	return nil
+}
+
 // XXX_OneofFuncs is for the internal use of the proto package.
 func (*GossipMessage) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer) error, func(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error), func(msg proto.Message) (n int), []interface{}) {
 	return _GossipMessage_OneofMarshaler, _GossipMessage_OneofUnmarshaler, _GossipMessage_OneofSizer, []interface{}{
@@ -698,6 +727,8 @@ func (*GossipMessage) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer)
 		(*GossipMessage_PrivateReq)(nil),
 		(*GossipMessage_PrivateRes)(nil),
 		(*GossipMessage_PrivateData)(nil),
+		(*GossipMessage_CollDataReq)(nil),
+		(*GossipMessage_CollDataRes)(nil),
 	}
 }

@@ -810,6 +841,16 @@ func _GossipMessage_OneofMarshaler(msg proto.Message, b *proto.Buffer) error {
 		if err := b.EncodeMessage(x.PrivateData); err != nil {
 			return err
 		}
+	case *GossipMessage_CollDataReq:
+		b.EncodeVarint(90<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.CollDataReq); err != nil {
+			return err
+		}
+	case *GossipMessage_CollDataRes:
+		b.EncodeVarint(91<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.CollDataRes); err != nil {
+			return err
+		}
 	case nil:
 	default:
 		return fmt.Errorf("GossipMessage.Content has unexpected type %T", x)
@@ -988,6 +1029,22 @@ func _GossipMessage_OneofUnmarshaler(msg proto.Message, tag, wire int, b *proto.
 		err := b.DecodeMessage(msg)
 		m.Content = &GossipMessage_PrivateData{msg}
 		return true, err
+	case 90: // content.collDataReq
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(RemoteCollDataRequest)
+		err := b.DecodeMessage(msg)
+		m.Content = &GossipMessage_CollDataReq{msg}
+		return true, err
+	case 91: // content.collDataRes
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(RemoteCollDataResponse)
+		err := b.DecodeMessage(msg)
+		m.Content = &GossipMessage_CollDataRes{msg}
+		return true, err
 	default:
 		return false, nil
 	}
@@ -1102,6 +1159,16 @@ func _GossipMessage_OneofSizer(msg proto.Message) (n int) {
 		n += 2 // tag and wire
 		n += proto.SizeVarint(uint64(s))
 		n += s
+	case *GossipMessage_CollDataReq:
+		s := proto.Size(x.CollDataReq)
+		n += 2 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
+	case *GossipMessage_CollDataRes:
+		s := proto.Size(x.CollDataRes)
+		n += 2 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
 	case nil:
 	default:
 		panic(fmt.Sprintf("proto: unexpected type %T in oneof", x))
@@ -1128,7 +1195,7 @@ func (m *StateInfo) Reset()         { *m = StateInfo{} }
 func (m *StateInfo) String() string { return proto.CompactTextString(m) }
 func (*StateInfo) ProtoMessage()    {}
 func (*StateInfo) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{4}
+	return fileDescriptor_message_563b77f4f682605d, []int{4}
 }
 func (m *StateInfo) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StateInfo.Unmarshal(m, b)
@@ -1180,6 +1247,7 @@ type Properties struct {
 	LedgerHeight         uint64       `protobuf:"varint,1,opt,name=ledger_height,json=ledgerHeight,proto3" json:"ledger_height,omitempty"`
 	LeftChannel          bool         `protobuf:"varint,2,opt,name=left_channel,json=leftChannel,proto3" json:"left_channel,omitempty"`
 	Chaincodes           []*Chaincode `protobuf:"bytes,3,rep,name=chaincodes,proto3" json:"chaincodes,omitempty"`
+	Roles                []string     `protobuf:"bytes,99,rep,name=roles,proto3" json:"roles,omitempty"`
 	XXX_NoUnkeyedLiteral struct{}     `json:"-"`
 	XXX_unrecognized     []byte       `json:"-"`
 	XXX_sizecache        int32        `json:"-"`
@@ -1189,7 +1257,7 @@ func (m *Properties) Reset()         { *m = Properties{} }
 func (m *Properties) String() string { return proto.CompactTextString(m) }
 func (*Properties) ProtoMessage()    {}
 func (*Properties) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{5}
+	return fileDescriptor_message_563b77f4f682605d, []int{5}
 }
 func (m *Properties) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Properties.Unmarshal(m, b)
@@ -1230,6 +1298,13 @@ func (m *Properties) GetChaincodes() []*Chaincode {
 	return nil
 }

+func (m *Properties) GetRoles() []string {
+	if m != nil {
+		return m.Roles
+	}
+	return nil
+}
+
 // StateInfoSnapshot is an aggregation of StateInfo messages
 type StateInfoSnapshot struct {
 	Elements             []*Envelope `protobuf:"bytes,1,rep,name=elements,proto3" json:"elements,omitempty"`
@@ -1242,7 +1317,7 @@ func (m *StateInfoSnapshot) Reset()         { *m = StateInfoSnapshot{} }
 func (m *StateInfoSnapshot) String() string { return proto.CompactTextString(m) }
 func (*StateInfoSnapshot) ProtoMessage()    {}
 func (*StateInfoSnapshot) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{6}
+	return fileDescriptor_message_563b77f4f682605d, []int{6}
 }
 func (m *StateInfoSnapshot) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StateInfoSnapshot.Unmarshal(m, b)
@@ -1285,7 +1360,7 @@ func (m *StateInfoPullRequest) Reset()         { *m = StateInfoPullRequest{} }
 func (m *StateInfoPullRequest) String() string { return proto.CompactTextString(m) }
 func (*StateInfoPullRequest) ProtoMessage()    {}
 func (*StateInfoPullRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{7}
+	return fileDescriptor_message_563b77f4f682605d, []int{7}
 }
 func (m *StateInfoPullRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StateInfoPullRequest.Unmarshal(m, b)
@@ -1328,7 +1403,7 @@ func (m *ConnEstablish) Reset()         { *m = ConnEstablish{} }
 func (m *ConnEstablish) String() string { return proto.CompactTextString(m) }
 func (*ConnEstablish) ProtoMessage()    {}
 func (*ConnEstablish) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{8}
+	return fileDescriptor_message_563b77f4f682605d, []int{8}
 }
 func (m *ConnEstablish) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_ConnEstablish.Unmarshal(m, b)
@@ -1385,7 +1460,7 @@ func (m *PeerIdentity) Reset()         { *m = PeerIdentity{} }
 func (m *PeerIdentity) String() string { return proto.CompactTextString(m) }
 func (*PeerIdentity) ProtoMessage()    {}
 func (*PeerIdentity) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{9}
+	return fileDescriptor_message_563b77f4f682605d, []int{9}
 }
 func (m *PeerIdentity) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PeerIdentity.Unmarshal(m, b)
@@ -1441,7 +1516,7 @@ func (m *DataRequest) Reset()         { *m = DataRequest{} }
 func (m *DataRequest) String() string { return proto.CompactTextString(m) }
 func (*DataRequest) ProtoMessage()    {}
 func (*DataRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{10}
+	return fileDescriptor_message_563b77f4f682605d, []int{10}
 }
 func (m *DataRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataRequest.Unmarshal(m, b)
@@ -1497,7 +1572,7 @@ func (m *GossipHello) Reset()         { *m = GossipHello{} }
 func (m *GossipHello) String() string { return proto.CompactTextString(m) }
 func (*GossipHello) ProtoMessage()    {}
 func (*GossipHello) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{11}
+	return fileDescriptor_message_563b77f4f682605d, []int{11}
 }
 func (m *GossipHello) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_GossipHello.Unmarshal(m, b)
@@ -1553,7 +1628,7 @@ func (m *DataUpdate) Reset()         { *m = DataUpdate{} }
 func (m *DataUpdate) String() string { return proto.CompactTextString(m) }
 func (*DataUpdate) ProtoMessage()    {}
 func (*DataUpdate) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{12}
+	return fileDescriptor_message_563b77f4f682605d, []int{12}
 }
 func (m *DataUpdate) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataUpdate.Unmarshal(m, b)
@@ -1609,7 +1684,7 @@ func (m *DataDigest) Reset()         { *m = DataDigest{} }
 func (m *DataDigest) String() string { return proto.CompactTextString(m) }
 func (*DataDigest) ProtoMessage()    {}
 func (*DataDigest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{13}
+	return fileDescriptor_message_563b77f4f682605d, []int{13}
 }
 func (m *DataDigest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataDigest.Unmarshal(m, b)
@@ -1662,7 +1737,7 @@ func (m *DataMessage) Reset()         { *m = DataMessage{} }
 func (m *DataMessage) String() string { return proto.CompactTextString(m) }
 func (*DataMessage) ProtoMessage()    {}
 func (*DataMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{14}
+	return fileDescriptor_message_563b77f4f682605d, []int{14}
 }
 func (m *DataMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataMessage.Unmarshal(m, b)
@@ -1703,7 +1778,7 @@ func (m *PrivateDataMessage) Reset()         { *m = PrivateDataMessage{} }
 func (m *PrivateDataMessage) String() string { return proto.CompactTextString(m) }
 func (*PrivateDataMessage) ProtoMessage()    {}
 func (*PrivateDataMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{15}
+	return fileDescriptor_message_563b77f4f682605d, []int{15}
 }
 func (m *PrivateDataMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PrivateDataMessage.Unmarshal(m, b)
@@ -1744,7 +1819,7 @@ func (m *Payload) Reset()         { *m = Payload{} }
 func (m *Payload) String() string { return proto.CompactTextString(m) }
 func (*Payload) ProtoMessage()    {}
 func (*Payload) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{16}
+	return fileDescriptor_message_563b77f4f682605d, []int{16}
 }
 func (m *Payload) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Payload.Unmarshal(m, b)
@@ -1804,7 +1879,7 @@ func (m *PrivatePayload) Reset()         { *m = PrivatePayload{} }
 func (m *PrivatePayload) String() string { return proto.CompactTextString(m) }
 func (*PrivatePayload) ProtoMessage()    {}
 func (*PrivatePayload) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{17}
+	return fileDescriptor_message_563b77f4f682605d, []int{17}
 }
 func (m *PrivatePayload) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PrivatePayload.Unmarshal(m, b)
@@ -1881,7 +1956,7 @@ func (m *AliveMessage) Reset()         { *m = AliveMessage{} }
 func (m *AliveMessage) String() string { return proto.CompactTextString(m) }
 func (*AliveMessage) ProtoMessage()    {}
 func (*AliveMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{18}
+	return fileDescriptor_message_563b77f4f682605d, []int{18}
 }
 func (m *AliveMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_AliveMessage.Unmarshal(m, b)
@@ -1937,7 +2012,7 @@ func (m *LeadershipMessage) Reset()         { *m = LeadershipMessage{} }
 func (m *LeadershipMessage) String() string { return proto.CompactTextString(m) }
 func (*LeadershipMessage) ProtoMessage()    {}
 func (*LeadershipMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{19}
+	return fileDescriptor_message_563b77f4f682605d, []int{19}
 }
 func (m *LeadershipMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_LeadershipMessage.Unmarshal(m, b)
@@ -1991,7 +2066,7 @@ func (m *PeerTime) Reset()         { *m = PeerTime{} }
 func (m *PeerTime) String() string { return proto.CompactTextString(m) }
 func (*PeerTime) ProtoMessage()    {}
 func (*PeerTime) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{20}
+	return fileDescriptor_message_563b77f4f682605d, []int{20}
 }
 func (m *PeerTime) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PeerTime.Unmarshal(m, b)
@@ -2039,7 +2114,7 @@ func (m *MembershipRequest) Reset()         { *m = MembershipRequest{} }
 func (m *MembershipRequest) String() string { return proto.CompactTextString(m) }
 func (*MembershipRequest) ProtoMessage()    {}
 func (*MembershipRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{21}
+	return fileDescriptor_message_563b77f4f682605d, []int{21}
 }
 func (m *MembershipRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_MembershipRequest.Unmarshal(m, b)
@@ -2086,7 +2161,7 @@ func (m *MembershipResponse) Reset()         { *m = MembershipResponse{} }
 func (m *MembershipResponse) String() string { return proto.CompactTextString(m) }
 func (*MembershipResponse) ProtoMessage()    {}
 func (*MembershipResponse) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{22}
+	return fileDescriptor_message_563b77f4f682605d, []int{22}
 }
 func (m *MembershipResponse) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_MembershipResponse.Unmarshal(m, b)
@@ -2135,7 +2210,7 @@ func (m *Member) Reset()         { *m = Member{} }
 func (m *Member) String() string { return proto.CompactTextString(m) }
 func (*Member) ProtoMessage()    {}
 func (*Member) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{23}
+	return fileDescriptor_message_563b77f4f682605d, []int{23}
 }
 func (m *Member) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Member.Unmarshal(m, b)
@@ -2187,7 +2262,7 @@ func (m *Empty) Reset()         { *m = Empty{} }
 func (m *Empty) String() string { return proto.CompactTextString(m) }
 func (*Empty) ProtoMessage()    {}
 func (*Empty) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{24}
+	return fileDescriptor_message_563b77f4f682605d, []int{24}
 }
 func (m *Empty) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Empty.Unmarshal(m, b)
@@ -2221,7 +2296,7 @@ func (m *RemoteStateRequest) Reset()         { *m = RemoteStateRequest{} }
 func (m *RemoteStateRequest) String() string { return proto.CompactTextString(m) }
 func (*RemoteStateRequest) ProtoMessage()    {}
 func (*RemoteStateRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{25}
+	return fileDescriptor_message_563b77f4f682605d, []int{25}
 }
 func (m *RemoteStateRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemoteStateRequest.Unmarshal(m, b)
@@ -2268,7 +2343,7 @@ func (m *RemoteStateResponse) Reset()         { *m = RemoteStateResponse{} }
 func (m *RemoteStateResponse) String() string { return proto.CompactTextString(m) }
 func (*RemoteStateResponse) ProtoMessage()    {}
 func (*RemoteStateResponse) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{26}
+	return fileDescriptor_message_563b77f4f682605d, []int{26}
 }
 func (m *RemoteStateResponse) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemoteStateResponse.Unmarshal(m, b)
@@ -2308,7 +2383,7 @@ func (m *RemotePvtDataRequest) Reset()         { *m = RemotePvtDataRequest{} }
 func (m *RemotePvtDataRequest) String() string { return proto.CompactTextString(m) }
 func (*RemotePvtDataRequest) ProtoMessage()    {}
 func (*RemotePvtDataRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{27}
+	return fileDescriptor_message_563b77f4f682605d, []int{27}
 }
 func (m *RemotePvtDataRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemotePvtDataRequest.Unmarshal(m, b)
@@ -2351,7 +2426,7 @@ func (m *PvtDataDigest) Reset()         { *m = PvtDataDigest{} }
 func (m *PvtDataDigest) String() string { return proto.CompactTextString(m) }
 func (*PvtDataDigest) ProtoMessage()    {}
 func (*PvtDataDigest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{28}
+	return fileDescriptor_message_563b77f4f682605d, []int{28}
 }
 func (m *PvtDataDigest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PvtDataDigest.Unmarshal(m, b)
@@ -2419,7 +2494,7 @@ func (m *RemotePvtDataResponse) Reset()         { *m = RemotePvtDataResponse{} }
 func (m *RemotePvtDataResponse) String() string { return proto.CompactTextString(m) }
 func (*RemotePvtDataResponse) ProtoMessage()    {}
 func (*RemotePvtDataResponse) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{29}
+	return fileDescriptor_message_563b77f4f682605d, []int{29}
 }
 func (m *RemotePvtDataResponse) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemotePvtDataResponse.Unmarshal(m, b)
@@ -2459,7 +2534,7 @@ func (m *PvtDataElement) Reset()         { *m = PvtDataElement{} }
 func (m *PvtDataElement) String() string { return proto.CompactTextString(m) }
 func (*PvtDataElement) ProtoMessage()    {}
 func (*PvtDataElement) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{30}
+	return fileDescriptor_message_563b77f4f682605d, []int{30}
 }
 func (m *PvtDataElement) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PvtDataElement.Unmarshal(m, b)
@@ -2509,7 +2584,7 @@ func (m *PvtDataPayload) Reset()         { *m = PvtDataPayload{} }
 func (m *PvtDataPayload) String() string { return proto.CompactTextString(m) }
 func (*PvtDataPayload) ProtoMessage()    {}
 func (*PvtDataPayload) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{31}
+	return fileDescriptor_message_563b77f4f682605d, []int{31}
 }
 func (m *PvtDataPayload) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PvtDataPayload.Unmarshal(m, b)
@@ -2554,7 +2629,7 @@ func (m *Acknowledgement) Reset()         { *m = Acknowledgement{} }
 func (m *Acknowledgement) String() string { return proto.CompactTextString(m) }
 func (*Acknowledgement) ProtoMessage()    {}
 func (*Acknowledgement) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{32}
+	return fileDescriptor_message_563b77f4f682605d, []int{32}
 }
 func (m *Acknowledgement) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Acknowledgement.Unmarshal(m, b)
@@ -2596,7 +2671,7 @@ func (m *Chaincode) Reset()         { *m = Chaincode{} }
 func (m *Chaincode) String() string { return proto.CompactTextString(m) }
 func (*Chaincode) ProtoMessage()    {}
 func (*Chaincode) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{33}
+	return fileDescriptor_message_563b77f4f682605d, []int{33}
 }
 func (m *Chaincode) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Chaincode.Unmarshal(m, b)
@@ -2637,6 +2712,283 @@ func (m *Chaincode) GetMetadata() []byte {
 	return nil
 }

+// ValidationResultsMessage is the message containing block validation results
+type ValidationResultsMessage struct {
+	SeqNum               uint64   `protobuf:"varint,1,opt,name=seq_num,json=seqNum,proto3" json:"seq_num,omitempty"`
+	TxFlags              []byte   `protobuf:"bytes,2,opt,name=txFlags,proto3" json:"txFlags,omitempty"`
+	Signature            []byte   `protobuf:"bytes,3,opt,name=signature,proto3" json:"signature,omitempty"`
+	Identity             []byte   `protobuf:"bytes,4,opt,name=identity,proto3" json:"identity,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *ValidationResultsMessage) Reset()         { *m = ValidationResultsMessage{} }
+func (m *ValidationResultsMessage) String() string { return proto.CompactTextString(m) }
+func (*ValidationResultsMessage) ProtoMessage()    {}
+func (*ValidationResultsMessage) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_563b77f4f682605d, []int{34}
+}
+func (m *ValidationResultsMessage) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_ValidationResultsMessage.Unmarshal(m, b)
+}
+func (m *ValidationResultsMessage) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_ValidationResultsMessage.Marshal(b, m, deterministic)
+}
+func (dst *ValidationResultsMessage) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_ValidationResultsMessage.Merge(dst, src)
+}
+func (m *ValidationResultsMessage) XXX_Size() int {
+	return xxx_messageInfo_ValidationResultsMessage.Size(m)
+}
+func (m *ValidationResultsMessage) XXX_DiscardUnknown() {
+	xxx_messageInfo_ValidationResultsMessage.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_ValidationResultsMessage proto.InternalMessageInfo
+
+func (m *ValidationResultsMessage) GetSeqNum() uint64 {
+	if m != nil {
+		return m.SeqNum
+	}
+	return 0
+}
+
+func (m *ValidationResultsMessage) GetTxFlags() []byte {
+	if m != nil {
+		return m.TxFlags
+	}
+	return nil
+}
+
+func (m *ValidationResultsMessage) GetSignature() []byte {
+	if m != nil {
+		return m.Signature
+	}
+	return nil
+}
+
+func (m *ValidationResultsMessage) GetIdentity() []byte {
+	if m != nil {
+		return m.Identity
+	}
+	return nil
+}
+
+// RemoteCollDataRequest message used to request
+// collection data
+type RemoteCollDataRequest struct {
+	Nonce                uint64            `protobuf:"varint,1,opt,name=nonce,proto3" json:"nonce,omitempty"`
+	Digests              []*CollDataDigest `protobuf:"bytes,2,rep,name=digests,proto3" json:"digests,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}          `json:"-"`
+	XXX_unrecognized     []byte            `json:"-"`
+	XXX_sizecache        int32             `json:"-"`
+}
+
+func (m *RemoteCollDataRequest) Reset()         { *m = RemoteCollDataRequest{} }
+func (m *RemoteCollDataRequest) String() string { return proto.CompactTextString(m) }
+func (*RemoteCollDataRequest) ProtoMessage()    {}
+func (*RemoteCollDataRequest) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_563b77f4f682605d, []int{35}
+}
+func (m *RemoteCollDataRequest) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_RemoteCollDataRequest.Unmarshal(m, b)
+}
+func (m *RemoteCollDataRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_RemoteCollDataRequest.Marshal(b, m, deterministic)
+}
+func (dst *RemoteCollDataRequest) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_RemoteCollDataRequest.Merge(dst, src)
+}
+func (m *RemoteCollDataRequest) XXX_Size() int {
+	return xxx_messageInfo_RemoteCollDataRequest.Size(m)
+}
+func (m *RemoteCollDataRequest) XXX_DiscardUnknown() {
+	xxx_messageInfo_RemoteCollDataRequest.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_RemoteCollDataRequest proto.InternalMessageInfo
+
+func (m *RemoteCollDataRequest) GetNonce() uint64 {
+	if m != nil {
+		return m.Nonce
+	}
+	return 0
+}
+
+func (m *RemoteCollDataRequest) GetDigests() []*CollDataDigest {
+	if m != nil {
+		return m.Digests
+	}
+	return nil
+}
+
+// CollDataDigest defines a digest of collection data
+type CollDataDigest struct {
+	Namespace            string   `protobuf:"bytes,1,opt,name=namespace,proto3" json:"namespace,omitempty"`
+	Collection           string   `protobuf:"bytes,2,opt,name=collection,proto3" json:"collection,omitempty"`
+	Key                  string   `protobuf:"bytes,3,opt,name=key,proto3" json:"key,omitempty"`
+	EndorsedAtTxID       string   `protobuf:"bytes,4,opt,name=endorsedAtTxID,proto3" json:"endorsedAtTxID,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *CollDataDigest) Reset()         { *m = CollDataDigest{} }
+func (m *CollDataDigest) String() string { return proto.CompactTextString(m) }
+func (*CollDataDigest) ProtoMessage()    {}
+func (*CollDataDigest) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_563b77f4f682605d, []int{36}
+}
+func (m *CollDataDigest) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_CollDataDigest.Unmarshal(m, b)
+}
+func (m *CollDataDigest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_CollDataDigest.Marshal(b, m, deterministic)
+}
+func (dst *CollDataDigest) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_CollDataDigest.Merge(dst, src)
+}
+func (m *CollDataDigest) XXX_Size() int {
+	return xxx_messageInfo_CollDataDigest.Size(m)
+}
+func (m *CollDataDigest) XXX_DiscardUnknown() {
+	xxx_messageInfo_CollDataDigest.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_CollDataDigest proto.InternalMessageInfo
+
+func (m *CollDataDigest) GetNamespace() string {
+	if m != nil {
+		return m.Namespace
+	}
+	return ""
+}
+
+func (m *CollDataDigest) GetCollection() string {
+	if m != nil {
+		return m.Collection
+	}
+	return ""
+}
+
+func (m *CollDataDigest) GetKey() string {
+	if m != nil {
+		return m.Key
+	}
+	return ""
+}
+
+func (m *CollDataDigest) GetEndorsedAtTxID() string {
+	if m != nil {
+		return m.EndorsedAtTxID
+	}
+	return ""
+}
+
+// RemoteCollDataResponse message used to respond to
+// collection data request
+type RemoteCollDataResponse struct {
+	Nonce                uint64             `protobuf:"varint,1,opt,name=nonce,proto3" json:"nonce,omitempty"`
+	Elements             []*CollDataElement `protobuf:"bytes,2,rep,name=elements,proto3" json:"elements,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
+	XXX_unrecognized     []byte             `json:"-"`
+	XXX_sizecache        int32              `json:"-"`
+}
+
+func (m *RemoteCollDataResponse) Reset()         { *m = RemoteCollDataResponse{} }
+func (m *RemoteCollDataResponse) String() string { return proto.CompactTextString(m) }
+func (*RemoteCollDataResponse) ProtoMessage()    {}
+func (*RemoteCollDataResponse) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_563b77f4f682605d, []int{37}
+}
+func (m *RemoteCollDataResponse) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_RemoteCollDataResponse.Unmarshal(m, b)
+}
+func (m *RemoteCollDataResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_RemoteCollDataResponse.Marshal(b, m, deterministic)
+}
+func (dst *RemoteCollDataResponse) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_RemoteCollDataResponse.Merge(dst, src)
+}
+func (m *RemoteCollDataResponse) XXX_Size() int {
+	return xxx_messageInfo_RemoteCollDataResponse.Size(m)
+}
+func (m *RemoteCollDataResponse) XXX_DiscardUnknown() {
+	xxx_messageInfo_RemoteCollDataResponse.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_RemoteCollDataResponse proto.InternalMessageInfo
+
+func (m *RemoteCollDataResponse) GetNonce() uint64 {
+	if m != nil {
+		return m.Nonce
+	}
+	return 0
+}
+
+func (m *RemoteCollDataResponse) GetElements() []*CollDataElement {
+	if m != nil {
+		return m.Elements
+	}
+	return nil
+}
+
+// CollDataElement contains the collection data digest and value
+type CollDataElement struct {
+	Digest               *CollDataDigest      `protobuf:"bytes,1,opt,name=digest,proto3" json:"digest,omitempty"`
+	Value                []byte               `protobuf:"bytes,2,opt,name=value,proto3" json:"value,omitempty"`
+	ExpiryTime           *timestamp.Timestamp `protobuf:"bytes,3,opt,name=expiryTime,proto3" json:"expiryTime,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
+	XXX_unrecognized     []byte               `json:"-"`
+	XXX_sizecache        int32                `json:"-"`
+}
+
+func (m *CollDataElement) Reset()         { *m = CollDataElement{} }
+func (m *CollDataElement) String() string { return proto.CompactTextString(m) }
+func (*CollDataElement) ProtoMessage()    {}
+func (*CollDataElement) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_563b77f4f682605d, []int{38}
+}
+func (m *CollDataElement) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_CollDataElement.Unmarshal(m, b)
+}
+func (m *CollDataElement) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_CollDataElement.Marshal(b, m, deterministic)
+}
+func (dst *CollDataElement) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_CollDataElement.Merge(dst, src)
+}
+func (m *CollDataElement) XXX_Size() int {
+	return xxx_messageInfo_CollDataElement.Size(m)
+}
+func (m *CollDataElement) XXX_DiscardUnknown() {
+	xxx_messageInfo_CollDataElement.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_CollDataElement proto.InternalMessageInfo
+
+func (m *CollDataElement) GetDigest() *CollDataDigest {
+	if m != nil {
+		return m.Digest
+	}
+	return nil
+}
+
+func (m *CollDataElement) GetValue() []byte {
+	if m != nil {
+		return m.Value
+	}
+	return nil
+}
+
+func (m *CollDataElement) GetExpiryTime() *timestamp.Timestamp {
+	if m != nil {
+		return m.ExpiryTime
+	}
+	return nil
+}
+
 func init() {
 	proto.RegisterType((*Envelope)(nil), "gossip.Envelope")
 	proto.RegisterType((*SecretEnvelope)(nil), "gossip.SecretEnvelope")
@@ -2672,6 +3024,11 @@ func init() {
 	proto.RegisterType((*PvtDataPayload)(nil), "gossip.PvtDataPayload")
 	proto.RegisterType((*Acknowledgement)(nil), "gossip.Acknowledgement")
 	proto.RegisterType((*Chaincode)(nil), "gossip.Chaincode")
+	proto.RegisterType((*ValidationResultsMessage)(nil), "gossip.ValidationResultsMessage")
+	proto.RegisterType((*RemoteCollDataRequest)(nil), "gossip.RemoteCollDataRequest")
+	proto.RegisterType((*CollDataDigest)(nil), "gossip.CollDataDigest")
+	proto.RegisterType((*RemoteCollDataResponse)(nil), "gossip.RemoteCollDataResponse")
+	proto.RegisterType((*CollDataElement)(nil), "gossip.CollDataElement")
 	proto.RegisterEnum("gossip.PullMsgType", PullMsgType_name, PullMsgType_value)
 	proto.RegisterEnum("gossip.GossipMessage_Tag", GossipMessage_Tag_name, GossipMessage_Tag_value)
 }
@@ -2818,126 +3175,141 @@ var _Gossip_serviceDesc = grpc.ServiceDesc{
 	Metadata: "gossip/message.proto",
 }

-func init() { proto.RegisterFile("gossip/message.proto", fileDescriptor_message_7c42328ef5ef9997) }
-
-var fileDescriptor_message_7c42328ef5ef9997 = []byte{
-	// 1874 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x58, 0x5b, 0x6f, 0xe3, 0xc6,
-	0x15, 0x16, 0x6d, 0x5d, 0x8f, 0x2e, 0x96, 0xc7, 0xde, 0x5d, 0xc6, 0x49, 0x13, 0x87, 0xed, 0x26,
-	0xdb, 0x7a, 0x23, 0x6f, 0x9d, 0x16, 0x0d, 0x90, 0xb6, 0x0b, 0x5b, 0x52, 0x2c, 0x21, 0x2b, 0xad,
-	0x4b, 0x7b, 0xd1, 0xba, 0x2f, 0xc4, 0x98, 0x1c, 0x53, 0xac, 0xc9, 0x21, 0xcd, 0x19, 0x3b, 0xf6,
-	0x63, 0xd1, 0x87, 0x00, 0x7d, 0xe9, 0x6f, 0xe8, 0x53, 0xff, 0x66, 0x31, 0x33, 0xbc, 0x4a, 0xf6,
-	0x02, 0x1b, 0x20, 0x6f, 0x3c, 0xf7, 0x99, 0x33, 0x67, 0xbe, 0x73, 0x86, 0xb0, 0xed, 0x86, 0x8c,
-	0x79, 0xd1, 0x7e, 0x40, 0x18, 0xc3, 0x2e, 0x19, 0x44, 0x71, 0xc8, 0x43, 0x54, 0x57, 0xdc, 0x9d,
-	0x67, 0x76, 0x18, 0x04, 0x21, 0xdd, 0xb7, 0x43, 0xdf, 0x27, 0x36, 0xf7, 0x42, 0xaa, 0x14, 0x8c,
-	0x7f, 0x69, 0xd0, 0x1c, 0xd3, 0x5b, 0xe2, 0x87, 0x11, 0x41, 0x3a, 0x34, 0x22, 0x7c, 0xef, 0x87,
-	0xd8, 0xd1, 0xb5, 0x5d, 0xed, 0x45, 0xc7, 0x4c, 0x49, 0xf4, 0x09, 0xb4, 0x98, 0xe7, 0x52, 0xcc,
-	0x6f, 0x62, 0xa2, 0xaf, 0x49, 0x59, 0xce, 0x40, 0xaf, 0x61, 0x83, 0x11, 0x3b, 0x26, 0xdc, 0x22,
-	0x89, 0x2b, 0x7d, 0x7d, 0x57, 0x7b, 0xd1, 0x3e, 0x78, 0x3a, 0x50, 0xf1, 0x07, 0xa7, 0x52, 0x9c,
-	0x06, 0x32, 0x7b, 0xac, 0x44, 0x1b, 0x13, 0xe8, 0x95, 0x35, 0x7e, 0xea, 0x52, 0x8c, 0x43, 0xa8,
-	0x2b, 0x4f, 0xe8, 0x25, 0xf4, 0x3d, 0xca, 0x49, 0x4c, 0xb1, 0x3f, 0xa6, 0x4e, 0x14, 0x7a, 0x94,
-	0x4b, 0x57, 0xad, 0x49, 0xc5, 0x5c, 0x91, 0x1c, 0xb5, 0xa0, 0x61, 0x87, 0x94, 0x13, 0xca, 0x8d,
-	0x1f, 0xdb, 0xd0, 0x3d, 0x96, 0xcb, 0x9e, 0xa9, 0x5c, 0xa2, 0x6d, 0xa8, 0xd1, 0x90, 0xda, 0x44,
-	0xda, 0x57, 0x4d, 0x45, 0x88, 0x25, 0xda, 0x0b, 0x4c, 0x29, 0xf1, 0x93, 0x65, 0xa4, 0x24, 0xda,
-	0x83, 0x75, 0x8e, 0x5d, 0x99, 0x83, 0xde, 0xc1, 0x47, 0x69, 0x0e, 0x4a, 0x3e, 0x07, 0x67, 0xd8,
-	0x35, 0x85, 0x16, 0xfa, 0x1a, 0x5a, 0xd8, 0xf7, 0x6e, 0x89, 0x15, 0x30, 0x57, 0xaf, 0xc9, 0xb4,
-	0x6d, 0xa7, 0x26, 0x87, 0x42, 0x90, 0x58, 0x4c, 0x2a, 0x66, 0x53, 0x2a, 0xce, 0x98, 0x8b, 0x7e,
-	0x07, 0x8d, 0x80, 0x04, 0x56, 0x4c, 0xae, 0xf5, 0xba, 0x34, 0xc9, 0xa2, 0xcc, 0x48, 0x70, 0x41,
-	0x62, 0xb6, 0xf0, 0x22, 0x93, 0x5c, 0xdf, 0x10, 0xc6, 0x27, 0x15, 0xb3, 0x1e, 0x90, 0xc0, 0x24,
-	0xd7, 0xe8, 0xf7, 0xa9, 0x15, 0xd3, 0x1b, 0xd2, 0x6a, 0xe7, 0x21, 0x2b, 0x16, 0x85, 0x94, 0x91,
-	0xcc, 0x8c, 0xa1, 0x57, 0xd0, 0x74, 0x30, 0xc7, 0x72, 0x81, 0x4d, 0x69, 0xb7, 0x95, 0xda, 0x8d,
-	0x30, 0xc7, 0xf9, 0xfa, 0x1a, 0x42, 0x4d, 0x2c, 0x6f, 0x0f, 0x6a, 0x0b, 0xe2, 0xfb, 0xa1, 0xde,
-	0x2a, 0xab, 0xab, 0x14, 0x4c, 0x84, 0x68, 0x52, 0x31, 0x95, 0x0e, 0xda, 0x4f, 0xdc, 0x3b, 0x9e,
-	0xab, 0x83, 0xd4, 0x47, 0x45, 0xf7, 0x23, 0xcf, 0x55, 0xbb, 0x90, 0xde, 0x47, 0x9e, 0x9b, 0xad,
-	0x47, 0xec, 0xbe, 0xbd, 0xba, 0x9e, 0x7c, 0xdf, 0xd2, 0x42, 0x6d, 0xbc, 0x2d, 0x2d, 0x6e, 0x22,
-	0x07, 0x73, 0xa2, 0x77, 0x56, 0xa3, 0xbc, 0x93, 0x92, 0x49, 0xc5, 0x04, 0x27, 0xa3, 0xd0, 0x73,
-	0xa8, 0x91, 0x20, 0xe2, 0xf7, 0x7a, 0x57, 0x1a, 0x74, 0x53, 0x83, 0xb1, 0x60, 0x8a, 0x0d, 0x48,
-	0x29, 0xda, 0x83, 0xaa, 0x1d, 0x52, 0xaa, 0xf7, 0xa4, 0xd6, 0x93, 0x54, 0x6b, 0x18, 0x52, 0x3a,
-	0x66, 0x1c, 0x5f, 0xf8, 0x1e, 0x5b, 0x4c, 0x2a, 0xa6, 0x54, 0x42, 0x07, 0x00, 0x8c, 0x63, 0x4e,
-	0x2c, 0x8f, 0x5e, 0x86, 0xfa, 0x86, 0x34, 0xd9, 0xcc, 0xae, 0x89, 0x90, 0x4c, 0xe9, 0xa5, 0xc8,
-	0x4e, 0x8b, 0xa5, 0x04, 0x3a, 0x82, 0x9e, 0xb2, 0x61, 0x14, 0x47, 0x6c, 0x11, 0x72, 0xbd, 0x5f,
-	0x3e, 0xf4, 0xcc, 0xee, 0x34, 0x51, 0x98, 0x54, 0xcc, 0xae, 0x34, 0x49, 0x19, 0x68, 0x06, 0x5b,
-	0x79, 0x5c, 0x2b, 0xba, 0xf1, 0x7d, 0x99, 0xbf, 0x4d, 0xe9, 0xe8, 0x93, 0x15, 0x47, 0x27, 0x37,
-	0xbe, 0x9f, 0x27, 0xb2, 0xcf, 0x96, 0xf8, 0xe8, 0x10, 0x94, 0x7f, 0xe1, 0x44, 0x28, 0xe9, 0xa8,
-	0x5c, 0x50, 0x26, 0x09, 0x42, 0x4e, 0xa4, 0xbb, 0xdc, 0x4d, 0x87, 0x15, 0x68, 0x34, 0x4a, 0x77,
-	0x15, 0x27, 0x25, 0xa7, 0x6f, 0x49, 0x1f, 0x1f, 0x3f, 0xe8, 0x23, 0xab, 0xca, 0x2e, 0x2b, 0x32,
-	0x44, 0x6e, 0x7c, 0x82, 0x1d, 0x55, 0xbc, 0xb2, 0x44, 0xb7, 0xcb, 0xb9, 0x79, 0x93, 0x49, 0xf3,
-	0x42, 0xed, 0xe6, 0x26, 0xa2, 0x5c, 0xbf, 0x85, 0x6e, 0x44, 0x48, 0x6c, 0x79, 0x0e, 0xa1, 0xdc,
-	0xe3, 0xf7, 0xfa, 0x93, 0xf2, 0x35, 0x3c, 0x21, 0x24, 0x9e, 0x26, 0x32, 0xb1, 0x8d, 0xa8, 0x40,
-	0x8b, 0xcb, 0x8e, 0xed, 0x2b, 0xfd, 0xa9, 0x34, 0x79, 0x96, 0xdd, 0x5c, 0xfb, 0x8a, 0x86, 0x3f,
-	0xf8, 0xc4, 0x71, 0x49, 0x40, 0xa8, 0xd8, 0xbc, 0xd0, 0x42, 0x7f, 0x06, 0x88, 0x62, 0xef, 0x56,
-	0x65, 0x41, 0x7f, 0x56, 0x4e, 0xbe, 0xda, 0xef, 0xc9, 0x2d, 0x2f, 0x57, 0x71, 0xc1, 0x02, 0xbd,
-	0x2e, 0xd8, 0x33, 0x5d, 0x97, 0xf6, 0xbf, 0x78, 0xc4, 0x3e, 0xcb, 0x58, 0xc1, 0x04, 0xbd, 0x86,
-	0x4e, 0x42, 0x59, 0xa2, 0xd0, 0xf5, 0x8f, 0xca, 0xc7, 0x76, 0xa2, 0x64, 0xe5, 0x6b, 0xdd, 0x8e,
-	0x72, 0xae, 0x61, 0xc1, 0xfa, 0x19, 0x76, 0x51, 0x17, 0x5a, 0xef, 0xe6, 0xa3, 0xf1, 0x77, 0xd3,
-	0xf9, 0x78, 0xd4, 0xaf, 0xa0, 0x16, 0xd4, 0xc6, 0xb3, 0x93, 0xb3, 0xf3, 0xbe, 0x86, 0x3a, 0xd0,
-	0x7c, 0x6b, 0x1e, 0x5b, 0x6f, 0xe7, 0x6f, 0xce, 0xfb, 0x6b, 0x42, 0x6f, 0x38, 0x39, 0x9c, 0x2b,
-	0x72, 0x1d, 0xf5, 0xa1, 0x23, 0xc9, 0xc3, 0xf9, 0xc8, 0x7a, 0x6b, 0x1e, 0xf7, 0xab, 0x68, 0x03,
-	0xda, 0x4a, 0xc1, 0x94, 0x8c, 0x5a, 0x11, 0x89, 0xff, 0xa7, 0x41, 0x2b, 0xab, 0x48, 0x34, 0x80,
-	0x16, 0xf7, 0x02, 0xc2, 0x38, 0x0e, 0x22, 0x89, 0xb8, 0xed, 0x83, 0x7e, 0xf1, 0x84, 0xce, 0xbc,
-	0x80, 0x98, 0xb9, 0x0a, 0x7a, 0x02, 0xf5, 0xe8, 0xca, 0xb3, 0x3c, 0x47, 0x02, 0x71, 0xc7, 0xac,
-	0x45, 0x57, 0xde, 0xd4, 0x41, 0x9f, 0x41, 0x3b, 0xc1, 0x69, 0x6b, 0x76, 0x38, 0xd4, 0xab, 0x52,
-	0x06, 0x09, 0x6b, 0x76, 0x38, 0x14, 0x37, 0x34, 0x8a, 0xc3, 0x88, 0xc4, 0xdc, 0x23, 0x2c, 0x41,
-	0x64, 0x94, 0x27, 0x28, 0x95, 0x98, 0x05, 0x2d, 0xe3, 0x47, 0x0d, 0x20, 0x17, 0xa1, 0x5f, 0x42,
-	0x57, 0x1e, 0x7d, 0x6c, 0x2d, 0x88, 0xe7, 0x2e, 0x78, 0xd2, 0x38, 0x3a, 0x8a, 0x39, 0x91, 0x3c,
-	0xf4, 0x39, 0x74, 0x7c, 0x72, 0xc9, 0xad, 0x62, 0x13, 0x69, 0x9a, 0x6d, 0xc1, 0x1b, 0x26, 0x8d,
-	0xe4, 0xb7, 0x20, 0x16, 0xe6, 0x51, 0x3b, 0x74, 0x08, 0xd3, 0xd7, 0x77, 0xd7, 0x8b, 0x60, 0x31,
-	0x4c, 0x25, 0x66, 0x41, 0xc9, 0x38, 0x84, 0xcd, 0x15, 0x34, 0x40, 0x2f, 0xa1, 0x49, 0x7c, 0x59,
-	0x88, 0x4c, 0xd7, 0xa4, 0x97, 0x2c, 0x73, 0x59, 0x4f, 0xce, 0x34, 0x8c, 0x3f, 0xc0, 0xf6, 0x43,
-	0x38, 0xb0, 0x9c, 0x39, 0x6d, 0x39, 0x73, 0xc6, 0x25, 0x74, 0x4b, 0xa0, 0x57, 0x38, 0x02, 0xad,
-	0x78, 0x04, 0x3b, 0xd0, 0xcc, 0xae, 0x9a, 0x6a, 0x9d, 0x19, 0x8d, 0x0c, 0xe8, 0x72, 0x9f, 0x59,
-	0x36, 0x89, 0xb9, 0xb5, 0xc0, 0x6c, 0x91, 0x1c, 0x5e, 0x9b, 0xfb, 0x6c, 0x48, 0x62, 0x3e, 0xc1,
-	0x6c, 0x61, 0xbc, 0x83, 0x4e, 0xf1, 0x4a, 0x3e, 0x16, 0x06, 0x41, 0x55, 0xb8, 0x49, 0x42, 0xc8,
-	0x6f, 0x11, 0x3a, 0x20, 0x1c, 0xcb, 0xda, 0x57, 0x9e, 0x33, 0xda, 0x08, 0xa0, 0x5d, 0xb8, 0x79,
-	0x8f, 0x77, 0x7d, 0x47, 0x76, 0x24, 0xa6, 0xaf, 0xed, 0xae, 0x8b, 0xae, 0x9f, 0x90, 0x68, 0x00,
-	0xcd, 0x80, 0xb9, 0x16, 0xbf, 0x4f, 0xc6, 0x9f, 0x5e, 0xde, 0x96, 0x44, 0x16, 0x67, 0xcc, 0x3d,
-	0xbb, 0x8f, 0x88, 0xd9, 0x08, 0xd4, 0x87, 0x11, 0x42, 0xbb, 0xd0, 0x0f, 0x1f, 0x09, 0x57, 0x5c,
-	0xef, 0x5a, 0x79, 0xbd, 0x1f, 0x1c, 0xf0, 0x0e, 0x20, 0x6f, 0x75, 0x8f, 0xc4, 0xfb, 0x15, 0x54,
-	0x93, 0x58, 0x0f, 0x57, 0x49, 0xf5, 0x27, 0x45, 0xf6, 0x55, 0x64, 0xd5, 0xca, 0x7f, 0xf6, 0xc4,
-	0x7e, 0xa3, 0xce, 0x31, 0x9d, 0xde, 0x7e, 0x5d, 0x1e, 0x25, 0xdb, 0x07, 0x1b, 0x99, 0xb5, 0x62,
-	0x67, 0xb3, 0xa5, 0xf1, 0x1d, 0xa0, 0x55, 0x04, 0x44, 0xaf, 0x96, 0x1d, 0x3c, 0x5d, 0x82, 0xcb,
-	0x15, 0x3f, 0xe7, 0xd0, 0x48, 0x78, 0xe8, 0x19, 0x34, 0x18, 0xb9, 0xb6, 0xe8, 0x4d, 0x90, 0x6c,
-	0xb7, 0xce, 0xc8, 0xf5, 0xfc, 0x26, 0x10, 0xd5, 0x59, 0x38, 0x55, 0x95, 0xd7, 0xcf, 0x97, 0xd0,
-	0x79, 0x5d, 0x26, 0xa2, 0x84, 0xbf, 0xff, 0x59, 0x83, 0x5e, 0x39, 0x2c, 0xfa, 0x12, 0x36, 0xf2,
-	0xb9, 0xde, 0xa2, 0x38, 0x50, 0x99, 0x6d, 0x99, 0xbd, 0x9c, 0x3d, 0xc7, 0x01, 0x11, 0xa3, 0xb3,
-	0x90, 0xb2, 0x08, 0xdb, 0x6a, 0x74, 0x6e, 0x99, 0x39, 0x03, 0x6d, 0x41, 0x8d, 0xdf, 0xa5, 0x70,
-	0xd9, 0x32, 0xab, 0xfc, 0x6e, 0xea, 0x08, 0x24, 0x4b, 0x57, 0x14, 0xff, 0xc0, 0x08, 0x4f, 0xf0,
-	0x32, 0x5d, 0xa6, 0x29, 0x78, 0xe8, 0x25, 0xa0, 0x54, 0x89, 0x79, 0x41, 0x8a, 0x79, 0x35, 0xb9,
-	0xdd, 0x7e, 0x22, 0x39, 0xf5, 0x82, 0x04, 0xf7, 0xe6, 0x80, 0x0a, 0xcb, 0xb5, 0x43, 0x7a, 0xe9,
-	0xb9, 0x2c, 0x19, 0x63, 0x3f, 0x1b, 0xa8, 0x87, 0xca, 0x60, 0x98, 0x69, 0x0c, 0xa5, 0xc2, 0x09,
-	0xb6, 0xaf, 0xb0, 0x4b, 0xcc, 0x4d, 0x7b, 0x49, 0xc0, 0x8c, 0x7f, 0x6b, 0xd0, 0x29, 0x0e, 0xca,
-	0x68, 0x00, 0x10, 0x64, 0xf3, 0x6c, 0x72, 0x64, 0xbd, 0xf2, 0xa4, 0x6b, 0x16, 0x34, 0x3e, 0xb8,
-	0xb1, 0x14, 0xe1, 0xab, 0x5a, 0x86, 0x2f, 0xe3, 0x9f, 0x1a, 0x6c, 0xae, 0x4c, 0x1c, 0x8f, 0x01,
-	0xd4, 0x87, 0x06, 0x7e, 0x0e, 0x3d, 0x8f, 0x59, 0x0e, 0xb1, 0x7d, 0x1c, 0x63, 0x91, 0x02, 0x79,
-	0x54, 0x4d, 0xb3, 0xeb, 0xb1, 0x51, 0xce, 0x34, 0xfe, 0x08, 0xcd, 0xd4, 0x5a, 0x94, 0x9f, 0x47,
-	0xed, 0x62, 0xf9, 0x79, 0xd4, 0x16, 0xe5, 0x57, 0xa8, 0xcb, 0xb5, 0x62, 0x5d, 0x1a, 0x97, 0xb0,
-	0xb9, 0xf2, 0x86, 0x40, 0xdf, 0x42, 0x9f, 0x11, 0xff, 0x52, 0x0e, 0x8f, 0x71, 0xa0, 0x62, 0x6b,
-	0xe5, 0x05, 0x67, 0x10, 0xb1, 0x21, 0x34, 0xa7, 0xb9, 0xa2, 0xb8, 0xef, 0x62, 0x18, 0xa2, 0xc9,
-	0xbd, 0x56, 0x84, 0x71, 0x01, 0x68, 0xf5, 0xd5, 0x81, 0xbe, 0x80, 0x9a, 0x7c, 0xe4, 0x3c, 0xda,
-	0xa6, 0x94, 0x58, 0xe2, 0x14, 0xc1, 0xce, 0x7b, 0x70, 0x8a, 0x60, 0xc7, 0xf8, 0x2b, 0xd4, 0x55,
-	0x0c, 0x71, 0x66, 0xa4, 0xf4, 0x0a, 0x34, 0x33, 0xfa, 0xbd, 0x18, 0xfb, 0xf0, 0x10, 0x61, 0x34,
-	0xa0, 0x26, 0x1f, 0x01, 0xc6, 0xdf, 0x00, 0xad, 0x8e, 0xba, 0xa2, 0x89, 0x31, 0x8e, 0x63, 0x6e,
-	0x95, 0xaf, 0x7e, 0x5b, 0x32, 0x4f, 0xd5, 0xfd, 0xff, 0x14, 0xda, 0x84, 0x3a, 0x56, 0xf9, 0x10,
-	0x5a, 0x84, 0x3a, 0x4a, 0x6e, 0x1c, 0xc1, 0xd6, 0x03, 0x03, 0x30, 0xda, 0x83, 0x66, 0x82, 0x32,
-	0x69, 0x2b, 0x5f, 0x81, 0xb3, 0x4c, 0xc1, 0x38, 0x86, 0xed, 0x87, 0x86, 0x4a, 0xb4, 0x9f, 0x63,
-	0xad, 0xf2, 0x91, 0x3d, 0x5a, 0x12, 0x45, 0x85, 0xd4, 0x19, 0x04, 0x1b, 0xff, 0xd5, 0xa0, 0x5b,
-	0x12, 0xe5, 0x68, 0xa1, 0x15, 0xd0, 0xe2, 0xfd, 0x00, 0xf3, 0x29, 0x40, 0x7e, 0x7b, 0x13, 0x94,
-	0x29, 0x70, 0xd0, 0xc7, 0xd0, 0xba, 0xf0, 0x43, 0xfb, 0x4a, 0xe4, 0x44, 0x5e, 0xac, 0xaa, 0xd9,
-	0x94, 0x8c, 0x53, 0x72, 0x8d, 0x76, 0xa1, 0x23, 0x52, 0xe5, 0x51, 0x4b, 0xb2, 0x12, 0x74, 0x01,
-	0x46, 0xae, 0xa7, 0xf4, 0x48, 0x70, 0x8c, 0xef, 0xe1, 0xc9, 0x83, 0x13, 0x30, 0x3a, 0x58, 0x99,
-	0x7e, 0x9e, 0x2e, 0x6d, 0x77, 0xac, 0xc4, 0x85, 0x19, 0xe8, 0x1c, 0x7a, 0x65, 0x19, 0xfa, 0x0a,
-	0xea, 0x2a, 0x1b, 0x49, 0xe1, 0x3f, 0x92, 0xb2, 0x44, 0xa9, 0xf8, 0x03, 0x23, 0x69, 0x67, 0x69,
-	0x73, 0xf8, 0x4b, 0xe6, 0x3a, 0x05, 0xf0, 0xe7, 0xb0, 0xc1, 0xef, 0xac, 0xd2, 0xf6, 0x92, 0x81,
-	0x91, 0xdf, 0x9d, 0x66, 0x1b, 0x2c, 0xbb, 0x2c, 0xfe, 0x13, 0x31, 0xbe, 0x84, 0x8d, 0xa5, 0x07,
-	0x87, 0xb8, 0x74, 0x24, 0x8e, 0xc3, 0x38, 0x39, 0x1f, 0x45, 0x18, 0xef, 0xa0, 0x95, 0x8d, 0x8d,
-	0xa2, 0x03, 0x15, 0x9a, 0x85, 0xfc, 0x16, 0x31, 0x6e, 0x49, 0xcc, 0xc4, 0x01, 0xa9, 0xf3, 0x4b,
-	0xc9, 0xf7, 0x4d, 0x4e, 0xbf, 0xf9, 0x13, 0xb4, 0x0b, 0x9d, 0x78, 0xf9, 0x71, 0xd0, 0x85, 0xd6,
-	0xd1, 0x9b, 0xb7, 0xc3, 0xef, 0xad, 0xd9, 0xe9, 0x71, 0x5f, 0x13, 0x6f, 0x80, 0xe9, 0x68, 0x3c,
-	0x3f, 0x9b, 0x9e, 0x9d, 0x4b, 0xce, 0xda, 0xc1, 0x3f, 0xa0, 0xae, 0x26, 0x21, 0xf4, 0x0d, 0x74,
-	0xd4, 0xd7, 0x29, 0x8f, 0x09, 0x0e, 0xd0, 0xca, 0xc5, 0xde, 0x59, 0xe1, 0x18, 0x95, 0x17, 0xda,
-	0x2b, 0x0d, 0x7d, 0x01, 0xd5, 0x13, 0x8f, 0xba, 0xa8, 0xfc, 0x48, 0xdf, 0x29, 0x93, 0x46, 0xe5,
-	0xe8, 0xab, 0xbf, 0xef, 0xb9, 0x1e, 0x5f, 0xdc, 0x5c, 0x88, 0x4e, 0xb3, 0xbf, 0xb8, 0x8f, 0x48,
-	0xac, 0xa6, 0xf2, 0xfd, 0x4b, 0x7c, 0x11, 0x7b, 0xf6, 0xbe, 0xfc, 0x2f, 0xc6, 0xf6, 0x95, 0xd9,
-	0x45, 0x5d, 0x92, 0x5f, 0xff, 0x3f, 0x00, 0x00, 0xff, 0xff, 0xdd, 0x1d, 0xb3, 0x7e, 0x5f, 0x13,
-	0x00, 0x00,
+func init() { proto.RegisterFile("gossip/message.proto", fileDescriptor_message_563b77f4f682605d) }
+
+var fileDescriptor_message_563b77f4f682605d = []byte{
+	// 2120 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x18, 0x4b, 0x73, 0xdc, 0x48,
+	0xd9, 0xf2, 0xcc, 0xd8, 0x33, 0xdf, 0x3c, 0x3c, 0xee, 0x38, 0x89, 0xd6, 0xbb, 0x24, 0x5e, 0x41,
+	0xb2, 0x81, 0x64, 0xc7, 0xc1, 0x81, 0x62, 0x8b, 0x05, 0x52, 0xf6, 0x8c, 0x93, 0x99, 0xda, 0xd8,
+	0x31, 0xb2, 0x03, 0x78, 0x39, 0xa8, 0x64, 0xa9, 0x47, 0x23, 0xac, 0x97, 0xd5, 0x3d, 0x5e, 0xfb,
+	0x48, 0x71, 0x80, 0xe2, 0x00, 0x07, 0xce, 0x1c, 0x38, 0xf1, 0x37, 0xa9, 0x7e, 0x48, 0x6a, 0xcd,
+	0xc3, 0x55, 0xd9, 0x2a, 0x6e, 0xfa, 0x9e, 0xfd, 0x7d, 0x5f, 0x7f, 0xaf, 0x16, 0x6c, 0x79, 0x31,
+	0x21, 0x7e, 0xb2, 0x1b, 0x62, 0x42, 0x6c, 0x0f, 0xf7, 0x92, 0x34, 0xa6, 0x31, 0x5a, 0x13, 0xd8,
+	0xed, 0x87, 0x4e, 0x1c, 0x86, 0x71, 0xb4, 0xeb, 0xc4, 0x41, 0x80, 0x1d, 0xea, 0xc7, 0x91, 0x60,
+	0xd8, 0x7e, 0xec, 0xc5, 0xb1, 0x17, 0xe0, 0x5d, 0x0e, 0x5d, 0x4c, 0xc7, 0xbb, 0xd4, 0x0f, 0x31,
+	0xa1, 0x76, 0x98, 0x08, 0x06, 0xe3, 0x2f, 0x1a, 0xd4, 0x0f, 0xa3, 0x6b, 0x1c, 0xc4, 0x09, 0x46,
+	0x3a, 0xac, 0x27, 0xf6, 0x6d, 0x10, 0xdb, 0xae, 0xae, 0xed, 0x68, 0xcf, 0x5a, 0x66, 0x06, 0xa2,
+	0xcf, 0xa0, 0x41, 0x7c, 0x2f, 0xb2, 0xe9, 0x34, 0xc5, 0xfa, 0x2a, 0xa7, 0x15, 0x08, 0xf4, 0x1a,
+	0x36, 0x08, 0x76, 0x52, 0x4c, 0x2d, 0x2c, 0x55, 0xe9, 0x95, 0x1d, 0xed, 0x59, 0x73, 0xef, 0x41,
+	0x4f, 0x18, 0xd8, 0x3b, 0xe5, 0xe4, 0xec, 0x20, 0xb3, 0x43, 0x4a, 0xb0, 0x31, 0x84, 0x4e, 0x99,
+	0xe3, 0xfb, 0x9a, 0x62, 0xec, 0xc3, 0x9a, 0xd0, 0x84, 0x5e, 0x40, 0xd7, 0x8f, 0x28, 0x4e, 0x23,
+	0x3b, 0x38, 0x8c, 0xdc, 0x24, 0xf6, 0x23, 0xca, 0x55, 0x35, 0x86, 0x2b, 0xe6, 0x1c, 0xe5, 0xa0,
+	0x01, 0xeb, 0x4e, 0x1c, 0x51, 0x1c, 0x51, 0xe3, 0x1f, 0x2d, 0x68, 0xbf, 0xe5, 0x66, 0x1f, 0x89,
+	0x60, 0xa3, 0x2d, 0xa8, 0x45, 0x71, 0xe4, 0x60, 0x2e, 0x5f, 0x35, 0x05, 0xc0, 0x4c, 0x74, 0x26,
+	0x76, 0x14, 0xe1, 0x40, 0x9a, 0x91, 0x81, 0xe8, 0x39, 0x54, 0xa8, 0xed, 0xf1, 0x18, 0x74, 0xf6,
+	0x3e, 0xc9, 0x62, 0x50, 0xd2, 0xd9, 0x3b, 0xb3, 0x3d, 0x93, 0x71, 0xa1, 0x57, 0xd0, 0xb0, 0x03,
+	0xff, 0x1a, 0x5b, 0x21, 0xf1, 0xf4, 0x1a, 0x0f, 0xdb, 0x56, 0x26, 0xb2, 0xcf, 0x08, 0x52, 0x62,
+	0xb8, 0x62, 0xd6, 0x39, 0xe3, 0x11, 0xf1, 0xd0, 0xcf, 0x60, 0x3d, 0xc4, 0xa1, 0x95, 0xe2, 0x2b,
+	0x7d, 0x8d, 0x8b, 0xe4, 0xa7, 0x1c, 0xe1, 0xf0, 0x02, 0xa7, 0x64, 0xe2, 0x27, 0x26, 0xbe, 0x9a,
+	0x62, 0x42, 0x87, 0x2b, 0xe6, 0x5a, 0x88, 0x43, 0x13, 0x5f, 0xa1, 0x9f, 0x67, 0x52, 0x44, 0x5f,
+	0xe7, 0x52, 0xdb, 0x8b, 0xa4, 0x48, 0x12, 0x47, 0x04, 0xe7, 0x62, 0x04, 0xbd, 0x84, 0xba, 0x6b,
+	0x53, 0x9b, 0x1b, 0x58, 0xe7, 0x72, 0xf7, 0x32, 0xb9, 0x81, 0x4d, 0xed, 0xc2, 0xbe, 0x75, 0xc6,
+	0xc6, 0xcc, 0x7b, 0x0e, 0xb5, 0x09, 0x0e, 0x82, 0x58, 0x6f, 0x94, 0xd9, 0x45, 0x08, 0x86, 0x8c,
+	0x34, 0x5c, 0x31, 0x05, 0x0f, 0xda, 0x95, 0xea, 0x5d, 0xdf, 0xd3, 0x81, 0xf3, 0x23, 0x55, 0xfd,
+	0xc0, 0xf7, 0x84, 0x17, 0x5c, 0xfb, 0xc0, 0xf7, 0x72, 0x7b, 0x98, 0xf7, 0xcd, 0x79, 0x7b, 0x0a,
+	0xbf, 0xb9, 0x84, 0x70, 0xbc, 0xc9, 0x25, 0xa6, 0x89, 0x6b, 0x53, 0xac, 0xb7, 0xe6, 0x4f, 0xf9,
+	0xc0, 0x29, 0xc3, 0x15, 0x13, 0xdc, 0x1c, 0x42, 0x4f, 0xa0, 0x86, 0xc3, 0x84, 0xde, 0xea, 0x6d,
+	0x2e, 0xd0, 0xce, 0x04, 0x0e, 0x19, 0x92, 0x39, 0xc0, 0xa9, 0xe8, 0x39, 0x54, 0x9d, 0x38, 0x8a,
+	0xf4, 0x0e, 0xe7, 0xba, 0x9f, 0x71, 0xf5, 0xe3, 0x28, 0x3a, 0x24, 0xd4, 0xbe, 0x08, 0x7c, 0x32,
+	0x19, 0xae, 0x98, 0x9c, 0x09, 0xed, 0x01, 0x10, 0x6a, 0x53, 0x6c, 0xf9, 0xd1, 0x38, 0xd6, 0x37,
+	0xb8, 0xc8, 0x66, 0x5e, 0x26, 0x8c, 0x32, 0x8a, 0xc6, 0x2c, 0x3a, 0x0d, 0x92, 0x01, 0xe8, 0x00,
+	0x3a, 0x42, 0x86, 0x44, 0x76, 0x42, 0x26, 0x31, 0xd5, 0xbb, 0xe5, 0x4b, 0xcf, 0xe5, 0x4e, 0x25,
+	0xc3, 0x70, 0xc5, 0x6c, 0x73, 0x91, 0x0c, 0x81, 0x8e, 0xe0, 0x5e, 0x71, 0xae, 0x95, 0x4c, 0x83,
+	0x80, 0xc7, 0x6f, 0x93, 0x2b, 0xfa, 0x6c, 0x4e, 0xd1, 0xc9, 0x34, 0x08, 0x8a, 0x40, 0x76, 0xc9,
+	0x0c, 0x1e, 0xed, 0x83, 0xd0, 0xcf, 0x94, 0x30, 0x26, 0x1d, 0x95, 0x13, 0xca, 0xc4, 0x61, 0x4c,
+	0x31, 0x57, 0x57, 0xa8, 0x69, 0x11, 0x05, 0x46, 0x83, 0xcc, 0xab, 0x54, 0xa6, 0x9c, 0x7e, 0x8f,
+	0xeb, 0xf8, 0x74, 0xa1, 0x8e, 0x3c, 0x2b, 0xdb, 0x44, 0x45, 0xb0, 0xd8, 0x04, 0xd8, 0x76, 0x45,
+	0xf2, 0xf2, 0x14, 0xdd, 0x2a, 0xc7, 0xe6, 0x5d, 0x4e, 0x2d, 0x12, 0xb5, 0x5d, 0x88, 0xb0, 0x74,
+	0xfd, 0x1a, 0xda, 0x09, 0xc6, 0xa9, 0xe5, 0xbb, 0x38, 0xa2, 0x3e, 0xbd, 0xd5, 0xef, 0x97, 0xcb,
+	0xf0, 0x04, 0xe3, 0x74, 0x24, 0x69, 0xcc, 0x8d, 0x44, 0x81, 0x59, 0xb1, 0xdb, 0xce, 0xa5, 0xfe,
+	0x80, 0x8b, 0x3c, 0xcc, 0x2b, 0xd7, 0xb9, 0x8c, 0xe2, 0xef, 0x02, 0xec, 0x7a, 0x38, 0xc4, 0x11,
+	0x73, 0x9e, 0x71, 0xa1, 0xdf, 0x00, 0x24, 0xa9, 0x7f, 0x2d, 0xa2, 0xa0, 0x3f, 0x2c, 0x07, 0x5f,
+	0xf8, 0x7b, 0x72, 0x4d, 0xcb, 0x59, 0xac, 0x48, 0xa0, 0xd7, 0x8a, 0x3c, 0xd1, 0x75, 0x2e, 0xff,
+	0x83, 0x25, 0xf2, 0x79, 0xc4, 0x14, 0x11, 0xf4, 0x1a, 0x5a, 0x12, 0xb2, 0x58, 0xa2, 0xeb, 0x9f,
+	0x94, 0xaf, 0xed, 0x44, 0xd0, 0xca, 0x65, 0xdd, 0x4c, 0x0a, 0x2c, 0xda, 0x87, 0x26, 0x9b, 0x32,
+	0xd2, 0x44, 0xfd, 0xdb, 0x45, 0x26, 0xf4, 0x0b, 0x06, 0xe9, 0x83, 0x2a, 0x83, 0x0e, 0x54, 0x15,
+	0x44, 0xff, 0x23, 0x57, 0xf1, 0x68, 0x99, 0x8a, 0xdc, 0x0d, 0x55, 0xc8, 0xb0, 0xa0, 0x72, 0x66,
+	0x7b, 0xa8, 0x0d, 0x8d, 0x0f, 0xc7, 0x83, 0xc3, 0x37, 0xa3, 0xe3, 0xc3, 0x41, 0x77, 0x05, 0x35,
+	0xa0, 0x76, 0x78, 0x74, 0x72, 0x76, 0xde, 0xd5, 0x50, 0x0b, 0xea, 0xef, 0xcd, 0xb7, 0xd6, 0xfb,
+	0xe3, 0x77, 0xe7, 0xdd, 0x55, 0xc6, 0xd7, 0x1f, 0xee, 0x1f, 0x0b, 0xb0, 0x82, 0xba, 0xd0, 0xe2,
+	0xe0, 0xfe, 0xf1, 0xc0, 0x7a, 0x6f, 0xbe, 0xed, 0x56, 0xd1, 0x06, 0x34, 0x05, 0x83, 0xc9, 0x11,
+	0x35, 0x75, 0x20, 0xfc, 0x57, 0x83, 0x46, 0x5e, 0x18, 0xa8, 0x07, 0x8d, 0x7c, 0x88, 0xf2, 0xc6,
+	0xdf, 0xdc, 0xeb, 0xaa, 0x89, 0x72, 0xe6, 0x87, 0xd8, 0x2c, 0x58, 0xd0, 0x7d, 0x58, 0x4b, 0x2e,
+	0x7d, 0xcb, 0x77, 0xf9, 0x3c, 0x68, 0x99, 0xb5, 0xe4, 0xd2, 0x1f, 0xb9, 0xe8, 0x31, 0x34, 0xe5,
+	0xb8, 0xb0, 0x8e, 0xf6, 0xfb, 0x7a, 0x95, 0xd3, 0x40, 0xa2, 0x8e, 0xf6, 0xfb, 0xac, 0x51, 0x24,
+	0x69, 0x9c, 0xe0, 0x94, 0xfa, 0x98, 0xc8, 0xc1, 0x80, 0x8a, 0x7b, 0xca, 0x28, 0xa6, 0xc2, 0x65,
+	0xfc, 0x5b, 0x03, 0x28, 0x48, 0xe8, 0x87, 0xd0, 0xe6, 0x19, 0x98, 0x5a, 0x13, 0xec, 0x7b, 0x13,
+	0x2a, 0xe7, 0x57, 0x4b, 0x20, 0x87, 0x1c, 0x87, 0x3e, 0x87, 0x56, 0x80, 0xc7, 0xd4, 0x52, 0x67,
+	0x59, 0xdd, 0x6c, 0x32, 0x5c, 0x5f, 0xce, 0xb3, 0x9f, 0x02, 0x33, 0xcc, 0x8f, 0x9c, 0xd8, 0xc5,
+	0x44, 0xaf, 0xec, 0x54, 0xd4, 0x9e, 0xd5, 0xcf, 0x28, 0xa6, 0xc2, 0xc4, 0x46, 0x66, 0x1a, 0x07,
+	0x98, 0xe8, 0xce, 0x4e, 0xe5, 0x59, 0xc3, 0x14, 0x80, 0xb1, 0x0f, 0x9b, 0x73, 0xad, 0x0a, 0xbd,
+	0x80, 0x3a, 0x0e, 0x78, 0x95, 0x10, 0x5d, 0xe3, 0xba, 0xf3, 0x78, 0xe6, 0x0b, 0x43, 0xce, 0x61,
+	0xfc, 0x02, 0xb6, 0x16, 0x35, 0xa9, 0xd9, 0x78, 0x6a, 0xb3, 0xf1, 0x34, 0xc6, 0xd0, 0x2e, 0x75,
+	0x64, 0xe5, 0x62, 0x34, 0xf5, 0x62, 0xb6, 0xa1, 0x9e, 0xf7, 0x01, 0x31, 0xd7, 0x73, 0x18, 0x19,
+	0xd0, 0xa6, 0x01, 0xb1, 0x1c, 0x9c, 0x52, 0x6b, 0x62, 0x93, 0x89, 0xbc, 0xd2, 0x26, 0x0d, 0x48,
+	0x1f, 0xa7, 0x74, 0x68, 0x93, 0x89, 0xf1, 0x01, 0x5a, 0x6a, 0xbf, 0x58, 0x76, 0x0c, 0x82, 0x2a,
+	0x53, 0x23, 0x8f, 0xe0, 0xdf, 0xec, 0xe8, 0x10, 0x53, 0x9b, 0x17, 0xa6, 0xd0, 0x9c, 0xc3, 0x46,
+	0x08, 0x4d, 0xa5, 0xa4, 0x96, 0xaf, 0x24, 0x2e, 0x1f, 0x97, 0x44, 0x5f, 0xdd, 0xa9, 0xb0, 0x95,
+	0x44, 0x82, 0xa8, 0x07, 0xf5, 0x90, 0x78, 0x16, 0xbd, 0x95, 0xbb, 0x59, 0xa7, 0x98, 0x99, 0x2c,
+	0x8a, 0x47, 0xc4, 0x3b, 0xbb, 0x4d, 0xb0, 0xb9, 0x1e, 0x8a, 0x0f, 0x23, 0x86, 0xa6, 0x32, 0xac,
+	0x97, 0x1c, 0xa7, 0xda, 0xbb, 0x5a, 0xb6, 0xf7, 0xa3, 0x0f, 0xbc, 0x01, 0x28, 0xe6, 0xf0, 0x92,
+	0xf3, 0x7e, 0x04, 0x55, 0x79, 0xd6, 0xe2, 0x2c, 0xa9, 0x7e, 0xaf, 0x93, 0x03, 0x71, 0xb2, 0xd8,
+	0x33, 0xfe, 0xef, 0x81, 0xfd, 0x4a, 0xdc, 0x63, 0xb6, 0x5a, 0xfe, 0xb8, 0xbc, 0xe7, 0x36, 0xf7,
+	0x36, 0x72, 0x69, 0x81, 0xce, 0x17, 0x5f, 0xe3, 0x0d, 0xa0, 0xf9, 0xf6, 0x8c, 0x5e, 0xce, 0x2a,
+	0x78, 0x30, 0xd3, 0xcb, 0xe7, 0xf4, 0x9c, 0xc3, 0xba, 0xc4, 0xa1, 0x87, 0xb0, 0x4e, 0xf0, 0x95,
+	0x15, 0x4d, 0x43, 0xe9, 0xee, 0x1a, 0xc1, 0x57, 0xc7, 0xd3, 0x90, 0x65, 0xa7, 0x72, 0xab, 0x22,
+	0xae, 0x9f, 0xcf, 0x8c, 0x8e, 0x0a, 0x0f, 0x84, 0x3a, 0x1c, 0x8c, 0x7f, 0xae, 0x42, 0xa7, 0x7c,
+	0x2c, 0xfa, 0x02, 0x36, 0x8a, 0x57, 0x89, 0x15, 0xd9, 0xa1, 0x88, 0x6c, 0xc3, 0xec, 0x14, 0xe8,
+	0x63, 0x3b, 0xc4, 0x6c, 0xaf, 0x67, 0x54, 0x92, 0xd8, 0x8e, 0xd8, 0xeb, 0x1b, 0x66, 0x81, 0x40,
+	0xf7, 0xa0, 0x46, 0x6f, 0xb2, 0x26, 0xda, 0x30, 0xab, 0xf4, 0x66, 0xe4, 0xb2, 0xfe, 0x96, 0x59,
+	0x94, 0x7e, 0x47, 0x30, 0x95, 0x5d, 0x34, 0x33, 0xd3, 0x64, 0x38, 0xf4, 0x02, 0x50, 0xc6, 0x44,
+	0xfc, 0x30, 0xeb, 0x84, 0x35, 0xee, 0x6e, 0x57, 0x52, 0x4e, 0xfd, 0x50, 0x76, 0xc3, 0x63, 0x40,
+	0x8a, 0xb9, 0x4e, 0x1c, 0x8d, 0x7d, 0x8f, 0xc8, 0x1d, 0xfb, 0x71, 0x4f, 0x3c, 0xb3, 0x7a, 0xfd,
+	0x9c, 0xa3, 0xcf, 0x19, 0x4e, 0x6c, 0xe7, 0xd2, 0xf6, 0xb0, 0xb9, 0xe9, 0xcc, 0x10, 0x88, 0xf1,
+	0x77, 0x0d, 0x5a, 0xea, 0x16, 0x8f, 0x7a, 0x00, 0x61, 0xbe, 0x6c, 0xcb, 0x2b, 0xeb, 0x94, 0xd7,
+	0x70, 0x53, 0xe1, 0xf8, 0xe8, 0x71, 0xa3, 0xb6, 0xaf, 0x6a, 0xb9, 0x7d, 0x19, 0x7f, 0xd6, 0x60,
+	0x73, 0x6e, 0x1d, 0x5a, 0xd6, 0xa0, 0x3e, 0xf6, 0xe0, 0x27, 0xd0, 0xf1, 0x89, 0xe5, 0x62, 0x27,
+	0xb0, 0x53, 0x9b, 0x85, 0x80, 0x5f, 0x55, 0xdd, 0x6c, 0xfb, 0x64, 0x50, 0x20, 0x8d, 0x5f, 0x41,
+	0x3d, 0x93, 0x66, 0xe9, 0xe7, 0x47, 0x8e, 0x9a, 0x7e, 0x7e, 0xe4, 0xb0, 0xf4, 0x53, 0xf2, 0x72,
+	0x55, 0xcd, 0x4b, 0x63, 0x0c, 0x9b, 0x73, 0x0f, 0x1c, 0xf4, 0x35, 0x74, 0x09, 0x0e, 0xc6, 0x7c,
+	0xb3, 0x4d, 0x43, 0x71, 0xb6, 0x56, 0x36, 0x38, 0x6f, 0x11, 0x1b, 0x8c, 0x73, 0x54, 0x30, 0xb2,
+	0x7a, 0x67, 0x9b, 0x5a, 0x24, 0xeb, 0x5a, 0x00, 0xc6, 0x05, 0xa0, 0xf9, 0x27, 0x11, 0x7a, 0x0a,
+	0x35, 0xfe, 0x02, 0x5b, 0x3a, 0xa6, 0x04, 0x99, 0xf7, 0x29, 0x6c, 0xbb, 0x77, 0xf4, 0x29, 0x6c,
+	0xbb, 0xc6, 0xef, 0x61, 0x4d, 0x9c, 0xc1, 0xee, 0x0c, 0x97, 0x9e, 0xa8, 0x66, 0x0e, 0xdf, 0xd9,
+	0x63, 0x17, 0xaf, 0x16, 0xc6, 0x3a, 0xd4, 0xf8, 0x0b, 0xc5, 0xf8, 0x03, 0xa0, 0xf9, 0x3d, 0x9c,
+	0x0d, 0x31, 0x42, 0xed, 0x94, 0x5a, 0xe5, 0xd2, 0x6f, 0x72, 0xe4, 0xa9, 0xa8, 0xff, 0x47, 0xd0,
+	0xc4, 0x91, 0x6b, 0x95, 0x2f, 0xa1, 0x81, 0x23, 0x57, 0xd0, 0x8d, 0x03, 0xb8, 0xb7, 0x60, 0x3b,
+	0x47, 0xcf, 0xa1, 0x2e, 0xbb, 0x4c, 0x36, 0xca, 0xe7, 0xda, 0x59, 0xce, 0x60, 0xbc, 0x85, 0xad,
+	0x45, 0x1b, 0x2f, 0xda, 0x2d, 0x7a, 0xad, 0xd0, 0x91, 0xbf, 0xa8, 0x24, 0xa3, 0xe8, 0xd4, 0x79,
+	0x0b, 0x36, 0xfe, 0xa3, 0x41, 0xbb, 0x44, 0x2a, 0xba, 0x85, 0xa6, 0x74, 0x8b, 0xbb, 0x1b, 0xcc,
+	0x23, 0x80, 0xa2, 0x7a, 0x65, 0x97, 0x51, 0x30, 0xe8, 0x53, 0x68, 0x5c, 0x04, 0xb1, 0x73, 0xc9,
+	0x62, 0xc2, 0x0b, 0xab, 0x6a, 0xd6, 0x39, 0xe2, 0x14, 0x5f, 0xa1, 0x1d, 0x68, 0xb1, 0x50, 0xf9,
+	0x91, 0xc5, 0x51, 0xb2, 0xbb, 0x00, 0xc1, 0x57, 0xa3, 0xe8, 0x80, 0x61, 0x8c, 0x6f, 0xe0, 0xfe,
+	0xc2, 0xf5, 0x1c, 0xed, 0xcd, 0x6d, 0x3f, 0x0f, 0x66, 0xdc, 0x3d, 0x14, 0x64, 0x65, 0x07, 0x3a,
+	0x87, 0x4e, 0x99, 0x86, 0xbe, 0x84, 0x35, 0x11, 0x0d, 0x99, 0xf8, 0x4b, 0x42, 0x26, 0x99, 0xd4,
+	0xbf, 0x2b, 0x72, 0x9c, 0x65, 0xc3, 0xe1, 0xb7, 0xb9, 0xea, 0xac, 0x81, 0x3f, 0x81, 0x0d, 0x7a,
+	0x63, 0x95, 0xdc, 0x93, 0x6b, 0x24, 0xbd, 0x39, 0xcd, 0x1d, 0x2c, 0xab, 0x54, 0x7f, 0xd8, 0x18,
+	0x5f, 0xc0, 0xc6, 0xcc, 0x6b, 0x88, 0x15, 0x1d, 0x4e, 0xd3, 0x38, 0x95, 0xf7, 0x23, 0x00, 0xe3,
+	0x03, 0x34, 0xf2, 0x65, 0x92, 0x4d, 0x20, 0x65, 0x58, 0xf0, 0x6f, 0x76, 0xc6, 0x35, 0x4e, 0x09,
+	0xbb, 0x20, 0x71, 0x7f, 0x19, 0x78, 0xe7, 0xe6, 0xf4, 0x57, 0x0d, 0xf4, 0xdf, 0xd9, 0x81, 0xef,
+	0xf2, 0x82, 0x37, 0x31, 0x99, 0x06, 0x94, 0x64, 0xcd, 0x6f, 0xe9, 0x04, 0xd4, 0x61, 0x9d, 0xde,
+	0xbc, 0x09, 0x6c, 0x8f, 0x64, 0xfe, 0x48, 0xb0, 0xfc, 0x03, 0xaa, 0x32, 0xfb, 0x2f, 0xec, 0xae,
+	0xfe, 0x6b, 0x65, 0x49, 0x30, 0xf3, 0x40, 0x5a, 0xb2, 0x74, 0xbc, 0x2c, 0x2f, 0x1d, 0x4a, 0x66,
+	0x64, 0xf2, 0xb3, 0x95, 0xf0, 0x37, 0x0d, 0x3a, 0x65, 0x5a, 0x39, 0xeb, 0xb5, 0xbb, 0xb3, 0x7e,
+	0x75, 0x2e, 0xeb, 0xbb, 0x50, 0xb9, 0xc4, 0xb7, 0xb2, 0x1c, 0xd8, 0x27, 0x7a, 0x0a, 0x1d, 0x1c,
+	0xb9, 0x71, 0x4a, 0xb0, 0xbb, 0x4f, 0xcf, 0x6e, 0x46, 0x03, 0xee, 0x65, 0xc3, 0x9c, 0xc1, 0x1a,
+	0x0e, 0x3c, 0x58, 0xfc, 0x92, 0x5b, 0xe2, 0xec, 0x2b, 0xa5, 0x0e, 0x84, 0xb7, 0x0f, 0x67, 0xbd,
+	0x9d, 0x2f, 0x84, 0x7f, 0x69, 0xb0, 0x31, 0x43, 0x45, 0xbd, 0x99, 0x52, 0x58, 0x16, 0xb4, 0xac,
+	0x16, 0xb6, 0xa0, 0x76, 0x6d, 0x07, 0xd3, 0xec, 0x5f, 0xa2, 0x00, 0xd0, 0x2f, 0x01, 0xf0, 0x4d,
+	0xe2, 0xa7, 0xb7, 0x6c, 0x50, 0xc9, 0xbf, 0x99, 0xec, 0x95, 0x1c, 0x7b, 0x81, 0xfc, 0xf9, 0x7a,
+	0x31, 0x1d, 0xf7, 0xce, 0xb2, 0xe9, 0x67, 0x2a, 0xdc, 0x3f, 0xf9, 0x35, 0x34, 0x95, 0xd5, 0x6f,
+	0xf6, 0x8d, 0xda, 0x86, 0xc6, 0xc1, 0xbb, 0xf7, 0xfd, 0x6f, 0xac, 0xa3, 0xd3, 0xb7, 0x5d, 0x8d,
+	0x3d, 0x45, 0x47, 0x83, 0xc3, 0xe3, 0xb3, 0xd1, 0xd9, 0x39, 0xc7, 0xac, 0xee, 0xfd, 0x09, 0xd6,
+	0xc4, 0xea, 0x8d, 0xbe, 0x82, 0x96, 0xf8, 0x3a, 0xa5, 0x29, 0xb6, 0x43, 0x34, 0x37, 0x49, 0xb6,
+	0xe7, 0x30, 0xc6, 0xca, 0x33, 0xed, 0xa5, 0x86, 0x9e, 0x42, 0xf5, 0xc4, 0x8f, 0x3c, 0x54, 0xfe,
+	0x65, 0xb5, 0x5d, 0x06, 0x8d, 0x95, 0x83, 0x2f, 0xbf, 0x7d, 0xee, 0xf9, 0x74, 0x32, 0xbd, 0x60,
+	0xab, 0xcd, 0xee, 0xe4, 0x36, 0xc1, 0xa9, 0x78, 0x1c, 0xee, 0x8e, 0xed, 0x8b, 0xd4, 0x77, 0xc4,
+	0x8f, 0x63, 0xb2, 0x2b, 0xc4, 0x2e, 0xd6, 0x38, 0xf8, 0xea, 0x7f, 0x01, 0x00, 0x00, 0xff, 0xff,
+	0xd7, 0x39, 0x29, 0xbd, 0x8e, 0x16, 0x00, 0x00,
 }
diff --git a/protos/gossip/message.proto b/protos/gossip/message.proto
index 56cb0cd0c..09c4104ff 100644
--- a/protos/gossip/message.proto
+++ b/protos/gossip/message.proto
@@ -9,6 +9,7 @@ option go_package = "github.com/hyperledger/fabric/protos/gossip" ;
 package gossip;

 import "common/collection.proto";
+import "google/protobuf/timestamp.proto";

 // Gossip
 service Gossip {
@@ -130,6 +131,12 @@ message GossipMessage {
         // Encapsulates private data used to distribute
         // private rwset after the endorsement
         PrivateDataMessage private_data = 25;
+
+        // Used to request collection data
+        RemoteCollDataRequest collDataReq = 90;
+
+        // Used to respond to collection data requests
+        RemoteCollDataResponse collDataRes = 91;
     }
 }

@@ -151,6 +158,7 @@ message Properties {
     uint64 ledger_height = 1;
     bool left_channel = 2;
     repeated Chaincode chaincodes = 3;
+    repeated string roles = 99; // NOT USED by Fabric 1.4.1 but added here for fabric-peer-ext dependency
 }

 // StateInfoSnapshot is an aggregation of StateInfo messages
@@ -369,4 +377,41 @@ message Chaincode {
     string name = 1;
     string version = 2;
     bytes metadata = 3;
+}
+
+// ValidationResultsMessage is the message containing block validation results
+message ValidationResultsMessage {
+    uint64 seq_num = 1;
+    bytes txFlags  = 2;
+    bytes signature = 3;
+    bytes identity = 4;
+}
+
+// RemoteCollDataRequest message used to request
+// collection data
+message RemoteCollDataRequest {
+    uint64 nonce = 1;
+    repeated CollDataDigest digests = 2;
+}
+
+// CollDataDigest defines a digest of collection data
+message CollDataDigest {
+    string namespace = 1;
+    string collection = 2;
+    string key = 3;
+    string endorsedAtTxID = 4;
+}
+
+// RemoteCollDataResponse message used to respond to
+// collection data request
+message RemoteCollDataResponse {
+    uint64 nonce = 1;
+    repeated CollDataElement elements = 2;
+}
+
+// CollDataElement contains the collection data digest and value
+message CollDataElement {
+    CollDataDigest digest = 1;
+    bytes value = 2;
+    google.protobuf.Timestamp expiryTime = 3;
 }
\ No newline at end of file
diff --git a/protoutil/blockutils.go b/protoutil/blockutils.go
new file mode 100644
index 000000000..c7535ac9f
--- /dev/null
+++ b/protoutil/blockutils.go
@@ -0,0 +1,124 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil
+
+import (
+	"github.com/golang/protobuf/proto"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+// GetChainIDFromBlockBytes returns chain ID given byte array which represents
+// the block
+func GetChainIDFromBlockBytes(bytes []byte) (string, error) {
+	block, err := GetBlockFromBlockBytes(bytes)
+	if err != nil {
+		return "", err
+	}
+
+	return GetChainIDFromBlock(block)
+}
+
+// GetChainIDFromBlock returns chain ID in the block
+func GetChainIDFromBlock(block *cb.Block) (string, error) {
+	if block == nil || block.Data == nil || block.Data.Data == nil || len(block.Data.Data) == 0 {
+		return "", errors.Errorf("failed to retrieve channel id - block is empty")
+	}
+	var err error
+	envelope, err := GetEnvelopeFromBlock(block.Data.Data[0])
+	if err != nil {
+		return "", err
+	}
+	payload, err := GetPayload(envelope)
+	if err != nil {
+		return "", err
+	}
+
+	if payload.Header == nil {
+		return "", errors.Errorf("failed to retrieve channel id - payload header is empty")
+	}
+	chdr, err := UnmarshalChannelHeader(payload.Header.ChannelHeader)
+	if err != nil {
+		return "", err
+	}
+
+	return chdr.ChannelId, nil
+}
+
+// GetMetadataFromBlock retrieves metadata at the specified index.
+func GetMetadataFromBlock(block *cb.Block, index cb.BlockMetadataIndex) (*cb.Metadata, error) {
+	md := &cb.Metadata{}
+	err := proto.Unmarshal(block.Metadata.Metadata[index], md)
+	if err != nil {
+		return nil, errors.Wrapf(err, "error unmarshaling metadata from block at index [%s]", index)
+	}
+	return md, nil
+}
+
+// GetMetadataFromBlockOrPanic retrieves metadata at the specified index, or
+// panics on error
+func GetMetadataFromBlockOrPanic(block *cb.Block, index cb.BlockMetadataIndex) *cb.Metadata {
+	md, err := GetMetadataFromBlock(block, index)
+	if err != nil {
+		panic(err)
+	}
+	return md
+}
+
+// GetLastConfigIndexFromBlock retrieves the index of the last config block as
+// encoded in the block metadata
+func GetLastConfigIndexFromBlock(block *cb.Block) (uint64, error) {
+	md, err := GetMetadataFromBlock(block, cb.BlockMetadataIndex_LAST_CONFIG)
+	if err != nil {
+		return 0, err
+	}
+	lc := &cb.LastConfig{}
+	err = proto.Unmarshal(md.Value, lc)
+	if err != nil {
+		return 0, errors.Wrap(err, "error unmarshaling LastConfig")
+	}
+	return lc.Index, nil
+}
+
+// GetLastConfigIndexFromBlockOrPanic retrieves the index of the last config
+// block as encoded in the block metadata, or panics on error
+func GetLastConfigIndexFromBlockOrPanic(block *cb.Block) uint64 {
+	index, err := GetLastConfigIndexFromBlock(block)
+	if err != nil {
+		panic(err)
+	}
+	return index
+}
+
+// GetBlockFromBlockBytes marshals the bytes into Block
+func GetBlockFromBlockBytes(blockBytes []byte) (*cb.Block, error) {
+	block := &cb.Block{}
+	err := proto.Unmarshal(blockBytes, block)
+	if err != nil {
+		return block, errors.Wrap(err, "error unmarshaling block")
+	}
+	return block, nil
+}
+
+// CopyBlockMetadata copies metadata from one block into another
+func CopyBlockMetadata(src *cb.Block, dst *cb.Block) {
+	dst.Metadata = src.Metadata
+	// Once copied initialize with rest of the
+	// required metadata positions.
+	InitBlockMetadata(dst)
+}
+
+// InitBlockMetadata copies metadata from one block into another
+func InitBlockMetadata(block *cb.Block) {
+	if block.Metadata == nil {
+		block.Metadata = &cb.BlockMetadata{Metadata: [][]byte{{}, {}, {}}}
+	} else if len(block.Metadata.Metadata) < int(cb.BlockMetadataIndex_TRANSACTIONS_FILTER+1) {
+		for i := int(len(block.Metadata.Metadata)); i <= int(cb.BlockMetadataIndex_TRANSACTIONS_FILTER); i++ {
+			block.Metadata.Metadata = append(block.Metadata.Metadata, []byte{})
+		}
+	}
+}
diff --git a/protoutil/blockutils_test.go b/protoutil/blockutils_test.go
new file mode 100644
index 000000000..1d30dbc8c
--- /dev/null
+++ b/protoutil/blockutils_test.go
@@ -0,0 +1,206 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil_test
+
+import (
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	configtxtest "github.com/hyperledger/fabric/common/configtx/test"
+	"github.com/hyperledger/fabric/protos/common"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/utils"
+	"github.com/stretchr/testify/assert"
+)
+
+var testChainID = "myuniquetestchainid"
+
+func TestGetChainIDFromBlockBytes(t *testing.T) {
+	gb, err := configtxtest.MakeGenesisBlock(testChainID)
+	assert.NoError(t, err, "Failed to create test configuration block")
+	bytes, err := proto.Marshal(gb)
+	cid, err := utils.GetChainIDFromBlockBytes(bytes)
+	assert.NoError(t, err)
+	assert.Equal(t, testChainID, cid, "Failed to return expected chain ID")
+
+	// bad block bytes
+	_, err = utils.GetChainIDFromBlockBytes([]byte("bad block"))
+	assert.Error(t, err, "Expected error with malformed block bytes")
+}
+
+func TestGetChainIDFromBlock(t *testing.T) {
+	var err error
+	var gb *common.Block
+	var cid string
+
+	// nil block
+	_, err = utils.GetChainIDFromBlock(gb)
+	assert.Error(t, err, "Expected error getting channel id from nil block")
+
+	gb, err = configtxtest.MakeGenesisBlock(testChainID)
+	assert.NoError(t, err, "Failed to create test configuration block")
+
+	cid, err = utils.GetChainIDFromBlock(gb)
+	assert.NoError(t, err, "Failed to get chain ID from block")
+	assert.Equal(t, testChainID, cid, "Failed to return expected chain ID")
+
+	// missing data
+	badBlock := gb
+	badBlock.Data = nil
+	_, err = utils.GetChainIDFromBlock(badBlock)
+	assert.Error(t, err, "Expected error with missing block data")
+
+	// no envelope
+	badBlock = &cb.Block{
+		Data: &cb.BlockData{
+			Data: [][]byte{[]byte("bad envelope")},
+		},
+	}
+	_, err = utils.GetChainIDFromBlock(badBlock)
+	assert.Error(t, err, "Expected error with no envelope in data")
+
+	// bad payload
+	env, _ := proto.Marshal(&cb.Envelope{
+		Payload: []byte("bad payload"),
+	})
+	badBlock = &cb.Block{
+		Data: &cb.BlockData{
+			Data: [][]byte{env},
+		},
+	}
+	_, err = utils.GetChainIDFromBlock(badBlock)
+	assert.Error(t, err, "Expected error - malformed payload")
+
+	// bad channel header
+	payload, _ := proto.Marshal(&cb.Payload{
+		Header: &cb.Header{
+			ChannelHeader: []byte("bad header"),
+		},
+	})
+	env, _ = proto.Marshal(&cb.Envelope{
+		Payload: payload,
+	})
+	badBlock = &cb.Block{
+		Data: &cb.BlockData{
+			Data: [][]byte{env},
+		},
+	}
+	_, err = utils.GetChainIDFromBlock(badBlock)
+	assert.Error(t, err, "Expected error with malformed channel header")
+
+	// nil payload header
+	payload, _ = proto.Marshal(&cb.Payload{})
+	env, _ = proto.Marshal(&cb.Envelope{
+		Payload: payload,
+	})
+	badBlock = &cb.Block{
+		Data: &cb.BlockData{
+			Data: [][]byte{env},
+		},
+	}
+	_, err = utils.GetChainIDFromBlock(badBlock)
+	assert.Error(t, err, "Expected error when payload header is nil")
+}
+
+func TestGetBlockFromBlockBytes(t *testing.T) {
+	testChainID := "myuniquetestchainid"
+	gb, err := configtxtest.MakeGenesisBlock(testChainID)
+	assert.NoError(t, err, "Failed to create test configuration block")
+	blockBytes, err := utils.Marshal(gb)
+	assert.NoError(t, err, "Failed to marshal block")
+	_, err = utils.GetBlockFromBlockBytes(blockBytes)
+	assert.NoError(t, err, "to get block from block bytes")
+
+	// bad block bytes
+	_, err = utils.GetBlockFromBlockBytes([]byte("bad block"))
+	assert.Error(t, err, "Expected error for malformed block bytes")
+}
+
+func TestGetMetadataFromNewBlock(t *testing.T) {
+	block := common.NewBlock(0, nil)
+	md, err := utils.GetMetadataFromBlock(block, cb.BlockMetadataIndex_ORDERER)
+	assert.NoError(t, err, "Unexpected error extracting metadata from new block")
+	assert.Nil(t, md.Value, "Expected metadata field value to be nil")
+	assert.Equal(t, 0, len(md.Value), "Expected length of metadata field value to be 0")
+	md = utils.GetMetadataFromBlockOrPanic(block, cb.BlockMetadataIndex_ORDERER)
+	assert.NotNil(t, md, "Expected to get metadata from block")
+
+	// malformed metadata
+	block.Metadata.Metadata[cb.BlockMetadataIndex_ORDERER] = []byte("bad metadata")
+	_, err = utils.GetMetadataFromBlock(block, cb.BlockMetadataIndex_ORDERER)
+	assert.Error(t, err, "Expected error with malformed metadata")
+	assert.Panics(t, func() {
+		_ = utils.GetMetadataFromBlockOrPanic(block, cb.BlockMetadataIndex_ORDERER)
+	}, "Expected panic with malformed metadata")
+}
+
+func TestInitBlockMeta(t *testing.T) {
+	// block with no metadata
+	block := &cb.Block{}
+	utils.InitBlockMetadata(block)
+	// should have 3 entries
+	assert.Equal(t, 3, len(block.Metadata.Metadata), "Expected block to have 3 metadata entries")
+
+	// block with a single entry
+	block = &cb.Block{
+		Metadata: &cb.BlockMetadata{},
+	}
+	block.Metadata.Metadata = append(block.Metadata.Metadata, []byte{})
+	utils.InitBlockMetadata(block)
+	// should have 3 entries
+	assert.Equal(t, 3, len(block.Metadata.Metadata), "Expected block to have 3 metadata entries")
+}
+
+func TestCopyBlockMetadata(t *testing.T) {
+	srcBlock := common.NewBlock(0, nil)
+	dstBlock := &cb.Block{}
+
+	metadata, _ := proto.Marshal(&cb.Metadata{
+		Value: []byte("orderer metadata"),
+	})
+	srcBlock.Metadata.Metadata[cb.BlockMetadataIndex_ORDERER] = metadata
+	utils.CopyBlockMetadata(srcBlock, dstBlock)
+
+	// check that the copy worked
+	assert.Equal(t, len(srcBlock.Metadata.Metadata), len(dstBlock.Metadata.Metadata),
+		"Expected target block to have same number of metadata entries after copy")
+	assert.Equal(t, metadata, dstBlock.Metadata.Metadata[cb.BlockMetadataIndex_ORDERER],
+		"Unexpected metadata from target block")
+}
+
+func TestGetLastConfigIndexFromBlock(t *testing.T) {
+	block := common.NewBlock(0, nil)
+	index := uint64(2)
+	lc, _ := proto.Marshal(&cb.LastConfig{
+		Index: index,
+	})
+	metadata, _ := proto.Marshal(&cb.Metadata{
+		Value: lc,
+	})
+	block.Metadata.Metadata[cb.BlockMetadataIndex_LAST_CONFIG] = metadata
+	result, err := utils.GetLastConfigIndexFromBlock(block)
+	assert.NoError(t, err, "Unexpected error returning last config index")
+	assert.Equal(t, index, result, "Unexpected last config index returned from block")
+	result = utils.GetLastConfigIndexFromBlockOrPanic(block)
+	assert.Equal(t, index, result, "Unexpected last config index returned from block")
+
+	// malformed metadata
+	block.Metadata.Metadata[cb.BlockMetadataIndex_LAST_CONFIG] = []byte("bad metadata")
+	_, err = utils.GetLastConfigIndexFromBlock(block)
+	assert.Error(t, err, "Expected error with malformed metadata")
+
+	// malformed last config
+	metadata, _ = proto.Marshal(&cb.Metadata{
+		Value: []byte("bad last config"),
+	})
+	block.Metadata.Metadata[cb.BlockMetadataIndex_LAST_CONFIG] = metadata
+	_, err = utils.GetLastConfigIndexFromBlock(block)
+	assert.Error(t, err, "Expected error with malformed last config metadata")
+	assert.Panics(t, func() {
+		_ = utils.GetLastConfigIndexFromBlockOrPanic(block)
+	}, "Expected panic with malformed last config metadata")
+}
diff --git a/protoutil/chaincodeutils.go b/protoutil/chaincodeutils.go
new file mode 100644
index 000000000..c0504d869
--- /dev/null
+++ b/protoutil/chaincodeutils.go
@@ -0,0 +1,25 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil
+
+import (
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/pkg/errors"
+)
+
+// UnmarshalChaincodeDeploymentSpec unmarshals a ChaincodeDeploymentSpec from
+// the provided bytes
+func UnmarshalChaincodeDeploymentSpec(cdsBytes []byte) (*peer.ChaincodeDeploymentSpec, error) {
+	cds := &peer.ChaincodeDeploymentSpec{}
+	err := proto.Unmarshal(cdsBytes, cds)
+	if err != nil {
+		return nil, errors.Wrap(err, "error unmarshaling ChaincodeDeploymentSpec")
+	}
+
+	return cds, nil
+}
diff --git a/protoutil/commonutils.go b/protoutil/commonutils.go
new file mode 100644
index 000000000..bced28e3c
--- /dev/null
+++ b/protoutil/commonutils.go
@@ -0,0 +1,339 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil
+
+import (
+	"fmt"
+	"time"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/golang/protobuf/ptypes/timestamp"
+	"github.com/hyperledger/fabric/common/crypto"
+	cb "github.com/hyperledger/fabric/protos/common"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/pkg/errors"
+)
+
+// MarshalOrPanic serializes a protobuf message and panics if this
+// operation fails
+func MarshalOrPanic(pb proto.Message) []byte {
+	data, err := proto.Marshal(pb)
+	if err != nil {
+		panic(err)
+	}
+	return data
+}
+
+// Marshal serializes a protobuf message.
+func Marshal(pb proto.Message) ([]byte, error) {
+	return proto.Marshal(pb)
+}
+
+// CreateNonceOrPanic generates a nonce using the common/crypto package
+// and panics if this operation fails.
+func CreateNonceOrPanic() []byte {
+	nonce, err := CreateNonce()
+	if err != nil {
+		panic(err)
+	}
+	return nonce
+}
+
+// CreateNonce generates a nonce using the common/crypto package.
+func CreateNonce() ([]byte, error) {
+	nonce, err := crypto.GetRandomNonce()
+	return nonce, errors.WithMessage(err, "error generating random nonce")
+}
+
+// UnmarshalPayloadOrPanic unmarshals bytes to a Payload structure or panics
+// on error
+func UnmarshalPayloadOrPanic(encoded []byte) *cb.Payload {
+	payload, err := UnmarshalPayload(encoded)
+	if err != nil {
+		panic(err)
+	}
+	return payload
+}
+
+// UnmarshalPayload unmarshals bytes to a Payload structure
+func UnmarshalPayload(encoded []byte) (*cb.Payload, error) {
+	payload := &cb.Payload{}
+	err := proto.Unmarshal(encoded, payload)
+	return payload, errors.Wrap(err, "error unmarshaling Payload")
+}
+
+// UnmarshalEnvelopeOrPanic unmarshals bytes to an Envelope structure or panics
+// on error
+func UnmarshalEnvelopeOrPanic(encoded []byte) *cb.Envelope {
+	envelope, err := UnmarshalEnvelope(encoded)
+	if err != nil {
+		panic(err)
+	}
+	return envelope
+}
+
+// UnmarshalEnvelope unmarshals bytes to an Envelope structure
+func UnmarshalEnvelope(encoded []byte) (*cb.Envelope, error) {
+	envelope := &cb.Envelope{}
+	err := proto.Unmarshal(encoded, envelope)
+	return envelope, errors.Wrap(err, "error unmarshaling Envelope")
+}
+
+// UnmarshalBlockOrPanic unmarshals bytes to an Block structure or panics
+// on error
+func UnmarshalBlockOrPanic(encoded []byte) *cb.Block {
+	block, err := UnmarshalBlock(encoded)
+	if err != nil {
+		panic(err)
+	}
+	return block
+}
+
+// UnmarshalBlock unmarshals bytes to an Block structure
+func UnmarshalBlock(encoded []byte) (*cb.Block, error) {
+	block := &cb.Block{}
+	err := proto.Unmarshal(encoded, block)
+	return block, errors.Wrap(err, "error unmarshaling Block")
+}
+
+// UnmarshalEnvelopeOfType unmarshals an envelope of the specified type,
+// including unmarshaling the payload data
+func UnmarshalEnvelopeOfType(envelope *cb.Envelope, headerType cb.HeaderType, message proto.Message) (*cb.ChannelHeader, error) {
+	payload, err := UnmarshalPayload(envelope.Payload)
+	if err != nil {
+		return nil, err
+	}
+
+	if payload.Header == nil {
+		return nil, errors.New("envelope must have a Header")
+	}
+
+	chdr, err := UnmarshalChannelHeader(payload.Header.ChannelHeader)
+	if err != nil {
+		return nil, err
+	}
+
+	if chdr.Type != int32(headerType) {
+		return nil, errors.Errorf("invalid type %s, expected %s", cb.HeaderType(chdr.Type), headerType)
+	}
+
+	err = proto.Unmarshal(payload.Data, message)
+	err = errors.Wrapf(err, "error unmarshaling message for type %s", headerType)
+	return chdr, err
+}
+
+// ExtractEnvelopeOrPanic retrieves the requested envelope from a given block
+// and unmarshals it -- it panics if either of these operations fail
+func ExtractEnvelopeOrPanic(block *cb.Block, index int) *cb.Envelope {
+	envelope, err := ExtractEnvelope(block, index)
+	if err != nil {
+		panic(err)
+	}
+	return envelope
+}
+
+// ExtractEnvelope retrieves the requested envelope from a given block and
+// unmarshals it
+func ExtractEnvelope(block *cb.Block, index int) (*cb.Envelope, error) {
+	if block.Data == nil {
+		return nil, errors.New("block data is nil")
+	}
+
+	envelopeCount := len(block.Data.Data)
+	if index < 0 || index >= envelopeCount {
+		return nil, errors.New("envelope index out of bounds")
+	}
+	marshaledEnvelope := block.Data.Data[index]
+	envelope, err := GetEnvelopeFromBlock(marshaledEnvelope)
+	err = errors.WithMessage(err, fmt.Sprintf("block data does not carry an envelope at index %d", index))
+	return envelope, err
+}
+
+// ExtractPayloadOrPanic retrieves the payload of a given envelope and
+// unmarshals it -- it panics if either of these operations fail
+func ExtractPayloadOrPanic(envelope *cb.Envelope) *cb.Payload {
+	payload, err := ExtractPayload(envelope)
+	if err != nil {
+		panic(err)
+	}
+	return payload
+}
+
+// ExtractPayload retrieves the payload of a given envelope and unmarshals it.
+func ExtractPayload(envelope *cb.Envelope) (*cb.Payload, error) {
+	payload := &cb.Payload{}
+	err := proto.Unmarshal(envelope.Payload, payload)
+	err = errors.Wrap(err, "no payload in envelope")
+	return payload, err
+}
+
+// MakeChannelHeader creates a ChannelHeader.
+func MakeChannelHeader(headerType cb.HeaderType, version int32, chainID string, epoch uint64) *cb.ChannelHeader {
+	return &cb.ChannelHeader{
+		Type:    int32(headerType),
+		Version: version,
+		Timestamp: &timestamp.Timestamp{
+			Seconds: time.Now().Unix(),
+			Nanos:   0,
+		},
+		ChannelId: chainID,
+		Epoch:     epoch,
+	}
+}
+
+// MakeSignatureHeader creates a SignatureHeader.
+func MakeSignatureHeader(serializedCreatorCertChain []byte, nonce []byte) *cb.SignatureHeader {
+	return &cb.SignatureHeader{
+		Creator: serializedCreatorCertChain,
+		Nonce:   nonce,
+	}
+}
+
+// SetTxID generates a transaction id based on the provided signature header
+// and sets the TxId field in the channel header
+func SetTxID(channelHeader *cb.ChannelHeader, signatureHeader *cb.SignatureHeader) error {
+	txid, err := ComputeTxID(
+		signatureHeader.Nonce,
+		signatureHeader.Creator,
+	)
+	if err != nil {
+		return err
+	}
+	channelHeader.TxId = txid
+	return nil
+}
+
+// MakePayloadHeader creates a Payload Header.
+func MakePayloadHeader(ch *cb.ChannelHeader, sh *cb.SignatureHeader) *cb.Header {
+	return &cb.Header{
+		ChannelHeader:   MarshalOrPanic(ch),
+		SignatureHeader: MarshalOrPanic(sh),
+	}
+}
+
+// NewSignatureHeaderOrPanic returns a signature header and panics on error.
+func NewSignatureHeaderOrPanic(signer crypto.LocalSigner) *cb.SignatureHeader {
+	if signer == nil {
+		panic(errors.New("invalid signer. cannot be nil"))
+	}
+
+	signatureHeader, err := signer.NewSignatureHeader()
+	if err != nil {
+		panic(fmt.Errorf("failed generating a new SignatureHeader: %s", err))
+	}
+	return signatureHeader
+}
+
+// SignOrPanic signs a message and panics on error.
+func SignOrPanic(signer crypto.LocalSigner, msg []byte) []byte {
+	if signer == nil {
+		panic(errors.New("invalid signer. cannot be nil"))
+	}
+
+	sigma, err := signer.Sign(msg)
+	if err != nil {
+		panic(fmt.Errorf("failed generating signature: %s", err))
+	}
+	return sigma
+}
+
+// UnmarshalChannelHeader returns a ChannelHeader from bytes
+func UnmarshalChannelHeader(bytes []byte) (*cb.ChannelHeader, error) {
+	chdr := &cb.ChannelHeader{}
+	err := proto.Unmarshal(bytes, chdr)
+	return chdr, errors.Wrap(err, "error unmarshaling ChannelHeader")
+}
+
+// UnmarshalChannelHeaderOrPanic unmarshals bytes to a ChannelHeader or panics
+// on error
+func UnmarshalChannelHeaderOrPanic(bytes []byte) *cb.ChannelHeader {
+	chdr, err := UnmarshalChannelHeader(bytes)
+	if err != nil {
+		panic(err)
+	}
+	return chdr
+}
+
+// UnmarshalChaincodeID returns a ChaincodeID from bytes
+func UnmarshalChaincodeID(bytes []byte) (*pb.ChaincodeID, error) {
+	ccid := &pb.ChaincodeID{}
+	err := proto.Unmarshal(bytes, ccid)
+	if err != nil {
+		return nil, errors.Wrap(err, "error unmarshaling ChaincodeID")
+	}
+
+	return ccid, nil
+}
+
+// IsConfigBlock validates whenever given block contains configuration
+// update transaction
+func IsConfigBlock(block *cb.Block) bool {
+	envelope, err := ExtractEnvelope(block, 0)
+	if err != nil {
+		return false
+	}
+
+	payload, err := GetPayload(envelope)
+	if err != nil {
+		return false
+	}
+
+	if payload.Header == nil {
+		return false
+	}
+
+	hdr, err := UnmarshalChannelHeader(payload.Header.ChannelHeader)
+	if err != nil {
+		return false
+	}
+
+	return cb.HeaderType(hdr.Type) == cb.HeaderType_CONFIG || cb.HeaderType(hdr.Type) == cb.HeaderType_ORDERER_TRANSACTION
+}
+
+// ChannelHeader returns the *cb.ChannelHeader for a given *cb.Envelope.
+func ChannelHeader(env *cb.Envelope) (*cb.ChannelHeader, error) {
+	envPayload, err := UnmarshalPayload(env.Payload)
+	if err != nil {
+		return nil, err
+	}
+
+	if envPayload.Header == nil {
+		return nil, errors.New("header not set")
+	}
+
+	if envPayload.Header.ChannelHeader == nil {
+		return nil, errors.New("channel header not set")
+	}
+
+	chdr, err := UnmarshalChannelHeader(envPayload.Header.ChannelHeader)
+	if err != nil {
+		return nil, errors.WithMessage(err, "error unmarshaling channel header")
+	}
+
+	return chdr, nil
+}
+
+// ChannelID returns the Channel ID for a given *cb.Envelope.
+func ChannelID(env *cb.Envelope) (string, error) {
+	chdr, err := ChannelHeader(env)
+	if err != nil {
+		return "", errors.WithMessage(err, "error retrieving channel header")
+	}
+
+	return chdr.ChannelId, nil
+}
+
+// EnvelopeToConfigUpdate is used to extract a ConfigUpdateEnvelope from an envelope of
+// type CONFIG_UPDATE
+func EnvelopeToConfigUpdate(configtx *cb.Envelope) (*cb.ConfigUpdateEnvelope, error) {
+	configUpdateEnv := &cb.ConfigUpdateEnvelope{}
+	_, err := UnmarshalEnvelopeOfType(configtx, cb.HeaderType_CONFIG_UPDATE, configUpdateEnv)
+	if err != nil {
+		return nil, err
+	}
+	return configUpdateEnv, nil
+}
diff --git a/protoutil/commonutils_test.go b/protoutil/commonutils_test.go
new file mode 100644
index 000000000..1fcedd872
--- /dev/null
+++ b/protoutil/commonutils_test.go
@@ -0,0 +1,429 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil
+
+import (
+	"bytes"
+	"errors"
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/crypto"
+	cb "github.com/hyperledger/fabric/protos/common"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestNonceRandomness(t *testing.T) {
+	n1, err := CreateNonce()
+	if err != nil {
+		t.Fatal(err)
+	}
+	n2, err := CreateNonce()
+	if err != nil {
+		t.Fatal(err)
+	}
+	if bytes.Equal(n1, n2) {
+		t.Fatalf("Expected nonces to be different, got %x and %x", n1, n2)
+	}
+}
+
+func TestNonceLength(t *testing.T) {
+	n, err := CreateNonce()
+	if err != nil {
+		t.Fatal(err)
+	}
+	actual := len(n)
+	expected := crypto.NonceSize
+	if actual != expected {
+		t.Fatalf("Expected nonce to be of size %d, got %d instead", expected, actual)
+	}
+
+}
+
+func TestUnmarshalPayload(t *testing.T) {
+	var payload *cb.Payload
+	good, _ := proto.Marshal(&cb.Payload{
+		Data: []byte("payload"),
+	})
+	payload, err := UnmarshalPayload(good)
+	assert.NoError(t, err, "Unexpected error unmarshaling payload")
+	assert.NotNil(t, payload, "Payload should not be nil")
+	payload = UnmarshalPayloadOrPanic(good)
+	assert.NotNil(t, payload, "Payload should not be nil")
+
+	bad := []byte("bad payload")
+	assert.Panics(t, func() {
+		_ = UnmarshalPayloadOrPanic(bad)
+	}, "Expected panic unmarshaling malformed payload")
+
+}
+
+func TestUnmarshalEnvelope(t *testing.T) {
+	var env *cb.Envelope
+	good, _ := proto.Marshal(&cb.Envelope{})
+	env, err := UnmarshalEnvelope(good)
+	assert.NoError(t, err, "Unexpected error unmarshaling envelope")
+	assert.NotNil(t, env, "Envelope should not be nil")
+	env = UnmarshalEnvelopeOrPanic(good)
+	assert.NotNil(t, env, "Envelope should not be nil")
+
+	bad := []byte("bad envelope")
+	assert.Panics(t, func() {
+		_ = UnmarshalEnvelopeOrPanic(bad)
+	}, "Expected panic unmarshaling malformed envelope")
+
+}
+
+func TestUnmarshalBlock(t *testing.T) {
+	var env *cb.Block
+	good, _ := proto.Marshal(&cb.Block{})
+	env, err := UnmarshalBlock(good)
+	assert.NoError(t, err, "Unexpected error unmarshaling block")
+	assert.NotNil(t, env, "Block should not be nil")
+	env = UnmarshalBlockOrPanic(good)
+	assert.NotNil(t, env, "Block should not be nil")
+
+	bad := []byte("bad block")
+	assert.Panics(t, func() {
+		_ = UnmarshalBlockOrPanic(bad)
+	}, "Expected panic unmarshaling malformed block")
+
+}
+
+func TestUnmarshalEnvelopeOfType(t *testing.T) {
+	env := &cb.Envelope{}
+
+	env.Payload = []byte("bad payload")
+	_, err := UnmarshalEnvelopeOfType(env, cb.HeaderType_CONFIG, nil)
+	assert.Error(t, err, "Expected error unmarshaling malformed envelope")
+
+	payload, _ := proto.Marshal(&cb.Payload{
+		Header: nil,
+	})
+	env.Payload = payload
+	_, err = UnmarshalEnvelopeOfType(env, cb.HeaderType_CONFIG, nil)
+	assert.Error(t, err, "Expected error with missing payload header")
+
+	payload, _ = proto.Marshal(&cb.Payload{
+		Header: &cb.Header{
+			ChannelHeader: []byte("bad header"),
+		},
+	})
+	env.Payload = payload
+	_, err = UnmarshalEnvelopeOfType(env, cb.HeaderType_CONFIG, nil)
+	assert.Error(t, err, "Expected error for malformed channel header")
+
+	chdr, _ := proto.Marshal(&cb.ChannelHeader{
+		Type: int32(cb.HeaderType_CHAINCODE_PACKAGE),
+	})
+	payload, _ = proto.Marshal(&cb.Payload{
+		Header: &cb.Header{
+			ChannelHeader: chdr,
+		},
+	})
+	env.Payload = payload
+	_, err = UnmarshalEnvelopeOfType(env, cb.HeaderType_CONFIG, nil)
+	assert.Error(t, err, "Expected error for wrong channel header type")
+
+	chdr, _ = proto.Marshal(&cb.ChannelHeader{
+		Type: int32(cb.HeaderType_CONFIG),
+	})
+	payload, _ = proto.Marshal(&cb.Payload{
+		Header: &cb.Header{
+			ChannelHeader: chdr,
+		},
+		Data: []byte("bad data"),
+	})
+	env.Payload = payload
+	_, err = UnmarshalEnvelopeOfType(env, cb.HeaderType_CONFIG, &cb.ConfigEnvelope{})
+	assert.Error(t, err, "Expected error for malformed payload data")
+
+	chdr, _ = proto.Marshal(&cb.ChannelHeader{
+		Type: int32(cb.HeaderType_CONFIG),
+	})
+	configEnv, _ := proto.Marshal(&cb.ConfigEnvelope{})
+	payload, _ = proto.Marshal(&cb.Payload{
+		Header: &cb.Header{
+			ChannelHeader: chdr,
+		},
+		Data: configEnv,
+	})
+	env.Payload = payload
+	_, err = UnmarshalEnvelopeOfType(env, cb.HeaderType_CONFIG, &cb.ConfigEnvelope{})
+	assert.NoError(t, err, "Unexpected error unmarshaling envelope")
+
+}
+
+func TestExtractEnvelopeNilData(t *testing.T) {
+	block := &cb.Block{}
+	_, err := ExtractEnvelope(block, 0)
+	assert.Error(t, err, "Nil data")
+}
+
+func TestExtractEnvelopeWrongIndex(t *testing.T) {
+	block := testBlock()
+	if _, err := ExtractEnvelope(block, len(block.GetData().Data)); err == nil {
+		t.Fatal("Expected envelope extraction to fail (wrong index)")
+	}
+}
+
+func TestExtractEnvelopeWrongIndexOrPanic(t *testing.T) {
+	defer func() {
+		if r := recover(); r == nil {
+			t.Fatal("Expected envelope extraction to panic (wrong index)")
+		}
+	}()
+
+	block := testBlock()
+	ExtractEnvelopeOrPanic(block, len(block.GetData().Data))
+}
+
+func TestExtractEnvelope(t *testing.T) {
+	if envelope, err := ExtractEnvelope(testBlock(), 0); err != nil {
+		t.Fatalf("Expected envelop extraction to succeed: %s", err)
+	} else if !proto.Equal(envelope, testEnvelope()) {
+		t.Fatal("Expected extracted envelope to match test envelope")
+	}
+}
+
+func TestExtractEnvelopeOrPanic(t *testing.T) {
+	defer func() {
+		if r := recover(); r != nil {
+			t.Fatal("Expected envelope extraction to succeed")
+		}
+	}()
+
+	if !proto.Equal(ExtractEnvelopeOrPanic(testBlock(), 0), testEnvelope()) {
+		t.Fatal("Expected extracted envelope to match test envelope")
+	}
+}
+
+func TestExtractPayload(t *testing.T) {
+	if payload, err := ExtractPayload(testEnvelope()); err != nil {
+		t.Fatalf("Expected payload extraction to succeed: %s", err)
+	} else if !proto.Equal(payload, testPayload()) {
+		t.Fatal("Expected extracted payload to match test payload")
+	}
+}
+
+func TestExtractPayloadOrPanic(t *testing.T) {
+	defer func() {
+		if r := recover(); r != nil {
+			t.Fatal("Expected payload extraction to succeed")
+		}
+	}()
+
+	if !proto.Equal(ExtractPayloadOrPanic(testEnvelope()), testPayload()) {
+		t.Fatal("Expected extracted payload to match test payload")
+	}
+}
+
+func TestUnmarshalChaincodeID(t *testing.T) {
+	ccname := "mychaincode"
+	ccversion := "myversion"
+	ccidbytes, _ := proto.Marshal(&pb.ChaincodeID{
+		Name:    ccname,
+		Version: ccversion,
+	})
+	ccid, err := UnmarshalChaincodeID(ccidbytes)
+	assert.Equal(t, ccname, ccid.Name, "Expected ccid names to match")
+	assert.Equal(t, ccversion, ccid.Version, "Expected ccid versions to match")
+
+	_, err = UnmarshalChaincodeID([]byte("bad chaincodeID"))
+	assert.Error(t, err, "Expected error marshaling malformed chaincode ID")
+}
+
+func TestNewSignatureHeaderOrPanic(t *testing.T) {
+	var sigHeader *cb.SignatureHeader
+
+	sigHeader = NewSignatureHeaderOrPanic(goodSigner)
+	assert.NotNil(t, sigHeader, "Signature header should not be nil")
+
+	assert.Panics(t, func() {
+		_ = NewSignatureHeaderOrPanic(nil)
+	}, "Expected panic with nil signer")
+
+	assert.Panics(t, func() {
+		_ = NewSignatureHeaderOrPanic(badSigner)
+	}, "Expected panic with signature header error")
+
+}
+
+func TestSignOrPanic(t *testing.T) {
+	msg := []byte("sign me")
+	sig := SignOrPanic(goodSigner, msg)
+	// mock signer returns message to be signed
+	assert.Equal(t, msg, sig, "Signature does not match expected value")
+
+	assert.Panics(t, func() {
+		_ = SignOrPanic(nil, []byte("sign me"))
+	}, "Expected panic with nil signer")
+
+	assert.Panics(t, func() {
+		_ = SignOrPanic(badSigner, []byte("sign me"))
+	}, "Expected panic with sign error")
+}
+
+// Helper functions
+
+func testPayload() *cb.Payload {
+	return &cb.Payload{
+		Header: MakePayloadHeader(
+			MakeChannelHeader(cb.HeaderType_MESSAGE, int32(1), "test", 0),
+			MakeSignatureHeader([]byte("creator"), []byte("nonce"))),
+		Data: []byte("test"),
+	}
+}
+
+func testEnvelope() *cb.Envelope {
+	// No need to set the signature
+	return &cb.Envelope{Payload: MarshalOrPanic(testPayload())}
+}
+
+func testBlock() *cb.Block {
+	// No need to set the block's Header, or Metadata
+	return &cb.Block{
+		Data: &cb.BlockData{
+			Data: [][]byte{MarshalOrPanic(testEnvelope())},
+		},
+	}
+}
+
+// mock
+var badSigner = &mockLocalSigner{
+	returnError: true,
+}
+
+var goodSigner = &mockLocalSigner{
+	returnError: false,
+}
+
+type mockLocalSigner struct {
+	returnError bool
+}
+
+func (m *mockLocalSigner) NewSignatureHeader() (*cb.SignatureHeader, error) {
+	if m.returnError {
+		return nil, errors.New("signature header error")
+	}
+	return &cb.SignatureHeader{}, nil
+}
+
+func (m *mockLocalSigner) Sign(message []byte) ([]byte, error) {
+	if m.returnError {
+		return nil, errors.New("sign error")
+	}
+	return message, nil
+}
+
+func TestChannelHeader(t *testing.T) {
+	makeEnvelope := func(payload *cb.Payload) *cb.Envelope {
+		return &cb.Envelope{
+			Payload: MarshalOrPanic(payload),
+		}
+	}
+
+	_, err := ChannelHeader(makeEnvelope(&cb.Payload{
+		Header: &cb.Header{
+			ChannelHeader: MarshalOrPanic(&cb.ChannelHeader{
+				ChannelId: "foo",
+			}),
+		},
+	}))
+	assert.NoError(t, err, "Channel header was present")
+
+	_, err = ChannelHeader(makeEnvelope(&cb.Payload{
+		Header: &cb.Header{},
+	}))
+	assert.Error(t, err, "ChannelHeader was missing")
+
+	_, err = ChannelHeader(makeEnvelope(&cb.Payload{}))
+	assert.Error(t, err, "Header was missing")
+
+	_, err = ChannelHeader(&cb.Envelope{})
+	assert.Error(t, err, "Payload was missing")
+}
+
+func TestIsConfigBlock(t *testing.T) {
+	newBlock := func(env *cb.Envelope) *cb.Block {
+		return &cb.Block{
+			Data: &cb.BlockData{
+				Data: [][]byte{MarshalOrPanic(env)},
+			},
+		}
+	}
+
+	newConfigEnv := func(envType int32) *cb.Envelope {
+		return &cb.Envelope{
+			Payload: MarshalOrPanic(&cb.Payload{
+				Header: &cb.Header{
+					ChannelHeader: MarshalOrPanic(&cb.ChannelHeader{
+						Type:      envType,
+						ChannelId: "test-chain",
+					}),
+				},
+				Data: []byte("test bytes"),
+			}), // common.Payload
+		} // LastUpdate
+	}
+
+	// scenario 1: CONFIG envelope
+	envType := int32(cb.HeaderType_CONFIG)
+	env := newConfigEnv(envType)
+	block := newBlock(env)
+
+	result := IsConfigBlock(block)
+	assert.True(t, result, "IsConfigBlock returns true for blocks with CONFIG envelope")
+
+	// scenario 2: ORDERER_TRANSACTION envelope
+	envType = int32(cb.HeaderType_ORDERER_TRANSACTION)
+	env = newConfigEnv(envType)
+	block = newBlock(env)
+
+	result = IsConfigBlock(block)
+	assert.True(t, result, "IsConfigBlock returns true for blocks with ORDERER_TRANSACTION envelope")
+
+	// scenario 3: MESSAGE envelope
+	envType = int32(cb.HeaderType_MESSAGE)
+	env = newConfigEnv(envType)
+	block = newBlock(env)
+
+	result = IsConfigBlock(block)
+	assert.False(t, result, "IsConfigBlock returns false for blocks with MESSAGE envelope")
+}
+
+func TestEnvelopeToConfigUpdate(t *testing.T) {
+
+	makeEnv := func(data []byte) *cb.Envelope {
+		return &cb.Envelope{
+			Payload: MarshalOrPanic(&cb.Payload{
+				Header: &cb.Header{
+					ChannelHeader: MarshalOrPanic(&cb.ChannelHeader{
+						Type:      int32(cb.HeaderType_CONFIG_UPDATE),
+						ChannelId: "test-chain",
+					}),
+				},
+				Data: data,
+			}), // common.Payload
+		} // LastUpdate
+	}
+
+	// scenario 1: for valid envelopes
+	configUpdateEnv := &cb.ConfigUpdateEnvelope{}
+	env := makeEnv(MarshalOrPanic(configUpdateEnv))
+	result, err := EnvelopeToConfigUpdate(env)
+
+	assert.NoError(t, err, "EnvelopeToConfigUpdate runs without error for valid CONFIG_UPDATE envelope")
+	assert.Equal(t, configUpdateEnv, result, "Correct configUpdateEnvelope returned")
+
+	// scenario 2: for invalid envelopes
+	env = makeEnv([]byte("test bytes"))
+	_, err = EnvelopeToConfigUpdate(env)
+
+	assert.Error(t, err, "EnvelopeToConfigUpdate fails with error for invalid CONFIG_UPDATE envelope")
+}
diff --git a/protoutil/proputils.go b/protoutil/proputils.go
new file mode 100644
index 000000000..7b67b0a6b
--- /dev/null
+++ b/protoutil/proputils.go
@@ -0,0 +1,642 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil
+
+import (
+	"encoding/binary"
+	"encoding/hex"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/bccsp"
+	"github.com/hyperledger/fabric/bccsp/factory"
+	"github.com/hyperledger/fabric/common/crypto"
+	"github.com/hyperledger/fabric/common/util"
+	"github.com/hyperledger/fabric/core/chaincode/platforms"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/pkg/errors"
+)
+
+// GetChaincodeInvocationSpec get the ChaincodeInvocationSpec from the proposal
+func GetChaincodeInvocationSpec(prop *peer.Proposal) (*peer.ChaincodeInvocationSpec, error) {
+	if prop == nil {
+		return nil, errors.New("proposal is nil")
+	}
+	_, err := GetHeader(prop.Header)
+	if err != nil {
+		return nil, err
+	}
+	ccPropPayload, err := GetChaincodeProposalPayload(prop.Payload)
+	if err != nil {
+		return nil, err
+	}
+	cis := &peer.ChaincodeInvocationSpec{}
+	err = proto.Unmarshal(ccPropPayload.Input, cis)
+	return cis, errors.Wrap(err, "error unmarshaling ChaincodeInvocationSpec")
+}
+
+// GetChaincodeProposalContext returns creator and transient
+func GetChaincodeProposalContext(prop *peer.Proposal) ([]byte, map[string][]byte, error) {
+	if prop == nil {
+		return nil, nil, errors.New("proposal is nil")
+	}
+	if len(prop.Header) == 0 {
+		return nil, nil, errors.New("proposal's header is nil")
+	}
+	if len(prop.Payload) == 0 {
+		return nil, nil, errors.New("proposal's payload is nil")
+	}
+	// get back the header
+	hdr, err := GetHeader(prop.Header)
+	if err != nil {
+		return nil, nil, errors.WithMessage(err, "error extracting header from proposal")
+	}
+	if hdr == nil {
+		return nil, nil, errors.New("unmarshaled header is nil")
+	}
+
+	chdr, err := UnmarshalChannelHeader(hdr.ChannelHeader)
+	if err != nil {
+		return nil, nil, errors.WithMessage(err, "error extracting channel header from proposal")
+	}
+
+	if err = validateChannelHeaderType(chdr, []common.HeaderType{common.HeaderType_ENDORSER_TRANSACTION, common.HeaderType_CONFIG}); err != nil {
+		return nil, nil, errors.WithMessage(err, "invalid proposal")
+	}
+
+	shdr, err := GetSignatureHeader(hdr.SignatureHeader)
+	if err != nil {
+		return nil, nil, errors.WithMessage(err, "error extracting signature header from proposal")
+	}
+
+	ccPropPayload, err := GetChaincodeProposalPayload(prop.Payload)
+	if err != nil {
+		return nil, nil, err
+	}
+
+	return shdr.Creator, ccPropPayload.TransientMap, nil
+}
+
+func validateChannelHeaderType(chdr *common.ChannelHeader, expectedTypes []common.HeaderType) error {
+	for _, t := range expectedTypes {
+		if common.HeaderType(chdr.Type) == t {
+			return nil
+		}
+	}
+	return errors.Errorf("invalid channel header type. expected one of %s, received %s", expectedTypes, common.HeaderType(chdr.Type))
+}
+
+// GetHeader Get Header from bytes
+func GetHeader(bytes []byte) (*common.Header, error) {
+	hdr := &common.Header{}
+	err := proto.Unmarshal(bytes, hdr)
+	return hdr, errors.Wrap(err, "error unmarshaling Header")
+}
+
+// GetNonce returns the nonce used in Proposal
+func GetNonce(prop *peer.Proposal) ([]byte, error) {
+	if prop == nil {
+		return nil, errors.New("proposal is nil")
+	}
+
+	// get back the header
+	hdr, err := GetHeader(prop.Header)
+	if err != nil {
+		return nil, err
+	}
+
+	chdr, err := UnmarshalChannelHeader(hdr.ChannelHeader)
+	if err != nil {
+		return nil, err
+	}
+
+	if err = validateChannelHeaderType(chdr, []common.HeaderType{common.HeaderType_ENDORSER_TRANSACTION, common.HeaderType_CONFIG}); err != nil {
+		return nil, errors.WithMessage(err, "invalid proposal")
+	}
+
+	shdr, err := GetSignatureHeader(hdr.SignatureHeader)
+	if err != nil {
+		return nil, err
+	}
+
+	if hdr.SignatureHeader == nil {
+		return nil, errors.New("invalid signature header. cannot be nil")
+	}
+
+	return shdr.Nonce, nil
+}
+
+// GetChaincodeHeaderExtension get chaincode header extension given header
+func GetChaincodeHeaderExtension(hdr *common.Header) (*peer.ChaincodeHeaderExtension, error) {
+	chdr, err := UnmarshalChannelHeader(hdr.ChannelHeader)
+	if err != nil {
+		return nil, err
+	}
+
+	chaincodeHdrExt := &peer.ChaincodeHeaderExtension{}
+	err = proto.Unmarshal(chdr.Extension, chaincodeHdrExt)
+	return chaincodeHdrExt, errors.Wrap(err, "error unmarshaling ChaincodeHeaderExtension")
+}
+
+// GetProposalResponse given proposal in bytes
+func GetProposalResponse(prBytes []byte) (*peer.ProposalResponse, error) {
+	proposalResponse := &peer.ProposalResponse{}
+	err := proto.Unmarshal(prBytes, proposalResponse)
+	return proposalResponse, errors.Wrap(err, "error unmarshaling ProposalResponse")
+}
+
+// GetChaincodeDeploymentSpec returns a ChaincodeDeploymentSpec given args
+func GetChaincodeDeploymentSpec(code []byte, pr *platforms.Registry) (*peer.ChaincodeDeploymentSpec, error) {
+	cds := &peer.ChaincodeDeploymentSpec{}
+	err := proto.Unmarshal(code, cds)
+	if err != nil {
+		return nil, errors.Wrap(err, "error unmarshaling ChaincodeDeploymentSpec")
+	}
+
+	// FAB-2122: Validate the CDS according to platform specific requirements
+	return cds, pr.ValidateDeploymentSpec(cds.CCType(), cds.Bytes())
+}
+
+// GetChaincodeAction gets the ChaincodeAction given chaicnode action bytes
+func GetChaincodeAction(caBytes []byte) (*peer.ChaincodeAction, error) {
+	chaincodeAction := &peer.ChaincodeAction{}
+	err := proto.Unmarshal(caBytes, chaincodeAction)
+	return chaincodeAction, errors.Wrap(err, "error unmarshaling ChaincodeAction")
+}
+
+// GetResponse gets the Response given response bytes
+func GetResponse(resBytes []byte) (*peer.Response, error) {
+	response := &peer.Response{}
+	err := proto.Unmarshal(resBytes, response)
+	return response, errors.Wrap(err, "error unmarshaling Response")
+}
+
+// GetChaincodeEvents gets the ChaincodeEvents given chaincode event bytes
+func GetChaincodeEvents(eBytes []byte) (*peer.ChaincodeEvent, error) {
+	chaincodeEvent := &peer.ChaincodeEvent{}
+	err := proto.Unmarshal(eBytes, chaincodeEvent)
+	return chaincodeEvent, errors.Wrap(err, "error unmarshaling ChaicnodeEvent")
+}
+
+// GetProposalResponsePayload gets the proposal response payload
+func GetProposalResponsePayload(prpBytes []byte) (*peer.ProposalResponsePayload, error) {
+	prp := &peer.ProposalResponsePayload{}
+	err := proto.Unmarshal(prpBytes, prp)
+	return prp, errors.Wrap(err, "error unmarshaling ProposalResponsePayload")
+}
+
+// GetProposal returns a Proposal message from its bytes
+func GetProposal(propBytes []byte) (*peer.Proposal, error) {
+	prop := &peer.Proposal{}
+	err := proto.Unmarshal(propBytes, prop)
+	return prop, errors.Wrap(err, "error unmarshaling Proposal")
+}
+
+// GetPayload Get Payload from Envelope message
+func GetPayload(e *common.Envelope) (*common.Payload, error) {
+	payload := &common.Payload{}
+	err := proto.Unmarshal(e.Payload, payload)
+	return payload, errors.Wrap(err, "error unmarshaling Payload")
+}
+
+// GetTransaction Get Transaction from bytes
+func GetTransaction(txBytes []byte) (*peer.Transaction, error) {
+	tx := &peer.Transaction{}
+	err := proto.Unmarshal(txBytes, tx)
+	return tx, errors.Wrap(err, "error unmarshaling Transaction")
+
+}
+
+// GetChaincodeActionPayload Get ChaincodeActionPayload from bytes
+func GetChaincodeActionPayload(capBytes []byte) (*peer.ChaincodeActionPayload, error) {
+	cap := &peer.ChaincodeActionPayload{}
+	err := proto.Unmarshal(capBytes, cap)
+	return cap, errors.Wrap(err, "error unmarshaling ChaincodeActionPayload")
+}
+
+// GetChaincodeProposalPayload Get ChaincodeProposalPayload from bytes
+func GetChaincodeProposalPayload(bytes []byte) (*peer.ChaincodeProposalPayload, error) {
+	cpp := &peer.ChaincodeProposalPayload{}
+	err := proto.Unmarshal(bytes, cpp)
+	return cpp, errors.Wrap(err, "error unmarshaling ChaincodeProposalPayload")
+}
+
+// GetSignatureHeader Get SignatureHeader from bytes
+func GetSignatureHeader(bytes []byte) (*common.SignatureHeader, error) {
+	sh := &common.SignatureHeader{}
+	err := proto.Unmarshal(bytes, sh)
+	return sh, errors.Wrap(err, "error unmarshaling SignatureHeader")
+}
+
+// CreateChaincodeProposal creates a proposal from given input.
+// It returns the proposal and the transaction id associated to the proposal
+func CreateChaincodeProposal(typ common.HeaderType, chainID string, cis *peer.ChaincodeInvocationSpec, creator []byte) (*peer.Proposal, string, error) {
+	return CreateChaincodeProposalWithTransient(typ, chainID, cis, creator, nil)
+}
+
+// CreateChaincodeProposalWithTransient creates a proposal from given input
+// It returns the proposal and the transaction id associated to the proposal
+func CreateChaincodeProposalWithTransient(typ common.HeaderType, chainID string, cis *peer.ChaincodeInvocationSpec, creator []byte, transientMap map[string][]byte) (*peer.Proposal, string, error) {
+	// generate a random nonce
+	nonce, err := crypto.GetRandomNonce()
+	if err != nil {
+		return nil, "", err
+	}
+
+	// compute txid
+	txid, err := ComputeTxID(nonce, creator)
+	if err != nil {
+		return nil, "", err
+	}
+
+	return CreateChaincodeProposalWithTxIDNonceAndTransient(txid, typ, chainID, cis, nonce, creator, transientMap)
+}
+
+// CreateChaincodeProposalWithTxIDAndTransient creates a proposal from given
+// input. It returns the proposal and the transaction id associated with the
+// proposal
+func CreateChaincodeProposalWithTxIDAndTransient(typ common.HeaderType, chainID string, cis *peer.ChaincodeInvocationSpec, creator []byte, txid string, transientMap map[string][]byte) (*peer.Proposal, string, error) {
+	// generate a random nonce
+	nonce, err := crypto.GetRandomNonce()
+	if err != nil {
+		return nil, "", err
+	}
+
+	// compute txid unless provided by tests
+	if txid == "" {
+		txid, err = ComputeTxID(nonce, creator)
+		if err != nil {
+			return nil, "", err
+		}
+	}
+
+	return CreateChaincodeProposalWithTxIDNonceAndTransient(txid, typ, chainID, cis, nonce, creator, transientMap)
+}
+
+// CreateChaincodeProposalWithTxIDNonceAndTransient creates a proposal from
+// given input
+func CreateChaincodeProposalWithTxIDNonceAndTransient(txid string, typ common.HeaderType, chainID string, cis *peer.ChaincodeInvocationSpec, nonce, creator []byte, transientMap map[string][]byte) (*peer.Proposal, string, error) {
+	ccHdrExt := &peer.ChaincodeHeaderExtension{ChaincodeId: cis.ChaincodeSpec.ChaincodeId}
+	ccHdrExtBytes, err := proto.Marshal(ccHdrExt)
+	if err != nil {
+		return nil, "", errors.Wrap(err, "error marshaling ChaincodeHeaderExtension")
+	}
+
+	cisBytes, err := proto.Marshal(cis)
+	if err != nil {
+		return nil, "", errors.Wrap(err, "error marshaling ChaincodeInvocationSpec")
+	}
+
+	ccPropPayload := &peer.ChaincodeProposalPayload{Input: cisBytes, TransientMap: transientMap}
+	ccPropPayloadBytes, err := proto.Marshal(ccPropPayload)
+	if err != nil {
+		return nil, "", errors.Wrap(err, "error marshaling ChaincodeProposalPayload")
+	}
+
+	// TODO: epoch is now set to zero. This must be changed once we
+	// get a more appropriate mechanism to handle it in.
+	var epoch uint64
+
+	timestamp := util.CreateUtcTimestamp()
+
+	hdr := &common.Header{
+		ChannelHeader: MarshalOrPanic(
+			&common.ChannelHeader{
+				Type:      int32(typ),
+				TxId:      txid,
+				Timestamp: timestamp,
+				ChannelId: chainID,
+				Extension: ccHdrExtBytes,
+				Epoch:     epoch,
+			},
+		),
+		SignatureHeader: MarshalOrPanic(
+			&common.SignatureHeader{
+				Nonce:   nonce,
+				Creator: creator,
+			},
+		),
+	}
+
+	hdrBytes, err := proto.Marshal(hdr)
+	if err != nil {
+		return nil, "", err
+	}
+
+	prop := &peer.Proposal{
+		Header:  hdrBytes,
+		Payload: ccPropPayloadBytes,
+	}
+	return prop, txid, nil
+}
+
+// GetBytesProposalResponsePayload gets proposal response payload
+func GetBytesProposalResponsePayload(hash []byte, response *peer.Response, result []byte, event []byte, ccid *peer.ChaincodeID) ([]byte, error) {
+	cAct := &peer.ChaincodeAction{
+		Events: event, Results: result,
+		Response:    response,
+		ChaincodeId: ccid,
+	}
+	cActBytes, err := proto.Marshal(cAct)
+	if err != nil {
+		return nil, errors.Wrap(err, "error marshaling ChaincodeAction")
+	}
+
+	prp := &peer.ProposalResponsePayload{
+		Extension:    cActBytes,
+		ProposalHash: hash,
+	}
+	prpBytes, err := proto.Marshal(prp)
+	return prpBytes, errors.Wrap(err, "error marshaling ProposalResponsePayload")
+}
+
+// GetBytesChaincodeProposalPayload gets the chaincode proposal payload
+func GetBytesChaincodeProposalPayload(cpp *peer.ChaincodeProposalPayload) ([]byte, error) {
+	cppBytes, err := proto.Marshal(cpp)
+	return cppBytes, errors.Wrap(err, "error marshaling ChaincodeProposalPayload")
+}
+
+// GetBytesResponse gets the bytes of Response
+func GetBytesResponse(res *peer.Response) ([]byte, error) {
+	resBytes, err := proto.Marshal(res)
+	return resBytes, errors.Wrap(err, "error marshaling Response")
+}
+
+// GetBytesChaincodeEvent gets the bytes of ChaincodeEvent
+func GetBytesChaincodeEvent(event *peer.ChaincodeEvent) ([]byte, error) {
+	eventBytes, err := proto.Marshal(event)
+	return eventBytes, errors.Wrap(err, "error marshaling ChaincodeEvent")
+}
+
+// GetBytesChaincodeActionPayload get the bytes of ChaincodeActionPayload from
+// the message
+func GetBytesChaincodeActionPayload(cap *peer.ChaincodeActionPayload) ([]byte, error) {
+	capBytes, err := proto.Marshal(cap)
+	return capBytes, errors.Wrap(err, "error marshaling ChaincodeActionPayload")
+}
+
+// GetBytesProposalResponse gets proposal bytes response
+func GetBytesProposalResponse(pr *peer.ProposalResponse) ([]byte, error) {
+	respBytes, err := proto.Marshal(pr)
+	return respBytes, errors.Wrap(err, "error marshaling ProposalResponse")
+}
+
+// GetBytesProposal returns the bytes of a proposal message
+func GetBytesProposal(prop *peer.Proposal) ([]byte, error) {
+	propBytes, err := proto.Marshal(prop)
+	return propBytes, errors.Wrap(err, "error marshaling Proposal")
+}
+
+// GetBytesHeader get the bytes of Header from the message
+func GetBytesHeader(hdr *common.Header) ([]byte, error) {
+	bytes, err := proto.Marshal(hdr)
+	return bytes, errors.Wrap(err, "error marshaling Header")
+}
+
+// GetBytesSignatureHeader get the bytes of SignatureHeader from the message
+func GetBytesSignatureHeader(hdr *common.SignatureHeader) ([]byte, error) {
+	bytes, err := proto.Marshal(hdr)
+	return bytes, errors.Wrap(err, "error marshaling SignatureHeader")
+}
+
+// GetBytesTransaction get the bytes of Transaction from the message
+func GetBytesTransaction(tx *peer.Transaction) ([]byte, error) {
+	bytes, err := proto.Marshal(tx)
+	return bytes, errors.Wrap(err, "error unmarshaling Transaction")
+}
+
+// GetBytesPayload get the bytes of Payload from the message
+func GetBytesPayload(payl *common.Payload) ([]byte, error) {
+	bytes, err := proto.Marshal(payl)
+	return bytes, errors.Wrap(err, "error marshaling Payload")
+}
+
+// GetBytesEnvelope get the bytes of Envelope from the message
+func GetBytesEnvelope(env *common.Envelope) ([]byte, error) {
+	bytes, err := proto.Marshal(env)
+	return bytes, errors.Wrap(err, "error marshaling Envelope")
+}
+
+// GetActionFromEnvelope extracts a ChaincodeAction message from a
+// serialized Envelope
+// TODO: fix function name as per FAB-11831
+func GetActionFromEnvelope(envBytes []byte) (*peer.ChaincodeAction, error) {
+	env, err := GetEnvelopeFromBlock(envBytes)
+	if err != nil {
+		return nil, err
+	}
+	return GetActionFromEnvelopeMsg(env)
+}
+
+func GetActionFromEnvelopeMsg(env *common.Envelope) (*peer.ChaincodeAction, error) {
+	payl, err := GetPayload(env)
+	if err != nil {
+		return nil, err
+	}
+
+	tx, err := GetTransaction(payl.Data)
+	if err != nil {
+		return nil, err
+	}
+
+	if len(tx.Actions) == 0 {
+		return nil, errors.New("at least one TransactionAction required")
+	}
+
+	_, respPayload, err := GetPayloads(tx.Actions[0])
+	return respPayload, err
+}
+
+// CreateProposalFromCISAndTxid returns a proposal given a serialized identity
+// and a ChaincodeInvocationSpec
+func CreateProposalFromCISAndTxid(txid string, typ common.HeaderType, chainID string, cis *peer.ChaincodeInvocationSpec, creator []byte) (*peer.Proposal, string, error) {
+	nonce, err := crypto.GetRandomNonce()
+	if err != nil {
+		return nil, "", err
+	}
+	return CreateChaincodeProposalWithTxIDNonceAndTransient(txid, typ, chainID, cis, nonce, creator, nil)
+}
+
+// CreateProposalFromCIS returns a proposal given a serialized identity and a
+// ChaincodeInvocationSpec
+func CreateProposalFromCIS(typ common.HeaderType, chainID string, cis *peer.ChaincodeInvocationSpec, creator []byte) (*peer.Proposal, string, error) {
+	return CreateChaincodeProposal(typ, chainID, cis, creator)
+}
+
+// CreateGetChaincodesProposal returns a GETCHAINCODES proposal given a
+// serialized identity
+func CreateGetChaincodesProposal(chainID string, creator []byte) (*peer.Proposal, string, error) {
+	ccinp := &peer.ChaincodeInput{Args: [][]byte{[]byte("getchaincodes")}}
+	lsccSpec := &peer.ChaincodeInvocationSpec{
+		ChaincodeSpec: &peer.ChaincodeSpec{
+			Type:        peer.ChaincodeSpec_GOLANG,
+			ChaincodeId: &peer.ChaincodeID{Name: "lscc"},
+			Input:       ccinp,
+		},
+	}
+	return CreateProposalFromCIS(common.HeaderType_ENDORSER_TRANSACTION, chainID, lsccSpec, creator)
+}
+
+// CreateGetInstalledChaincodesProposal returns a GETINSTALLEDCHAINCODES
+// proposal given a serialized identity
+func CreateGetInstalledChaincodesProposal(creator []byte) (*peer.Proposal, string, error) {
+	ccinp := &peer.ChaincodeInput{Args: [][]byte{[]byte("getinstalledchaincodes")}}
+	lsccSpec := &peer.ChaincodeInvocationSpec{
+		ChaincodeSpec: &peer.ChaincodeSpec{
+			Type:        peer.ChaincodeSpec_GOLANG,
+			ChaincodeId: &peer.ChaincodeID{Name: "lscc"},
+			Input:       ccinp,
+		},
+	}
+	return CreateProposalFromCIS(common.HeaderType_ENDORSER_TRANSACTION, "", lsccSpec, creator)
+}
+
+// CreateInstallProposalFromCDS returns a install proposal given a serialized
+// identity and a ChaincodeDeploymentSpec
+func CreateInstallProposalFromCDS(ccpack proto.Message, creator []byte) (*peer.Proposal, string, error) {
+	return createProposalFromCDS("", ccpack, creator, "install")
+}
+
+// CreateDeployProposalFromCDS returns a deploy proposal given a serialized
+// identity and a ChaincodeDeploymentSpec
+func CreateDeployProposalFromCDS(
+	chainID string,
+	cds *peer.ChaincodeDeploymentSpec,
+	creator []byte,
+	policy []byte,
+	escc []byte,
+	vscc []byte,
+	collectionConfig []byte) (*peer.Proposal, string, error) {
+	if collectionConfig == nil {
+		return createProposalFromCDS(chainID, cds, creator, "deploy", policy, escc, vscc)
+	}
+	return createProposalFromCDS(chainID, cds, creator, "deploy", policy, escc, vscc, collectionConfig)
+}
+
+// CreateUpgradeProposalFromCDS returns a upgrade proposal given a serialized
+// identity and a ChaincodeDeploymentSpec
+func CreateUpgradeProposalFromCDS(
+	chainID string,
+	cds *peer.ChaincodeDeploymentSpec,
+	creator []byte,
+	policy []byte,
+	escc []byte,
+	vscc []byte,
+	collectionConfig []byte) (*peer.Proposal, string, error) {
+	if collectionConfig == nil {
+		return createProposalFromCDS(chainID, cds, creator, "upgrade", policy, escc, vscc)
+	}
+	return createProposalFromCDS(chainID, cds, creator, "upgrade", policy, escc, vscc, collectionConfig)
+}
+
+// createProposalFromCDS returns a deploy or upgrade proposal given a
+// serialized identity and a ChaincodeDeploymentSpec
+func createProposalFromCDS(chainID string, msg proto.Message, creator []byte, propType string, args ...[]byte) (*peer.Proposal, string, error) {
+	// in the new mode, cds will be nil, "deploy" and "upgrade" are instantiates.
+	var ccinp *peer.ChaincodeInput
+	var b []byte
+	var err error
+	if msg != nil {
+		b, err = proto.Marshal(msg)
+		if err != nil {
+			return nil, "", err
+		}
+	}
+	switch propType {
+	case "deploy":
+		fallthrough
+	case "upgrade":
+		cds, ok := msg.(*peer.ChaincodeDeploymentSpec)
+		if !ok || cds == nil {
+			return nil, "", errors.New("invalid message for creating lifecycle chaincode proposal")
+		}
+		Args := [][]byte{[]byte(propType), []byte(chainID), b}
+		Args = append(Args, args...)
+
+		ccinp = &peer.ChaincodeInput{Args: Args}
+	case "install":
+		ccinp = &peer.ChaincodeInput{Args: [][]byte{[]byte(propType), b}}
+	}
+
+	// wrap the deployment in an invocation spec to lscc...
+	lsccSpec := &peer.ChaincodeInvocationSpec{
+		ChaincodeSpec: &peer.ChaincodeSpec{
+			Type:        peer.ChaincodeSpec_GOLANG,
+			ChaincodeId: &peer.ChaincodeID{Name: "lscc"},
+			Input:       ccinp,
+		},
+	}
+
+	// ...and get the proposal for it
+	return CreateProposalFromCIS(common.HeaderType_ENDORSER_TRANSACTION, chainID, lsccSpec, creator)
+}
+
+// ComputeTxID computes TxID as the Hash computed
+// over the concatenation of nonce and creator.
+func ComputeTxID(nonce, creator []byte) (string, error) {
+	// TODO: Get the Hash function to be used from
+	// channel configuration
+	digest, err := factory.GetDefault().Hash(
+		append(nonce, creator...),
+		&bccsp.SHA256Opts{})
+	if err != nil {
+		return "", err
+	}
+	return hex.EncodeToString(digest), nil
+}
+
+// CheckTxID checks that txid is equal to the Hash computed
+// over the concatenation of nonce and creator.
+func CheckTxID(txid string, nonce, creator []byte) error {
+	computedTxID, err := ComputeTxID(nonce, creator)
+	if err != nil {
+		return errors.WithMessage(err, "error computing target txid")
+	}
+
+	if txid != computedTxID {
+		return errors.Errorf("invalid txid. got [%s], expected [%s]", txid, computedTxID)
+	}
+
+	return nil
+}
+
+// ComputeProposalBinding computes the binding of a proposal
+func ComputeProposalBinding(proposal *peer.Proposal) ([]byte, error) {
+	if proposal == nil {
+		return nil, errors.New("proposal is nil")
+	}
+	if len(proposal.Header) == 0 {
+		return nil, errors.New("proposal's header is nil")
+	}
+
+	h, err := GetHeader(proposal.Header)
+	if err != nil {
+		return nil, err
+	}
+
+	chdr, err := UnmarshalChannelHeader(h.ChannelHeader)
+	if err != nil {
+		return nil, err
+	}
+	shdr, err := GetSignatureHeader(h.SignatureHeader)
+	if err != nil {
+		return nil, err
+	}
+
+	return computeProposalBindingInternal(shdr.Nonce, shdr.Creator, chdr.Epoch)
+}
+
+func computeProposalBindingInternal(nonce, creator []byte, epoch uint64) ([]byte, error) {
+	epochBytes := make([]byte, 8)
+	binary.LittleEndian.PutUint64(epochBytes, epoch)
+
+	// TODO: add to genesis block the hash function used for
+	// the binding computation
+	return factory.GetDefault().Hash(
+		append(append(nonce, creator...), epochBytes...),
+		&bccsp.SHA256Opts{})
+}
diff --git a/protoutil/proputils_test.go b/protoutil/proputils_test.go
new file mode 100644
index 000000000..ca7f54b3c
--- /dev/null
+++ b/protoutil/proputils_test.go
@@ -0,0 +1,665 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil
+
+import (
+	"bytes"
+	"crypto/sha256"
+	"encoding/hex"
+	"fmt"
+	"os"
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/util"
+	"github.com/hyperledger/fabric/core/chaincode/platforms"
+	"github.com/hyperledger/fabric/core/chaincode/platforms/golang"
+	"github.com/hyperledger/fabric/msp"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	msptesttools "github.com/hyperledger/fabric/msp/mgmt/testtools"
+	"github.com/hyperledger/fabric/protos/common"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/hyperledger/fabric/protos/utils"
+	"github.com/stretchr/testify/assert"
+)
+
+func createCIS() *pb.ChaincodeInvocationSpec {
+	return &pb.ChaincodeInvocationSpec{
+		ChaincodeSpec: &pb.ChaincodeSpec{
+			Type:        pb.ChaincodeSpec_GOLANG,
+			ChaincodeId: &pb.ChaincodeID{Name: "chaincode_name"},
+			Input:       &pb.ChaincodeInput{Args: [][]byte{[]byte("arg1"), []byte("arg2")}}}}
+}
+
+func TestNilProposal(t *testing.T) {
+	// pass nil to all function which accept *peer.Proposal
+	_, err := utils.GetChaincodeInvocationSpec(nil)
+	assert.Error(t, err, "Expected error with nil proposal")
+	_, _, err = utils.GetChaincodeProposalContext(nil)
+	assert.Error(t, err, "Expected error with nil proposal")
+	_, err = utils.GetNonce(nil)
+	assert.Error(t, err, "Expected error with nil proposal")
+	_, err = utils.GetBytesProposal(nil)
+	assert.Error(t, err, "Expected error with nil proposal")
+	_, err = utils.ComputeProposalBinding(nil)
+	assert.Error(t, err, "Expected error with nil proposal")
+}
+
+func TestBadProposalHeaders(t *testing.T) {
+	// NOTE:  There is a lot of repetitive proposal validation code
+	// in multiple functions which should be refactored in the future.
+	// For now, simply consolidating the test cases
+
+	// empty header
+	prop := &pb.Proposal{
+		Header: []byte{},
+	}
+	_, _, err := utils.GetChaincodeProposalContext(prop)
+	assert.Error(t, err, "Expected error with empty proposal header")
+	_, err = utils.ComputeProposalBinding(prop)
+	assert.Error(t, err, "Expected error with empty proposal header")
+
+	// empty payload
+	prop = &pb.Proposal{
+		Header: []byte("header"),
+	}
+	_, _, err = utils.GetChaincodeProposalContext(prop)
+	assert.Error(t, err, "Expected error with empty proposal payload")
+
+	// malformed proposal header
+	prop = &pb.Proposal{
+		Header:  []byte("bad header"),
+		Payload: []byte("payload"),
+	}
+	_, err = utils.GetHeader(prop.Header)
+	assert.Error(t, err, "Expected error with malformed proposal header")
+	_, err = utils.GetChaincodeInvocationSpec(prop)
+	assert.Error(t, err, "Expected error with malformed proposal header")
+	_, _, err = utils.GetChaincodeProposalContext(prop)
+	assert.Error(t, err, "Expected error with malformed proposal header")
+	_, err = utils.GetNonce(prop)
+	assert.Error(t, err, "Expected error with malformed proposal header")
+	_, err = utils.ComputeProposalBinding(prop)
+	assert.Error(t, err, "Expected error with malformed proposal header")
+
+	// malformed signature header
+	chdr, _ := proto.Marshal(&common.ChannelHeader{
+		Type: int32(common.HeaderType_ENDORSER_TRANSACTION),
+	})
+	hdr := &common.Header{
+		ChannelHeader:   chdr,
+		SignatureHeader: []byte("bad signature header"),
+	}
+	_, err = utils.GetSignatureHeader(hdr.SignatureHeader)
+	assert.Error(t, err, "Expected error with malformed signature header")
+	hdrBytes, _ := proto.Marshal(hdr)
+	prop.Header = hdrBytes
+	_, err = utils.GetChaincodeInvocationSpec(prop)
+	assert.Error(t, err, "Expected error with malformed signature header")
+	_, _, err = utils.GetChaincodeProposalContext(prop)
+	assert.Error(t, err, "Expected error with malformed signature header")
+	_, err = utils.GetNonce(prop)
+	assert.Error(t, err, "Expected error with malformed signature header")
+	_, err = utils.ComputeProposalBinding(prop)
+	assert.Error(t, err, "Expected error with malformed signature header")
+
+	// wrong channel header type
+	chdr, _ = proto.Marshal(&common.ChannelHeader{
+		Type: int32(common.HeaderType_DELIVER_SEEK_INFO),
+	})
+	hdr.ChannelHeader = chdr
+	hdrBytes, _ = proto.Marshal(hdr)
+	prop.Header = hdrBytes
+	_, _, err = utils.GetChaincodeProposalContext(prop)
+	assert.Error(t, err, "Expected error with wrong header type")
+	assert.Contains(t, err.Error(), "invalid proposal: invalid channel header type")
+	_, err = utils.GetNonce(prop)
+	assert.Error(t, err, "Expected error with wrong header type")
+
+	// malformed channel header
+	hdr.ChannelHeader = []byte("bad channel header")
+	hdrBytes, _ = proto.Marshal(hdr)
+	prop.Header = hdrBytes
+	_, _, err = utils.GetChaincodeProposalContext(prop)
+	assert.Error(t, err, "Expected error with malformed channel header")
+	_, err = utils.GetNonce(prop)
+	assert.Error(t, err, "Expected error with malformed channel header")
+	_, err = utils.GetChaincodeHeaderExtension(hdr)
+	assert.Error(t, err, "Expected error with malformed channel header")
+	_, err = utils.ComputeProposalBinding(prop)
+	assert.Error(t, err, "Expected error with malformed channel header")
+
+}
+
+func TestGetNonce(t *testing.T) {
+	chdr, _ := proto.Marshal(&common.ChannelHeader{
+		Type: int32(common.HeaderType_ENDORSER_TRANSACTION),
+	})
+	hdr, _ := proto.Marshal(&common.Header{
+		ChannelHeader:   chdr,
+		SignatureHeader: []byte{},
+	})
+	prop := &pb.Proposal{
+		Header: hdr,
+	}
+	_, err := utils.GetNonce(prop)
+	assert.Error(t, err, "Expected error with nil signature header")
+
+	shdr, _ := proto.Marshal(&common.SignatureHeader{
+		Nonce: []byte("nonce"),
+	})
+	hdr, _ = proto.Marshal(&common.Header{
+		ChannelHeader:   chdr,
+		SignatureHeader: shdr,
+	})
+	prop = &pb.Proposal{
+		Header: hdr,
+	}
+	nonce, err := utils.GetNonce(prop)
+	assert.NoError(t, err, "Unexpected error getting nonce")
+	assert.Equal(t, "nonce", string(nonce), "Failed to return the expected nonce")
+
+}
+
+func TestGetChaincodeDeploymentSpec(t *testing.T) {
+	pr := platforms.NewRegistry(&golang.Platform{})
+
+	_, err := utils.GetChaincodeDeploymentSpec([]byte("bad spec"), pr)
+	assert.Error(t, err, "Expected error with malformed spec")
+
+	cds, _ := proto.Marshal(&pb.ChaincodeDeploymentSpec{
+		ChaincodeSpec: &pb.ChaincodeSpec{
+			Type: pb.ChaincodeSpec_GOLANG,
+		},
+	})
+	_, err = utils.GetChaincodeDeploymentSpec(cds, pr)
+	assert.NoError(t, err, "Unexpected error getting deployment spec")
+
+	cds, _ = proto.Marshal(&pb.ChaincodeDeploymentSpec{
+		ChaincodeSpec: &pb.ChaincodeSpec{
+			Type: pb.ChaincodeSpec_UNDEFINED,
+		},
+	})
+	_, err = utils.GetChaincodeDeploymentSpec(cds, pr)
+	assert.Error(t, err, "Expected error with invalid spec type")
+
+}
+
+func TestCDSProposals(t *testing.T) {
+	var prop *pb.Proposal
+	var err error
+	var txid string
+	creator := []byte("creator")
+	cds := &pb.ChaincodeDeploymentSpec{
+		ChaincodeSpec: &pb.ChaincodeSpec{
+			Type: pb.ChaincodeSpec_GOLANG,
+		},
+	}
+	policy := []byte("policy")
+	escc := []byte("escc")
+	vscc := []byte("vscc")
+	chainID := "testchainid"
+
+	// install
+	prop, txid, err = utils.CreateInstallProposalFromCDS(cds, creator)
+	assert.NotNil(t, prop, "Install proposal should not be nil")
+	assert.NoError(t, err, "Unexpected error creating install proposal")
+	assert.NotEqual(t, "", txid, "txid should not be empty")
+
+	// deploy
+	prop, txid, err = utils.CreateDeployProposalFromCDS(chainID, cds, creator, policy, escc, vscc, nil)
+	assert.NotNil(t, prop, "Deploy proposal should not be nil")
+	assert.NoError(t, err, "Unexpected error creating deploy proposal")
+	assert.NotEqual(t, "", txid, "txid should not be empty")
+
+	// upgrade
+	prop, txid, err = utils.CreateUpgradeProposalFromCDS(chainID, cds, creator, policy, escc, vscc, nil)
+	assert.NotNil(t, prop, "Upgrade proposal should not be nil")
+	assert.NoError(t, err, "Unexpected error creating upgrade proposal")
+	assert.NotEqual(t, "", txid, "txid should not be empty")
+
+}
+
+func TestComputeProposalBinding(t *testing.T) {
+	expectedDigestHex := "5093dd4f4277e964da8f4afbde0a9674d17f2a6a5961f0670fc21ae9b67f2983"
+	expectedDigest, _ := hex.DecodeString(expectedDigestHex)
+	chdr, _ := proto.Marshal(&common.ChannelHeader{
+		Epoch: uint64(10),
+	})
+	shdr, _ := proto.Marshal(&common.SignatureHeader{
+		Nonce:   []byte("nonce"),
+		Creator: []byte("creator"),
+	})
+	hdr, _ := proto.Marshal(&common.Header{
+		ChannelHeader:   chdr,
+		SignatureHeader: shdr,
+	})
+	prop := &pb.Proposal{
+		Header: hdr,
+	}
+	binding, _ := utils.ComputeProposalBinding(prop)
+	assert.Equal(t, expectedDigest, binding, "Binding does not match expected digest")
+}
+
+func TestProposal(t *testing.T) {
+	// create a proposal from a ChaincodeInvocationSpec
+	prop, _, err := utils.CreateChaincodeProposalWithTransient(
+		common.HeaderType_ENDORSER_TRANSACTION,
+		util.GetTestChainID(), createCIS(),
+		[]byte("creator"),
+		map[string][]byte{"certx": []byte("transient")})
+	if err != nil {
+		t.Fatalf("Could not create chaincode proposal, err %s\n", err)
+		return
+	}
+
+	// serialize the proposal
+	pBytes, err := utils.GetBytesProposal(prop)
+	if err != nil {
+		t.Fatalf("Could not serialize the chaincode proposal, err %s\n", err)
+		return
+	}
+
+	// deserialize it and expect it to be the same
+	propBack, err := utils.GetProposal(pBytes)
+	if err != nil {
+		t.Fatalf("Could not deserialize the chaincode proposal, err %s\n", err)
+		return
+	}
+	if !proto.Equal(prop, propBack) {
+		t.Fatalf("Proposal and deserialized proposals don't match\n")
+		return
+	}
+
+	// get back the header
+	hdr, err := utils.GetHeader(prop.Header)
+	if err != nil {
+		t.Fatalf("Could not extract the header from the proposal, err %s\n", err)
+	}
+
+	hdrBytes, err := utils.GetBytesHeader(hdr)
+	if err != nil {
+		t.Fatalf("Could not marshal the header, err %s\n", err)
+	}
+
+	hdr, err = utils.GetHeader(hdrBytes)
+	if err != nil {
+		t.Fatalf("Could not unmarshal the header, err %s\n", err)
+	}
+
+	chdr, err := utils.UnmarshalChannelHeader(hdr.ChannelHeader)
+	if err != nil {
+		t.Fatalf("Could not unmarshal channel header, err %s", err)
+	}
+
+	shdr, err := utils.GetSignatureHeader(hdr.SignatureHeader)
+	if err != nil {
+		t.Fatalf("Could not unmarshal signature header, err %s", err)
+	}
+
+	_, err = utils.GetBytesSignatureHeader(shdr)
+	if err != nil {
+		t.Fatalf("Could not marshal signature header, err %s", err)
+	}
+
+	// sanity check on header
+	if chdr.Type != int32(common.HeaderType_ENDORSER_TRANSACTION) ||
+		shdr.Nonce == nil ||
+		string(shdr.Creator) != "creator" {
+		t.Fatalf("Invalid header after unmarshalling\n")
+		return
+	}
+
+	// get back the header extension
+	hdrExt, err := utils.GetChaincodeHeaderExtension(hdr)
+	if err != nil {
+		t.Fatalf("Could not extract the header extensions from the proposal, err %s\n", err)
+		return
+	}
+
+	// sanity check on header extension
+	if string(hdrExt.ChaincodeId.Name) != "chaincode_name" {
+		t.Fatalf("Invalid header extension after unmarshalling\n")
+		return
+	}
+
+	// get back the ChaincodeInvocationSpec
+	cis, err := utils.GetChaincodeInvocationSpec(prop)
+	if err != nil {
+		t.Fatalf("Could not extract chaincode invocation spec from header, err %s\n", err)
+		return
+	}
+
+	// sanity check on cis
+	if cis.ChaincodeSpec.Type != pb.ChaincodeSpec_GOLANG ||
+		cis.ChaincodeSpec.ChaincodeId.Name != "chaincode_name" ||
+		len(cis.ChaincodeSpec.Input.Args) != 2 ||
+		string(cis.ChaincodeSpec.Input.Args[0]) != "arg1" ||
+		string(cis.ChaincodeSpec.Input.Args[1]) != "arg2" {
+		t.Fatalf("Invalid chaincode invocation spec after unmarshalling\n")
+		return
+	}
+
+	creator, transient, err := utils.GetChaincodeProposalContext(prop)
+	if err != nil {
+		t.Fatalf("Failed getting chaincode proposal context [%s]", err)
+	}
+	if string(creator) != "creator" {
+		t.Fatalf("Failed checking Creator field. Invalid value, expectext 'creator', got [%s]", string(creator))
+		return
+	}
+	value, ok := transient["certx"]
+	if !ok || string(value) != "transient" {
+		t.Fatalf("Failed checking Transient field. Invalid value, expectext 'transient', got [%s]", string(value))
+		return
+	}
+}
+
+func TestProposalWithTxID(t *testing.T) {
+	// create a proposal from a ChaincodeInvocationSpec
+	prop, txid, err := utils.CreateChaincodeProposalWithTxIDAndTransient(
+		common.HeaderType_ENDORSER_TRANSACTION,
+		util.GetTestChainID(),
+		createCIS(),
+		[]byte("creator"),
+		"testtx",
+		map[string][]byte{"certx": []byte("transient")},
+	)
+	assert.Nil(t, err)
+	assert.NotNil(t, prop)
+	assert.Equal(t, txid, "testtx")
+
+	prop, txid, err = utils.CreateChaincodeProposalWithTxIDAndTransient(
+		common.HeaderType_ENDORSER_TRANSACTION,
+		util.GetTestChainID(),
+		createCIS(),
+		[]byte("creator"),
+		"",
+		map[string][]byte{"certx": []byte("transient")},
+	)
+	assert.Nil(t, err)
+	assert.NotNil(t, prop)
+	assert.NotEmpty(t, txid)
+}
+
+func TestProposalResponse(t *testing.T) {
+	events := &pb.ChaincodeEvent{
+		ChaincodeId: "ccid",
+		EventName:   "EventName",
+		Payload:     []byte("EventPayload"),
+		TxId:        "TxID"}
+	ccid := &pb.ChaincodeID{
+		Name:    "ccid",
+		Version: "v1",
+	}
+
+	pHashBytes := []byte("proposal_hash")
+	pResponse := &pb.Response{Status: 200}
+	results := []byte("results")
+	eventBytes, err := utils.GetBytesChaincodeEvent(events)
+	if err != nil {
+		t.Fatalf("Failure while marshalling the ProposalResponsePayload")
+		return
+	}
+
+	// get the bytes of the response
+	pResponseBytes, err := utils.GetBytesResponse(pResponse)
+	if err != nil {
+		t.Fatalf("Failure while marshalling the Response")
+		return
+	}
+
+	// get the response from bytes
+	_, err = utils.GetResponse(pResponseBytes)
+	if err != nil {
+		t.Fatalf("Failure while unmarshalling the Response")
+		return
+	}
+
+	// get the bytes of the ProposalResponsePayload
+	prpBytes, err := utils.GetBytesProposalResponsePayload(pHashBytes, pResponse, results, eventBytes, ccid)
+	if err != nil {
+		t.Fatalf("Failure while marshalling the ProposalResponsePayload")
+		return
+	}
+
+	// get the ProposalResponsePayload message
+	prp, err := utils.GetProposalResponsePayload(prpBytes)
+	if err != nil {
+		t.Fatalf("Failure while unmarshalling the ProposalResponsePayload")
+		return
+	}
+
+	// get the ChaincodeAction message
+	act, err := utils.GetChaincodeAction(prp.Extension)
+	if err != nil {
+		t.Fatalf("Failure while unmarshalling the ChaincodeAction")
+		return
+	}
+
+	// sanity check on the action
+	if string(act.Results) != "results" {
+		t.Fatalf("Invalid actions after unmarshalling")
+		return
+	}
+
+	event, err := utils.GetChaincodeEvents(act.Events)
+	if err != nil {
+		t.Fatalf("Failure while unmarshalling the ChainCodeEvents")
+		return
+	}
+
+	// sanity check on the event
+	if string(event.ChaincodeId) != "ccid" {
+		t.Fatalf("Invalid actions after unmarshalling")
+		return
+	}
+
+	pr := &pb.ProposalResponse{
+		Payload:     prpBytes,
+		Endorsement: &pb.Endorsement{Endorser: []byte("endorser"), Signature: []byte("signature")},
+		Version:     1, // TODO: pick right version number
+		Response:    &pb.Response{Status: 200, Message: "OK"}}
+
+	// create a proposal response
+	prBytes, err := utils.GetBytesProposalResponse(pr)
+	if err != nil {
+		t.Fatalf("Failure while marshalling the ProposalResponse")
+		return
+	}
+
+	// get the proposal response message back
+	prBack, err := utils.GetProposalResponse(prBytes)
+	if err != nil {
+		t.Fatalf("Failure while unmarshalling the ProposalResponse")
+		return
+	}
+
+	// sanity check on pr
+	if prBack.Response.Status != 200 ||
+		string(prBack.Endorsement.Signature) != "signature" ||
+		string(prBack.Endorsement.Endorser) != "endorser" ||
+		bytes.Compare(prBack.Payload, prpBytes) != 0 {
+		t.Fatalf("Invalid ProposalResponse after unmarshalling")
+		return
+	}
+}
+
+func TestEnvelope(t *testing.T) {
+	// create a proposal from a ChaincodeInvocationSpec
+	prop, _, err := utils.CreateChaincodeProposal(common.HeaderType_ENDORSER_TRANSACTION, util.GetTestChainID(), createCIS(), signerSerialized)
+	if err != nil {
+		t.Fatalf("Could not create chaincode proposal, err %s\n", err)
+		return
+	}
+
+	response := &pb.Response{Status: 200, Payload: []byte("payload")}
+	result := []byte("res")
+	ccid := &pb.ChaincodeID{Name: "foo", Version: "v1"}
+
+	presp, err := utils.CreateProposalResponse(prop.Header, prop.Payload, response, result, nil, ccid, nil, signer)
+	if err != nil {
+		t.Fatalf("Could not create proposal response, err %s\n", err)
+		return
+	}
+
+	tx, err := utils.CreateSignedTx(prop, signer, presp)
+	if err != nil {
+		t.Fatalf("Could not create signed tx, err %s\n", err)
+		return
+	}
+
+	envBytes, err := utils.GetBytesEnvelope(tx)
+	if err != nil {
+		t.Fatalf("Could not marshal envelope, err %s\n", err)
+		return
+	}
+
+	tx, err = utils.GetEnvelopeFromBlock(envBytes)
+	if err != nil {
+		t.Fatalf("Could not unmarshal envelope, err %s\n", err)
+		return
+	}
+
+	act2, err := utils.GetActionFromEnvelope(envBytes)
+	if err != nil {
+		t.Fatalf("Could not extract actions from envelop, err %s\n", err)
+		return
+	}
+
+	if act2.Response.Status != response.Status {
+		t.Fatalf("response staus don't match")
+		return
+	}
+	if bytes.Compare(act2.Response.Payload, response.Payload) != 0 {
+		t.Fatalf("response payload don't match")
+		return
+	}
+
+	if bytes.Compare(act2.Results, result) != 0 {
+		t.Fatalf("results don't match")
+		return
+	}
+
+	txpayl, err := utils.GetPayload(tx)
+	if err != nil {
+		t.Fatalf("Could not unmarshal payload, err %s\n", err)
+		return
+	}
+
+	tx2, err := utils.GetTransaction(txpayl.Data)
+	if err != nil {
+		t.Fatalf("Could not unmarshal Transaction, err %s\n", err)
+		return
+	}
+
+	sh, err := utils.GetSignatureHeader(tx2.Actions[0].Header)
+	if err != nil {
+		t.Fatalf("Could not unmarshal SignatureHeader, err %s\n", err)
+		return
+	}
+
+	if bytes.Compare(sh.Creator, signerSerialized) != 0 {
+		t.Fatalf("creator does not match")
+		return
+	}
+
+	cap, err := utils.GetChaincodeActionPayload(tx2.Actions[0].Payload)
+	if err != nil {
+		t.Fatalf("Could not unmarshal ChaincodeActionPayload, err %s\n", err)
+		return
+	}
+	assert.NotNil(t, cap)
+
+	prp, err := utils.GetProposalResponsePayload(cap.Action.ProposalResponsePayload)
+	if err != nil {
+		t.Fatalf("Could not unmarshal ProposalResponsePayload, err %s\n", err)
+		return
+	}
+
+	ca, err := utils.GetChaincodeAction(prp.Extension)
+	if err != nil {
+		t.Fatalf("Could not unmarshal ChaincodeAction, err %s\n", err)
+		return
+	}
+
+	if ca.Response.Status != response.Status {
+		t.Fatalf("response staus don't match")
+		return
+	}
+	if bytes.Compare(ca.Response.Payload, response.Payload) != 0 {
+		t.Fatalf("response payload don't match")
+		return
+	}
+
+	if bytes.Compare(ca.Results, result) != 0 {
+		t.Fatalf("results don't match")
+		return
+	}
+}
+
+func TestProposalTxID(t *testing.T) {
+	nonce := []byte{1}
+	creator := []byte{2}
+
+	txid, err := utils.ComputeTxID(nonce, creator)
+	assert.NotEmpty(t, txid, "TxID cannot be empty.")
+	assert.NoError(t, err, "Failed computing txID")
+	assert.Nil(t, utils.CheckTxID(txid, nonce, creator))
+	assert.Error(t, utils.CheckTxID("", nonce, creator))
+
+	txid, err = utils.ComputeTxID(nil, nil)
+	assert.NotEmpty(t, txid, "TxID cannot be empty.")
+	assert.NoError(t, err, "Failed computing txID")
+}
+
+func TestComputeProposalTxID(t *testing.T) {
+	txid, err := utils.ComputeTxID([]byte{1}, []byte{1})
+	assert.NoError(t, err, "Failed computing TxID")
+
+	// Compute the function computed by ComputeTxID,
+	// namely, base64(sha256(nonce||creator))
+	hf := sha256.New()
+	hf.Write([]byte{1})
+	hf.Write([]byte{1})
+	hashOut := hf.Sum(nil)
+	txid2 := hex.EncodeToString(hashOut)
+
+	t.Logf("% x\n", hashOut)
+	t.Logf("% s\n", txid)
+	t.Logf("% s\n", txid2)
+
+	assert.Equal(t, txid, txid2)
+}
+
+var signer msp.SigningIdentity
+var signerSerialized []byte
+
+func TestMain(m *testing.M) {
+	// setup the MSP manager so that we can sign/verify
+	err := msptesttools.LoadMSPSetupForTesting()
+	if err != nil {
+		os.Exit(-1)
+		fmt.Printf("Could not initialize msp")
+		return
+	}
+	signer, err = mspmgmt.GetLocalMSP().GetDefaultSigningIdentity()
+	if err != nil {
+		os.Exit(-1)
+		fmt.Printf("Could not get signer")
+		return
+	}
+
+	signerSerialized, err = signer.Serialize()
+	if err != nil {
+		os.Exit(-1)
+		fmt.Printf("Could not serialize identity")
+		return
+	}
+
+	os.Exit(m.Run())
+}
diff --git a/protoutil/readme.md b/protoutil/readme.md
new file mode 100644
index 000000000..b6ce68e1f
--- /dev/null
+++ b/protoutil/readme.md
@@ -0,0 +1,3 @@
+this package is an exact copy of protos/util
+
+it is copied here to fix the dependency of trustbloc/fabric-peer-ext during the peer docker image build
\ No newline at end of file
diff --git a/protoutil/txutils.go b/protoutil/txutils.go
new file mode 100644
index 000000000..20b6b5a75
--- /dev/null
+++ b/protoutil/txutils.go
@@ -0,0 +1,453 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil
+
+import (
+	"bytes"
+	"fmt"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/bccsp"
+	"github.com/hyperledger/fabric/bccsp/factory"
+	"github.com/hyperledger/fabric/common/crypto"
+	"github.com/hyperledger/fabric/msp"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/pkg/errors"
+)
+
+// GetPayloads gets the underlying payload objects in a TransactionAction
+func GetPayloads(txActions *peer.TransactionAction) (*peer.ChaincodeActionPayload, *peer.ChaincodeAction, error) {
+	// TODO: pass in the tx type (in what follows we're assuming the
+	// type is ENDORSER_TRANSACTION)
+	ccPayload, err := GetChaincodeActionPayload(txActions.Payload)
+	if err != nil {
+		return nil, nil, err
+	}
+
+	if ccPayload.Action == nil || ccPayload.Action.ProposalResponsePayload == nil {
+		return nil, nil, errors.New("no payload in ChaincodeActionPayload")
+	}
+	pRespPayload, err := GetProposalResponsePayload(ccPayload.Action.ProposalResponsePayload)
+	if err != nil {
+		return nil, nil, err
+	}
+
+	if pRespPayload.Extension == nil {
+		return nil, nil, errors.New("response payload is missing extension")
+	}
+
+	respPayload, err := GetChaincodeAction(pRespPayload.Extension)
+	if err != nil {
+		return ccPayload, nil, err
+	}
+	return ccPayload, respPayload, nil
+}
+
+// GetEnvelopeFromBlock gets an envelope from a block's Data field.
+func GetEnvelopeFromBlock(data []byte) (*common.Envelope, error) {
+	// Block always begins with an envelope
+	var err error
+	env := &common.Envelope{}
+	if err = proto.Unmarshal(data, env); err != nil {
+		return nil, errors.Wrap(err, "error unmarshaling Envelope")
+	}
+
+	return env, nil
+}
+
+// CreateSignedEnvelope creates a signed envelope of the desired type, with
+// marshaled dataMsg and signs it
+func CreateSignedEnvelope(txType common.HeaderType, channelID string, signer crypto.LocalSigner, dataMsg proto.Message, msgVersion int32, epoch uint64) (*common.Envelope, error) {
+	return CreateSignedEnvelopeWithTLSBinding(txType, channelID, signer, dataMsg, msgVersion, epoch, nil)
+}
+
+// CreateSignedEnvelopeWithTLSBinding creates a signed envelope of the desired
+// type, with marshaled dataMsg and signs it. It also includes a TLS cert hash
+// into the channel header
+func CreateSignedEnvelopeWithTLSBinding(txType common.HeaderType, channelID string, signer crypto.LocalSigner, dataMsg proto.Message, msgVersion int32, epoch uint64, tlsCertHash []byte) (*common.Envelope, error) {
+	payloadChannelHeader := MakeChannelHeader(txType, msgVersion, channelID, epoch)
+	payloadChannelHeader.TlsCertHash = tlsCertHash
+	var err error
+	payloadSignatureHeader := &common.SignatureHeader{}
+
+	if signer != nil {
+		payloadSignatureHeader, err = signer.NewSignatureHeader()
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	data, err := proto.Marshal(dataMsg)
+	if err != nil {
+		return nil, errors.Wrap(err, "error marshaling")
+	}
+
+	paylBytes := MarshalOrPanic(
+		&common.Payload{
+			Header: MakePayloadHeader(payloadChannelHeader, payloadSignatureHeader),
+			Data:   data,
+		},
+	)
+
+	var sig []byte
+	if signer != nil {
+		sig, err = signer.Sign(paylBytes)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	env := &common.Envelope{
+		Payload:   paylBytes,
+		Signature: sig,
+	}
+
+	return env, nil
+}
+
+// CreateSignedTx assembles an Envelope message from proposal, endorsements,
+// and a signer. This function should be called by a client when it has
+// collected enough endorsements for a proposal to create a transaction and
+// submit it to peers for ordering
+func CreateSignedTx(proposal *peer.Proposal, signer msp.SigningIdentity, resps ...*peer.ProposalResponse) (*common.Envelope, error) {
+	if len(resps) == 0 {
+		return nil, errors.New("at least one proposal response is required")
+	}
+
+	// the original header
+	hdr, err := GetHeader(proposal.Header)
+	if err != nil {
+		return nil, err
+	}
+
+	// the original payload
+	pPayl, err := GetChaincodeProposalPayload(proposal.Payload)
+	if err != nil {
+		return nil, err
+	}
+
+	// check that the signer is the same that is referenced in the header
+	// TODO: maybe worth removing?
+	signerBytes, err := signer.Serialize()
+	if err != nil {
+		return nil, err
+	}
+
+	shdr, err := GetSignatureHeader(hdr.SignatureHeader)
+	if err != nil {
+		return nil, err
+	}
+
+	if bytes.Compare(signerBytes, shdr.Creator) != 0 {
+		return nil, errors.New("signer must be the same as the one referenced in the header")
+	}
+
+	// get header extensions so we have the visibility field
+	hdrExt, err := GetChaincodeHeaderExtension(hdr)
+	if err != nil {
+		return nil, err
+	}
+
+	// ensure that all actions are bitwise equal and that they are successful
+	var a1 []byte
+	for n, r := range resps {
+		if n == 0 {
+			a1 = r.Payload
+			if r.Response.Status < 200 || r.Response.Status >= 400 {
+				return nil, errors.Errorf("proposal response was not successful, error code %d, msg %s", r.Response.Status, r.Response.Message)
+			}
+			continue
+		}
+
+		if bytes.Compare(a1, r.Payload) != 0 {
+			return nil, errors.New("ProposalResponsePayloads do not match")
+		}
+	}
+
+	// fill endorsements
+	endorsements := make([]*peer.Endorsement, len(resps))
+	for n, r := range resps {
+		endorsements[n] = r.Endorsement
+	}
+
+	// create ChaincodeEndorsedAction
+	cea := &peer.ChaincodeEndorsedAction{ProposalResponsePayload: resps[0].Payload, Endorsements: endorsements}
+
+	// obtain the bytes of the proposal payload that will go to the transaction
+	propPayloadBytes, err := GetBytesProposalPayloadForTx(pPayl, hdrExt.PayloadVisibility)
+	if err != nil {
+		return nil, err
+	}
+
+	// serialize the chaincode action payload
+	cap := &peer.ChaincodeActionPayload{ChaincodeProposalPayload: propPayloadBytes, Action: cea}
+	capBytes, err := GetBytesChaincodeActionPayload(cap)
+	if err != nil {
+		return nil, err
+	}
+
+	// create a transaction
+	taa := &peer.TransactionAction{Header: hdr.SignatureHeader, Payload: capBytes}
+	taas := make([]*peer.TransactionAction, 1)
+	taas[0] = taa
+	tx := &peer.Transaction{Actions: taas}
+
+	// serialize the tx
+	txBytes, err := GetBytesTransaction(tx)
+	if err != nil {
+		return nil, err
+	}
+
+	// create the payload
+	payl := &common.Payload{Header: hdr, Data: txBytes}
+	paylBytes, err := GetBytesPayload(payl)
+	if err != nil {
+		return nil, err
+	}
+
+	// sign the payload
+	sig, err := signer.Sign(paylBytes)
+	if err != nil {
+		return nil, err
+	}
+
+	// here's the envelope
+	return &common.Envelope{Payload: paylBytes, Signature: sig}, nil
+}
+
+// CreateProposalResponse creates a proposal response.
+func CreateProposalResponse(hdrbytes []byte, payl []byte, response *peer.Response, results []byte, events []byte, ccid *peer.ChaincodeID, visibility []byte, signingEndorser msp.SigningIdentity) (*peer.ProposalResponse, error) {
+	hdr, err := GetHeader(hdrbytes)
+	if err != nil {
+		return nil, err
+	}
+
+	// obtain the proposal hash given proposal header, payload and the
+	// requested visibility
+	pHashBytes, err := GetProposalHash1(hdr, payl, visibility)
+	if err != nil {
+		return nil, errors.WithMessage(err, "error computing proposal hash")
+	}
+
+	// get the bytes of the proposal response payload - we need to sign them
+	prpBytes, err := GetBytesProposalResponsePayload(pHashBytes, response, results, events, ccid)
+	if err != nil {
+		return nil, err
+	}
+
+	// serialize the signing identity
+	endorser, err := signingEndorser.Serialize()
+	if err != nil {
+		return nil, errors.WithMessage(err, fmt.Sprintf("error serializing signing identity for %s", signingEndorser.GetIdentifier()))
+	}
+
+	// sign the concatenation of the proposal response and the serialized
+	// endorser identity with this endorser's key
+	signature, err := signingEndorser.Sign(append(prpBytes, endorser...))
+	if err != nil {
+		return nil, errors.WithMessage(err, "could not sign the proposal response payload")
+	}
+
+	resp := &peer.ProposalResponse{
+		// Timestamp: TODO!
+		Version: 1, // TODO: pick right version number
+		Endorsement: &peer.Endorsement{
+			Signature: signature,
+			Endorser:  endorser,
+		},
+		Payload: prpBytes,
+		Response: &peer.Response{
+			Status:  200,
+			Message: "OK",
+		},
+	}
+
+	return resp, nil
+}
+
+// CreateProposalResponseFailure creates a proposal response for cases where
+// endorsement proposal fails either due to a endorsement failure or a
+// chaincode failure (chaincode response status >= shim.ERRORTHRESHOLD)
+func CreateProposalResponseFailure(hdrbytes []byte, payl []byte, response *peer.Response, results []byte, events []byte, ccid *peer.ChaincodeID, visibility []byte) (*peer.ProposalResponse, error) {
+	hdr, err := GetHeader(hdrbytes)
+	if err != nil {
+		return nil, err
+	}
+
+	// obtain the proposal hash given proposal header, payload and the requested visibility
+	pHashBytes, err := GetProposalHash1(hdr, payl, visibility)
+	if err != nil {
+		return nil, errors.WithMessage(err, "error computing proposal hash")
+	}
+
+	// get the bytes of the proposal response payload
+	prpBytes, err := GetBytesProposalResponsePayload(pHashBytes, response, results, events, ccid)
+	if err != nil {
+		return nil, err
+	}
+
+	resp := &peer.ProposalResponse{
+		// Timestamp: TODO!
+		Payload:  prpBytes,
+		Response: response,
+	}
+
+	return resp, nil
+}
+
+// GetSignedProposal returns a signed proposal given a Proposal message and a
+// signing identity
+func GetSignedProposal(prop *peer.Proposal, signer msp.SigningIdentity) (*peer.SignedProposal, error) {
+	// check for nil argument
+	if prop == nil || signer == nil {
+		return nil, errors.New("nil arguments")
+	}
+
+	propBytes, err := GetBytesProposal(prop)
+	if err != nil {
+		return nil, err
+	}
+
+	signature, err := signer.Sign(propBytes)
+	if err != nil {
+		return nil, err
+	}
+
+	return &peer.SignedProposal{ProposalBytes: propBytes, Signature: signature}, nil
+}
+
+// MockSignedEndorserProposalOrPanic creates a SignedProposal with the
+// passed arguments
+func MockSignedEndorserProposalOrPanic(chainID string, cs *peer.ChaincodeSpec, creator, signature []byte) (*peer.SignedProposal, *peer.Proposal) {
+	prop, _, err := CreateChaincodeProposal(
+		common.HeaderType_ENDORSER_TRANSACTION,
+		chainID,
+		&peer.ChaincodeInvocationSpec{ChaincodeSpec: cs},
+		creator)
+	if err != nil {
+		panic(err)
+	}
+
+	propBytes, err := GetBytesProposal(prop)
+	if err != nil {
+		panic(err)
+	}
+
+	return &peer.SignedProposal{ProposalBytes: propBytes, Signature: signature}, prop
+}
+
+func MockSignedEndorserProposal2OrPanic(chainID string, cs *peer.ChaincodeSpec, signer msp.SigningIdentity) (*peer.SignedProposal, *peer.Proposal) {
+	serializedSigner, err := signer.Serialize()
+	if err != nil {
+		panic(err)
+	}
+
+	prop, _, err := CreateChaincodeProposal(
+		common.HeaderType_ENDORSER_TRANSACTION,
+		chainID,
+		&peer.ChaincodeInvocationSpec{ChaincodeSpec: &peer.ChaincodeSpec{}},
+		serializedSigner)
+	if err != nil {
+		panic(err)
+	}
+
+	sProp, err := GetSignedProposal(prop, signer)
+	if err != nil {
+		panic(err)
+	}
+
+	return sProp, prop
+}
+
+// GetBytesProposalPayloadForTx takes a ChaincodeProposalPayload and returns
+// its serialized version according to the visibility field
+func GetBytesProposalPayloadForTx(payload *peer.ChaincodeProposalPayload, visibility []byte) ([]byte, error) {
+	// check for nil argument
+	if payload == nil {
+		return nil, errors.New("nil arguments")
+	}
+
+	// strip the transient bytes off the payload - this needs to be done no
+	// matter the visibility mode
+	cppNoTransient := &peer.ChaincodeProposalPayload{Input: payload.Input, TransientMap: nil}
+	cppBytes, err := GetBytesChaincodeProposalPayload(cppNoTransient)
+	if err != nil {
+		return nil, err
+	}
+
+	// currently the fabric only supports full visibility: this means that
+	// there are no restrictions on which parts of the proposal payload will
+	// be visible in the final transaction; this default approach requires
+	// no additional instructions in the PayloadVisibility field; however
+	// the fabric may be extended to encode more elaborate visibility
+	// mechanisms that shall be encoded in this field (and handled
+	// appropriately by the peer)
+
+	return cppBytes, nil
+}
+
+// GetProposalHash2 gets the proposal hash - this version
+// is called by the committer where the visibility policy
+// has already been enforced and so we already get what
+// we have to get in ccPropPayl
+func GetProposalHash2(header *common.Header, ccPropPayl []byte) ([]byte, error) {
+	// check for nil argument
+	if header == nil ||
+		header.ChannelHeader == nil ||
+		header.SignatureHeader == nil ||
+		ccPropPayl == nil {
+		return nil, errors.New("nil arguments")
+	}
+
+	hash, err := factory.GetDefault().GetHash(&bccsp.SHA256Opts{})
+	if err != nil {
+		return nil, errors.WithMessage(err, "error instantiating hash function")
+	}
+	// hash the serialized Channel Header object
+	hash.Write(header.ChannelHeader)
+	// hash the serialized Signature Header object
+	hash.Write(header.SignatureHeader)
+	// hash the bytes of the chaincode proposal payload that we are given
+	hash.Write(ccPropPayl)
+	return hash.Sum(nil), nil
+}
+
+// GetProposalHash1 gets the proposal hash bytes after sanitizing the
+// chaincode proposal payload according to the rules of visibility
+func GetProposalHash1(header *common.Header, ccPropPayl []byte, visibility []byte) ([]byte, error) {
+	// check for nil argument
+	if header == nil ||
+		header.ChannelHeader == nil ||
+		header.SignatureHeader == nil ||
+		ccPropPayl == nil {
+		return nil, errors.New("nil arguments")
+	}
+
+	// unmarshal the chaincode proposal payload
+	cpp, err := GetChaincodeProposalPayload(ccPropPayl)
+	if err != nil {
+		return nil, err
+	}
+
+	ppBytes, err := GetBytesProposalPayloadForTx(cpp, visibility)
+	if err != nil {
+		return nil, err
+	}
+
+	hash2, err := factory.GetDefault().GetHash(&bccsp.SHA256Opts{})
+	if err != nil {
+		return nil, errors.WithMessage(err, "error instantiating hash function")
+	}
+	// hash the serialized Channel Header object
+	hash2.Write(header.ChannelHeader)
+	// hash the serialized Signature Header object
+	hash2.Write(header.SignatureHeader)
+	// hash of the part of the chaincode proposal payload that will go to the tx
+	hash2.Write(ppBytes)
+	return hash2.Sum(nil), nil
+}
diff --git a/protoutil/txutils_test.go b/protoutil/txutils_test.go
new file mode 100644
index 000000000..df14255ef
--- /dev/null
+++ b/protoutil/txutils_test.go
@@ -0,0 +1,522 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package protoutil_test
+
+import (
+	"encoding/hex"
+	"errors"
+	"strconv"
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	mockmsp "github.com/hyperledger/fabric/common/mocks/msp"
+	"github.com/hyperledger/fabric/common/util"
+	cb "github.com/hyperledger/fabric/protos/common"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/hyperledger/fabric/protos/utils"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestGetPayloads(t *testing.T) {
+	var txAction *pb.TransactionAction
+	var err error
+
+	// good
+	ccActionBytes, _ := proto.Marshal(&pb.ChaincodeAction{
+		Results: []byte("results"),
+	})
+	proposalResponsePayload := &pb.ProposalResponsePayload{
+		Extension: ccActionBytes,
+	}
+	proposalResponseBytes, err := proto.Marshal(proposalResponsePayload)
+	ccActionPayload := &pb.ChaincodeActionPayload{
+		Action: &pb.ChaincodeEndorsedAction{
+			ProposalResponsePayload: proposalResponseBytes,
+		},
+	}
+	ccActionPayloadBytes, _ := proto.Marshal(ccActionPayload)
+	txAction = &pb.TransactionAction{
+		Payload: ccActionPayloadBytes,
+	}
+	_, _, err = utils.GetPayloads(txAction)
+	assert.NoError(t, err, "Unexpected error getting payload bytes")
+	t.Logf("error1 [%s]", err)
+
+	// nil proposal response extension
+	proposalResponseBytes, err = proto.Marshal(&pb.ProposalResponsePayload{
+		Extension: nil,
+	})
+	ccActionPayloadBytes, _ = proto.Marshal(&pb.ChaincodeActionPayload{
+		Action: &pb.ChaincodeEndorsedAction{
+			ProposalResponsePayload: proposalResponseBytes,
+		},
+	})
+	txAction = &pb.TransactionAction{
+		Payload: ccActionPayloadBytes,
+	}
+	_, _, err = utils.GetPayloads(txAction)
+	assert.Error(t, err, "Expected error with nil proposal response extension")
+	t.Logf("error2 [%s]", err)
+
+	// malformed proposal response payload
+	ccActionPayloadBytes, _ = proto.Marshal(&pb.ChaincodeActionPayload{
+		Action: &pb.ChaincodeEndorsedAction{
+			ProposalResponsePayload: []byte("bad payload"),
+		},
+	})
+	txAction = &pb.TransactionAction{
+		Payload: ccActionPayloadBytes,
+	}
+	_, _, err = utils.GetPayloads(txAction)
+	assert.Error(t, err, "Expected error with malformed proposal response payload")
+	t.Logf("error3 [%s]", err)
+
+	// malformed proposal response payload extension
+	proposalResponseBytes, _ = proto.Marshal(&pb.ProposalResponsePayload{
+		Extension: []byte("bad extension"),
+	})
+	ccActionPayloadBytes, _ = proto.Marshal(&pb.ChaincodeActionPayload{
+		Action: &pb.ChaincodeEndorsedAction{
+			ProposalResponsePayload: proposalResponseBytes,
+		},
+	})
+	txAction = &pb.TransactionAction{
+		Payload: ccActionPayloadBytes,
+	}
+	_, _, err = utils.GetPayloads(txAction)
+	assert.Error(t, err, "Expected error with malformed proposal response extension")
+	t.Logf("error4 [%s]", err)
+
+	// nil proposal response payload extension
+	proposalResponseBytes, _ = proto.Marshal(&pb.ProposalResponsePayload{
+		ProposalHash: []byte("hash"),
+	})
+	ccActionPayloadBytes, _ = proto.Marshal(&pb.ChaincodeActionPayload{
+		Action: &pb.ChaincodeEndorsedAction{
+			ProposalResponsePayload: proposalResponseBytes,
+		},
+	})
+	txAction = &pb.TransactionAction{
+		Payload: ccActionPayloadBytes,
+	}
+	_, _, err = utils.GetPayloads(txAction)
+	assert.Error(t, err, "Expected error with nil proposal response extension")
+	t.Logf("error5 [%s]", err)
+
+	// malformed transaction action payload
+	txAction = &pb.TransactionAction{
+		Payload: []byte("bad payload"),
+	}
+	_, _, err = utils.GetPayloads(txAction)
+	assert.Error(t, err, "Expected error with malformed transaction action payload")
+	t.Logf("error6 [%s]", err)
+
+}
+
+func TestCreateSignedTx(t *testing.T) {
+	var err error
+	prop := &pb.Proposal{}
+
+	signID, err := mockmsp.NewNoopMsp().GetDefaultSigningIdentity()
+	assert.NoError(t, err, "Unexpected error getting signing identity")
+	signerBytes, err := signID.Serialize()
+	assert.NoError(t, err, "Unexpected error serializing signing identity")
+
+	ccHeaderExtensionBytes, _ := proto.Marshal(&pb.ChaincodeHeaderExtension{})
+	chdrBytes, _ := proto.Marshal(&cb.ChannelHeader{
+		Extension: ccHeaderExtensionBytes,
+	})
+	shdrBytes, _ := proto.Marshal(&cb.SignatureHeader{
+		Creator: signerBytes,
+	})
+	responses := []*pb.ProposalResponse{{}}
+
+	// malformed chaincode header extension
+	headerBytes, _ := proto.Marshal(&cb.Header{
+		ChannelHeader:   []byte("bad channel header"),
+		SignatureHeader: shdrBytes,
+	})
+	prop.Header = headerBytes
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with malformed chaincode extension")
+
+	// malformed signature header
+	headerBytes, _ = proto.Marshal(&cb.Header{
+		SignatureHeader: []byte("bad signature header"),
+	})
+	prop.Header = headerBytes
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with malformed signature header")
+
+	// set up the header bytes for the remaining tests
+	headerBytes, _ = proto.Marshal(&cb.Header{
+		ChannelHeader:   chdrBytes,
+		SignatureHeader: shdrBytes,
+	})
+	prop.Header = headerBytes
+
+	// non-matching responses
+	responses = []*pb.ProposalResponse{{
+		Payload: []byte("payload"),
+		Response: &pb.Response{
+			Status: int32(200),
+		},
+	}}
+	responses = append(responses, &pb.ProposalResponse{
+		Payload: []byte("payload2"),
+		Response: &pb.Response{
+			Status: int32(200),
+		},
+	})
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with non-matching responses")
+
+	// no endorsement
+	responses = []*pb.ProposalResponse{{
+		Payload: []byte("payload"),
+		Response: &pb.Response{
+			Status: int32(200),
+		},
+	}}
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with no endorsements")
+
+	// success
+	responses = []*pb.ProposalResponse{{
+		Payload:     []byte("payload"),
+		Endorsement: &pb.Endorsement{},
+		Response: &pb.Response{
+			Status: int32(200),
+		},
+	}}
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.NoError(t, err, "Unexpected error creating signed transaction")
+	t.Logf("error: [%s]", err)
+
+	//
+	//
+	// additional failure cases
+	prop = &pb.Proposal{}
+	responses = []*pb.ProposalResponse{}
+	// no proposal responses
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with no proposal responses")
+
+	// missing proposal header
+	responses = append(responses, &pb.ProposalResponse{})
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with no proposal header")
+
+	// bad proposal payload
+	prop.Payload = []byte("bad payload")
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with malformed proposal payload")
+
+	// bad payload header
+	prop.Header = []byte("bad header")
+	_, err = utils.CreateSignedTx(prop, signID, responses...)
+	assert.Error(t, err, "Expected error with malformed proposal header")
+
+}
+
+func TestCreateSignedTxStatus(t *testing.T) {
+	serializedExtension, err := proto.Marshal(&pb.ChaincodeHeaderExtension{})
+	assert.NoError(t, err)
+	serializedChannelHeader, err := proto.Marshal(&cb.ChannelHeader{
+		Extension: serializedExtension,
+	})
+	assert.NoError(t, err)
+
+	signingID, err := mockmsp.NewNoopMsp().GetDefaultSigningIdentity()
+	assert.NoError(t, err)
+	serializedSigningID, err := signingID.Serialize()
+	assert.NoError(t, err)
+	serializedSignatureHeader, err := proto.Marshal(&cb.SignatureHeader{
+		Creator: serializedSigningID,
+	})
+	assert.NoError(t, err)
+
+	header := &cb.Header{
+		ChannelHeader:   serializedChannelHeader,
+		SignatureHeader: serializedSignatureHeader,
+	}
+
+	serializedHeader, err := proto.Marshal(header)
+	assert.NoError(t, err)
+
+	proposal := &pb.Proposal{
+		Header: serializedHeader,
+	}
+
+	tests := []struct {
+		status      int32
+		expectedErr string
+	}{
+		{status: 0, expectedErr: "proposal response was not successful, error code 0, msg response-message"},
+		{status: 199, expectedErr: "proposal response was not successful, error code 199, msg response-message"},
+		{status: 200, expectedErr: ""},
+		{status: 201, expectedErr: ""},
+		{status: 399, expectedErr: ""},
+		{status: 400, expectedErr: "proposal response was not successful, error code 400, msg response-message"},
+	}
+	for _, tc := range tests {
+		t.Run(strconv.Itoa(int(tc.status)), func(t *testing.T) {
+			response := &pb.ProposalResponse{
+				Payload:     []byte("payload"),
+				Endorsement: &pb.Endorsement{},
+				Response: &pb.Response{
+					Status:  tc.status,
+					Message: "response-message",
+				},
+			}
+
+			_, err := utils.CreateSignedTx(proposal, signingID, response)
+			if tc.expectedErr == "" {
+				assert.NoError(t, err)
+			} else {
+				assert.EqualError(t, err, tc.expectedErr)
+			}
+		})
+	}
+}
+
+func TestCreateSignedEnvelope(t *testing.T) {
+	var env *cb.Envelope
+	channelID := "mychannelID"
+	msg := &cb.ConfigEnvelope{}
+
+	env, err := utils.CreateSignedEnvelope(cb.HeaderType_CONFIG, channelID,
+		goodSigner, msg, int32(1), uint64(1))
+	assert.NoError(t, err, "Unexpected error creating signed envelope")
+	assert.NotNil(t, env, "Envelope should not be nil")
+	// mock sign returns the bytes to be signed
+	assert.Equal(t, env.Payload, env.Signature, "Unexpected signature returned")
+	payload := &cb.Payload{}
+	err = proto.Unmarshal(env.Payload, payload)
+	assert.NoError(t, err, "Failed to unmarshal payload")
+	data := &cb.ConfigEnvelope{}
+	err = proto.Unmarshal(payload.Data, data)
+	assert.NoError(t, err, "Expected payload data to be a config envelope")
+	assert.Equal(t, msg, data, "Payload data does not match expected value")
+
+	_, err = utils.CreateSignedEnvelope(cb.HeaderType_CONFIG, channelID,
+		badSigner, &cb.ConfigEnvelope{}, int32(1), uint64(1))
+	assert.Error(t, err, "Expected sign error")
+}
+
+func TestCreateSignedEnvelopeNilSigner(t *testing.T) {
+	var env *cb.Envelope
+	channelID := "mychannelID"
+	msg := &cb.ConfigEnvelope{}
+
+	env, err := utils.CreateSignedEnvelope(cb.HeaderType_CONFIG, channelID,
+		nil, msg, int32(1), uint64(1))
+	assert.NoError(t, err, "Unexpected error creating signed envelope")
+	assert.NotNil(t, env, "Envelope should not be nil")
+	assert.Empty(t, env.Signature, "Signature should have been empty")
+	payload := &cb.Payload{}
+	err = proto.Unmarshal(env.Payload, payload)
+	assert.NoError(t, err, "Failed to unmarshal payload")
+	data := &cb.ConfigEnvelope{}
+	err = proto.Unmarshal(payload.Data, data)
+	assert.NoError(t, err, "Expected payload data to be a config envelope")
+	assert.Equal(t, msg, data, "Payload data does not match expected value")
+}
+
+func TestGetSignedProposal(t *testing.T) {
+	var signedProp *pb.SignedProposal
+	var err error
+
+	signID, err := mockmsp.NewNoopMsp().GetDefaultSigningIdentity()
+	assert.NoError(t, err, "Unexpected error getting signing identity")
+
+	prop := &pb.Proposal{}
+	propBytes, _ := proto.Marshal(prop)
+	signedProp, err = utils.GetSignedProposal(prop, signID)
+	assert.NoError(t, err, "Unexpected error getting signed proposal")
+	assert.Equal(t, propBytes, signedProp.ProposalBytes,
+		"Proposal bytes did not match expected value")
+	assert.Equal(t, []byte("signature"), signedProp.Signature,
+		"Signature did not match expected value")
+
+	_, err = utils.GetSignedProposal(nil, signID)
+	assert.Error(t, err, "Expected error with nil proposal")
+	_, err = utils.GetSignedProposal(prop, nil)
+	assert.Error(t, err, "Expected error with nil signing identity")
+
+}
+
+func TestMockSignedEndorserProposalOrPanic(t *testing.T) {
+	var prop *pb.Proposal
+	var signedProp *pb.SignedProposal
+
+	ccProposal := &pb.ChaincodeProposalPayload{}
+	cis := &pb.ChaincodeInvocationSpec{}
+	chainID := "testchainid"
+	sig := []byte("signature")
+	creator := []byte("creator")
+	cs := &pb.ChaincodeSpec{
+		ChaincodeId: &pb.ChaincodeID{
+			Name: "mychaincode",
+		},
+	}
+
+	signedProp, prop = utils.MockSignedEndorserProposalOrPanic(chainID, cs,
+		creator, sig)
+	assert.Equal(t, sig, signedProp.Signature,
+		"Signature did not match expected result")
+	propBytes, _ := proto.Marshal(prop)
+	assert.Equal(t, propBytes, signedProp.ProposalBytes,
+		"Proposal bytes do not match expected value")
+	err := proto.Unmarshal(prop.Payload, ccProposal)
+	assert.NoError(t, err, "Expected ChaincodeProposalPayload")
+	err = proto.Unmarshal(ccProposal.Input, cis)
+	assert.NoError(t, err, "Expected ChaincodeInvocationSpec")
+	assert.Equal(t, cs.ChaincodeId.Name, cis.ChaincodeSpec.ChaincodeId.Name,
+		"Chaincode name did not match expected value")
+}
+
+func TestMockSignedEndorserProposal2OrPanic(t *testing.T) {
+	var prop *pb.Proposal
+	var signedProp *pb.SignedProposal
+
+	ccProposal := &pb.ChaincodeProposalPayload{}
+	cis := &pb.ChaincodeInvocationSpec{}
+	chainID := "testchainid"
+	sig := []byte("signature")
+	signID, err := mockmsp.NewNoopMsp().GetDefaultSigningIdentity()
+	assert.NoError(t, err, "Unexpected error getting signing identity")
+
+	signedProp, prop = utils.MockSignedEndorserProposal2OrPanic(chainID,
+		&pb.ChaincodeSpec{}, signID)
+	assert.Equal(t, sig, signedProp.Signature,
+		"Signature did not match expected result")
+	propBytes, _ := proto.Marshal(prop)
+	assert.Equal(t, propBytes, signedProp.ProposalBytes,
+		"Proposal bytes do not match expected value")
+	err = proto.Unmarshal(prop.Payload, ccProposal)
+	assert.NoError(t, err, "Expected ChaincodeProposalPayload")
+	err = proto.Unmarshal(ccProposal.Input, cis)
+	assert.NoError(t, err, "Expected ChaincodeInvocationSpec")
+}
+
+func TestGetBytesProposalPayloadForTx(t *testing.T) {
+	input := &pb.ChaincodeProposalPayload{
+		Input:        []byte("input"),
+		TransientMap: make(map[string][]byte),
+	}
+	expected, _ := proto.Marshal(&pb.ChaincodeProposalPayload{
+		Input: []byte("input"),
+	})
+
+	result, err := utils.GetBytesProposalPayloadForTx(input, []byte{})
+	assert.NoError(t, err, "Unexpected error getting proposal payload")
+	assert.Equal(t, expected, result, "Payload does not match expected value")
+
+	_, err = utils.GetBytesProposalPayloadForTx(nil, []byte{})
+	assert.Error(t, err, "Expected error with nil proposal payload")
+}
+
+func TestGetProposalHash2(t *testing.T) {
+	expectedHashHex := "7b622ef4e1ab9b7093ec3bbfbca17d5d6f14a437914a6839319978a7034f7960"
+	expectedHash, _ := hex.DecodeString(expectedHashHex)
+	hdr := &cb.Header{
+		ChannelHeader:   []byte("chdr"),
+		SignatureHeader: []byte("shdr"),
+	}
+	propHash, err := utils.GetProposalHash2(hdr, []byte("ccproppayload"))
+	assert.NoError(t, err, "Unexpected error getting hash2 for proposal")
+	t.Logf("%x", propHash)
+	assert.Equal(t, expectedHash, propHash,
+		"Proposal hash did not match expected hash")
+
+	propHash, err = utils.GetProposalHash2(&cb.Header{},
+		[]byte("ccproppayload"))
+	assert.Error(t, err, "Expected error with nil arguments")
+}
+
+func TestGetProposalHash1(t *testing.T) {
+	expectedHashHex := "d4c1e3cac2105da5fddc2cfe776d6ec28e4598cf1e6fa51122c7f70d8076437b"
+	expectedHash, _ := hex.DecodeString(expectedHashHex)
+	hdr := &cb.Header{
+		ChannelHeader:   []byte("chdr"),
+		SignatureHeader: []byte("shdr"),
+	}
+
+	ccProposal, _ := proto.Marshal(&pb.ChaincodeProposalPayload{})
+
+	propHash, err := utils.GetProposalHash1(hdr, ccProposal, []byte{})
+	assert.NoError(t, err, "Unexpected error getting hash for proposal")
+	t.Logf("%x", propHash)
+	assert.Equal(t, expectedHash, propHash,
+		"Proposal hash did not match expected hash")
+
+	propHash, err = utils.GetProposalHash1(hdr,
+		[]byte("ccproppayload"), []byte{})
+	assert.Error(t, err,
+		"Expected error with malformed chaincode proposal payload")
+
+	propHash, err = utils.GetProposalHash1(&cb.Header{},
+		[]byte("ccproppayload"), []byte{})
+	assert.Error(t, err, "Expected error with nil arguments")
+}
+
+func TestCreateProposalResponseFailure(t *testing.T) {
+	// create a proposal from a ChaincodeInvocationSpec
+	prop, _, err := utils.CreateChaincodeProposal(cb.HeaderType_ENDORSER_TRANSACTION, util.GetTestChainID(), createCIS(), signerSerialized)
+	if err != nil {
+		t.Fatalf("Could not create chaincode proposal, err %s\n", err)
+		return
+	}
+
+	response := &pb.Response{Status: 502, Payload: []byte("Invalid function name")}
+	result := []byte("res")
+	ccid := &pb.ChaincodeID{Name: "foo", Version: "v1"}
+
+	prespFailure, err := utils.CreateProposalResponseFailure(prop.Header, prop.Payload, response, result, nil, ccid, nil)
+	if err != nil {
+		t.Fatalf("Could not create proposal response failure, err %s\n", err)
+		return
+	}
+
+	assert.Equal(t, int32(502), prespFailure.Response.Status)
+	// drilldown into the response to find the chaincode response
+	pRespPayload, err := utils.GetProposalResponsePayload(prespFailure.Payload)
+	assert.NoError(t, err, "Error while unmarshaling proposal response payload: %s", err)
+	ca, err := utils.GetChaincodeAction(pRespPayload.Extension)
+	assert.NoError(t, err, "Error while unmarshaling chaincode action: %s", err)
+
+	assert.Equal(t, int32(502), ca.Response.Status)
+	assert.Equal(t, "Invalid function name", string(ca.Response.Payload))
+}
+
+// mock
+var badSigner = &mockLocalSigner{
+	returnError: true,
+}
+
+var goodSigner = &mockLocalSigner{
+	returnError: false,
+}
+
+type mockLocalSigner struct {
+	returnError bool
+}
+
+func (m *mockLocalSigner) NewSignatureHeader() (*cb.SignatureHeader, error) {
+	if m.returnError {
+		return nil, errors.New("signature header error")
+	}
+	return &cb.SignatureHeader{}, nil
+}
+
+func (m *mockLocalSigner) Sign(message []byte) ([]byte, error) {
+	if m.returnError {
+		return nil, errors.New("sign error")
+	}
+	return message, nil
+}
diff --git a/vendor/github.com/bluele/gcache/LICENSE b/vendor/github.com/bluele/gcache/LICENSE
new file mode 100644
index 000000000..d1e7b03e3
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/LICENSE
@@ -0,0 +1,21 @@
+The MIT License (MIT)
+
+Copyright (c) 2017 Jun Kimura
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
diff --git a/vendor/github.com/bluele/gcache/arc.go b/vendor/github.com/bluele/gcache/arc.go
new file mode 100644
index 000000000..5215dca9e
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/arc.go
@@ -0,0 +1,452 @@
+package gcache
+
+import (
+	"container/list"
+	"time"
+)
+
+// Constantly balances between LRU and LFU, to improve the combined result.
+type ARC struct {
+	baseCache
+	items map[interface{}]*arcItem
+
+	part int
+	t1   *arcList
+	t2   *arcList
+	b1   *arcList
+	b2   *arcList
+}
+
+func newARC(cb *CacheBuilder) *ARC {
+	c := &ARC{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *ARC) init() {
+	c.items = make(map[interface{}]*arcItem)
+	c.t1 = newARCList()
+	c.t2 = newARCList()
+	c.b1 = newARCList()
+	c.b2 = newARCList()
+}
+
+func (c *ARC) replace(key interface{}) {
+	if !c.isCacheFull() {
+		return
+	}
+	var old interface{}
+	if c.t1.Len() > 0 && ((c.b2.Has(key) && c.t1.Len() == c.part) || (c.t1.Len() > c.part)) {
+		old = c.t1.RemoveTail()
+		c.b1.PushFront(old)
+	} else if c.t2.Len() > 0 {
+		old = c.t2.RemoveTail()
+		c.b2.PushFront(old)
+	} else {
+		old = c.t1.RemoveTail()
+		c.b1.PushFront(old)
+	}
+	item, ok := c.items[old]
+	if ok {
+		delete(c.items, old)
+		if c.evictedFunc != nil {
+			c.evictedFunc(item.key, item.value)
+		}
+	}
+}
+
+func (c *ARC) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *ARC) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*arcItem).expiration = &t
+	return nil
+}
+
+func (c *ARC) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	item, ok := c.items[key]
+	if ok {
+		item.value = value
+	} else {
+		item = &arcItem{
+			clock: c.clock,
+			key:   key,
+			value: value,
+		}
+		c.items[key] = item
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	defer func() {
+		if c.addedFunc != nil {
+			c.addedFunc(key, value)
+		}
+	}()
+
+	if c.t1.Has(key) || c.t2.Has(key) {
+		return item, nil
+	}
+
+	if elt := c.b1.Lookup(key); elt != nil {
+		c.setPart(minInt(c.size, c.part+maxInt(c.b2.Len()/c.b1.Len(), 1)))
+		c.replace(key)
+		c.b1.Remove(key, elt)
+		c.t2.PushFront(key)
+		return item, nil
+	}
+
+	if elt := c.b2.Lookup(key); elt != nil {
+		c.setPart(maxInt(0, c.part-maxInt(c.b1.Len()/c.b2.Len(), 1)))
+		c.replace(key)
+		c.b2.Remove(key, elt)
+		c.t2.PushFront(key)
+		return item, nil
+	}
+
+	if c.isCacheFull() && c.t1.Len()+c.b1.Len() == c.size {
+		if c.t1.Len() < c.size {
+			c.b1.RemoveTail()
+			c.replace(key)
+		} else {
+			pop := c.t1.RemoveTail()
+			item, ok := c.items[pop]
+			if ok {
+				delete(c.items, pop)
+				if c.evictedFunc != nil {
+					c.evictedFunc(item.key, item.value)
+				}
+			}
+		}
+	} else {
+		total := c.t1.Len() + c.b1.Len() + c.t2.Len() + c.b2.Len()
+		if total >= c.size {
+			if total == (2 * c.size) {
+				if c.b2.Len() > 0 {
+					c.b2.RemoveTail()
+				} else {
+					c.b1.RemoveTail()
+				}
+			}
+			c.replace(key)
+		}
+	}
+	c.t1.PushFront(key)
+	return item, nil
+}
+
+// Get a value from cache pool using key if it exists. If not exists and it has LoaderFunc, it will generate the value using you have specified LoaderFunc method returns value.
+func (c *ARC) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *ARC) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, err
+}
+
+func (c *ARC) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *ARC) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	if elt := c.t1.Lookup(key); elt != nil {
+		c.t1.Remove(key, elt)
+		item := c.items[key]
+		if !item.IsExpired(nil) {
+			c.t2.PushFront(key)
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return item.value, nil
+		} else {
+			delete(c.items, key)
+			c.b1.PushFront(key)
+			if c.evictedFunc != nil {
+				c.evictedFunc(item.key, item.value)
+			}
+		}
+	}
+	if elt := c.t2.Lookup(key); elt != nil {
+		item := c.items[key]
+		if !item.IsExpired(nil) {
+			c.t2.MoveToFront(elt)
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return item.value, nil
+		} else {
+			delete(c.items, key)
+			c.t2.Remove(key, elt)
+			c.b2.PushFront(key)
+			if c.evictedFunc != nil {
+				c.evictedFunc(item.key, item.value)
+			}
+		}
+	}
+
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *ARC) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*arcItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+// Has checks if key exists in cache
+func (c *ARC) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *ARC) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *ARC) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *ARC) remove(key interface{}) bool {
+	if elt := c.t1.Lookup(key); elt != nil {
+		c.t1.Remove(key, elt)
+		item := c.items[key]
+		delete(c.items, key)
+		c.b1.PushFront(key)
+		if c.evictedFunc != nil {
+			c.evictedFunc(key, item.value)
+		}
+		return true
+	}
+
+	if elt := c.t2.Lookup(key); elt != nil {
+		c.t2.Remove(key, elt)
+		item := c.items[key]
+		delete(c.items, key)
+		c.b2.PushFront(key)
+		if c.evictedFunc != nil {
+			c.evictedFunc(key, item.value)
+		}
+		return true
+	}
+
+	return false
+}
+
+func (c *ARC) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *ARC) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *ARC) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *ARC) Len() int {
+	return len(c.GetALL())
+}
+
+// Purge is used to completely clear the cache
+func (c *ARC) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for _, item := range c.items {
+			c.purgeVisitorFunc(item.key, item.value)
+		}
+	}
+
+	c.init()
+}
+
+func (c *ARC) setPart(p int) {
+	if c.isCacheFull() {
+		c.part = p
+	}
+}
+
+func (c *ARC) isCacheFull() bool {
+	return (c.t1.Len() + c.t2.Len()) == c.size
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (it *arcItem) IsExpired(now *time.Time) bool {
+	if it.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := it.clock.Now()
+		now = &t
+	}
+	return it.expiration.Before(*now)
+}
+
+type arcList struct {
+	l    *list.List
+	keys map[interface{}]*list.Element
+}
+
+type arcItem struct {
+	clock      Clock
+	key        interface{}
+	value      interface{}
+	expiration *time.Time
+}
+
+func newARCList() *arcList {
+	return &arcList{
+		l:    list.New(),
+		keys: make(map[interface{}]*list.Element),
+	}
+}
+
+func (al *arcList) Has(key interface{}) bool {
+	_, ok := al.keys[key]
+	return ok
+}
+
+func (al *arcList) Lookup(key interface{}) *list.Element {
+	elt := al.keys[key]
+	return elt
+}
+
+func (al *arcList) MoveToFront(elt *list.Element) {
+	al.l.MoveToFront(elt)
+}
+
+func (al *arcList) PushFront(key interface{}) {
+	if elt, ok := al.keys[key]; ok {
+		al.l.MoveToFront(elt)
+		return
+	}
+	elt := al.l.PushFront(key)
+	al.keys[key] = elt
+}
+
+func (al *arcList) Remove(key interface{}, elt *list.Element) {
+	delete(al.keys, key)
+	al.l.Remove(elt)
+}
+
+func (al *arcList) RemoveTail() interface{} {
+	elt := al.l.Back()
+	al.l.Remove(elt)
+
+	key := elt.Value
+	delete(al.keys, key)
+
+	return key
+}
+
+func (al *arcList) Len() int {
+	return al.l.Len()
+}
diff --git a/vendor/github.com/bluele/gcache/cache.go b/vendor/github.com/bluele/gcache/cache.go
new file mode 100644
index 000000000..0ac855721
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/cache.go
@@ -0,0 +1,205 @@
+package gcache
+
+import (
+	"errors"
+	"fmt"
+	"sync"
+	"time"
+)
+
+const (
+	TYPE_SIMPLE = "simple"
+	TYPE_LRU    = "lru"
+	TYPE_LFU    = "lfu"
+	TYPE_ARC    = "arc"
+)
+
+var KeyNotFoundError = errors.New("Key not found.")
+
+type Cache interface {
+	Set(interface{}, interface{}) error
+	SetWithExpire(interface{}, interface{}, time.Duration) error
+	Get(interface{}) (interface{}, error)
+	GetIFPresent(interface{}) (interface{}, error)
+	GetALL() map[interface{}]interface{}
+	Has(interface{}) bool
+	get(interface{}, bool) (interface{}, error)
+	Remove(interface{}) bool
+	Purge()
+	Keys() []interface{}
+	Len() int
+
+	statsAccessor
+}
+
+type baseCache struct {
+	clock            Clock
+	size             int
+	loaderExpireFunc LoaderExpireFunc
+	evictedFunc      EvictedFunc
+	purgeVisitorFunc PurgeVisitorFunc
+	addedFunc        AddedFunc
+	deserializeFunc  DeserializeFunc
+	serializeFunc    SerializeFunc
+	expiration       *time.Duration
+	mu               sync.RWMutex
+	loadGroup        Group
+	*stats
+}
+
+type (
+	LoaderFunc       func(interface{}) (interface{}, error)
+	LoaderExpireFunc func(interface{}) (interface{}, *time.Duration, error)
+	EvictedFunc      func(interface{}, interface{})
+	PurgeVisitorFunc func(interface{}, interface{})
+	AddedFunc        func(interface{}, interface{})
+	DeserializeFunc  func(interface{}, interface{}) (interface{}, error)
+	SerializeFunc    func(interface{}, interface{}) (interface{}, error)
+)
+
+type CacheBuilder struct {
+	clock            Clock
+	tp               string
+	size             int
+	loaderExpireFunc LoaderExpireFunc
+	evictedFunc      EvictedFunc
+	purgeVisitorFunc PurgeVisitorFunc
+	addedFunc        AddedFunc
+	expiration       *time.Duration
+	deserializeFunc  DeserializeFunc
+	serializeFunc    SerializeFunc
+}
+
+func New(size int) *CacheBuilder {
+	return &CacheBuilder{
+		clock: NewRealClock(),
+		tp:    TYPE_SIMPLE,
+		size:  size,
+	}
+}
+
+func (cb *CacheBuilder) Clock(clock Clock) *CacheBuilder {
+	cb.clock = clock
+	return cb
+}
+
+// Set a loader function.
+// loaderFunc: create a new value with this function if cached value is expired.
+func (cb *CacheBuilder) LoaderFunc(loaderFunc LoaderFunc) *CacheBuilder {
+	cb.loaderExpireFunc = func(k interface{}) (interface{}, *time.Duration, error) {
+		v, err := loaderFunc(k)
+		return v, nil, err
+	}
+	return cb
+}
+
+// Set a loader function with expiration.
+// loaderExpireFunc: create a new value with this function if cached value is expired.
+// If nil returned instead of time.Duration from loaderExpireFunc than value will never expire.
+func (cb *CacheBuilder) LoaderExpireFunc(loaderExpireFunc LoaderExpireFunc) *CacheBuilder {
+	cb.loaderExpireFunc = loaderExpireFunc
+	return cb
+}
+
+func (cb *CacheBuilder) EvictType(tp string) *CacheBuilder {
+	cb.tp = tp
+	return cb
+}
+
+func (cb *CacheBuilder) Simple() *CacheBuilder {
+	return cb.EvictType(TYPE_SIMPLE)
+}
+
+func (cb *CacheBuilder) LRU() *CacheBuilder {
+	return cb.EvictType(TYPE_LRU)
+}
+
+func (cb *CacheBuilder) LFU() *CacheBuilder {
+	return cb.EvictType(TYPE_LFU)
+}
+
+func (cb *CacheBuilder) ARC() *CacheBuilder {
+	return cb.EvictType(TYPE_ARC)
+}
+
+func (cb *CacheBuilder) EvictedFunc(evictedFunc EvictedFunc) *CacheBuilder {
+	cb.evictedFunc = evictedFunc
+	return cb
+}
+
+func (cb *CacheBuilder) PurgeVisitorFunc(purgeVisitorFunc PurgeVisitorFunc) *CacheBuilder {
+	cb.purgeVisitorFunc = purgeVisitorFunc
+	return cb
+}
+
+func (cb *CacheBuilder) AddedFunc(addedFunc AddedFunc) *CacheBuilder {
+	cb.addedFunc = addedFunc
+	return cb
+}
+
+func (cb *CacheBuilder) DeserializeFunc(deserializeFunc DeserializeFunc) *CacheBuilder {
+	cb.deserializeFunc = deserializeFunc
+	return cb
+}
+
+func (cb *CacheBuilder) SerializeFunc(serializeFunc SerializeFunc) *CacheBuilder {
+	cb.serializeFunc = serializeFunc
+	return cb
+}
+
+func (cb *CacheBuilder) Expiration(expiration time.Duration) *CacheBuilder {
+	cb.expiration = &expiration
+	return cb
+}
+
+func (cb *CacheBuilder) Build() Cache {
+	if cb.size <= 0 && cb.tp != TYPE_SIMPLE {
+		panic("gcache: Cache size <= 0")
+	}
+
+	return cb.build()
+}
+
+func (cb *CacheBuilder) build() Cache {
+	switch cb.tp {
+	case TYPE_SIMPLE:
+		return newSimpleCache(cb)
+	case TYPE_LRU:
+		return newLRUCache(cb)
+	case TYPE_LFU:
+		return newLFUCache(cb)
+	case TYPE_ARC:
+		return newARC(cb)
+	default:
+		panic("gcache: Unknown type " + cb.tp)
+	}
+}
+
+func buildCache(c *baseCache, cb *CacheBuilder) {
+	c.clock = cb.clock
+	c.size = cb.size
+	c.loaderExpireFunc = cb.loaderExpireFunc
+	c.expiration = cb.expiration
+	c.addedFunc = cb.addedFunc
+	c.deserializeFunc = cb.deserializeFunc
+	c.serializeFunc = cb.serializeFunc
+	c.evictedFunc = cb.evictedFunc
+	c.purgeVisitorFunc = cb.purgeVisitorFunc
+	c.stats = &stats{}
+}
+
+// load a new value using by specified key.
+func (c *baseCache) load(key interface{}, cb func(interface{}, *time.Duration, error) (interface{}, error), isWait bool) (interface{}, bool, error) {
+	v, called, err := c.loadGroup.Do(key, func() (v interface{}, e error) {
+		defer func() {
+			if r := recover(); r != nil {
+				e = fmt.Errorf("Loader panics: %v", r)
+			}
+		}()
+		return cb(c.loaderExpireFunc(key))
+	}, isWait)
+	if err != nil {
+		return nil, called, err
+	}
+	return v, called, nil
+}
diff --git a/vendor/github.com/bluele/gcache/clock.go b/vendor/github.com/bluele/gcache/clock.go
new file mode 100644
index 000000000..3acc3f0db
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/clock.go
@@ -0,0 +1,53 @@
+package gcache
+
+import (
+	"sync"
+	"time"
+)
+
+type Clock interface {
+	Now() time.Time
+}
+
+type RealClock struct{}
+
+func NewRealClock() Clock {
+	return RealClock{}
+}
+
+func (rc RealClock) Now() time.Time {
+	t := time.Now()
+	return t
+}
+
+type FakeClock interface {
+	Clock
+
+	Advance(d time.Duration)
+}
+
+func NewFakeClock() FakeClock {
+	return &fakeclock{
+		// Taken from github.com/jonboulle/clockwork: use a fixture that does not fulfill Time.IsZero()
+		now: time.Date(1984, time.April, 4, 0, 0, 0, 0, time.UTC),
+	}
+}
+
+type fakeclock struct {
+	now time.Time
+
+	mutex sync.RWMutex
+}
+
+func (fc *fakeclock) Now() time.Time {
+	fc.mutex.RLock()
+	defer fc.mutex.RUnlock()
+	t := fc.now
+	return t
+}
+
+func (fc *fakeclock) Advance(d time.Duration) {
+	fc.mutex.Lock()
+	defer fc.mutex.Unlock()
+	fc.now = fc.now.Add(d)
+}
diff --git a/vendor/github.com/bluele/gcache/lfu.go b/vendor/github.com/bluele/gcache/lfu.go
new file mode 100644
index 000000000..915ac8cc4
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/lfu.go
@@ -0,0 +1,335 @@
+package gcache
+
+import (
+	"container/list"
+	"time"
+)
+
+// Discards the least frequently used items first.
+type LFUCache struct {
+	baseCache
+	items    map[interface{}]*lfuItem
+	freqList *list.List // list for freqEntry
+}
+
+func newLFUCache(cb *CacheBuilder) *LFUCache {
+	c := &LFUCache{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *LFUCache) init() {
+	c.freqList = list.New()
+	c.items = make(map[interface{}]*lfuItem, c.size+1)
+	c.freqList.PushFront(&freqEntry{
+		freq:  0,
+		items: make(map[*lfuItem]struct{}),
+	})
+}
+
+// Set a new key-value pair
+func (c *LFUCache) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *LFUCache) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*lfuItem).expiration = &t
+	return nil
+}
+
+func (c *LFUCache) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	// Check for existing item
+	item, ok := c.items[key]
+	if ok {
+		item.value = value
+	} else {
+		// Verify size not exceeded
+		if len(c.items) >= c.size {
+			c.evict(1)
+		}
+		item = &lfuItem{
+			clock:       c.clock,
+			key:         key,
+			value:       value,
+			freqElement: nil,
+		}
+		el := c.freqList.Front()
+		fe := el.Value.(*freqEntry)
+		fe.items[item] = struct{}{}
+
+		item.freqElement = el
+		c.items[key] = item
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	if c.addedFunc != nil {
+		c.addedFunc(key, value)
+	}
+
+	return item, nil
+}
+
+// Get a value from cache pool using key if it exists.
+// If it dose not exists key and has LoaderFunc,
+// generate a value using `LoaderFunc` method returns value.
+func (c *LFUCache) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *LFUCache) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, err
+}
+
+func (c *LFUCache) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *LFUCache) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	item, ok := c.items[key]
+	if ok {
+		if !item.IsExpired(nil) {
+			c.increment(item)
+			v := item.value
+			c.mu.Unlock()
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return v, nil
+		}
+		c.removeItem(item)
+	}
+	c.mu.Unlock()
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *LFUCache) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*lfuItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+func (c *LFUCache) increment(item *lfuItem) {
+	currentFreqElement := item.freqElement
+	currentFreqEntry := currentFreqElement.Value.(*freqEntry)
+	nextFreq := currentFreqEntry.freq + 1
+	delete(currentFreqEntry.items, item)
+
+	nextFreqElement := currentFreqElement.Next()
+	if nextFreqElement == nil {
+		nextFreqElement = c.freqList.InsertAfter(&freqEntry{
+			freq:  nextFreq,
+			items: make(map[*lfuItem]struct{}),
+		}, currentFreqElement)
+	}
+	nextFreqElement.Value.(*freqEntry).items[item] = struct{}{}
+	item.freqElement = nextFreqElement
+}
+
+// evict removes the least frequence item from the cache.
+func (c *LFUCache) evict(count int) {
+	entry := c.freqList.Front()
+	for i := 0; i < count; {
+		if entry == nil {
+			return
+		} else {
+			for item, _ := range entry.Value.(*freqEntry).items {
+				if i >= count {
+					return
+				}
+				c.removeItem(item)
+				i++
+			}
+			entry = entry.Next()
+		}
+	}
+}
+
+// Has checks if key exists in cache
+func (c *LFUCache) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *LFUCache) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *LFUCache) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *LFUCache) remove(key interface{}) bool {
+	if item, ok := c.items[key]; ok {
+		c.removeItem(item)
+		return true
+	}
+	return false
+}
+
+// removeElement is used to remove a given list element from the cache
+func (c *LFUCache) removeItem(item *lfuItem) {
+	delete(c.items, item.key)
+	delete(item.freqElement.Value.(*freqEntry).items, item)
+	if c.evictedFunc != nil {
+		c.evictedFunc(item.key, item.value)
+	}
+}
+
+func (c *LFUCache) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *LFUCache) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *LFUCache) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *LFUCache) Len() int {
+	return len(c.GetALL())
+}
+
+// Completely clear the cache
+func (c *LFUCache) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for key, item := range c.items {
+			c.purgeVisitorFunc(key, item.value)
+		}
+	}
+
+	c.init()
+}
+
+type freqEntry struct {
+	freq  uint
+	items map[*lfuItem]struct{}
+}
+
+type lfuItem struct {
+	clock       Clock
+	key         interface{}
+	value       interface{}
+	freqElement *list.Element
+	expiration  *time.Time
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (it *lfuItem) IsExpired(now *time.Time) bool {
+	if it.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := it.clock.Now()
+		now = &t
+	}
+	return it.expiration.Before(*now)
+}
diff --git a/vendor/github.com/bluele/gcache/lru.go b/vendor/github.com/bluele/gcache/lru.go
new file mode 100644
index 000000000..10061b08c
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/lru.go
@@ -0,0 +1,301 @@
+package gcache
+
+import (
+	"container/list"
+	"time"
+)
+
+// Discards the least recently used items first.
+type LRUCache struct {
+	baseCache
+	items     map[interface{}]*list.Element
+	evictList *list.List
+}
+
+func newLRUCache(cb *CacheBuilder) *LRUCache {
+	c := &LRUCache{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *LRUCache) init() {
+	c.evictList = list.New()
+	c.items = make(map[interface{}]*list.Element, c.size+1)
+}
+
+func (c *LRUCache) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	// Check for existing item
+	var item *lruItem
+	if it, ok := c.items[key]; ok {
+		c.evictList.MoveToFront(it)
+		item = it.Value.(*lruItem)
+		item.value = value
+	} else {
+		// Verify size not exceeded
+		if c.evictList.Len() >= c.size {
+			c.evict(1)
+		}
+		item = &lruItem{
+			clock: c.clock,
+			key:   key,
+			value: value,
+		}
+		c.items[key] = c.evictList.PushFront(item)
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	if c.addedFunc != nil {
+		c.addedFunc(key, value)
+	}
+
+	return item, nil
+}
+
+// set a new key-value pair
+func (c *LRUCache) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *LRUCache) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*lruItem).expiration = &t
+	return nil
+}
+
+// Get a value from cache pool using key if it exists.
+// If it dose not exists key and has LoaderFunc,
+// generate a value using `LoaderFunc` method returns value.
+func (c *LRUCache) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *LRUCache) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, err
+}
+
+func (c *LRUCache) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *LRUCache) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	item, ok := c.items[key]
+	if ok {
+		it := item.Value.(*lruItem)
+		if !it.IsExpired(nil) {
+			c.evictList.MoveToFront(item)
+			v := it.value
+			c.mu.Unlock()
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return v, nil
+		}
+		c.removeElement(item)
+	}
+	c.mu.Unlock()
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *LRUCache) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*lruItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+// evict removes the oldest item from the cache.
+func (c *LRUCache) evict(count int) {
+	for i := 0; i < count; i++ {
+		ent := c.evictList.Back()
+		if ent == nil {
+			return
+		} else {
+			c.removeElement(ent)
+		}
+	}
+}
+
+// Has checks if key exists in cache
+func (c *LRUCache) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *LRUCache) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.Value.(*lruItem).IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *LRUCache) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *LRUCache) remove(key interface{}) bool {
+	if ent, ok := c.items[key]; ok {
+		c.removeElement(ent)
+		return true
+	}
+	return false
+}
+
+func (c *LRUCache) removeElement(e *list.Element) {
+	c.evictList.Remove(e)
+	entry := e.Value.(*lruItem)
+	delete(c.items, entry.key)
+	if c.evictedFunc != nil {
+		entry := e.Value.(*lruItem)
+		c.evictedFunc(entry.key, entry.value)
+	}
+}
+
+func (c *LRUCache) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *LRUCache) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *LRUCache) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *LRUCache) Len() int {
+	return len(c.GetALL())
+}
+
+// Completely clear the cache
+func (c *LRUCache) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for key, item := range c.items {
+			it := item.Value.(*lruItem)
+			v := it.value
+			c.purgeVisitorFunc(key, v)
+		}
+	}
+
+	c.init()
+}
+
+type lruItem struct {
+	clock      Clock
+	key        interface{}
+	value      interface{}
+	expiration *time.Time
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (it *lruItem) IsExpired(now *time.Time) bool {
+	if it.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := it.clock.Now()
+		now = &t
+	}
+	return it.expiration.Before(*now)
+}
diff --git a/vendor/github.com/bluele/gcache/simple.go b/vendor/github.com/bluele/gcache/simple.go
new file mode 100644
index 000000000..befc03685
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/simple.go
@@ -0,0 +1,289 @@
+package gcache
+
+import "time"
+
+// SimpleCache has no clear priority for evict cache. It depends on key-value map order.
+type SimpleCache struct {
+	baseCache
+	items map[interface{}]*simpleItem
+}
+
+func newSimpleCache(cb *CacheBuilder) *SimpleCache {
+	c := &SimpleCache{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *SimpleCache) init() {
+	if c.size <= 0 {
+		c.items = make(map[interface{}]*simpleItem)
+	} else {
+		c.items = make(map[interface{}]*simpleItem, c.size)
+	}
+}
+
+// Set a new key-value pair
+func (c *SimpleCache) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *SimpleCache) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*simpleItem).expiration = &t
+	return nil
+}
+
+func (c *SimpleCache) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	// Check for existing item
+	item, ok := c.items[key]
+	if ok {
+		item.value = value
+	} else {
+		// Verify size not exceeded
+		if (len(c.items) >= c.size) && c.size > 0 {
+			c.evict(1)
+		}
+		item = &simpleItem{
+			clock: c.clock,
+			value: value,
+		}
+		c.items[key] = item
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	if c.addedFunc != nil {
+		c.addedFunc(key, value)
+	}
+
+	return item, nil
+}
+
+// Get a value from cache pool using key if it exists.
+// If it dose not exists key and has LoaderFunc,
+// generate a value using `LoaderFunc` method returns value.
+func (c *SimpleCache) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *SimpleCache) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, nil
+}
+
+func (c *SimpleCache) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *SimpleCache) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	item, ok := c.items[key]
+	if ok {
+		if !item.IsExpired(nil) {
+			v := item.value
+			c.mu.Unlock()
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return v, nil
+		}
+		c.remove(key)
+	}
+	c.mu.Unlock()
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *SimpleCache) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*simpleItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+func (c *SimpleCache) evict(count int) {
+	now := c.clock.Now()
+	current := 0
+	for key, item := range c.items {
+		if current >= count {
+			return
+		}
+		if item.expiration == nil || now.After(*item.expiration) {
+			defer c.remove(key)
+			current++
+		}
+	}
+}
+
+// Has checks if key exists in cache
+func (c *SimpleCache) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *SimpleCache) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *SimpleCache) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *SimpleCache) remove(key interface{}) bool {
+	item, ok := c.items[key]
+	if ok {
+		delete(c.items, key)
+		if c.evictedFunc != nil {
+			c.evictedFunc(key, item.value)
+		}
+		return true
+	}
+	return false
+}
+
+// Returns a slice of the keys in the cache.
+func (c *SimpleCache) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *SimpleCache) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *SimpleCache) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *SimpleCache) Len() int {
+	return len(c.GetALL())
+}
+
+// Completely clear the cache
+func (c *SimpleCache) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for key, item := range c.items {
+			c.purgeVisitorFunc(key, item.value)
+		}
+	}
+
+	c.init()
+}
+
+type simpleItem struct {
+	clock      Clock
+	value      interface{}
+	expiration *time.Time
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (si *simpleItem) IsExpired(now *time.Time) bool {
+	if si.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := si.clock.Now()
+		now = &t
+	}
+	return si.expiration.Before(*now)
+}
diff --git a/vendor/github.com/bluele/gcache/singleflight.go b/vendor/github.com/bluele/gcache/singleflight.go
new file mode 100644
index 000000000..2c6285e82
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/singleflight.go
@@ -0,0 +1,82 @@
+package gcache
+
+/*
+Copyright 2012 Google Inc.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+// This module provides a duplicate function call suppression
+// mechanism.
+
+import "sync"
+
+// call is an in-flight or completed Do call
+type call struct {
+	wg  sync.WaitGroup
+	val interface{}
+	err error
+}
+
+// Group represents a class of work and forms a namespace in which
+// units of work can be executed with duplicate suppression.
+type Group struct {
+	cache Cache
+	mu    sync.Mutex            // protects m
+	m     map[interface{}]*call // lazily initialized
+}
+
+// Do executes and returns the results of the given function, making
+// sure that only one execution is in-flight for a given key at a
+// time. If a duplicate comes in, the duplicate caller waits for the
+// original to complete and receives the same results.
+func (g *Group) Do(key interface{}, fn func() (interface{}, error), isWait bool) (interface{}, bool, error) {
+	g.mu.Lock()
+	v, err := g.cache.get(key, true)
+	if err == nil {
+		g.mu.Unlock()
+		return v, false, nil
+	}
+	if g.m == nil {
+		g.m = make(map[interface{}]*call)
+	}
+	if c, ok := g.m[key]; ok {
+		g.mu.Unlock()
+		if !isWait {
+			return nil, false, KeyNotFoundError
+		}
+		c.wg.Wait()
+		return c.val, false, c.err
+	}
+	c := new(call)
+	c.wg.Add(1)
+	g.m[key] = c
+	g.mu.Unlock()
+	if !isWait {
+		go g.call(c, key, fn)
+		return nil, false, KeyNotFoundError
+	}
+	v, err = g.call(c, key, fn)
+	return v, true, err
+}
+
+func (g *Group) call(c *call, key interface{}, fn func() (interface{}, error)) (interface{}, error) {
+	c.val, c.err = fn()
+	c.wg.Done()
+
+	g.mu.Lock()
+	delete(g.m, key)
+	g.mu.Unlock()
+
+	return c.val, c.err
+}
diff --git a/vendor/github.com/bluele/gcache/stats.go b/vendor/github.com/bluele/gcache/stats.go
new file mode 100644
index 000000000..ca0bf3185
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/stats.go
@@ -0,0 +1,53 @@
+package gcache
+
+import (
+	"sync/atomic"
+)
+
+type statsAccessor interface {
+	HitCount() uint64
+	MissCount() uint64
+	LookupCount() uint64
+	HitRate() float64
+}
+
+// statistics
+type stats struct {
+	hitCount  uint64
+	missCount uint64
+}
+
+// increment hit count
+func (st *stats) IncrHitCount() uint64 {
+	return atomic.AddUint64(&st.hitCount, 1)
+}
+
+// increment miss count
+func (st *stats) IncrMissCount() uint64 {
+	return atomic.AddUint64(&st.missCount, 1)
+}
+
+// HitCount returns hit count
+func (st *stats) HitCount() uint64 {
+	return atomic.LoadUint64(&st.hitCount)
+}
+
+// MissCount returns miss count
+func (st *stats) MissCount() uint64 {
+	return atomic.LoadUint64(&st.missCount)
+}
+
+// LookupCount returns lookup count
+func (st *stats) LookupCount() uint64 {
+	return st.HitCount() + st.MissCount()
+}
+
+// HitRate returns rate for cache hitting
+func (st *stats) HitRate() float64 {
+	hc, mc := st.HitCount(), st.MissCount()
+	total := hc + mc
+	if total == 0 {
+		return 0.0
+	}
+	return float64(hc) / float64(total)
+}
diff --git a/vendor/github.com/bluele/gcache/utils.go b/vendor/github.com/bluele/gcache/utils.go
new file mode 100644
index 000000000..1f784e4c4
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/utils.go
@@ -0,0 +1,15 @@
+package gcache
+
+func minInt(x, y int) int {
+	if x < y {
+		return x
+	}
+	return y
+}
+
+func maxInt(x, y int) int {
+	if x > y {
+		return x
+	}
+	return y
+}
diff --git a/vendor/github.com/btcsuite/btcutil/LICENSE b/vendor/github.com/btcsuite/btcutil/LICENSE
new file mode 100644
index 000000000..3e7b16791
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/LICENSE
@@ -0,0 +1,16 @@
+ISC License
+
+Copyright (c) 2013-2017 The btcsuite developers
+Copyright (c) 2016-2017 The Lightning Network Developers
+
+Permission to use, copy, modify, and distribute this software for any
+purpose with or without fee is hereby granted, provided that the above
+copyright notice and this permission notice appear in all copies.
+
+THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
+ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
+ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
+OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
diff --git a/vendor/github.com/btcsuite/btcutil/base58/alphabet.go b/vendor/github.com/btcsuite/btcutil/base58/alphabet.go
new file mode 100644
index 000000000..6bb39fef1
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/alphabet.go
@@ -0,0 +1,49 @@
+// Copyright (c) 2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+// AUTOGENERATED by genalphabet.go; do not edit.
+
+package base58
+
+const (
+	// alphabet is the modified base58 alphabet used by Bitcoin.
+	alphabet = "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz"
+
+	alphabetIdx0 = '1'
+)
+
+var b58 = [256]byte{
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 0, 1, 2, 3, 4, 5, 6,
+	7, 8, 255, 255, 255, 255, 255, 255,
+	255, 9, 10, 11, 12, 13, 14, 15,
+	16, 255, 17, 18, 19, 20, 21, 255,
+	22, 23, 24, 25, 26, 27, 28, 29,
+	30, 31, 32, 255, 255, 255, 255, 255,
+	255, 33, 34, 35, 36, 37, 38, 39,
+	40, 41, 42, 43, 255, 44, 45, 46,
+	47, 48, 49, 50, 51, 52, 53, 54,
+	55, 56, 57, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+}
diff --git a/vendor/github.com/btcsuite/btcutil/base58/base58.go b/vendor/github.com/btcsuite/btcutil/base58/base58.go
new file mode 100644
index 000000000..19a72de2c
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/base58.go
@@ -0,0 +1,75 @@
+// Copyright (c) 2013-2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+package base58
+
+import (
+	"math/big"
+)
+
+//go:generate go run genalphabet.go
+
+var bigRadix = big.NewInt(58)
+var bigZero = big.NewInt(0)
+
+// Decode decodes a modified base58 string to a byte slice.
+func Decode(b string) []byte {
+	answer := big.NewInt(0)
+	j := big.NewInt(1)
+
+	scratch := new(big.Int)
+	for i := len(b) - 1; i >= 0; i-- {
+		tmp := b58[b[i]]
+		if tmp == 255 {
+			return []byte("")
+		}
+		scratch.SetInt64(int64(tmp))
+		scratch.Mul(j, scratch)
+		answer.Add(answer, scratch)
+		j.Mul(j, bigRadix)
+	}
+
+	tmpval := answer.Bytes()
+
+	var numZeros int
+	for numZeros = 0; numZeros < len(b); numZeros++ {
+		if b[numZeros] != alphabetIdx0 {
+			break
+		}
+	}
+	flen := numZeros + len(tmpval)
+	val := make([]byte, flen)
+	copy(val[numZeros:], tmpval)
+
+	return val
+}
+
+// Encode encodes a byte slice to a modified base58 string.
+func Encode(b []byte) string {
+	x := new(big.Int)
+	x.SetBytes(b)
+
+	answer := make([]byte, 0, len(b)*136/100)
+	for x.Cmp(bigZero) > 0 {
+		mod := new(big.Int)
+		x.DivMod(x, bigRadix, mod)
+		answer = append(answer, alphabet[mod.Int64()])
+	}
+
+	// leading zero bytes
+	for _, i := range b {
+		if i != 0 {
+			break
+		}
+		answer = append(answer, alphabetIdx0)
+	}
+
+	// reverse
+	alen := len(answer)
+	for i := 0; i < alen/2; i++ {
+		answer[i], answer[alen-1-i] = answer[alen-1-i], answer[i]
+	}
+
+	return string(answer)
+}
diff --git a/vendor/github.com/btcsuite/btcutil/base58/base58check.go b/vendor/github.com/btcsuite/btcutil/base58/base58check.go
new file mode 100644
index 000000000..7cdafeeec
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/base58check.go
@@ -0,0 +1,52 @@
+// Copyright (c) 2013-2014 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+package base58
+
+import (
+	"crypto/sha256"
+	"errors"
+)
+
+// ErrChecksum indicates that the checksum of a check-encoded string does not verify against
+// the checksum.
+var ErrChecksum = errors.New("checksum error")
+
+// ErrInvalidFormat indicates that the check-encoded string has an invalid format.
+var ErrInvalidFormat = errors.New("invalid format: version and/or checksum bytes missing")
+
+// checksum: first four bytes of sha256^2
+func checksum(input []byte) (cksum [4]byte) {
+	h := sha256.Sum256(input)
+	h2 := sha256.Sum256(h[:])
+	copy(cksum[:], h2[:4])
+	return
+}
+
+// CheckEncode prepends a version byte and appends a four byte checksum.
+func CheckEncode(input []byte, version byte) string {
+	b := make([]byte, 0, 1+len(input)+4)
+	b = append(b, version)
+	b = append(b, input[:]...)
+	cksum := checksum(b)
+	b = append(b, cksum[:]...)
+	return Encode(b)
+}
+
+// CheckDecode decodes a string that was encoded with CheckEncode and verifies the checksum.
+func CheckDecode(input string) (result []byte, version byte, err error) {
+	decoded := Decode(input)
+	if len(decoded) < 5 {
+		return nil, 0, ErrInvalidFormat
+	}
+	version = decoded[0]
+	var cksum [4]byte
+	copy(cksum[:], decoded[len(decoded)-4:])
+	if checksum(decoded[:len(decoded)-4]) != cksum {
+		return nil, 0, ErrChecksum
+	}
+	payload := decoded[1 : len(decoded)-4]
+	result = append(result, payload...)
+	return
+}
diff --git a/vendor/github.com/btcsuite/btcutil/base58/doc.go b/vendor/github.com/btcsuite/btcutil/base58/doc.go
new file mode 100644
index 000000000..9a2c0e6e3
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/doc.go
@@ -0,0 +1,29 @@
+// Copyright (c) 2014 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+/*
+Package base58 provides an API for working with modified base58 and Base58Check
+encodings.
+
+Modified Base58 Encoding
+
+Standard base58 encoding is similar to standard base64 encoding except, as the
+name implies, it uses a 58 character alphabet which results in an alphanumeric
+string and allows some characters which are problematic for humans to be
+excluded.  Due to this, there can be various base58 alphabets.
+
+The modified base58 alphabet used by Bitcoin, and hence this package, omits the
+0, O, I, and l characters that look the same in many fonts and are therefore
+hard to humans to distinguish.
+
+Base58Check Encoding Scheme
+
+The Base58Check encoding scheme is primarily used for Bitcoin addresses at the
+time of this writing, however it can be used to generically encode arbitrary
+byte arrays into human-readable strings along with a version byte that can be
+used to differentiate the same payload.  For Bitcoin addresses, the extra
+version is used to differentiate the network of otherwise identical public keys
+which helps prevent using an address intended for one network on another.
+*/
+package base58
diff --git a/vendor/github.com/btcsuite/btcutil/base58/genalphabet.go b/vendor/github.com/btcsuite/btcutil/base58/genalphabet.go
new file mode 100644
index 000000000..010cbee39
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/genalphabet.go
@@ -0,0 +1,79 @@
+// Copyright (c) 2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+//+build ignore
+
+package main
+
+import (
+	"bytes"
+	"io"
+	"log"
+	"os"
+	"strconv"
+)
+
+var (
+	start = []byte(`// Copyright (c) 2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+// AUTOGENERATED by genalphabet.go; do not edit.
+
+package base58
+
+const (
+	// alphabet is the modified base58 alphabet used by Bitcoin.
+	alphabet = "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz"
+
+	alphabetIdx0 = '1'
+)
+
+var b58 = [256]byte{`)
+
+	end = []byte(`}`)
+
+	alphabet = []byte("123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz")
+	tab      = []byte("\t")
+	invalid  = []byte("255")
+	comma    = []byte(",")
+	space    = []byte(" ")
+	nl       = []byte("\n")
+)
+
+func write(w io.Writer, b []byte) {
+	_, err := w.Write(b)
+	if err != nil {
+		log.Fatal(err)
+	}
+}
+
+func main() {
+	fi, err := os.Create("alphabet.go")
+	if err != nil {
+		log.Fatal(err)
+	}
+	defer fi.Close()
+
+	write(fi, start)
+	write(fi, nl)
+	for i := byte(0); i < 32; i++ {
+		write(fi, tab)
+		for j := byte(0); j < 8; j++ {
+			idx := bytes.IndexByte(alphabet, i*8+j)
+			if idx == -1 {
+				write(fi, invalid)
+			} else {
+				write(fi, strconv.AppendInt(nil, int64(idx), 10))
+			}
+			write(fi, comma)
+			if j != 7 {
+				write(fi, space)
+			}
+		}
+		write(fi, nl)
+	}
+	write(fi, end)
+	write(fi, nl)
+}
diff --git a/vendor/github.com/magiconair/properties/assert/assert.go b/vendor/github.com/magiconair/properties/assert/assert.go
new file mode 100644
index 000000000..d0f270467
--- /dev/null
+++ b/vendor/github.com/magiconair/properties/assert/assert.go
@@ -0,0 +1,90 @@
+// Copyright 2018 Frank Schroeder. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package assert provides helper functions for testing.
+package assert
+
+import (
+	"fmt"
+	"path/filepath"
+	"reflect"
+	"regexp"
+	"runtime"
+	"strings"
+	"testing"
+)
+
+// skip defines the default call depth
+const skip = 2
+
+// Equal asserts that got and want are equal as defined by
+// reflect.DeepEqual. The test fails with msg if they are not equal.
+func Equal(t *testing.T, got, want interface{}, msg ...string) {
+	if x := equal(2, got, want, msg...); x != "" {
+		fmt.Println(x)
+		t.Fail()
+	}
+}
+
+func equal(skip int, got, want interface{}, msg ...string) string {
+	if !reflect.DeepEqual(got, want) {
+		return fail(skip, "got %v want %v %s", got, want, strings.Join(msg, " "))
+	}
+	return ""
+}
+
+// Panic asserts that function fn() panics.
+// It assumes that recover() either returns a string or
+// an error and fails if the message does not match
+// the regular expression in 'matches'.
+func Panic(t *testing.T, fn func(), matches string) {
+	if x := doesPanic(2, fn, matches); x != "" {
+		fmt.Println(x)
+		t.Fail()
+	}
+}
+
+func doesPanic(skip int, fn func(), expr string) (err string) {
+	defer func() {
+		r := recover()
+		if r == nil {
+			err = fail(skip, "did not panic")
+			return
+		}
+		var v string
+		switch r.(type) {
+		case error:
+			v = r.(error).Error()
+		case string:
+			v = r.(string)
+		}
+		err = matches(skip, v, expr)
+	}()
+	fn()
+	return ""
+}
+
+// Matches asserts that a value matches a given regular expression.
+func Matches(t *testing.T, value, expr string) {
+	if x := matches(2, value, expr); x != "" {
+		fmt.Println(x)
+		t.Fail()
+	}
+}
+
+func matches(skip int, value, expr string) string {
+	ok, err := regexp.MatchString(expr, value)
+	if err != nil {
+		return fail(skip, "invalid pattern %q. %s", expr, err)
+	}
+	if !ok {
+		return fail(skip, "got %s which does not match %s", value, expr)
+	}
+	return ""
+}
+
+func fail(skip int, format string, args ...interface{}) string {
+	_, file, line, _ := runtime.Caller(skip)
+	return fmt.Sprintf("\t%s:%d: %s\n", filepath.Base(file), line, fmt.Sprintf(format, args...))
+}
diff --git a/vendor/github.com/pkg/errors/errors.go b/vendor/github.com/pkg/errors/errors.go
index 842ee8045..7421f326f 100644
--- a/vendor/github.com/pkg/errors/errors.go
+++ b/vendor/github.com/pkg/errors/errors.go
@@ -6,7 +6,7 @@
 //             return err
 //     }
 //
-// which applied recursively up the call stack results in error reports
+// which when applied recursively up the call stack results in error reports
 // without context or debugging information. The errors package allows
 // programmers to add context to the failure path in their code in a way
 // that does not destroy the original value of the error.
@@ -15,16 +15,17 @@
 //
 // The errors.Wrap function returns a new error that adds context to the
 // original error by recording a stack trace at the point Wrap is called,
-// and the supplied message. For example
+// together with the supplied message. For example
 //
 //     _, err := ioutil.ReadAll(r)
 //     if err != nil {
 //             return errors.Wrap(err, "read failed")
 //     }
 //
-// If additional control is required the errors.WithStack and errors.WithMessage
-// functions destructure errors.Wrap into its component operations of annotating
-// an error with a stack trace and an a message, respectively.
+// If additional control is required, the errors.WithStack and
+// errors.WithMessage functions destructure errors.Wrap into its component
+// operations: annotating an error with a stack trace and with a message,
+// respectively.
 //
 // Retrieving the cause of an error
 //
@@ -38,7 +39,7 @@
 //     }
 //
 // can be inspected by errors.Cause. errors.Cause will recursively retrieve
-// the topmost error which does not implement causer, which is assumed to be
+// the topmost error that does not implement causer, which is assumed to be
 // the original cause. For example:
 //
 //     switch err := errors.Cause(err).(type) {
@@ -48,16 +49,16 @@
 //             // unknown error
 //     }
 //
-// causer interface is not exported by this package, but is considered a part
-// of stable public API.
+// Although the causer interface is not exported by this package, it is
+// considered a part of its stable public interface.
 //
 // Formatted printing of errors
 //
 // All error values returned from this package implement fmt.Formatter and can
-// be formatted by the fmt package. The following verbs are supported
+// be formatted by the fmt package. The following verbs are supported:
 //
 //     %s    print the error. If the error has a Cause it will be
-//           printed recursively
+//           printed recursively.
 //     %v    see %s
 //     %+v   extended format. Each Frame of the error's StackTrace will
 //           be printed in detail.
@@ -65,13 +66,13 @@
 // Retrieving the stack trace of an error or wrapper
 //
 // New, Errorf, Wrap, and Wrapf record a stack trace at the point they are
-// invoked. This information can be retrieved with the following interface.
+// invoked. This information can be retrieved with the following interface:
 //
 //     type stackTracer interface {
 //             StackTrace() errors.StackTrace
 //     }
 //
-// Where errors.StackTrace is defined as
+// The returned errors.StackTrace type is defined as
 //
 //     type StackTrace []Frame
 //
@@ -85,8 +86,8 @@
 //             }
 //     }
 //
-// stackTracer interface is not exported by this package, but is considered a part
-// of stable public API.
+// Although the stackTracer interface is not exported by this package, it is
+// considered a part of its stable public interface.
 //
 // See the documentation for Frame.Format for more details.
 package errors
@@ -192,7 +193,7 @@ func Wrap(err error, message string) error {
 }

 // Wrapf returns an error annotating err with a stack trace
-// at the point Wrapf is call, and the format specifier.
+// at the point Wrapf is called, and the format specifier.
 // If err is nil, Wrapf returns nil.
 func Wrapf(err error, format string, args ...interface{}) error {
 	if err == nil {
@@ -220,6 +221,18 @@ func WithMessage(err error, message string) error {
 	}
 }

+// WithMessagef annotates err with the format specifier.
+// If err is nil, WithMessagef returns nil.
+func WithMessagef(err error, format string, args ...interface{}) error {
+	if err == nil {
+		return nil
+	}
+	return &withMessage{
+		cause: err,
+		msg:   fmt.Sprintf(format, args...),
+	}
+}
+
 type withMessage struct {
 	cause error
 	msg   string
diff --git a/vendor/github.com/pkg/errors/stack.go b/vendor/github.com/pkg/errors/stack.go
index 6b1f2891a..2874a048c 100644
--- a/vendor/github.com/pkg/errors/stack.go
+++ b/vendor/github.com/pkg/errors/stack.go
@@ -46,7 +46,8 @@ func (f Frame) line() int {
 //
 // Format accepts flags that alter the printing of some verbs, as follows:
 //
-//    %+s   path of source file relative to the compile time GOPATH
+//    %+s   function name and path of source file relative to the compile time
+//          GOPATH separated by \n\t (<funcname>\n\t<path>)
 //    %+v   equivalent to %+s:%d
 func (f Frame) Format(s fmt.State, verb rune) {
 	switch verb {
@@ -79,6 +80,14 @@ func (f Frame) Format(s fmt.State, verb rune) {
 // StackTrace is stack of Frames from innermost (newest) to outermost (oldest).
 type StackTrace []Frame

+// Format formats the stack of Frames according to the fmt.Formatter interface.
+//
+//    %s	lists source files for each Frame in the stack
+//    %v	lists the source file and line number for each Frame in the stack
+//
+// Format accepts flags that alter the printing of some verbs, as follows:
+//
+//    %+v   Prints filename, function, and line number for each Frame in the stack.
 func (st StackTrace) Format(s fmt.State, verb rune) {
 	switch verb {
 	case 'v':
@@ -136,43 +145,3 @@ func funcname(name string) string {
 	i = strings.Index(name, ".")
 	return name[i+1:]
 }
-
-func trimGOPATH(name, file string) string {
-	// Here we want to get the source file path relative to the compile time
-	// GOPATH. As of Go 1.6.x there is no direct way to know the compiled
-	// GOPATH at runtime, but we can infer the number of path segments in the
-	// GOPATH. We note that fn.Name() returns the function name qualified by
-	// the import path, which does not include the GOPATH. Thus we can trim
-	// segments from the beginning of the file path until the number of path
-	// separators remaining is one more than the number of path separators in
-	// the function name. For example, given:
-	//
-	//    GOPATH     /home/user
-	//    file       /home/user/src/pkg/sub/file.go
-	//    fn.Name()  pkg/sub.Type.Method
-	//
-	// We want to produce:
-	//
-	//    pkg/sub/file.go
-	//
-	// From this we can easily see that fn.Name() has one less path separator
-	// than our desired output. We count separators from the end of the file
-	// path until it finds two more than in the function name and then move
-	// one character forward to preserve the initial path segment without a
-	// leading separator.
-	const sep = "/"
-	goal := strings.Count(name, sep) + 2
-	i := len(file)
-	for n := 0; n < goal; n++ {
-		i = strings.LastIndex(file[:i], sep)
-		if i == -1 {
-			// not enough separators found, set i so that the slice expression
-			// below leaves file unmodified
-			i = -len(sep)
-			break
-		}
-	}
-	// get back to 0 or trim the leading separator
-	file = file[i+len(sep):]
-	return file
-}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/LICENSE b/vendor/github.com/trustbloc/fabric-peer-ext/LICENSE
new file mode 100644
index 000000000..261eeb9e9
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/block_serialization.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/block_serialization.go
new file mode 100644
index 000000000..75c327d4a
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/block_serialization.go
@@ -0,0 +1,67 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cdbblkstorage
+
+import (
+	"github.com/pkg/errors"
+
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protoutil"
+)
+
+func extractTxIDFromEnvelope(txEnvelope *common.Envelope) (string, error) {
+	payload, err := protoutil.GetPayload(txEnvelope)
+	if err != nil {
+		return "", nil
+	}
+
+	payloadHeader := payload.Header
+	channelHeader, err := protoutil.UnmarshalChannelHeader(payloadHeader.ChannelHeader)
+	if err != nil {
+		return "", err
+	}
+
+	return channelHeader.TxId, nil
+}
+
+func extractTxnEnvelopeFromBlock(block *common.Block, txID string) (*common.Envelope, error) {
+	blockData := block.GetData()
+	for _, txEnvelopeBytes := range blockData.GetData() {
+		envelope, err := protoutil.GetEnvelopeFromBlock(txEnvelopeBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		id, err := extractTxIDFromEnvelope(envelope)
+		if err != nil {
+			return nil, err
+		}
+		if id != txID {
+			continue
+		}
+
+		txEnvelope, err := protoutil.GetEnvelopeFromBlock(txEnvelopeBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		return txEnvelope, nil
+	}
+
+	return nil, errors.Errorf("transaction not found [%s]", txID)
+}
+
+func extractEnvelopeFromBlock(block *common.Block, tranNum uint64) (*common.Envelope, error) {
+	blockData := block.GetData()
+	envelopes := blockData.GetData()
+	envelopesLen := uint64(len(envelopes))
+	if envelopesLen-1 < tranNum {
+		blockNum := block.GetHeader().GetNumber()
+		return nil, errors.Errorf("transaction number is invalid [%d, %d, %d]", blockNum, envelopesLen, tranNum)
+	}
+	return protoutil.GetEnvelopeFromBlock(envelopes[tranNum])
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/blocks_itr.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/blocks_itr.go
new file mode 100644
index 000000000..db281826c
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/blocks_itr.go
@@ -0,0 +1,73 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cdbblkstorage
+
+import (
+	"sync"
+
+	"github.com/hyperledger/fabric/common/ledger"
+)
+
+// blocksItr - an iterator for iterating over a sequence of blocks
+type blocksItr struct {
+	cdbBlockStore        *cdbBlockStore
+	maxBlockNumAvailable uint64
+	blockNumToRetrieve   uint64
+	closeMarker          bool
+	closeMarkerLock      *sync.Mutex
+}
+
+func newBlockItr(cdbBlockStore *cdbBlockStore, startBlockNum uint64) *blocksItr {
+	return &blocksItr{cdbBlockStore, cdbBlockStore.cpInfo.lastBlockNumber, startBlockNum, false, &sync.Mutex{}}
+}
+
+func (itr *blocksItr) waitForBlock(blockNum uint64) uint64 {
+	itr.cdbBlockStore.cpInfoCond.L.Lock()
+	defer itr.cdbBlockStore.cpInfoCond.L.Unlock()
+	for itr.cdbBlockStore.cpInfo.lastBlockNumber < blockNum && !itr.shouldClose() {
+		logger.Debugf("Going to wait for newer blocks. maxAvailaBlockNumber=[%d], waitForBlockNum=[%d]",
+			itr.cdbBlockStore.cpInfo.lastBlockNumber, blockNum)
+		itr.cdbBlockStore.cpInfoCond.Wait()
+		logger.Debugf("Came out of wait. maxAvailaBlockNumber=[%d]", itr.cdbBlockStore.cpInfo.lastBlockNumber)
+	}
+	return itr.cdbBlockStore.cpInfo.lastBlockNumber
+}
+
+func (itr *blocksItr) shouldClose() bool {
+	itr.closeMarkerLock.Lock()
+	defer itr.closeMarkerLock.Unlock()
+	return itr.closeMarker
+}
+
+// Next moves the cursor to next block and returns true iff the iterator is not exhausted
+func (itr *blocksItr) Next() (ledger.QueryResult, error) {
+	if itr.maxBlockNumAvailable < itr.blockNumToRetrieve {
+		itr.maxBlockNumAvailable = itr.waitForBlock(itr.blockNumToRetrieve)
+	}
+	itr.closeMarkerLock.Lock()
+	defer itr.closeMarkerLock.Unlock()
+	if itr.closeMarker {
+		return nil, nil
+	}
+
+	nextBlock, err := itr.cdbBlockStore.RetrieveBlockByNumber(itr.blockNumToRetrieve)
+	if err != nil {
+		return nil, err
+	}
+	itr.blockNumToRetrieve++
+	return nextBlock, nil
+}
+
+// Close releases any resources held by the iterator
+func (itr *blocksItr) Close() {
+	itr.cdbBlockStore.cpInfoCond.L.Lock()
+	defer itr.cdbBlockStore.cpInfoCond.L.Unlock()
+	itr.closeMarkerLock.Lock()
+	defer itr.closeMarkerLock.Unlock()
+	itr.closeMarker = true
+	itr.cdbBlockStore.cpInfoCond.Broadcast()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage.go
new file mode 100644
index 000000000..753b92ec2
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage.go
@@ -0,0 +1,368 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cdbblkstorage
+
+import (
+	"bytes"
+	"encoding/hex"
+	"fmt"
+	"math"
+	"strconv"
+	"sync"
+
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+
+	"github.com/hyperledger/fabric/protoutil"
+
+	"github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/common/ledger/blkstorage"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/pkg/errors"
+)
+
+// cdbBlockStore ...
+type cdbBlockStore struct {
+	blockStore *couchdb.CouchDatabase
+	txnStore   *couchdb.CouchDatabase
+	ledgerID   string
+	cpInfo     *checkpointInfo
+	cpInfoCond *sync.Cond
+	cp         *checkpoint
+	attachTxn  bool
+}
+
+// newCDBBlockStore constructs block store based on CouchDB
+func newCDBBlockStore(blockStore *couchdb.CouchDatabase, txnStore *couchdb.CouchDatabase, ledgerID string) *cdbBlockStore {
+	cp := newCheckpoint(blockStore)
+
+	cdbBlockStore := &cdbBlockStore{
+		blockStore: blockStore,
+		txnStore:   txnStore,
+		ledgerID:   ledgerID,
+		cp:         cp,
+		attachTxn:  false,
+	}
+
+	// cp = checkpointInfo, retrieve from the database the last block number that was written to that db.
+	cpInfo := cdbBlockStore.cp.getCheckpointInfo()
+	err := cdbBlockStore.cp.saveCurrentInfo(cpInfo)
+	if err != nil {
+		panic(fmt.Sprintf("Could not save cpInfo info to db: %s", err))
+	}
+
+	// Update the manager with the checkpoint info and the file writer
+	cdbBlockStore.cpInfo = cpInfo
+
+	// Create a checkpoint condition (event) variable, for the  goroutine waiting for
+	// or announcing the occurrence of an event.
+	cdbBlockStore.cpInfoCond = sync.NewCond(&sync.Mutex{})
+
+	return cdbBlockStore
+}
+
+// AddBlock adds a new block
+func (s *cdbBlockStore) AddBlock(block *common.Block) error {
+
+	if !roles.IsCommitter() {
+		// Nothing to do if not a committer
+		return nil
+	}
+
+	err := s.validateBlock(block)
+	if err != nil {
+		return err
+	}
+
+	err = s.storeBlock(block)
+	if err != nil {
+		return err
+	}
+
+	err = s.storeTransactions(block)
+	if err != nil {
+		return err
+	}
+
+	return s.checkpointBlock(block)
+}
+
+//validateBlock validates block before adding to store
+func (s *cdbBlockStore) validateBlock(block *common.Block) error {
+
+	if s.cpInfo.isChainEmpty {
+		//chain is empty, no need to validate, first block it is.
+		return nil
+	}
+	if block.Header.Number != s.cpInfo.lastBlockNumber+1 {
+		return errors.Errorf(
+			"block number should have been %d but was %d",
+			s.cpInfo.lastBlockNumber+1, block.Header.Number,
+		)
+	}
+
+	// Add the previous hash check - Though, not essential but may not be a bad idea to
+	// verify the field `block.Header.PreviousHash` present in the block.
+	// This check is a simple bytes comparison and hence does not cause any observable performance penalty
+	// and may help in detecting a rare scenario if there is any bug in the ordering service.
+	if !bytes.Equal(block.Header.PreviousHash, s.cpInfo.currentHash) {
+		return errors.Errorf(
+			"unexpected Previous block hash. Expected PreviousHash = [%x], PreviousHash referred in the latest block= [%x]",
+			s.cpInfo.currentHash, block.Header.PreviousHash,
+		)
+	}
+
+	return nil
+}
+
+func (s *cdbBlockStore) storeBlock(block *common.Block) error {
+	doc, err := blockToCouchDoc(block)
+	if err != nil {
+		return errors.WithMessage(err, "converting block to couchDB document failed")
+	}
+
+	id := blockNumberToKey(block.GetHeader().GetNumber())
+
+	rev, err := s.blockStore.SaveDoc(id, "", doc)
+	if err != nil {
+		return errors.WithMessage(err, "adding block to couchDB failed")
+	}
+	logger.Debugf("block added to couchDB [%d, %s]", block.GetHeader().GetNumber(), rev)
+	return nil
+}
+
+func (s *cdbBlockStore) storeTransactions(block *common.Block) error {
+	docs, err := blockToTxnCouchDocs(block, s.attachTxn)
+	if err != nil {
+		return errors.WithMessage(err, "converting block to couchDB txn documents failed")
+	}
+
+	if len(docs) == 0 {
+		return nil
+	}
+
+	_, err = s.txnStore.BatchUpdateDocuments(docs)
+	if err != nil {
+		return errors.WithMessage(err, "adding block to couchDB failed")
+	}
+	logger.Debugf("block transactions added to couchDB [%d]", block.GetHeader().GetNumber())
+	return nil
+}
+
+func (s *cdbBlockStore) checkpointBlock(block *common.Block) error {
+	//Update the checkpoint info with the results of adding the new block
+	newCPInfo := &checkpointInfo{
+		isChainEmpty:    false,
+		lastBlockNumber: block.Header.Number,
+		currentHash:     protoutil.BlockHeaderHash(block.Header),
+	}
+	//save the checkpoint information in the database
+	err := s.cp.saveCurrentInfo(newCPInfo)
+	if err != nil {
+		return errors.WithMessage(err, "adding cpInfo to couchDB failed")
+	}
+	//update the checkpoint info (for storage) and the blockchain info (for APIs) in the manager
+	s.updateCheckpoint(newCPInfo)
+	return nil
+}
+
+// GetBlockchainInfo returns the current info about blockchain
+func (s *cdbBlockStore) GetBlockchainInfo() (*common.BlockchainInfo, error) {
+	cpInfo := s.cp.getCheckpointInfo()
+	s.cpInfo = cpInfo
+	bcInfo := &common.BlockchainInfo{
+		Height: 0,
+	}
+	if !cpInfo.isChainEmpty {
+		//If start up is a restart of an existing storage, update BlockchainInfo for external API's
+		lastBlock, err := s.RetrieveBlockByNumber(cpInfo.lastBlockNumber)
+		if err != nil {
+			return nil, fmt.Errorf("RetrieveBlockByNumber return error: %s", err)
+		}
+
+		lastBlockHeader := lastBlock.GetHeader()
+		lastBlockHash := protoutil.BlockHeaderHash(lastBlockHeader)
+		previousBlockHash := lastBlockHeader.GetPreviousHash()
+		bcInfo = &common.BlockchainInfo{
+			Height:            lastBlockHeader.GetNumber() + 1,
+			CurrentBlockHash:  lastBlockHash,
+			PreviousBlockHash: previousBlockHash,
+		}
+	}
+	return bcInfo, nil
+}
+
+// RetrieveBlocks returns an iterator that can be used for iterating over a range of blocks
+func (s *cdbBlockStore) RetrieveBlocks(startNum uint64) (ledger.ResultsIterator, error) {
+	return newBlockItr(s, startNum), nil
+}
+
+// RetrieveBlockByHash returns the block for given block-hash
+func (s *cdbBlockStore) RetrieveBlockByHash(blockHash []byte) (*common.Block, error) {
+	blockHashHex := hex.EncodeToString(blockHash)
+	const queryFmt = `
+	{
+		"selector": {
+			"` + blockHeaderField + `.` + blockHashField + `": {
+				"$eq": "%s"
+			}
+		},
+		"use_index": ["_design/` + blockHashIndexDoc + `", "` + blockHashIndexName + `"]
+	}`
+
+	block, err := retrieveBlockQuery(s.blockStore, fmt.Sprintf(queryFmt, blockHashHex))
+	if err != nil {
+		// note: allow ErrNotFoundInIndex to pass through
+		return nil, err
+	}
+	return block, nil
+}
+
+// RetrieveBlockByNumber returns the block at a given blockchain height
+func (s *cdbBlockStore) RetrieveBlockByNumber(blockNum uint64) (*common.Block, error) {
+	// interpret math.MaxUint64 as a request for last block
+	if blockNum == math.MaxUint64 {
+		bcinfo, err := s.GetBlockchainInfo()
+		if err != nil {
+			return nil, errors.WithMessage(err, "retrieval of blockchain info failed")
+		}
+		blockNum = bcinfo.Height - 1
+	}
+
+	id := blockNumberToKey(blockNum)
+
+	doc, _, err := s.blockStore.ReadDoc(id)
+	if err != nil {
+		return nil, errors.WithMessage(err, fmt.Sprintf("retrieval of block from couchDB failed [%d]", blockNum))
+	}
+	if doc == nil {
+		return nil, blkstorage.ErrNotFoundInIndex
+	}
+
+	block, err := couchDocToBlock(doc)
+	if err != nil {
+		return nil, errors.WithMessage(err, fmt.Sprintf("unmarshal of block from couchDB failed [%d]", blockNum))
+	}
+
+	return block, nil
+}
+
+// RetrieveTxByID returns a transaction for given transaction id
+func (s *cdbBlockStore) RetrieveTxByID(txID string) (*common.Envelope, error) {
+	doc, _, err := s.txnStore.ReadDoc(txID)
+	if err != nil {
+		// note: allow ErrNotFoundInIndex to pass through
+		return nil, err
+	}
+	if doc == nil {
+		return nil, blkstorage.ErrNotFoundInIndex
+	}
+
+	// If this transaction includes the envelope as a valid attachment then can return immediately.
+	if len(doc.Attachments) > 0 {
+		attachedEnv, e := couchAttachmentsToTxnEnvelope(doc.Attachments)
+		if e == nil {
+			return attachedEnv, nil
+		}
+		logger.Debugf("transaction has attachment but failed to be extracted into envelope [%s]", err)
+	}
+
+	// Otherwise, we need to extract the transaction from the block document.
+	block, err := s.RetrieveBlockByTxID(txID)
+	if err != nil {
+		// note: allow ErrNotFoundInIndex to pass through
+		return nil, err
+	}
+
+	return extractTxnEnvelopeFromBlock(block, txID)
+}
+
+// RetrieveTxByBlockNumTranNum returns a transaction for given block ID and transaction ID
+func (s *cdbBlockStore) RetrieveTxByBlockNumTranNum(blockNum uint64, tranNum uint64) (*common.Envelope, error) {
+	block, err := s.RetrieveBlockByNumber(blockNum)
+	if err != nil {
+		// note: allow ErrNotFoundInIndex to pass through
+		return nil, err
+	}
+	return extractEnvelopeFromBlock(block, tranNum)
+}
+
+// RetrieveBlockByTxID returns a block for a given transaction ID
+func (s *cdbBlockStore) RetrieveBlockByTxID(txID string) (*common.Block, error) {
+	blockHash, err := s.retrieveBlockHashByTxID(txID)
+	if err != nil {
+		return nil, err
+	}
+
+	return s.RetrieveBlockByHash(blockHash)
+}
+
+func (s *cdbBlockStore) retrieveBlockHashByTxID(txID string) ([]byte, error) {
+	jsonResult, err := retrieveJSONQuery(s.txnStore, txID)
+	if err != nil {
+		// note: allow ErrNotFoundInIndex to pass through
+		logger.Errorf("retrieving transaction document from DB failed : %s", err)
+		return nil, blkstorage.ErrNotFoundInIndex
+	}
+
+	blockHashStoredUT, ok := jsonResult[txnBlockHashField]
+	if !ok {
+		return nil, errors.Errorf("block hash was not found for transaction ID [%s]", txID)
+	}
+
+	blockHashStored, ok := blockHashStoredUT.(string)
+	if !ok {
+		return nil, errors.Errorf("block hash has invalid type for transaction ID [%s]", txID)
+	}
+
+	blockHash, err := hex.DecodeString(blockHashStored)
+	if err != nil {
+		return nil, errors.Wrapf(err, "block hash was invalid for transaction ID [%s]", txID)
+	}
+
+	return blockHash, nil
+}
+
+// RetrieveTxValidationCodeByTxID returns a TX validation code for a given transaction ID
+func (s *cdbBlockStore) RetrieveTxValidationCodeByTxID(txID string) (peer.TxValidationCode, error) {
+	jsonResult, err := retrieveJSONQuery(s.txnStore, txID)
+	if err != nil {
+		// note: allow ErrNotFoundInIndex to pass through
+		return peer.TxValidationCode(-1), err
+	}
+
+	txnValidationCodeStoredUT, ok := jsonResult[txnValidationCode]
+	if !ok {
+		return peer.TxValidationCode_INVALID_OTHER_REASON, errors.Errorf("validation code was not found for transaction ID [%s]", txID)
+	}
+
+	txnValidationCodeStored, ok := txnValidationCodeStoredUT.(string)
+	if !ok {
+		return peer.TxValidationCode_INVALID_OTHER_REASON, errors.Errorf("validation code has invalid type for transaction ID [%s]", txID)
+	}
+
+	const sizeOfTxValidationCode = 32
+	txnValidationCode, err := strconv.ParseInt(txnValidationCodeStored, txnValidationCodeBase, sizeOfTxValidationCode)
+	if err != nil {
+		return peer.TxValidationCode_INVALID_OTHER_REASON, errors.Wrapf(err, "validation code was invalid for transaction ID [%s]", txID)
+	}
+
+	return peer.TxValidationCode(txnValidationCode), nil
+}
+
+// Shutdown closes the storage instance
+func (s *cdbBlockStore) Shutdown() {
+}
+
+func (s *cdbBlockStore) updateCheckpoint(cpInfo *checkpointInfo) {
+	s.cpInfoCond.L.Lock()
+	defer s.cpInfoCond.L.Unlock()
+	s.cpInfo = cpInfo
+	logger.Debugf("Broadcasting about update checkpointInfo: %s", cpInfo)
+	s.cpInfoCond.Broadcast()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage_provider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage_provider.go
new file mode 100644
index 000000000..3eb86e9f2
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_blkstorage_provider.go
@@ -0,0 +1,159 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cdbblkstorage
+
+import (
+	"strings"
+
+	"github.com/hyperledger/fabric/core/ledger"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/blkstorage"
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+var logger = flogging.MustGetLogger("cdbblkstorage")
+
+const (
+	blockStoreName = "blocks"
+	txnStoreName   = "transactions"
+)
+
+// CDBBlockstoreProvider provides block storage in CouchDB
+type CDBBlockstoreProvider struct {
+	couchInstance *couchdb.CouchInstance
+	indexConfig   *blkstorage.IndexConfig
+}
+
+// NewProvider creates a new CouchDB BlockStoreProvider
+func NewProvider(indexConfig *blkstorage.IndexConfig, ledgerconfig *ledger.Config) (blkstorage.BlockStoreProvider, error) {
+	logger.Debugf("constructing CouchDB block storage provider")
+	couchDBConfig := ledgerconfig.StateDB.CouchDB
+	couchInstance, err := couchdb.CreateCouchInstance(couchDBConfig, &disabled.Provider{})
+	if err != nil {
+		return nil, errors.WithMessage(err, "obtaining CouchDB instance failed")
+	}
+	return &CDBBlockstoreProvider{couchInstance, indexConfig}, nil
+}
+
+// CreateBlockStore creates a block store instance for the given ledger ID
+func (p *CDBBlockstoreProvider) CreateBlockStore(ledgerid string) (blkstorage.BlockStore, error) {
+	return p.OpenBlockStore(ledgerid)
+}
+
+// OpenBlockStore opens the block store for the given ledger ID
+func (p *CDBBlockstoreProvider) OpenBlockStore(ledgerid string) (blkstorage.BlockStore, error) {
+	id := strings.ToLower(ledgerid)
+	blockStoreDBName := couchdb.ConstructBlockchainDBName(id, blockStoreName)
+	txnStoreDBName := couchdb.ConstructBlockchainDBName(id, txnStoreName)
+
+	if roles.IsCommitter() {
+		return p.createCommitterBlockStore(ledgerid, blockStoreDBName, txnStoreDBName)
+	}
+
+	return p.createNonCommitterBlockStore(ledgerid, blockStoreDBName, txnStoreDBName)
+}
+
+//createCommitterBlockStore creates new couch db with gievn db name if doesn't exists
+func (p *CDBBlockstoreProvider) createCommitterBlockStore(ledgerid, blockStoreDBName, txnStoreDBName string) (blkstorage.BlockStore, error) {
+
+	blockStoreDB, err := couchdb.CreateCouchDatabase(p.couchInstance, blockStoreDBName)
+	if err != nil {
+		return nil, err
+	}
+
+	txnStoreDB, err := couchdb.CreateCouchDatabase(p.couchInstance, txnStoreDBName)
+	if err != nil {
+		return nil, err
+	}
+
+	err = p.createBlockStoreIndices(blockStoreDB)
+	if err != nil {
+		return nil, err
+	}
+
+	return newCDBBlockStore(blockStoreDB, txnStoreDB, ledgerid), nil
+}
+
+//createBlockStore opens existing couch db with given db name with retry
+func (p *CDBBlockstoreProvider) createNonCommitterBlockStore(ledgerid, blockStoreDBName, txnStoreDBName string) (blkstorage.BlockStore, error) {
+
+	//create new block store db
+	blockStoreDB, err := p.openCouchDB(blockStoreDBName)
+	if err != nil {
+		return nil, err
+	}
+
+	//check if indexes exists
+	indexExists, err := blockStoreDB.IndexDesignDocExistsWithRetry(blockHashIndexDoc)
+	if err != nil {
+		return nil, err
+	}
+	if !indexExists {
+		return nil, errors.Errorf("DB index not found: [%s]", blockStoreDBName)
+	}
+
+	//create new txn store db
+	txnStoreDB, err := p.openCouchDB(txnStoreDBName)
+	if err != nil {
+		return nil, err
+	}
+
+	return newCDBBlockStore(blockStoreDB, txnStoreDB, ledgerid), nil
+}
+
+func (p *CDBBlockstoreProvider) openCouchDB(dbName string) (*couchdb.CouchDatabase, error) {
+	//create new store db
+	cdb, err := couchdb.NewCouchDatabase(p.couchInstance, dbName)
+	if err != nil {
+		return nil, err
+	}
+
+	//check if store db exists
+	dbExists, err := cdb.ExistsWithRetry()
+	if err != nil {
+		return nil, err
+	}
+	if !dbExists {
+		return nil, errors.Errorf("DB not found: [%s]", dbName)
+	}
+
+	return cdb, nil
+}
+
+func (p *CDBBlockstoreProvider) createBlockStoreIndices(db *couchdb.CouchDatabase) error {
+	_, err := db.CreateIndex(blockHashIndexDef)
+	if err != nil {
+		return errors.WithMessage(err, "creation of block hash index failed")
+	}
+
+	return nil
+}
+
+// Exists returns whether or not the given ledger ID exists
+func (p *CDBBlockstoreProvider) Exists(ledgerid string) (bool, error) {
+	id := strings.ToLower(ledgerid)
+	blockStoreDBName := couchdb.ConstructBlockchainDBName(id, blockStoreName)
+	blockStoreDB, err := couchdb.NewCouchDatabase(p.couchInstance, blockStoreDBName)
+	if err != nil {
+		return false, err
+	}
+
+	return blockStoreDB.Exists()
+}
+
+// List returns the available ledger IDs, not supported in couchdb block storage
+func (p *CDBBlockstoreProvider) List() ([]string, error) {
+	panic("not supported")
+}
+
+// Close cleans up the Provider
+func (p *CDBBlockstoreProvider) Close() {
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_checkpoint.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_checkpoint.go
new file mode 100644
index 000000000..813ffd022
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/cdb_checkpoint.go
@@ -0,0 +1,121 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cdbblkstorage
+
+import (
+	"fmt"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/pkg/errors"
+)
+
+const blkMgrInfoKey = "blkMgrInfo"
+
+type checkpoint struct {
+	db *couchdb.CouchDatabase
+}
+
+// checkpointInfo
+type checkpointInfo struct {
+	isChainEmpty    bool
+	lastBlockNumber uint64
+	currentHash     []byte
+}
+
+func newCheckpoint(db *couchdb.CouchDatabase) *checkpoint {
+	return &checkpoint{db: db}
+}
+
+func (cp *checkpoint) getCheckpointInfo() *checkpointInfo {
+	cpInfo, err := cp.loadCurrentInfo()
+	if err != nil {
+		panic(fmt.Sprintf("Could not get block file info for current block file from db: %s", err))
+	}
+	if cpInfo == nil {
+		cpInfo = &checkpointInfo{
+			isChainEmpty:    true,
+			lastBlockNumber: 0}
+	}
+	return cpInfo
+}
+
+//Get the current checkpoint information that is stored in the database
+func (cp *checkpoint) loadCurrentInfo() (*checkpointInfo, error) {
+	doc, _, err := cp.db.ReadDoc(blkMgrInfoKey)
+	if err != nil {
+		return nil, errors.WithMessage(err, fmt.Sprintf("retrieval of checkpointInfo from couchDB failed [%s]", blkMgrInfoKey))
+	}
+	if doc == nil {
+		return nil, nil
+	}
+	checkpointInfo, err := couchDocToCheckpointInfo(doc)
+	if err != nil {
+		return nil, errors.WithMessage(err, fmt.Sprintf("unmarshal of checkpointInfo from couchDB failed [%s]", blkMgrInfoKey))
+	}
+	logger.Debugf("loaded checkpointInfo:%s", checkpointInfo)
+	return checkpointInfo, nil
+}
+
+func (cp *checkpoint) saveCurrentInfo(i *checkpointInfo) error {
+	doc, err := checkpointInfoToCouchDoc(i)
+	if err != nil {
+		return errors.WithMessage(err, "converting checkpointInfo to couchDB document failed")
+	}
+	_, err = cp.db.SaveDoc(blkMgrInfoKey, "", doc)
+	if err != nil {
+		return errors.WithMessage(err, "adding checkpointInfo to couchDB failed")
+	}
+	return nil
+}
+
+func (i *checkpointInfo) marshal() ([]byte, error) {
+	buffer := proto.NewBuffer([]byte{})
+	var err error
+	if err = buffer.EncodeVarint(i.lastBlockNumber); err != nil {
+		return nil, err
+	}
+
+	if err = buffer.EncodeRawBytes(i.currentHash); err != nil {
+		return nil, err
+	}
+
+	var chainEmptyMarker uint64
+	if i.isChainEmpty {
+		chainEmptyMarker = 1
+	}
+	if err = buffer.EncodeVarint(chainEmptyMarker); err != nil {
+		return nil, err
+	}
+
+	return buffer.Bytes(), nil
+}
+
+func (i *checkpointInfo) unmarshal(b []byte) error {
+	buffer := proto.NewBuffer(b)
+	var chainEmptyMarker uint64
+	var err error
+
+	if i.lastBlockNumber, err = buffer.DecodeVarint(); err != nil {
+		return err
+	}
+
+	if i.currentHash, err = buffer.DecodeRawBytes(false); err != nil {
+		return err
+	}
+
+	if len(i.currentHash) == 0 {
+		i.currentHash = nil
+	}
+
+	if chainEmptyMarker, err = buffer.DecodeVarint(); err != nil {
+		return err
+	}
+	i.isChainEmpty = chainEmptyMarker == 1
+
+	return nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/couchdoc_conv.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/couchdoc_conv.go
new file mode 100644
index 000000000..c70ae3a12
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/blkstorage/cdbblkstorage/couchdoc_conv.go
@@ -0,0 +1,377 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cdbblkstorage
+
+import (
+	"bytes"
+	"encoding/hex"
+	"encoding/json"
+	"strconv"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/ledger/blkstorage"
+	ledgerUtil "github.com/hyperledger/fabric/core/ledger/util"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/peer"
+	"github.com/hyperledger/fabric/protoutil"
+	"github.com/pkg/errors"
+)
+
+// block document
+const (
+	idField             = "_id"
+	blockHashField      = "hash"
+	blockTxnsField      = "transactions"
+	blockTxnIDField     = "id"
+	blockHashIndexName  = "by_hash"
+	blockHashIndexDoc   = "indexHash"
+	blockAttachmentName = "block"
+	blockKeyPrefix      = ""
+	blockHeaderField    = "header"
+)
+
+// txn document
+const (
+	txnBlockNumberField   = "block_number"
+	txnBlockHashField     = "block_hash"
+	txnAttachmentName     = "transaction"
+	txnValidationCode     = "validation_code"
+	txnValidationCodeBase = 16
+)
+
+// checkpoint document
+const (
+	cpiAttachmentName        = "checkpointinfo"
+	cpiAttachmentContentType = "application/octet-stream"
+)
+
+const blockHashIndexDef = `
+	{
+		"index": {
+			"fields": ["` + blockHeaderField + `.` + blockHashField + `"]
+		},
+		"name": "` + blockHashIndexName + `",
+		"ddoc": "` + blockHashIndexDoc + `",
+		"type": "json"
+	}`
+
+type jsonValue map[string]interface{}
+
+func (v jsonValue) toBytes() ([]byte, error) {
+	return json.Marshal(v)
+}
+
+func blockToCouchDoc(block *common.Block) (*couchdb.CouchDoc, error) {
+	jsonMap := make(jsonValue)
+
+	blockHeader := block.GetHeader()
+
+	key := blockNumberToKey(blockHeader.GetNumber())
+	blockHashHex := hex.EncodeToString(protoutil.BlockHeaderHash(blockHeader))
+	blockTxns, err := blockToTransactionsField(block)
+	if err != nil {
+		return nil, err
+	}
+
+	jsonMap[idField] = key
+	header := make(jsonValue)
+	header[blockHashField] = blockHashHex
+	jsonMap[blockHeaderField] = header
+	jsonMap[blockTxnsField] = blockTxns
+
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+	couchDoc := &couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	attachment, err := blockToAttachment(block)
+	if err != nil {
+		return nil, err
+	}
+
+	attachments := append([]*couchdb.AttachmentInfo{}, attachment)
+	couchDoc.Attachments = attachments
+	return couchDoc, nil
+}
+
+func blockToTxnCouchDocs(block *common.Block, attachTxn bool) ([]*couchdb.CouchDoc, error) {
+	blockHeader := block.GetHeader()
+	blockNumber := blockNumberToKey(blockHeader.GetNumber())
+	blockHash := hex.EncodeToString(protoutil.BlockHeaderHash(blockHeader))
+
+	blockData := block.GetData()
+
+	blockMetadata := block.GetMetadata()
+	txValidationFlags := ledgerUtil.TxValidationFlags(blockMetadata.GetMetadata()[common.BlockMetadataIndex_TRANSACTIONS_FILTER])
+
+	txnDocs := make([]*couchdb.CouchDoc, 0)
+
+	for i, txEnvelopeBytes := range blockData.GetData() {
+		envelope, err := protoutil.GetEnvelopeFromBlock(txEnvelopeBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		txnDoc, err := blockTxnToCouchDoc(blockNumber, blockHash, envelope, txValidationFlags.Flag(i), attachTxn)
+		if err == errorNoTxID {
+			continue
+		} else if err != nil {
+			return nil, err
+		}
+
+		txnDocs = append(txnDocs, txnDoc)
+	}
+
+	return txnDocs, nil
+}
+
+var errorNoTxID = errors.New("missing transaction ID")
+
+func blockTxnToCouchDoc(blockNumber string, blockHash string, txEnvelope *common.Envelope, validationCode peer.TxValidationCode, attachTxn bool) (*couchdb.CouchDoc, error) {
+	txID, err := extractTxIDFromEnvelope(txEnvelope)
+	if err != nil {
+		return nil, errors.WithMessage(err, "transaction ID could not be extracted")
+	}
+
+	// TODO: is the empty transaction queryable? If so, need to change this to a default transaction ID.
+	if txID == "" {
+		return nil, errorNoTxID
+	}
+
+	jsonMap := make(jsonValue)
+	jsonMap[idField] = txID
+	jsonMap[txnBlockHashField] = blockHash
+	jsonMap[txnBlockNumberField] = blockNumber
+	jsonMap[txnValidationCode] = strconv.FormatInt(int64(validationCode), txnValidationCodeBase)
+
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+	couchDoc := &couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	if attachTxn {
+		attachment, err := txnEnvelopeToAttachment(txEnvelope)
+		if err != nil {
+			return nil, err
+		}
+
+		attachments := append([]*couchdb.AttachmentInfo{}, attachment)
+		couchDoc.Attachments = attachments
+	}
+	return couchDoc, nil
+}
+
+func checkpointInfoToCouchDoc(i *checkpointInfo) (*couchdb.CouchDoc, error) {
+	jsonMap := make(jsonValue)
+
+	jsonMap[idField] = blkMgrInfoKey
+
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+	couchDoc := &couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	attachment, err := checkpointInfoToAttachment(i)
+	if err != nil {
+		return nil, err
+	}
+
+	attachments := append([]*couchdb.AttachmentInfo{}, attachment)
+	couchDoc.Attachments = attachments
+	return couchDoc, nil
+}
+
+func checkpointInfoToAttachment(i *checkpointInfo) (*couchdb.AttachmentInfo, error) {
+	checkpointInfoBytes, err := i.marshal()
+	if err != nil {
+		return nil, errors.Wrapf(err, "marshaling checkpointInfo failed")
+	}
+
+	attachment := &couchdb.AttachmentInfo{}
+	attachment.AttachmentBytes = checkpointInfoBytes
+	attachment.ContentType = cpiAttachmentContentType
+	attachment.Name = cpiAttachmentName
+
+	return attachment, nil
+}
+
+func blockToTransactionsField(block *common.Block) ([]jsonValue, error) {
+	blockData := block.GetData()
+
+	var txns []jsonValue
+
+	for _, txEnvelopeBytes := range blockData.GetData() {
+		envelope, err := protoutil.GetEnvelopeFromBlock(txEnvelopeBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		txID, err := extractTxIDFromEnvelope(envelope)
+		if err != nil {
+			return nil, errors.WithMessage(err, "transaction ID could not be extracted")
+		}
+
+		txField := make(jsonValue)
+		txField[blockTxnIDField] = txID
+
+		txns = append(txns, txField)
+	}
+
+	return txns, nil
+}
+
+func txnEnvelopeToAttachment(txEnvelope *common.Envelope) (*couchdb.AttachmentInfo, error) {
+	txEnvelopeBytes, err := proto.Marshal(txEnvelope)
+	if err != nil {
+		return nil, errors.Wrapf(err, "marshaling block failed")
+	}
+
+	attachment := &couchdb.AttachmentInfo{}
+	attachment.AttachmentBytes = txEnvelopeBytes
+	attachment.ContentType = cpiAttachmentContentType
+	attachment.Name = txnAttachmentName
+
+	return attachment, nil
+}
+
+func blockToAttachment(block *common.Block) (*couchdb.AttachmentInfo, error) {
+	blockBytes, err := proto.Marshal(block)
+	if err != nil {
+		return nil, errors.Wrapf(err, "marshaling block failed")
+	}
+
+	attachment := &couchdb.AttachmentInfo{}
+	attachment.AttachmentBytes = blockBytes
+	attachment.ContentType = cpiAttachmentContentType
+	attachment.Name = blockAttachmentName
+
+	return attachment, nil
+}
+
+func couchDocToBlock(doc *couchdb.CouchDoc) (*common.Block, error) {
+	return couchAttachmentsToBlock(doc.Attachments)
+}
+
+func couchAttachmentsToBlock(attachments []*couchdb.AttachmentInfo) (*common.Block, error) {
+	var blockBytes []byte
+	block := common.Block{}
+
+	// get binary data from attachment
+	for _, a := range attachments {
+		if a.Name == blockAttachmentName {
+			blockBytes = a.AttachmentBytes
+		}
+	}
+
+	if len(blockBytes) == 0 {
+		return nil, errors.New("block is not within couchDB document")
+	}
+
+	err := proto.Unmarshal(blockBytes, &block)
+	if err != nil {
+		return nil, errors.Wrapf(err, "block from couchDB document could not be unmarshaled")
+	}
+
+	return &block, nil
+}
+
+func couchAttachmentsToTxnEnvelope(attachments []*couchdb.AttachmentInfo) (*common.Envelope, error) {
+	var envelope common.Envelope
+	var txnBytes []byte
+
+	// get binary data from attachment
+	for _, a := range attachments {
+		if a.Name == txnAttachmentName {
+			txnBytes = a.AttachmentBytes
+		}
+	}
+
+	if len(txnBytes) == 0 {
+		return nil, errors.New("transaction envelope is not within couchDB document")
+	}
+
+	err := proto.Unmarshal(txnBytes, &envelope)
+	if err != nil {
+		return nil, errors.Wrapf(err, "transaction from couchDB document could not be unmarshaled")
+	}
+
+	return &envelope, nil
+}
+
+func couchDocToCheckpointInfo(doc *couchdb.CouchDoc) (*checkpointInfo, error) {
+	return couchAttachmentsToCheckpointInfo(doc.Attachments)
+}
+
+func couchAttachmentsToCheckpointInfo(attachments []*couchdb.AttachmentInfo) (*checkpointInfo, error) {
+	var checkpointInfoBytes []byte
+	cpInfo := checkpointInfo{}
+	// get binary data from attachment
+	for _, a := range attachments {
+		if a.Name == cpiAttachmentName {
+			checkpointInfoBytes = a.AttachmentBytes
+		}
+	}
+	if len(checkpointInfoBytes) == 0 {
+		return nil, errors.New("checkpointInfo is not within couchDB document")
+	}
+	err := cpInfo.unmarshal(checkpointInfoBytes)
+	if err != nil {
+		return nil, errors.Wrapf(err, "checkpointInfo from couchDB document could not be unmarshaled")
+	}
+	return &cpInfo, nil
+}
+
+func blockNumberToKey(blockNum uint64) string {
+	return blockKeyPrefix + strconv.FormatUint(blockNum, 10)
+}
+
+func retrieveBlockQuery(db *couchdb.CouchDatabase, query string) (*common.Block, error) {
+	results, _, err := db.QueryDocuments(query)
+	if err != nil {
+		return nil, err
+	}
+
+	if len(results) == 0 {
+		return nil, blkstorage.ErrNotFoundInIndex
+	}
+
+	if len(results[0].Attachments) == 0 {
+		return nil, errors.New("block bytes not found")
+	}
+
+	return couchAttachmentsToBlock(results[0].Attachments)
+}
+
+func retrieveJSONQuery(db *couchdb.CouchDatabase, id string) (jsonValue, error) {
+	doc, _, err := db.ReadDoc(id)
+	if err != nil {
+		return nil, err
+	}
+	if doc == nil {
+		return nil, blkstorage.ErrNotFoundInIndex
+	}
+
+	return couchDocToJSON(doc)
+}
+
+func couchDocToJSON(doc *couchdb.CouchDoc) (jsonValue, error) {
+	// create a generic map unmarshal the json
+	jsonResult := make(map[string]interface{})
+	decoder := json.NewDecoder(bytes.NewBuffer(doc.JSONValue))
+	decoder.UseNumber()
+
+	err := decoder.Decode(&jsonResult)
+	if err != nil {
+		return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+	}
+
+	return jsonResult, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api/offledger.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api/offledger.go
new file mode 100644
index 000000000..c0b429c0a
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api/offledger.go
@@ -0,0 +1,56 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// Store manages the storage of private data collections.
+type Store interface {
+	// Persist stores the private write set of a transaction.
+	Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error
+
+	// PutData stores the key/value.
+	PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error
+
+	// GetData gets the value for the given item
+	GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple items in a single call
+	GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+
+	// Close closes the store
+	Close()
+}
+
+// StoreProvider is an interface to open/close a store
+type StoreProvider interface {
+	// OpenStore creates a handle to the private data store for the given ledger ID
+	OpenStore(ledgerid string) (Store, error)
+
+	// Close cleans up the provider
+	Close()
+}
+
+// Retriever retrieves data
+type Retriever interface {
+	// GetData gets the value for the given data item
+	GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple data items in a single call
+	GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+}
+
+// Provider provides data retrievers
+type Provider interface {
+	RetrieverForChannel(channel string) Retriever
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/cas.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/cas.go
new file mode 100644
index 000000000..c00ab0064
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/cas.go
@@ -0,0 +1,42 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dcas
+
+import (
+	"crypto"
+	"encoding/base64"
+
+	"github.com/btcsuite/btcutil/base58"
+)
+
+// GetCASKey returns the content-addressable key for the given content.
+func GetCASKey(content []byte) string {
+	hash := getHash(content)
+	buf := make([]byte, base64.URLEncoding.EncodedLen(len(hash)))
+	base64.URLEncoding.Encode(buf, hash)
+	return string(buf)
+}
+
+// GetFabricCASKey returns the content-addressable key for the given content,
+// encoded in base58 so that it may be used as a key in Fabric.
+func GetFabricCASKey(content []byte) string {
+	return Base58Encode(GetCASKey(content))
+}
+
+// getHash will compute the hash for the supplied bytes using SHA256
+func getHash(bytes []byte) []byte {
+	h := crypto.SHA256.New()
+	// added no lint directive because there's no error from source code
+	// error cannot be produced, checked google source
+	h.Write(bytes) //nolint
+	return h.Sum(nil)
+}
+
+// Base58Encode encodes the given string in base 58
+func Base58Encode(s string) string {
+	return base58.Encode([]byte(s))
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/dcas.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/dcas.go
new file mode 100644
index 000000000..c715d1797
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas/dcas.go
@@ -0,0 +1,67 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dcas
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/pkg/errors"
+)
+
+// Validator is an off-ledger validator that validates the CAS key against the value
+func Validator(_, _, _, key string, value []byte) error {
+	if value == nil {
+		return errors.Errorf("nil value for key [%s]", key)
+	}
+	expectedKey := GetCASKey(value)
+	if key != expectedKey {
+		return errors.Errorf("Invalid CAS key [%s] - the key should be the hash of the value", key)
+	}
+	return nil
+}
+
+// Decorator is an off-ledger decorator that ensures the given key is the hash of the value. If the key is not
+// specified then it is generated. If the key is provided then it is validated against the value.
+func Decorator(key *storeapi.Key, value *storeapi.ExpiringValue) (*storeapi.Key, *storeapi.ExpiringValue, error) {
+	dcasKey, err := validateCASKey(key.Key, value.Value)
+	if err != nil {
+		return nil, nil, err
+	}
+
+	// The key needs to be base58 encoded since Fabric doesn't allow
+	// certain characters to be used in the key.
+	newKey := &storeapi.Key{
+		EndorsedAtTxID: key.EndorsedAtTxID,
+		Namespace:      key.Namespace,
+		Collection:     key.Collection,
+		Key:            Base58Encode(dcasKey),
+	}
+
+	return newKey, value, nil
+}
+
+// KeyDecorator is an off-ledger decorator that ensures the given key is base58 encoded
+// since Fabric doesn't allow certain characters to be used in the key.
+func KeyDecorator(key *storeapi.Key) (*storeapi.Key, error) {
+	return &storeapi.Key{
+		EndorsedAtTxID: key.EndorsedAtTxID,
+		Namespace:      key.Namespace,
+		Collection:     key.Collection,
+		Key:            Base58Encode(key.Key),
+	}, nil
+}
+
+func validateCASKey(key string, value []byte) (string, error) {
+	if value == nil {
+		return "", errors.Errorf("attempt to put nil value for key [%s]", key)
+	}
+
+	casKey := GetCASKey(value)
+	if key != "" && key != casKey {
+		return casKey, errors.New("invalid CAS key - the key should be the hash of the value")
+	}
+	return casKey, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminationplan.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminationplan.go
new file mode 100644
index 000000000..c27e8a2c8
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminationplan.go
@@ -0,0 +1,96 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	protobuf "github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	gdiscovery "github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/gossip"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	"github.com/pkg/errors"
+	"github.com/spf13/viper"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas"
+)
+
+type gossipAdapter interface {
+	PeersOfChannel(gcommon.ChainID) []gdiscovery.NetworkMember
+	SelfMembershipInfo() gdiscovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+// ComputeDisseminationPlan returns the dissemination plan for off ledger data
+func ComputeDisseminationPlan(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	collConfig *cb.StaticCollectionConfig,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *protoext.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	logger.Debugf("Computing dissemination plan for [%s:%s]", ns, rwSet.CollectionName)
+
+	kvRwSet := &kvrwset.KVRWSet{}
+	if err := protobuf.Unmarshal(rwSet.Rwset, kvRwSet); err != nil {
+		return nil, true, errors.WithMessage(err, "error unmarshalling KV read/write set")
+	}
+
+	if err := validateAll(collConfig.Type, kvRwSet); err != nil {
+		return nil, false, errors.WithMessagef(err, "one or more keys did not validate for collection [%s:%s]", ns, rwSet.CollectionName)
+	}
+
+	peers := New(channelID, ns, rwSet.CollectionName, colAP, gossipAdapter).resolvePeersForDissemination().Remote()
+
+	logger.Debugf("Peers for dissemination of collection [%s:%s]: %s", ns, rwSet.CollectionName, peers)
+
+	routingFilter := func(member gdiscovery.NetworkMember) bool {
+		if peers.ContainsPeer(member.Endpoint) {
+			logger.Debugf("Including peer [%s] for dissemination of [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+			return true
+		}
+
+		logger.Debugf("Not including peer [%s] for dissemination of [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+		return false
+	}
+
+	sc := gossip.SendCriteria{
+		Timeout:    viper.GetDuration("peer.gossip.pvtData.pushAckTimeout"),
+		Channel:    gcommon.ChainID(channelID),
+		MaxPeers:   len(peers),
+		MinAck:     colAP.RequiredPeerCount(),
+		IsEligible: routingFilter,
+	}
+
+	return []*dissemination.Plan{{
+		Criteria: sc,
+		Msg:      pvtDataMsg,
+	}}, true, nil
+}
+
+func validateAll(collType cb.CollectionType, kvRWSet *kvrwset.KVRWSet) error {
+	for _, ws := range kvRWSet.Writes {
+		if err := validate(collType, ws); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+func validate(collType cb.CollectionType, ws *kvrwset.KVWrite) error {
+	if collType == cb.CollectionType_COL_DCAS && ws.Value != nil {
+		expectedKey := dcas.GetCASKey(ws.Value)
+		if ws.Key != expectedKey {
+			return errors.Errorf("invalid CAS key [%s] - the key should be the hash of the value", ws.Key)
+		}
+	}
+	return nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminator.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminator.go
new file mode 100644
index 000000000..037dbe270
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination/disseminator.go
@@ -0,0 +1,126 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/discovery"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+var logger = flogging.MustGetLogger("ext_offledger")
+
+// Disseminator disseminates collection data to other endorsers
+type Disseminator struct {
+	*discovery.Discovery
+	namespace  string
+	collection string
+	policy     privdata.CollectionAccessPolicy
+}
+
+// New returns a new disseminator
+func New(channelID, namespace, collection string, policy privdata.CollectionAccessPolicy, gossip gossipAdapter) *Disseminator {
+	return &Disseminator{
+		Discovery:  discovery.New(channelID, gossip),
+		namespace:  namespace,
+		collection: collection,
+		policy:     policy,
+	}
+}
+
+// resolvePeersForDissemination resolves to a set of committers to which data should be disseminated
+func (d *Disseminator) resolvePeersForDissemination() discovery.PeerGroup {
+	orgs := d.policy.MemberOrgs()
+	maxPeerCount := d.policy.MaximumPeerCount()
+
+	logger.Debugf("[%s] Member orgs: %s", d.ChannelID(), orgs)
+
+	// Include all committers
+	peersForDissemination := d.getPeersWithRole(roles.CommitterRole, orgs)
+
+	if len(peersForDissemination) < maxPeerCount {
+		logger.Debugf("[%s] MaximumPeerCount in collection policy is %d and we only have %d committers. Adding some endorsers too...", d.ChannelID(), maxPeerCount, len(peersForDissemination))
+		for _, peer := range d.getPeersWithRole(roles.EndorserRole, orgs).Remote().Shuffle() {
+			if len(peersForDissemination) >= maxPeerCount {
+				// We have enough peers
+				break
+			}
+			logger.Debugf("Adding endorser [%s] ...", peer)
+			peersForDissemination = append(peersForDissemination, peer)
+		}
+	}
+
+	logger.Debugf("[%s] Peers for dissemination from orgs %s: %s", d.ChannelID(), orgs, peersForDissemination)
+
+	return peersForDissemination
+}
+
+// ResolvePeersForRetrieval resolves to a set of peers from which data should may be retrieved
+func (d *Disseminator) ResolvePeersForRetrieval() discovery.PeerGroup {
+	orgs := d.policy.MemberOrgs()
+
+	logger.Debugf("[%s] Member orgs: %s", d.ChannelID(), orgs)
+
+	// Maximum number of peers to ask for the data
+	maxPeers := getMaxPeersForRetrieval()
+
+	var peersForRetrieval discovery.PeerGroup
+	for _, peer := range d.getPeersWithRole(roles.EndorserRole, orgs).Remote().Shuffle() {
+		if len(peersForRetrieval) >= maxPeers {
+			// We have enough peers
+			break
+		}
+		logger.Debugf("Adding endorser [%s] ...", peer)
+		peersForRetrieval = append(peersForRetrieval, peer)
+	}
+
+	if len(peersForRetrieval) < maxPeers {
+		// Add some committers too
+		for _, peer := range d.getPeersWithRole(roles.CommitterRole, orgs).Remote().Shuffle() {
+			if len(peersForRetrieval) >= maxPeers {
+				// We have enough peers
+				break
+			}
+			logger.Debugf("Adding committer [%s] ...", peer)
+			peersForRetrieval = append(peersForRetrieval, peer)
+		}
+	}
+
+	logger.Debugf("[%s] Peers for retrieval from orgs %s: %s", d.ChannelID(), orgs, peersForRetrieval)
+
+	return peersForRetrieval
+}
+
+func (d *Disseminator) getPeersWithRole(role roles.Role, mspIDs []string) discovery.PeerGroup {
+	return d.GetMembers(func(m *discovery.Member) bool {
+		if !m.HasRole(role) {
+			logger.Debugf("[%s] Not adding peer [%s] since it does not have the role [%s]", d.ChannelID(), m.Endpoint, role)
+			return false
+		}
+		if !contains(mspIDs, m.MSPID) {
+			logger.Debugf("[%s] Not adding peer [%s] since it is not in any of the orgs [%s]", d.ChannelID(), m.Endpoint, mspIDs)
+			return false
+		}
+		return true
+	})
+}
+
+func contains(mspIDs []string, mspID string) bool {
+	for _, m := range mspIDs {
+		if m == mspID {
+			return true
+		}
+	}
+	return false
+}
+
+// getMaxPeersForRetrieval may be overridden by unit tests
+var getMaxPeersForRetrieval = func() int {
+	return config.GetOLCollMaxPeersForRetrieval()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/mocks/mockprovider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/mocks/mockprovider.go
new file mode 100644
index 000000000..efb33e706
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/mocks/mockprovider.go
@@ -0,0 +1,40 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+)
+
+// Provider is a mock off-ledger data data provider
+type Provider struct {
+}
+
+// RetrieverForChannel returns the retriever for the given channel
+func (p *Provider) RetrieverForChannel(channel string) olapi.Retriever {
+	return &retriever{}
+}
+
+type retriever struct {
+}
+
+// GetData gets data for the given key
+func (m *retriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return &storeapi.ExpiringValue{Value: []byte(key.Key)}, nil
+}
+
+// GetDataMultipleKeys gets data for multiple keys
+func (m *retriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = &storeapi.ExpiringValue{Value: []byte(k)}
+	}
+	return values, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/policy/validator.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/policy/validator.go
new file mode 100644
index 000000000..03d3e0c82
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/policy/validator.go
@@ -0,0 +1,42 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package policy
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+// ValidateConfig validates the Off-Ledger Collection configuration
+func ValidateConfig(config *common.StaticCollectionConfig) error {
+	if config.Type != common.CollectionType_COL_OFFLEDGER && config.Type != common.CollectionType_COL_DCAS {
+		return errors.Errorf("unsupported off-ledger collection type: %s", config.Type)
+	}
+
+	if config.RequiredPeerCount <= 0 {
+		return errors.Errorf("collection-name: %s -- required peer count must be greater than 0", config.Name)
+	}
+
+	if config.RequiredPeerCount > config.MaximumPeerCount {
+		return errors.Errorf("collection-name: %s -- maximum peer count (%d) must be greater than or equal to required peer count (%d)", config.Name, config.MaximumPeerCount, config.RequiredPeerCount)
+	}
+
+	if config.BlockToLive != 0 {
+		return errors.Errorf("collection-name: %s -- block-to-live not supported", config.Name)
+	}
+
+	if config.TimeToLive != "" {
+		_, err := time.ParseDuration(config.TimeToLive)
+		if err != nil {
+			return errors.Errorf("collection-name: %s -- invalid time format for time to live: %s", config.Name, err)
+		}
+	}
+
+	return nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/retriever/olretriever.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/retriever/olretriever.go
new file mode 100644
index 000000000..96e0501ee
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/retriever/olretriever.go
@@ -0,0 +1,470 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	"context"
+	"sync"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	cb "github.com/hyperledger/fabric/protos/common"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/pkg/errors"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dissemination"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/discovery"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/multirequest"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/requestmgr"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+var logger = flogging.MustGetLogger("ext_offledger")
+
+type support interface {
+	Config(channelID, ns, coll string) (*cb.StaticCollectionConfig, error)
+	Policy(channel, ns, collection string) (privdata.CollectionAccessPolicy, error)
+	BlockPublisher(channelID string) gossipapi.BlockPublisher
+}
+
+// Validator is a key/value validator
+type Validator func(txID, ns, coll, key string, value []byte) error
+
+// KeyDecorator allows for modification of the provided key
+type KeyDecorator func(key *storeapi.Key) (*storeapi.Key, error)
+
+// Provider is a collection data data provider.
+type Provider struct {
+	support
+	storeForChannel func(channelID string) olapi.Store
+	gossipAdapter   func() supportapi.GossipAdapter
+	validators      map[cb.CollectionType]Validator
+	keyDecorators   map[cb.CollectionType]KeyDecorator
+}
+
+// Option is a provider option
+type Option func(p *Provider)
+
+// WithValidator sets the key/value validator
+func WithValidator(collType cb.CollectionType, validator Validator) Option {
+	return func(p *Provider) {
+		p.validators[collType] = validator
+	}
+}
+
+// WithKeyDecorator sets the key decorator
+func WithKeyDecorator(collType cb.CollectionType, decorator KeyDecorator) Option {
+	return func(p *Provider) {
+		p.keyDecorators[collType] = decorator
+	}
+}
+
+// NewProvider returns a new collection data provider
+func NewProvider(storeProvider func(channelID string) olapi.Store, support support, gossipProvider func() supportapi.GossipAdapter, opts ...Option) olapi.Provider {
+	p := &Provider{
+		support:         support,
+		storeForChannel: storeProvider,
+		gossipAdapter:   gossipProvider,
+		validators:      make(map[cb.CollectionType]Validator),
+		keyDecorators:   make(map[cb.CollectionType]KeyDecorator),
+	}
+
+	// Apply options
+	for _, opt := range opts {
+		opt(p)
+	}
+	return p
+}
+
+// RetrieverForChannel returns the collection data retriever for the given channel
+func (p *Provider) RetrieverForChannel(channelID string) olapi.Retriever {
+	r := &retriever{
+		support:       p.support,
+		gossipAdapter: p.gossipAdapter(),
+		store:         p.storeForChannel(channelID),
+		channelID:     channelID,
+		reqMgr:        requestmgr.Get(channelID),
+		resolvers:     make(map[collKey]resolver),
+		validators:    p.validators,
+		keyDecorators: p.keyDecorators,
+	}
+
+	// Add a handler so that we can remove the resolver for a chaincode that has been upgraded
+	p.support.BlockPublisher(channelID).AddCCUpgradeHandler(func(blockNum uint64, txID string, chaincodeID string) error {
+		logger.Infof("[%s] Chaincode [%s] has been upgraded. Clearing resolver cache for chaincode.", channelID, chaincodeID)
+		r.removeResolvers(chaincodeID)
+		return nil
+	})
+
+	return r
+}
+
+type resolver interface {
+	// ResolvePeersForRetrieval resolves to a set of peers from which data should be retrieved
+	ResolvePeersForRetrieval() discovery.PeerGroup
+}
+
+type collKey struct {
+	ns   string
+	coll string
+}
+
+func newCollKey(ns, coll string) collKey {
+	return collKey{ns: ns, coll: coll}
+}
+
+type retriever struct {
+	support
+	gossipAdapter supportapi.GossipAdapter
+	channelID     string
+	store         olapi.Store
+	resolvers     map[collKey]resolver
+	lock          sync.RWMutex
+	reqMgr        requestmgr.RequestMgr
+	validators    map[cb.CollectionType]Validator
+	keyDecorators map[cb.CollectionType]KeyDecorator
+}
+
+// GetData gets the values for the data item
+func (r *retriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	values, err := r.GetDataMultipleKeys(ctxt, storeapi.NewMultiKey(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Key))
+	if err != nil {
+		return nil, err
+	}
+
+	if values.Values().IsEmpty() {
+		return nil, nil
+	}
+
+	return values[0], nil
+}
+
+// GetDataMultipleKeys gets the values for the multiple data items in a single call
+func (r *retriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	authorized, err := r.isAuthorized(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, err
+	}
+	if !authorized {
+		logger.Infof("[%s] This peer does not have access to the collection [%s:%s]", r.channelID, key.Namespace, key.Collection)
+		return nil, nil
+	}
+
+	localValues, err := r.getMultipleKeysFromLocal(key)
+	if err != nil {
+		return nil, err
+	}
+	if localValues.Values().AllSet() {
+		err = r.validateValues(key, localValues)
+		if err != nil {
+			logger.Warningf(err.Error())
+			return nil, err
+		}
+		return localValues, nil
+	}
+
+	res, err := r.getResolver(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, errors.WithMessagef(err, "unable to get resolver for channel [%s] and [%s:%s]", r.channelID, key.Namespace, key.Collection)
+	}
+
+	// Retrieve from the remote peers
+	cReq := multirequest.New()
+	for _, peer := range res.ResolvePeersForRetrieval() {
+		logger.Debugf("Adding request to get data for [%s] from [%s] ...", key, peer)
+		cReq.Add(peer.String(), r.getDataFromPeer(key, peer))
+	}
+
+	response := cReq.Execute(ctxt)
+
+	// Merge the values with the values received locally
+	values := asExpiringValues(localValues.Values().Merge(response.Values))
+
+	if err := r.validateValues(key, values); err != nil {
+		logger.Warningf(err.Error())
+		return nil, err
+	}
+
+	if roles.IsCommitter() {
+		r.persistMissingKeys(key, localValues, values)
+	}
+
+	return values, nil
+}
+
+func (r *retriever) getMultipleKeysFromLocal(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	decorateKey, err := r.getKeyDecorator(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, err
+	}
+
+	localValues := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		lkey, e := getKey(decorateKey, key.EndorsedAtTxID, key.Namespace, key.Collection, k)
+		if e != nil {
+			return nil, e
+		}
+		value, retrieveErr := r.store.GetData(lkey)
+		if retrieveErr != nil {
+			logger.Warningf("[%s] Error getting data from local store for [%s]: %s", r.channelID, key, retrieveErr)
+			return nil, errors.WithMessagef(retrieveErr, "unable to get data for channel [%s] and [%s]", r.channelID, key)
+		}
+		localValues[i] = value
+	}
+	return localValues, nil
+}
+
+func (r *retriever) getKeyDecorator(ns, coll string) (KeyDecorator, error) {
+	collConfig, err := r.Config(r.channelID, ns, coll)
+	if err != nil {
+		return nil, err
+	}
+	return r.keyDecorators[collConfig.Type], nil
+}
+
+func getKey(decorateKey KeyDecorator, txID, ns, coll, k string) (*storeapi.Key, error) {
+	key := storeapi.NewKey(txID, ns, coll, k)
+	if decorateKey == nil {
+		return key, nil
+	}
+	return decorateKey(key)
+}
+
+func getMissingKeyIndexes(values []*storeapi.ExpiringValue) []int {
+	var missingIndexes []int
+	for i, v := range values {
+		if v == nil {
+			missingIndexes = append(missingIndexes, i)
+		}
+	}
+	return missingIndexes
+}
+
+// persistMissingKeys persists the keys that were missing from the local store
+func (r *retriever) persistMissingKeys(key *storeapi.MultiKey, localValues, values storeapi.ExpiringValues) {
+	for _, i := range getMissingKeyIndexes(localValues) {
+		v := values[i]
+		if v != nil {
+			r.persistMissingKey(
+				storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Keys[i]),
+				&storeapi.ExpiringValue{Value: v.Value, Expiry: v.Expiry},
+			)
+		}
+	}
+}
+
+func (r *retriever) persistMissingKey(k *storeapi.Key, val *storeapi.ExpiringValue) {
+	logger.Debugf("Persisting key [%s] that was missing from the local store", k)
+	collConfig, err := r.Config(r.channelID, k.Namespace, k.Collection)
+	if err != nil {
+		logger.Warningf("Error persisting key [%s] that was missing from the local store: %s", k, err)
+	}
+	if err := r.store.PutData(collConfig, k, val); err != nil {
+		logger.Warningf("Error persisting key [%s] that was missing from the local store: %s", k, err)
+	}
+}
+
+func (r *retriever) validateValues(key *storeapi.MultiKey, values storeapi.ExpiringValues) error {
+	config, err := r.Config(r.channelID, key.Namespace, key.Collection)
+	if err != nil {
+		return err
+	}
+
+	validate, ok := r.validators[config.Type]
+	if !ok {
+		// No validator for config
+		return nil
+	}
+
+	for i, v := range values {
+		if v != nil && v.Value != nil {
+			err := validate(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Keys[i], v.Value)
+			if err != nil {
+				return err
+			}
+		}
+	}
+	return nil
+}
+
+func (r *retriever) getDataFromPeer(key *storeapi.MultiKey, endorser *discovery.Member) multirequest.Request {
+	return func(ctxt context.Context) (common.Values, error) {
+		logger.Debugf("Getting data for [%s] from [%s] ...", key, endorser)
+
+		values, err := r.getData(ctxt, key, endorser)
+		if err != nil {
+			if err == context.Canceled {
+				logger.Debugf("[%s] Request to get data from [%s] for [%s] was cancelled", r.channelID, endorser, key)
+			} else {
+				logger.Debugf("[%s] Error getting data from [%s] for [%s]: %s", r.channelID, endorser, key, err)
+			}
+			return nil, err
+		}
+
+		return values.Values(), nil
+	}
+}
+
+func (r *retriever) getResolver(ns, coll string) (resolver, error) {
+	key := newCollKey(ns, coll)
+
+	r.lock.RLock()
+	resolver, ok := r.resolvers[key]
+	r.lock.RUnlock()
+
+	if ok {
+		return resolver, nil
+	}
+
+	return r.getOrCreateResolver(key)
+}
+
+func (r *retriever) getOrCreateResolver(key collKey) (resolver, error) {
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	resolver, ok := r.resolvers[key]
+	if ok {
+		return resolver, nil
+	}
+
+	policy, err := r.Policy(r.channelID, key.ns, key.coll)
+	if err != nil {
+		return nil, err
+	}
+
+	resolver = dissemination.New(r.channelID, key.ns, key.coll, policy, r.gossipAdapter)
+
+	r.resolvers[key] = resolver
+
+	return resolver, nil
+}
+
+func (r *retriever) removeResolvers(ns string) {
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	for key := range r.resolvers {
+		if key.ns == ns {
+			logger.Debugf("[%s] Removing resolver [%s:%s] from cache", r.channelID, key.ns, key.coll)
+			delete(r.resolvers, key)
+		}
+	}
+}
+
+func (r *retriever) getData(ctxt context.Context, key *storeapi.MultiKey, peers ...*discovery.Member) (storeapi.ExpiringValues, error) {
+	logger.Debugf("[%s] Sending Gossip request to %s for data for [%s]", r.channelID, peers, key)
+
+	req := r.reqMgr.NewRequest()
+
+	logger.Debugf("[%s] Creating Gossip request %d for data for [%s]", r.channelID, req.ID(), key)
+	msg := r.createCollDataRequestMsg(req, key)
+
+	logger.Debugf("[%s] Sending Gossip request %d for data for [%s]", r.channelID, req.ID(), key)
+	r.gossipAdapter.Send(msg, asRemotePeers(peers)...)
+
+	logger.Debugf("[%s] Waiting for response for %d for data for [%s]", r.channelID, req.ID(), key)
+	res, err := req.GetResponse(ctxt)
+	if err != nil {
+		return nil, err
+	}
+
+	logger.Debugf("[%s] Got response for %d for data for [%s]", r.channelID, req.ID(), key)
+
+	data := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		d, ok := res.Data.Get(key.Namespace, key.Collection, k)
+		if !ok {
+			return nil, errors.Errorf("the response does not contain a value for key [%s:%s:%s]", key.Namespace, key.Collection, k)
+		}
+		if d.Value == nil {
+			data[i] = nil
+		} else {
+			data[i] = &storeapi.ExpiringValue{Value: d.Value, Expiry: d.Expiry}
+		}
+	}
+
+	return data, nil
+}
+
+// isAuthorized returns true if the local peer has access to the given collection
+func (r *retriever) isAuthorized(ns, coll string) (bool, error) {
+	policy, err := r.Policy(r.channelID, ns, coll)
+	if err != nil {
+		return false, errors.WithMessagef(err, "unable to get policy for [%s:%s]", ns, coll)
+	}
+
+	localMSPID, err := getLocalMSPID()
+	if err != nil {
+		return false, errors.WithMessagef(err, "unable to get local MSP ID")
+	}
+
+	for _, mspID := range policy.MemberOrgs() {
+		if mspID == localMSPID {
+			return true, nil
+		}
+	}
+
+	return false, nil
+}
+
+func (r *retriever) createCollDataRequestMsg(req requestmgr.Request, key *storeapi.MultiKey) *gproto.GossipMessage {
+	var digests []*gproto.CollDataDigest
+	for _, k := range key.Keys {
+		digests = append(digests, &gproto.CollDataDigest{
+			Namespace:      key.Namespace,
+			Collection:     key.Collection,
+			Key:            k,
+			EndorsedAtTxID: key.EndorsedAtTxID,
+		})
+	}
+
+	return &gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(r.channelID),
+		Content: &gproto.GossipMessage_CollDataReq{
+			CollDataReq: &gproto.RemoteCollDataRequest{
+				Nonce:   req.ID(),
+				Digests: digests,
+			},
+		},
+	}
+}
+
+func asRemotePeers(members []*discovery.Member) []*comm.RemotePeer {
+	var peers []*comm.RemotePeer
+	for _, m := range members {
+		peers = append(peers, &comm.RemotePeer{
+			Endpoint: m.Endpoint,
+			PKIID:    m.PKIid,
+		})
+	}
+	return peers
+}
+
+// getLocalMSPID returns the MSP ID of the local peer. This variable may be overridden by unit tests.
+var getLocalMSPID = func() (string, error) {
+	return mspmgmt.GetLocalMSP().GetIdentifier()
+}
+
+func asExpiringValues(cv common.Values) storeapi.ExpiringValues {
+	vals := make(storeapi.ExpiringValues, len(cv))
+	for i, v := range cv {
+		if common.IsNil(v) {
+			vals[i] = nil
+		} else {
+			vals[i] = v.(*storeapi.ExpiringValue)
+		}
+	}
+	return vals
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstore.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstore.go
new file mode 100644
index 000000000..703763e16
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstore.go
@@ -0,0 +1,341 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	"github.com/hyperledger/fabric/protos/common"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/cache"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+var logger = flogging.MustGetLogger("ext_offledger")
+
+type store struct {
+	channelID   string
+	dbProvider  api.DBProvider
+	cache       *cache.Cache
+	collConfigs map[common.CollectionType]*collTypeConfig
+}
+
+func newStore(channelID string, dbProvider api.DBProvider, collConfigs map[common.CollectionType]*collTypeConfig) *store {
+	logger.Debugf("constructing collection data store")
+	return &store{
+		channelID:   channelID,
+		collConfigs: collConfigs,
+		dbProvider:  dbProvider,
+		cache:       cache.New(channelID, dbProvider, config.GetOLCollCacheSize()),
+	}
+}
+
+// Close closes the store
+func (s *store) Close() {
+	s.dbProvider.Close()
+}
+
+// Persist persists all data within the private data simulation results
+func (s *store) Persist(txID string, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	rwSet, err := rwsetutil.TxPvtRwSetFromProtoMsg(privateSimulationResultsWithConfig.PvtRwset)
+	if err != nil {
+		return errors.WithMessage(err, "error getting pvt RW set from bytes")
+	}
+
+	for _, nsRWSet := range rwSet.NsPvtRwSet {
+		for _, collRWSet := range nsRWSet.CollPvtRwSets {
+			if err := s.persistColl(txID, nsRWSet.NameSpace, privateSimulationResultsWithConfig.CollectionConfigs, collRWSet); err != nil {
+				return err
+			}
+		}
+	}
+
+	return nil
+}
+
+// PutData returns the  data for the given key
+func (s *store) PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error {
+	if value.Value == nil {
+		return errors.Errorf("attempt to put nil value for key [%s]", key)
+	}
+	if config.Name != key.Collection {
+		return errors.Errorf("invalid collection config for key [%s]", key)
+	}
+
+	key, value, err := s.decorate(config, key, value)
+	if err != nil {
+		return err
+	}
+
+	if !value.Expiry.IsZero() {
+		if value.Expiry.Before(time.Now()) {
+			// Already expired
+			logger.Debugf("[%s] Key [%s] already expired", s.channelID, key)
+			return nil
+		}
+	}
+
+	db, err := s.dbProvider.GetDB(key.Namespace, key.Collection)
+	if err != nil {
+		return err
+	}
+
+	logger.Debugf("[%s] Putting key [%s] to DB", s.channelID, key)
+	err = db.Put(api.NewKeyValue(key.Key, value.Value, key.EndorsedAtTxID, value.Expiry))
+	if err != nil {
+		return err
+	}
+
+	logger.Debugf("[%s] Putting key [%s] to cache", s.channelID, key)
+	s.cache.Put(key.Namespace, key.Collection, key.Key,
+		&api.Value{
+			Value:      value.Value,
+			TxID:       key.EndorsedAtTxID,
+			ExpiryTime: value.Expiry,
+		},
+	)
+
+	return nil
+}
+
+// GetData returns the  data for the given key
+func (s *store) GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return s.getData(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Key)
+}
+
+// tDataMultipleKeys returns the  data for the given keys
+func (s *store) GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return s.getDataMultipleKeys(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Keys...)
+}
+
+func (s *store) persistColl(txID string, ns string, collConfigPkgs map[string]*common.CollectionConfigPackage, collRWSet *rwsetutil.CollPvtRwSet) error {
+	config, exists := s.getCollectionConfig(collConfigPkgs, ns, collRWSet.CollectionName)
+	if !exists {
+		logger.Debugf("[%s]  config for collection [%s:%s] not found in config packages", s.channelID, ns, collRWSet.CollectionName)
+		return nil
+	}
+
+	authorized, err := s.isAuthorized(ns, config)
+	if err != nil {
+		return err
+	}
+	if !authorized {
+		logger.Infof("[%s] Will not store  collection [%s:%s] since local peer is not authorized.", s.channelID, ns, collRWSet.CollectionName)
+		return nil
+	}
+
+	logger.Debugf("[%s] Collection [%s:%s] is a  collection", s.channelID, ns, collRWSet.CollectionName)
+
+	expiryTime, err := s.getExpirationTime(config)
+	if err != nil {
+		return err
+	}
+
+	batch, err := s.createBatch(txID, ns, config, collRWSet, expiryTime)
+	if err != nil {
+		return err
+	}
+
+	db, err := s.dbProvider.GetDB(ns, collRWSet.CollectionName)
+	if err != nil {
+		return err
+	}
+
+	err = db.Put(batch...)
+	if err != nil {
+		return errors.WithMessagef(err, "error persisting to [%s:%s]", ns, collRWSet.CollectionName)
+	}
+
+	for _, kv := range batch {
+		if kv.Value != nil {
+			logger.Infof("[%s] Putting key [%s:%s:%s] in Tx [%s]", s.channelID, ns, collRWSet.CollectionName, kv.Key, kv.TxID)
+			s.cache.Put(ns, collRWSet.CollectionName, kv.Key, kv.Value)
+		} else {
+			logger.Infof("[%s] Deleting key [%s:%s:%s]", s.channelID, ns, collRWSet.CollectionName, kv.Key)
+			s.cache.Delete(ns, collRWSet.CollectionName, kv.Key)
+		}
+	}
+
+	return nil
+}
+
+func (s *store) getData(txID, ns, coll, key string) (*storeapi.ExpiringValue, error) {
+	value, err := s.cache.Get(ns, coll, key)
+	if err != nil {
+		return nil, err
+	}
+
+	if value == nil {
+		return nil, nil
+	}
+
+	logger.Debugf("[%s] Got value for key [%s:%s:%s] which was persisted in transaction [%s]. Current tx [%s]", s.channelID, ns, coll, key, value.TxID, txID)
+	if value.TxID == txID {
+		logger.Debugf("[%s] Key [%s:%s:%s] was persisted in same transaction [%s] as caller. Returning nil.", s.channelID, ns, coll, key, txID)
+		return nil, nil
+	}
+
+	return &storeapi.ExpiringValue{Value: value.Value, Expiry: value.ExpiryTime}, nil
+}
+
+func (s *store) getDataMultipleKeys(txID, ns, coll string, keys ...string) (storeapi.ExpiringValues, error) {
+	values, err := s.cache.GetMultiple(ns, coll, keys...)
+	if err != nil {
+		return nil, err
+	}
+
+	if len(values) != len(keys) {
+		return nil, errors.New("not all of the values were returned for the set of keys")
+	}
+
+	var ret storeapi.ExpiringValues
+	for i, value := range values {
+		var v *storeapi.ExpiringValue
+		if value != nil {
+			logger.Debugf("[%s] Got value for key [%s:%s:%s] which was persisted in transaction [%s]. Current tx [%s]", s.channelID, ns, coll, keys[i], value.TxID, txID)
+			if value.TxID == txID {
+				logger.Debugf("[%s] Key [%s:%s:%s] was persisted in same transaction [%s] as caller. Returning nil.", s.channelID, ns, coll, keys[i], txID)
+			} else {
+				v = &storeapi.ExpiringValue{Value: value.Value, Expiry: value.ExpiryTime}
+			}
+		}
+		ret = append(ret, v)
+	}
+
+	return ret, nil
+}
+
+func (s *store) createBatch(txID, ns string, config *cb.StaticCollectionConfig, collRWSet *rwsetutil.CollPvtRwSet, expiryTime time.Time) ([]*api.KeyValue, error) {
+	var batch []*api.KeyValue
+	for _, w := range collRWSet.KvRwSet.Writes {
+		kv, err := s.newKeyValue(txID, ns, config, expiryTime, w)
+		if err != nil {
+			return nil, err
+		}
+		batch = append(batch, kv)
+	}
+	return batch, nil
+}
+
+func (s *store) newKeyValue(txID, ns string, config *cb.StaticCollectionConfig, expiryTime time.Time, w *kvrwset.KVWrite) (*api.KeyValue, error) {
+	key := storeapi.NewKey(txID, ns, config.Name, w.Key)
+	if w.IsDelete {
+		dKey, err := s.decorateKey(config, key)
+		if err != nil {
+			return nil, err
+		}
+		return &api.KeyValue{Key: dKey.Key}, nil
+	}
+
+	dKey, value, err := s.decorate(config, key,
+		&storeapi.ExpiringValue{
+			Value:  w.Value,
+			Expiry: expiryTime,
+		},
+	)
+	if err != nil {
+		return nil, err
+	}
+	return api.NewKeyValue(dKey.Key, value.Value, txID, value.Expiry), nil
+}
+
+func (s *store) isAuthorized(ns string, config *common.StaticCollectionConfig) (bool, error) {
+	policy, err := s.loadPolicy(ns, config)
+	if err != nil {
+		logger.Errorf("[%s] Error loading policy for collection [%s:%s]: %s", s.channelID, ns, config.Name, err)
+		return false, err
+	}
+
+	localMSPID, err := getLocalMSPID()
+	if err != nil {
+		logger.Errorf("[%s] Error getting local MSP ID: %s", s.channelID, err)
+		return false, err
+	}
+	for _, mspID := range policy.MemberOrgs() {
+		if mspID == localMSPID {
+			return true, nil
+		}
+	}
+	return false, nil
+}
+
+// TODO: Consider caching policies to avoid marshalling every time
+func (s *store) loadPolicy(ns string, config *common.StaticCollectionConfig) (privdata.CollectionAccessPolicy, error) {
+	logger.Debugf("[%s] Loading collection policy for [%s:%s]", s.channelID, ns, config.Name)
+
+	colAP := &privdata.SimpleCollection{}
+	err := colAP.Setup(config, mspmgmt.GetIdentityDeserializer(s.channelID))
+	if err != nil {
+		return nil, errors.Wrapf(err, "error setting up collection policy %s", config.Name)
+	}
+
+	return colAP, nil
+}
+
+func (s *store) getExpirationTime(config *common.StaticCollectionConfig) (time.Time, error) {
+	var expiryTime time.Time
+	if config.TimeToLive == "" {
+		return expiryTime, nil
+	}
+	ttl, e := time.ParseDuration(config.TimeToLive)
+	if e != nil {
+		// This shouldn't happen since the config was validated before being persisted
+		return expiryTime, errors.Wrapf(e, "error parsing time-to-live for collection [%s]", config.Name)
+	}
+	return time.Now().Add(ttl), nil
+}
+
+func (s *store) getCollectionConfig(collConfigPkgs map[string]*common.CollectionConfigPackage, namespace, collName string) (*common.StaticCollectionConfig, bool) {
+	collConfigPkg, ok := collConfigPkgs[namespace]
+	if !ok {
+		return nil, false
+	}
+
+	for _, collConfig := range collConfigPkg.Config {
+		config := collConfig.GetStaticCollectionConfig()
+		if config != nil && config.Name == collName && s.collTypeSupported(config.Type) {
+			return config, true
+		}
+	}
+
+	return nil, false
+}
+
+func (s *store) decorate(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) (*storeapi.Key, *storeapi.ExpiringValue, error) {
+	cfg, ok := s.collConfigs[config.Type]
+	if !ok || cfg.decorator == nil {
+		return key, value, nil
+	}
+	return cfg.decorator(key, value)
+}
+
+func (s *store) decorateKey(config *cb.StaticCollectionConfig, key *storeapi.Key) (*storeapi.Key, error) {
+	cfg, ok := s.collConfigs[config.Type]
+	if !ok || cfg.keyDecorator == nil {
+		return key, nil
+	}
+	return cfg.keyDecorator(key)
+}
+
+func (s *store) collTypeSupported(collType cb.CollectionType) bool {
+	_, ok := s.collConfigs[collType]
+	return ok
+}
+
+// getLocalMSPID returns the MSP ID of the local peer. This variable may be overridden by unit tests.
+var getLocalMSPID = func() (string, error) {
+	return mspmgmt.GetLocalMSP().GetIdentifier()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstoreprovider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstoreprovider.go
new file mode 100644
index 000000000..c6c2290f4
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/olstoreprovider.go
@@ -0,0 +1,119 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"sync"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/protos/common"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore"
+)
+
+// Option is a store provider option
+type Option func(p *StoreProvider)
+
+// CollOption is a collection option
+type CollOption func(c *collTypeConfig)
+
+// WithCollectionType adds a collection type to the set of collection types supported by the off-ledger store
+// along with any options
+func WithCollectionType(collType common.CollectionType, opts ...CollOption) Option {
+	return func(p *StoreProvider) {
+		c := &collTypeConfig{}
+		p.collConfigs[collType] = c
+
+		for _, opt := range opts {
+			opt(c)
+		}
+	}
+}
+
+// Decorator allows the key/value to be modified/validated before being persisted
+type Decorator func(key *storeapi.Key, value *storeapi.ExpiringValue) (*storeapi.Key, *storeapi.ExpiringValue, error)
+
+// WithDecorator sets a decorator for a collection type allowing the key/value to be validated/modified before being persisted
+func WithDecorator(decorator Decorator) CollOption {
+	return func(c *collTypeConfig) {
+		c.decorator = decorator
+	}
+}
+
+// KeyDecorator allows the key to be modified/validated
+type KeyDecorator func(key *storeapi.Key) (*storeapi.Key, error)
+
+// WithKeyDecorator sets a key decorator for a collection type allowing the key to be validated/modified
+func WithKeyDecorator(decorator KeyDecorator) CollOption {
+	return func(c *collTypeConfig) {
+		c.keyDecorator = decorator
+	}
+}
+
+type collTypeConfig struct {
+	decorator    Decorator
+	keyDecorator KeyDecorator
+}
+
+// New returns a store provider factory
+func New(opts ...Option) *StoreProvider {
+	p := &StoreProvider{
+		stores:      make(map[string]olapi.Store),
+		dbProvider:  getDBProvider(),
+		collConfigs: make(map[common.CollectionType]*collTypeConfig),
+	}
+
+	// OFF_LEDGER collection type supported by default
+	opts = append(opts, WithCollectionType(common.CollectionType_COL_OFFLEDGER))
+
+	// Apply options
+	for _, opt := range opts {
+		opt(p)
+	}
+	return p
+}
+
+// StoreProvider is a store provider
+type StoreProvider struct {
+	stores map[string]olapi.Store
+	sync.RWMutex
+	dbProvider  api.DBProvider
+	collConfigs map[common.CollectionType]*collTypeConfig
+}
+
+// StoreForChannel returns the store for the given channel
+func (sp *StoreProvider) StoreForChannel(channelID string) olapi.Store {
+	sp.RLock()
+	defer sp.RUnlock()
+	return sp.stores[channelID]
+}
+
+// OpenStore opens the store for the given channel
+func (sp *StoreProvider) OpenStore(channelID string) (olapi.Store, error) {
+	sp.Lock()
+	defer sp.Unlock()
+
+	store, ok := sp.stores[channelID]
+	if !ok {
+		store = newStore(channelID, sp.dbProvider, sp.collConfigs)
+		sp.stores[channelID] = store
+	}
+	return store, nil
+}
+
+// Close shuts down all of the stores
+func (sp *StoreProvider) Close() {
+	for _, s := range sp.stores {
+		s.Close()
+	}
+}
+
+// getDBProvider returns the DB provider. This var may be overridden by unit tests
+var getDBProvider = func() api.DBProvider {
+	return couchdbstore.NewDBProvider()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api/api.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api/api.go
new file mode 100644
index 000000000..e5fc5fda3
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api/api.go
@@ -0,0 +1,61 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	"time"
+)
+
+// Value is a data value
+type Value struct {
+	Value      []byte
+	TxID       string
+	ExpiryTime time.Time
+}
+
+// KeyValue is a struct to store a key value pair
+type KeyValue struct {
+	*Value
+	Key string
+}
+
+// NewKeyValue returns a new key
+func NewKeyValue(key string, value []byte, txID string, expiryTime time.Time) *KeyValue {
+	return &KeyValue{
+		Key:   key,
+		Value: &Value{Value: value, TxID: txID, ExpiryTime: expiryTime},
+	}
+}
+
+// String returns the string representation of the key
+func (k *KeyValue) String() string {
+	return k.Key
+}
+
+// DB persists collection data.
+type DB interface {
+	// Put stores the given set of keys/values. If expiry time is 0 then the data lives forever.
+	Put(keyVal ...*KeyValue) error
+
+	// Get returns the value for the given key or nil if the key doesn't exist
+	Get(key string) (*Value, error)
+
+	// Get returns the values for multiple keys. The values are returned in the same order as the keys.
+	GetMultiple(keys ...string) ([]*Value, error)
+
+	// DeleteExpiredKeys deletes all of the expired keys
+	DeleteExpiredKeys() error
+}
+
+// DBProvider returns the persister for the given namespace/collection
+type DBProvider interface {
+	// GetDB return the DB for the given namespace/collection
+	GetDB(ns, coll string) (DB, error)
+
+	// Close closes the DB provider
+	Close()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/cache/cache.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/cache/cache.go
new file mode 100644
index 000000000..0113daed4
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/cache/cache.go
@@ -0,0 +1,170 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cache
+
+import (
+	"fmt"
+	"time"
+
+	"github.com/bluele/gcache"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api"
+)
+
+var logger = flogging.MustGetLogger("ext_offledger")
+
+// Cache implements a cache for collection data
+type Cache struct {
+	channelID  string
+	cache      gcache.Cache
+	dbProvider api.DBProvider
+}
+
+type cacheKey struct {
+	namespace  string
+	collection string
+	key        string
+}
+
+func (k cacheKey) String() string {
+	return fmt.Sprintf("%s:%s:%s", k.namespace, k.collection, k.key)
+}
+
+// New returns a new collection data cache
+func New(channelID string, dbProvider api.DBProvider, size int) *Cache {
+	c := &Cache{
+		channelID:  channelID,
+		dbProvider: dbProvider,
+	}
+	c.cache = gcache.New(size).ARC().LoaderExpireFunc(
+		func(k interface{}) (interface{}, *time.Duration, error) {
+			key := k.(cacheKey)
+			v, remainingTime, err := c.load(key)
+			if err != nil {
+				logger.Warningf("[%s] Error loading key [%s]: %s", c.channelID, key, err)
+				return nil, nil, err
+			}
+			return v, remainingTime, nil
+		}).Build()
+	return c
+}
+
+// Put adds the value for the given key.
+func (c *Cache) Put(ns, coll, key string, value *api.Value) {
+	cKey := cacheKey{
+		namespace:  ns,
+		collection: coll,
+		key:        key,
+	}
+
+	var err error
+	if value.ExpiryTime.IsZero() {
+		logger.Debugf("[%s] Putting key [%s]. Expires: NEVER", c.channelID, cKey)
+		err = c.cache.Set(cKey, value)
+	} else if value.ExpiryTime.Before(time.Now()) {
+		logger.Debugf("[%s] Expiry time for key [%s] occurs in the past. Value will not be added", c.channelID, cKey)
+	} else {
+		logger.Debugf("[%s] Putting key [%s]. Expires: %s", c.channelID, cKey, value.ExpiryTime)
+		err = c.cache.SetWithExpire(cKey, value, time.Until(value.ExpiryTime))
+	}
+
+	if err != nil {
+		panic("Set must never return an error")
+	}
+}
+
+// Delete deletes the given key.
+func (c *Cache) Delete(ns, coll, key string) {
+	cKey := cacheKey{
+		namespace:  ns,
+		collection: coll,
+		key:        key,
+	}
+	logger.Debugf("[%s] Removing key [%s]", c.channelID, cKey)
+	c.cache.Remove(cKey)
+}
+
+// Get returns the values for the given keys
+func (c *Cache) Get(ns, coll, key string) (*api.Value, error) {
+	cKey := cacheKey{
+		namespace:  ns,
+		collection: coll,
+		key:        key,
+	}
+
+	value, err := c.cache.Get(cKey)
+	if err != nil {
+		logger.Warningf("[%s] Error getting key [%s]: %s", c.channelID, cKey, err)
+		return nil, err
+	}
+
+	if value == nil {
+		logger.Debugf("[%s] Key not found [%s]", c.channelID, cKey)
+		return nil, nil
+	}
+
+	v, ok := value.(*api.Value)
+	if !ok {
+		panic("Invalid value type!")
+	}
+	if v == nil {
+		logger.Debugf("[%s] Key not found [%s]", c.channelID, cKey)
+		return nil, nil
+	}
+
+	logger.Debugf("[%s] Got key [%s]. Expires: %s", c.channelID, cKey, v.ExpiryTime)
+	return v, nil
+}
+
+// GetMultiple returns the values for the given keys
+func (c *Cache) GetMultiple(ns, coll string, keys ...string) ([]*api.Value, error) {
+	values := make([]*api.Value, len(keys))
+	for i, key := range keys {
+		value, err := c.Get(ns, coll, key)
+		if err != nil {
+			return nil, err
+		}
+		values[i] = value
+	}
+	return values, nil
+}
+
+func (c *Cache) load(key cacheKey) (*api.Value, *time.Duration, error) {
+	db, err := c.dbProvider.GetDB(key.namespace, key.collection)
+	if err != nil {
+		return nil, nil, errors.WithMessage(err, "error getting database")
+	}
+
+	logger.Debugf("Loading value for key %s from DB", key)
+	v, err := db.Get(key.key)
+	if err != nil {
+		return nil, nil, errors.WithMessage(err, "error loading value")
+	}
+
+	logger.Debugf("Loaded value %v for key %s from DB", v, key)
+	if v == nil {
+		logger.Debugf("[%s] Value not found for key [%s]", c.channelID, key)
+		return nil, nil, nil
+	}
+
+	logger.Debugf("Checking expiry time for key %s", key)
+	if v.ExpiryTime.IsZero() {
+		logger.Debugf("[%s] Loaded key [%s]. Expires: NEVER", c.channelID, key)
+		return v, nil, nil
+
+	}
+	remainingTime := time.Until(v.ExpiryTime)
+	if remainingTime < 0 {
+		// Already expired
+		logger.Debugf("[%s] Loaded key [%s]. Expires: NOW", c.channelID, key)
+		return nil, nil, nil
+	}
+
+	logger.Debugf("[%s] Loaded key [%s]. Expires: %s", c.channelID, key, v.ExpiryTime)
+	return v, &remainingTime, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore.go
new file mode 100644
index 000000000..3c80ee404
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore.go
@@ -0,0 +1,232 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdbstore
+
+import (
+	"encoding/json"
+	"fmt"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api"
+)
+
+var (
+	logger          = flogging.MustGetLogger("ext_offledger")
+	compositeKeySep = "!"
+)
+
+const (
+	fetchExpiryDataQuery = `
+	{
+		"selector": {
+			"` + expiryField + `": {
+				"$lt": %v
+			}
+		},
+		"use_index": ["_design/` + expiryIndexDoc + `", "` + expiryIndexName + `"]
+	}`
+)
+
+// dataModel couch doc dataModel
+type dataModel struct {
+	ID      string `json:"_id"`
+	Rev     string `json:"_rev,omitempty"`
+	Data    string `json:"dataModel"`
+	TxnID   string `json:"txnID"`
+	Expiry  int64  `json:"expiry"`
+	Deleted bool   `json:"_deleted"`
+}
+
+type dbstore struct {
+	dbName string
+	db     *couchdb.CouchDatabase
+}
+
+// newDBStore constructs an instance of db store
+func newDBStore(db *couchdb.CouchDatabase, dbName string) *dbstore {
+	return &dbstore{dbName, db}
+}
+
+//-----------------Interface implementation functions--------------------//
+// AddKey adds dataModel to db
+func (s *dbstore) Put(keyVal ...*api.KeyValue) error {
+	var docs []*couchdb.CouchDoc
+	for _, kv := range keyVal {
+		dataDoc, err := s.createCouchDoc(string(encodeKey(kv.Key, time.Time{})), kv.Value)
+		if err != nil {
+			return err
+		}
+		if dataDoc != nil {
+			docs = append(docs, dataDoc)
+		}
+	}
+
+	if len(docs) == 0 {
+		logger.Debugf("[%s] Nothing to do", s.dbName)
+		return nil
+	}
+
+	_, err := s.db.BatchUpdateDocuments(docs)
+	if nil != err {
+		return errors.WithMessage(err, fmt.Sprintf("BatchUpdateDocuments failed for [%d] documents", len(docs)))
+	}
+
+	return nil
+}
+
+// GetKey get dataModel based on key from db
+func (s *dbstore) Get(key string) (*api.Value, error) {
+	data, err := fetchData(s.db, string(encodeKey(key, time.Time{})))
+	if err != nil {
+		return nil, errors.Wrapf(err, "Failed to load key [%s] from db", key)
+	}
+
+	if data != nil {
+		val := &api.Value{Value: []byte(data.Data), TxID: data.TxnID, ExpiryTime: time.Unix(0, data.Expiry)}
+		return val, nil
+	}
+
+	return nil, nil
+}
+
+// GetMultiple retrieves values for multiple keys at once
+func (s *dbstore) GetMultiple(keys ...string) ([]*api.Value, error) {
+	values := make([]*api.Value, len(keys))
+	for i, k := range keys {
+		v, err := s.Get(k)
+		if err != nil {
+			return nil, err
+		}
+		values[i] = v
+	}
+
+	return values, nil
+}
+
+// DeleteExpiredKeys delete expired keys from db
+func (s *dbstore) DeleteExpiredKeys() error {
+	data, err := fetchExpiryData(s.db, time.Now())
+	if err != nil {
+		return err
+	}
+	if len(data) == 0 {
+		logger.Debugf("No keys to delete from db")
+		return nil
+	}
+
+	docs := make([]*couchdb.CouchDoc, 0)
+	docIDs := make([]string, 0)
+	for _, doc := range data {
+		updateDoc := &dataModel{ID: doc.ID, Rev: doc.Rev, Deleted: true}
+		jsonBytes, err := json.Marshal(updateDoc)
+		if err != nil {
+			return err
+		}
+		couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+		docs = append(docs, &couchDoc)
+		docIDs = append(docIDs, updateDoc.ID)
+	}
+
+	if len(docs) > 0 {
+		_, err := s.db.BatchUpdateDocuments(docs)
+		if err != nil {
+			return errors.WithMessage(err, fmt.Sprintf("BatchUpdateDocuments failed for [%d] documents", len(docs)))
+		}
+		logger.Debugf("Deleted expired keys %s from db", docIDs)
+	}
+
+	return nil
+}
+
+// Close db
+func (s *dbstore) Close() {
+}
+
+//-----------------helper functions--------------------//
+func encodeKey(key string, expiryTime time.Time) []byte {
+	var compositeKey []byte
+	if !expiryTime.IsZero() {
+		compositeKey = append(compositeKey, []byte(fmt.Sprintf("%d", expiryTime.UnixNano()/int64(time.Millisecond)))...)
+		compositeKey = append(compositeKey, compositeKeySep...)
+	}
+	compositeKey = append(compositeKey, []byte(key)...)
+	return compositeKey
+}
+
+//-----------------database helper functions--------------------//
+func fetchData(db *couchdb.CouchDatabase, key string) (*dataModel, error) {
+	doc, _, err := db.ReadDoc(key)
+	if err != nil {
+		return nil, err
+	}
+
+	if doc == nil {
+		return nil, nil
+	}
+
+	var data dataModel
+	err = json.Unmarshal(doc.JSONValue, &data)
+	if err != nil {
+		return nil, errors.Wrapf(err, "Result from DB is not JSON encoded")
+	}
+
+	return &data, nil
+}
+
+func (s *dbstore) createCouchDoc(key string, value *api.Value) (*couchdb.CouchDoc, error) {
+	var data *dataModel
+	if value == nil {
+		logger.Debugf("[%s] Deleting key [%s]", s.dbName, key)
+
+		// Get the revision on the current doc
+		current, err := fetchData(s.db, string(encodeKey(key, time.Time{})))
+		if err != nil {
+			return nil, errors.Wrapf(err, "Failed to load key [%s] from db", key)
+		}
+		if current == nil {
+			logger.Debugf("[%s] Current key not found to delete [%s]", s.dbName, key)
+			return nil, nil
+		}
+		data = &dataModel{ID: key, Rev: current.Rev, Deleted: true}
+	} else {
+		data = &dataModel{ID: key, Data: string(value.Value), TxnID: value.TxID, Expiry: value.ExpiryTime.UnixNano() / int64(time.Millisecond)}
+	}
+
+	jsonBytes, err := json.Marshal(data)
+	if err != nil {
+		return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	return &couchDoc, nil
+}
+
+func fetchExpiryData(db *couchdb.CouchDatabase, expiry time.Time) ([]*dataModel, error) {
+	results, _, err := db.QueryDocuments(fmt.Sprintf(fetchExpiryDataQuery, expiry.UnixNano()/int64(time.Millisecond)))
+	if err != nil {
+		return nil, err
+	}
+
+	if len(results) == 0 {
+		return nil, nil
+	}
+
+	var responses []*dataModel
+	for _, result := range results {
+		var data dataModel
+		err = json.Unmarshal(result.Value, &data)
+		if err != nil {
+			return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+		}
+		responses = append(responses, &data)
+	}
+
+	return responses, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go
new file mode 100644
index 000000000..d7807a75f
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go
@@ -0,0 +1,207 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdbstore
+
+import (
+	"fmt"
+	"path/filepath"
+	"sync"
+	"time"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	coreconfig "github.com/hyperledger/fabric/core/config"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/spf13/viper"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider/store/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+const (
+	expiryField     = "expiry"
+	expiryIndexName = "by_expiry"
+	expiryIndexDoc  = "indexExpiry"
+	expiryIndexDef  = `
+	{
+		"index": {
+			"fields": ["` + expiryField + `"]
+		},
+		"name": "` + expiryIndexName + `",
+		"ddoc": "` + expiryIndexDoc + `",
+		"type": "json"
+	}`
+)
+
+// CouchDBProvider provides an handle to a db
+type CouchDBProvider struct {
+	couchInstance *couchdb.CouchInstance
+	cimutex       sync.RWMutex
+	stores        map[string]*dbstore
+	mutex         sync.RWMutex
+	done          chan struct{}
+	closed        bool
+}
+
+// NewDBProvider creates a CouchDB Provider
+func NewDBProvider() *CouchDBProvider {
+	return &CouchDBProvider{
+		done:   make(chan struct{}, 1),
+		stores: make(map[string]*dbstore),
+	}
+}
+
+//GetDB based on ns%coll
+func (p *CouchDBProvider) GetDB(ns, coll string) (api.DB, error) {
+	dbName := dbName(ns, coll)
+
+	p.mutex.RLock()
+	s, ok := p.stores[dbName]
+	p.mutex.RUnlock()
+
+	if ok {
+		return s, nil
+	}
+
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	if !ok {
+		ci, err := p.getCouchInstance()
+		if err != nil {
+			logger.Error(err)
+			return nil, err
+		}
+		db, err := couchdb.CreateCouchDatabase(ci, dbName)
+		if nil != err {
+			logger.Error(err)
+			return nil, nil
+		}
+		s = newDBStore(db, dbName)
+
+		err = db.CreateNewIndexWithRetry(expiryIndexDef, expiryIndexDoc)
+		if err != nil {
+			return nil, err
+		}
+		p.stores[dbName] = s
+	}
+
+	return s, nil
+}
+
+// Close cleans up the Provider
+func (p *CouchDBProvider) Close() {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	if !p.closed {
+		p.done <- struct{}{}
+		p.closed = true
+	}
+}
+
+func (p *CouchDBProvider) getCouchInstance() (*couchdb.CouchInstance, error) {
+	p.cimutex.RLock()
+	ci := p.couchInstance
+	p.cimutex.RUnlock()
+
+	if ci != nil {
+		return ci, nil
+	}
+
+	return p.createCouchInstance()
+}
+
+func (p *CouchDBProvider) createCouchInstance() (*couchdb.CouchInstance, error) {
+	p.cimutex.Lock()
+	defer p.cimutex.Unlock()
+
+	if p.couchInstance != nil {
+		return p.couchInstance, nil
+	}
+
+	var err error
+	p.couchInstance, err = couchdb.CreateCouchInstance(getCouchDBConfig(), &disabled.Provider{})
+	if err != nil {
+		return nil, err
+	}
+
+	p.periodicPurge()
+
+	return p.couchInstance, nil
+}
+
+// periodicPurge goroutine to purge dataModel based on config interval time
+func (p *CouchDBProvider) periodicPurge() {
+	ticker := time.NewTicker(config.GetOLCollExpirationCheckInterval())
+	go func() {
+		defer ticker.Stop()
+		for {
+			select {
+			case <-ticker.C:
+				for _, s := range p.getStores() {
+					err := s.DeleteExpiredKeys()
+					if err != nil {
+						logger.Errorf("Error deleting expired keys for [%s]", s.dbName)
+					}
+				}
+			case <-p.done:
+				logger.Infof("Periodic purge is exiting")
+				return
+			}
+		}
+	}()
+}
+
+// getStores retrieves dbstores contained in the provider
+func (p *CouchDBProvider) getStores() []*dbstore {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	var stores []*dbstore
+	for _, s := range p.stores {
+		stores = append(stores, s)
+	}
+	return stores
+}
+
+func dbName(ns, coll string) string {
+	return fmt.Sprintf("%s$%s", ns, coll)
+}
+
+// getCouchDBConfig return the couchdb config
+// TODO The ledgerconfig can't be passed to offledger provider as cscc calls the createChain which inturn initiates
+// CollectionDataStoreFactory(https://github.com/trustbloc/fabric-mod/blob/f195099d41db44623724131f2f487474707e84f2/core/peer/peer.go#L471).
+// More over this is using state couchdb configurations. Need to have configs specific to feature/functionality(blockstorage/offledger).
+// Created an issue https://github.com/trustbloc/fabric-peer-ext/issues/149. Also, added this as private function to avoid access from external packages.
+func getCouchDBConfig() *couchdb.Config {
+	// set defaults
+	warmAfterNBlocks := 1
+	if viper.IsSet("ledger.state.couchDBConfig.warmIndexesAfterNBlocks") {
+		warmAfterNBlocks = viper.GetInt("ledger.state.couchDBConfig.warmIndexesAfterNBlocks")
+	}
+	internalQueryLimit := 1000
+	if viper.IsSet("ledger.state.couchDBConfig.internalQueryLimit") {
+		internalQueryLimit = viper.GetInt("ledger.state.couchDBConfig.internalQueryLimit")
+	}
+	maxBatchUpdateSize := 500
+	if viper.IsSet("ledger.state.couchDBConfig.maxBatchUpdateSize") {
+		maxBatchUpdateSize = viper.GetInt("ledger.state.couchDBConfig.maxBatchUpdateSize")
+	}
+	rootFSPath := filepath.Join(coreconfig.GetPath("peer.fileSystemPath"), "ledgersData")
+
+	return &couchdb.Config{
+		Address:                 viper.GetString("ledger.state.couchDBConfig.couchDBAddress"),
+		Username:                viper.GetString("ledger.state.couchDBConfig.username"),
+		Password:                viper.GetString("ledger.state.couchDBConfig.password"),
+		MaxRetries:              viper.GetInt("ledger.state.couchDBConfig.maxRetries"),
+		MaxRetriesOnStartup:     viper.GetInt("ledger.state.couchDBConfig.maxRetriesOnStartup"),
+		RequestTimeout:          viper.GetDuration("ledger.state.couchDBConfig.requestTimeout"),
+		InternalQueryLimit:      internalQueryLimit,
+		MaxBatchUpdateSize:      maxBatchUpdateSize,
+		WarmIndexesAfterNBlocks: warmAfterNBlocks,
+		CreateGlobalChangesDB:   viper.GetBool("ledger.state.couchDBConfig.createGlobalChangesDB"),
+		RedoLogPath:             filepath.Join(rootFSPath, "couchdbRedoLogs"),
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatahandler/pvtdatahandler.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatahandler/pvtdatahandler.go
new file mode 100644
index 000000000..859eaf233
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatahandler/pvtdatahandler.go
@@ -0,0 +1,156 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatahandler
+
+import (
+	"context"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+var logger = flogging.MustGetLogger("ext_pvtdatahandler")
+
+// Handler handles the retrieval of kevlar-defined collection types
+type Handler struct {
+	channelID        string
+	collDataProvider storeapi.Provider
+}
+
+// New returns a new Handler
+func New(channelID string, collDataProvider storeapi.Provider) *Handler {
+	return &Handler{
+		channelID:        channelID,
+		collDataProvider: collDataProvider,
+	}
+}
+
+// HandleGetPrivateData if the collection is one of the custom Kevlar collections then the private data is returned
+func (h *Handler) HandleGetPrivateData(txID, ns string, config *common.StaticCollectionConfig, key string) ([]byte, bool, error) {
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		logger.Debugf("Collection [%s:%s] is of type TransientData. Returning transient data for key [%s]", ns, config.Name, key)
+		value, err := h.getTransientData(txID, ns, config.Name, key)
+		if err != nil {
+			return nil, true, err
+		}
+		return value, true, nil
+	case common.CollectionType_COL_DCAS:
+		fallthrough
+	case common.CollectionType_COL_OFFLEDGER:
+		logger.Debugf("Collection [%s:%s] is an off-ledger store. Returning data for key [%s]", ns, config.Name, key)
+		value, err := h.getData(txID, ns, config.Name, key)
+		if err != nil {
+			return nil, true, err
+		}
+		return value, true, nil
+	default:
+		return nil, false, nil
+	}
+}
+
+// HandleGetPrivateDataMultipleKeys if the collection is one of the custom Kevlar collections then the private data is returned
+func (h *Handler) HandleGetPrivateDataMultipleKeys(txID, ns string, config *common.StaticCollectionConfig, keys []string) ([][]byte, bool, error) {
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		logger.Debugf("Collection [%s:%s] is of type TransientData. Returning transient data for keys [%s]", ns, config.Name, keys)
+		values, err := h.getTransientDataMultipleKeys(txID, ns, config.Name, keys)
+		if err != nil {
+			return nil, true, err
+		}
+		return values, true, nil
+	case common.CollectionType_COL_DCAS:
+		fallthrough
+	case common.CollectionType_COL_OFFLEDGER:
+		logger.Debugf("Collection [%s:%s] is of an off-ledger store. Returning data for keys [%s]", ns, config.Name, keys)
+		values, err := h.getDataMultipleKeys(txID, ns, config.Name, keys)
+		if err != nil {
+			return nil, true, err
+		}
+		return values, true, nil
+	default:
+		return nil, false, nil
+	}
+}
+
+func (h *Handler) getTransientData(txID, ns, coll, key string) ([]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetTransientDataPullTimeout())
+	defer cancel()
+
+	v, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetTransientData(ctxt, storeapi.NewKey(txID, ns, coll, key))
+	if err != nil {
+		return nil, err
+	}
+
+	if v == nil {
+		return nil, nil
+	}
+
+	return v.Value, nil
+}
+
+func (h *Handler) getTransientDataMultipleKeys(txID, ns, coll string, keys []string) ([][]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetTransientDataPullTimeout())
+	defer cancel()
+
+	vals, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetTransientDataMultipleKeys(ctxt, storeapi.NewMultiKey(txID, ns, coll, keys...))
+	if err != nil {
+		return nil, err
+	}
+
+	values := make([][]byte, len(vals))
+	for i, v := range vals {
+		if v == nil {
+			values[i] = nil
+		} else {
+			values[i] = v.Value
+		}
+	}
+	return values, nil
+}
+
+func (h *Handler) getData(txID, ns, coll, key string) ([]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetOLCollPullTimeout())
+	defer cancel()
+
+	v, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetData(ctxt, storeapi.NewKey(txID, ns, coll, key))
+	if err != nil {
+		return nil, err
+	}
+
+	if v == nil {
+		return nil, nil
+	}
+
+	return v.Value, nil
+}
+
+func (h *Handler) getDataMultipleKeys(txID, ns, coll string, keys []string) ([][]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetOLCollPullTimeout())
+	defer cancel()
+
+	vals, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetDataMultipleKeys(ctxt, storeapi.NewMultiKey(txID, ns, coll, keys...))
+	if err != nil {
+		return nil, err
+	}
+
+	values := make([][]byte, len(vals))
+	for i, v := range vals {
+		if v == nil {
+			values[i] = nil
+		} else {
+			values[i] = v.Value
+		}
+	}
+	return values, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatastore/pvtdatastore.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatastore/pvtdatastore.go
new file mode 100644
index 000000000..50d69e45d
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/pvtdatastore/pvtdatastore.go
@@ -0,0 +1,190 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastore
+
+import (
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("ext_pvtdatastore")
+
+type transientStore interface {
+	// PersistWithConfig stores the private write set of a transaction along with the collection config
+	// in the transient store based on txid and the block height the private data was received at
+	PersistWithConfig(txid string, blockHeight uint64, privateSimulationResultsWithConfig *transientstore.TxPvtReadWriteSetWithConfigInfo) error
+}
+
+// Store persists private data from collection R/W sets
+type Store struct {
+	channelID      string
+	transientStore transientStore
+	collDataStore  storeapi.Store
+}
+
+// New returns a new Store
+func New(channelID string, transientStore transientStore, collDataStore storeapi.Store) *Store {
+	return &Store{
+		channelID:      channelID,
+		transientStore: transientStore,
+		collDataStore:  collDataStore,
+	}
+}
+
+// StorePvtData used to persist private date into transient and/or extended collection store(s)
+func (c *Store) StorePvtData(txID string, privData *transientstore.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error {
+	// Filter out all of the R/W sets that are not to be persisted to the ledger, i.e. we just
+	// want the regular private data R/W sets
+	pvtRWSet, err := newPvtRWSetFilter(c.channelID, txID, blkHeight, privData).apply()
+	if err != nil {
+		logger.Errorf("[%s:%d:%s] Unable to extract transient r/w set from private data: %s", c.channelID, blkHeight, txID, err)
+		return err
+	}
+
+	// Persist the extended collection data (this includes Transient Data, etc.)
+	err = c.collDataStore.Persist(txID, privData)
+	if err != nil {
+		logger.Errorf("[%s:%d:%s] Unable to persist collection data: %s", c.channelID, blkHeight, txID, err)
+		return err
+	}
+
+	if pvtRWSet == nil {
+		logger.Debugf("[%s:%d:%s] Nothing to persist to transient store", c.channelID, blkHeight, txID)
+		return nil
+	}
+
+	logger.Debugf("[%s:%d:%s] Persisting private data to transient store", c.channelID, blkHeight, txID)
+
+	return c.transientStore.PersistWithConfig(
+		txID, blkHeight,
+		&transientstore.TxPvtReadWriteSetWithConfigInfo{
+			PvtRwset:          pvtRWSet,
+			CollectionConfigs: privData.CollectionConfigs,
+			EndorsedAt:        privData.EndorsedAt,
+		})
+}
+
+// pvtRWSetFilter filters out all of the R/W sets that are not to be persisted to the ledger,
+// i.e. we just want the regular private data R/W sets
+type pvtRWSetFilter struct {
+	channelID string
+	txID      string
+	blkHeight uint64
+	privData  *transientstore.TxPvtReadWriteSetWithConfigInfo
+}
+
+func newPvtRWSetFilter(channelID, txID string, blkHeight uint64, privData *transientstore.TxPvtReadWriteSetWithConfigInfo) *pvtRWSetFilter {
+	return &pvtRWSetFilter{
+		channelID: channelID,
+		txID:      txID,
+		blkHeight: blkHeight,
+		privData:  privData,
+	}
+}
+
+func (f *pvtRWSetFilter) apply() (*rwset.TxPvtReadWriteSet, error) {
+	txPvtRWSet, err := rwsetutil.TxPvtRwSetFromProtoMsg(f.privData.PvtRwset)
+	if err != nil {
+		return nil, errors.New("error getting pvt RW set from bytes")
+	}
+
+	nsPvtRwSet, modified, err := f.extractPvtRWSets(txPvtRWSet.NsPvtRwSet)
+	if err != nil {
+		return nil, err
+	}
+	if !modified {
+		logger.Debugf("[%s:%d:%s] Rewrite to NsPvtRwSet not required for transient store", f.channelID, f.blkHeight, f.txID)
+		return f.privData.PvtRwset, nil
+	}
+	if len(nsPvtRwSet) == 0 {
+		logger.Debugf("[%s:%d:%s] Didn't find any private data to persist to transient store", f.channelID, f.blkHeight, f.txID)
+		return nil, nil
+	}
+
+	logger.Debugf("[%s:%d:%s] Rewriting NsPvtRwSet for transient store", f.channelID, f.blkHeight, f.txID)
+	txPvtRWSet.NsPvtRwSet = nsPvtRwSet
+	newPvtRwset, err := txPvtRWSet.ToProtoMsg()
+	if err != nil {
+		return nil, errors.WithMessage(err, "error marshalling private data r/w set")
+	}
+
+	logger.Debugf("[%s:%d:%s] Rewriting private data r/w set since it was modified", f.channelID, f.blkHeight, f.txID)
+	return newPvtRwset, nil
+}
+
+func (f *pvtRWSetFilter) extractPvtRWSets(srcNsPvtRwSets []*rwsetutil.NsPvtRwSet) ([]*rwsetutil.NsPvtRwSet, bool, error) {
+	modified := false
+	var nsPvtRWSets []*rwsetutil.NsPvtRwSet
+	for _, nsRWSet := range srcNsPvtRwSets {
+		collPvtRwSets, err := f.extractPvtCollPvtRWSets(nsRWSet)
+		if err != nil {
+			return nil, false, err
+		}
+		if len(collPvtRwSets) != len(nsRWSet.CollPvtRwSets) {
+			modified = true
+		}
+		if len(collPvtRwSets) > 0 {
+			if len(collPvtRwSets) != len(nsRWSet.CollPvtRwSets) {
+				logger.Debugf("[%s:%d:%s] Rewriting collections for [%s] in transient store", f.channelID, f.blkHeight, f.txID, nsRWSet.NameSpace)
+				nsRWSet.CollPvtRwSets = collPvtRwSets
+				modified = true
+			} else {
+				logger.Debugf("[%s:%d:%s] Not touching collections for [%s] in transient store", f.channelID, f.blkHeight, f.txID, nsRWSet.NameSpace)
+			}
+			logger.Debugf("[%s:%d:%s] Adding NsPvtRwSet for [%s] in transient store", f.channelID, f.blkHeight, f.txID, nsRWSet.NameSpace)
+			nsPvtRWSets = append(nsPvtRWSets, nsRWSet)
+		} else {
+			logger.Debugf("[%s:%d:%s] NOT adding NsPvtRwSet for [%s] in transient store since no private data collections found", f.channelID, f.blkHeight, f.txID, nsRWSet.NameSpace)
+		}
+	}
+	return nsPvtRWSets, modified, nil
+}
+
+func (f *pvtRWSetFilter) extractPvtCollPvtRWSets(nsPvtRWSet *rwsetutil.NsPvtRwSet) ([]*rwsetutil.CollPvtRwSet, error) {
+	var filteredCollPvtRwSets []*rwsetutil.CollPvtRwSet
+	for _, collRWSet := range nsPvtRWSet.CollPvtRwSets {
+		ok, e := f.isPvtData(nsPvtRWSet.NameSpace, collRWSet.CollectionName)
+		if e != nil {
+			return nil, errors.WithMessage(e, "error in collection config")
+		}
+		if !ok {
+			logger.Debugf("[%s:%d:%s] Not persisting collection [%s:%s] in transient store", f.channelID, f.blkHeight, f.txID, nsPvtRWSet.NameSpace, collRWSet.CollectionName)
+			continue
+		}
+		logger.Debugf("[%s:%d:%s] Persisting collection [%s:%s] in transient store", f.channelID, f.blkHeight, f.txID, nsPvtRWSet.NameSpace, collRWSet.CollectionName)
+		filteredCollPvtRwSets = append(filteredCollPvtRwSets, collRWSet)
+	}
+	return filteredCollPvtRwSets, nil
+}
+
+// isPvtData returns true if the given collection is a standard private data collection
+func (f *pvtRWSetFilter) isPvtData(ns, coll string) (bool, error) {
+	pkg, ok := f.privData.CollectionConfigs[ns]
+	if !ok {
+		return false, errors.Errorf("could not find collection configs for namespace [%s]", ns)
+	}
+
+	var config *common.StaticCollectionConfig
+	for _, c := range pkg.Config {
+		staticConfig := c.GetStaticCollectionConfig()
+		if staticConfig.Name == coll {
+			config = staticConfig
+			break
+		}
+	}
+
+	if config == nil {
+		return false, errors.Errorf("could not find collection config for collection [%s:%s]", ns, coll)
+	}
+
+	return config.Type == common.CollectionType_COL_UNKNOWN || config.Type == common.CollectionType_COL_PRIVATE, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/retriever.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/retriever.go
new file mode 100644
index 000000000..3272908c1
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/retriever.go
@@ -0,0 +1,124 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	"context"
+	"sync"
+
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	cb "github.com/hyperledger/fabric/protos/common"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas"
+	olretriever "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/retriever"
+	tdataapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+	tretriever "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/retriever"
+	supp "github.com/trustbloc/fabric-peer-ext/pkg/common/support"
+)
+
+// Provider is a transient data provider.
+type Provider struct {
+	transientDataProvider tdataapi.Provider
+	offLedgerProvider     olapi.Provider
+	retrievers            map[string]*retriever
+	mutex                 sync.RWMutex
+}
+
+// NewProvider returns a new transient data Retriever provider
+func NewProvider(
+	storeProvider func(channelID string) storeapi.Store,
+	ledgerProvider func(channelID string) ledger.PeerLedger,
+	gossipProvider func() supportapi.GossipAdapter,
+	blockPublisherProvider func(channelID string) gossipapi.BlockPublisher) storeapi.Provider {
+
+	support := supp.New(ledgerProvider, blockPublisherProvider)
+
+	tdataStoreProvider := func(channelID string) tdataapi.Store { return storeProvider(channelID) }
+	offLedgerStoreProvider := func(channelID string) olapi.Store { return storeProvider(channelID) }
+
+	return &Provider{
+		transientDataProvider: getTransientDataProvider(tdataStoreProvider, support, gossipProvider),
+		offLedgerProvider:     getOffLedgerProvider(offLedgerStoreProvider, support, gossipProvider),
+		retrievers:            make(map[string]*retriever),
+	}
+}
+
+// RetrieverForChannel returns the collection retriever for the given channel
+func (p *Provider) RetrieverForChannel(channelID string) storeapi.Retriever {
+	p.mutex.RLock()
+	r, ok := p.retrievers[channelID]
+	p.mutex.RUnlock()
+
+	if ok {
+		return r
+	}
+
+	return p.getOrCreateRetriever(channelID)
+}
+
+func (p *Provider) getOrCreateRetriever(channelID string) storeapi.Retriever {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	r, ok := p.retrievers[channelID]
+	if !ok {
+		r = &retriever{
+			transientDataRetriever: p.transientDataProvider.RetrieverForChannel(channelID),
+			offLedgerRetriever:     p.offLedgerProvider.RetrieverForChannel(channelID),
+		}
+		p.retrievers[channelID] = r
+	}
+
+	return r
+}
+
+type retriever struct {
+	transientDataRetriever tdataapi.Retriever
+	offLedgerRetriever     olapi.Retriever
+}
+
+// GetTransientData returns the transient data for the given key
+func (r *retriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return r.transientDataRetriever.GetTransientData(ctxt, key)
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (r *retriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return r.transientDataRetriever.GetTransientDataMultipleKeys(ctxt, key)
+}
+
+// GetData gets the value for the given data item
+func (r *retriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return r.offLedgerRetriever.GetData(ctxt, key)
+}
+
+// GetDataMultipleKeys gets the values for the multiple data items in a single call
+func (r *retriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return r.offLedgerRetriever.GetDataMultipleKeys(ctxt, key)
+}
+
+// Support defines the supporting functions required by the transient data provider
+type Support interface {
+	Config(channelID, ns, coll string) (*cb.StaticCollectionConfig, error)
+	Policy(channel, ns, collection string) (privdata.CollectionAccessPolicy, error)
+	BlockPublisher(channelID string) gossipapi.BlockPublisher
+}
+
+var getTransientDataProvider = func(storeProvider func(channelID string) tdataapi.Store, support Support, gossipProvider func() supportapi.GossipAdapter) tdataapi.Provider {
+	return tretriever.NewProvider(storeProvider, support, gossipProvider)
+}
+
+var getOffLedgerProvider = func(storeProvider func(channelID string) olapi.Store, support Support, gossipProvider func() supportapi.GossipAdapter) olapi.Provider {
+	return olretriever.NewProvider(storeProvider, support, gossipProvider,
+		olretriever.WithValidator(cb.CollectionType_COL_DCAS, dcas.Validator),
+		olretriever.WithKeyDecorator(cb.CollectionType_COL_DCAS, dcas.KeyDecorator),
+	)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/test_exports.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/test_exports.go
new file mode 100644
index 000000000..24cb1b363
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/retriever/test_exports.go
@@ -0,0 +1,25 @@
+// +build testing
+
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+	tdataapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+)
+
+// SetTransientDataProvider sets the transient data Retriever provider for unit tests
+func SetTransientDataProvider(provider func(storeProvider func(channelID string) tdataapi.Store, support Support, gossipProvider func() supportapi.GossipAdapter) tdataapi.Provider) {
+	getTransientDataProvider = provider
+}
+
+// SetOffLedgerProvider sets the off-ledger Retriever provider for unit tests
+func SetOffLedgerProvider(provider func(storeProvider func(channelID string) olapi.Store, support Support, gossipProvider func() supportapi.GossipAdapter) olapi.Provider) {
+	getOffLedgerProvider = provider
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/store.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/store.go
new file mode 100644
index 000000000..36854104a
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/store.go
@@ -0,0 +1,86 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	cb "github.com/hyperledger/fabric/protos/common"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+	tdapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+type targetStores struct {
+	transientDataStore tdapi.Store
+	offLedgerStore     olapi.Store
+}
+
+type store struct {
+	channelID string
+	targetStores
+}
+
+func newDelegatingStore(channelID string, targets targetStores) *store {
+	return &store{
+		channelID:    channelID,
+		targetStores: targets,
+	}
+}
+
+// Persist persists all transient data within the private data simulation results
+func (d *store) Persist(txID string, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	if err := d.transientDataStore.Persist(txID, privateSimulationResultsWithConfig); err != nil {
+		return errors.WithMessage(err, "error persisting transient data")
+	}
+
+	// Off-ledger data should only be persisted on committers
+	if isCommitter() {
+		if err := d.offLedgerStore.Persist(txID, privateSimulationResultsWithConfig); err != nil {
+			return errors.WithMessage(err, "error persisting off-ledger data")
+		}
+	}
+
+	return nil
+}
+
+// GetTransientData returns the transient data for the given key
+func (d *store) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return d.transientDataStore.GetTransientData(key)
+}
+
+// GetTransientData returns the transient data for the given keys
+func (d *store) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return d.transientDataStore.GetTransientDataMultipleKeys(key)
+}
+
+// GetData gets the value for the given key
+func (d *store) GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return d.offLedgerStore.GetData(key)
+}
+
+// PutData stores the key/value.
+func (d *store) PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error {
+	return d.offLedgerStore.PutData(config, key, value)
+}
+
+// GetDataMultipleKeys gets the values for multiple keys in a single call
+func (d *store) GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return d.offLedgerStore.GetDataMultipleKeys(key)
+}
+
+// Close closes all of the stores store
+func (d *store) Close() {
+	d.transientDataStore.Close()
+	d.offLedgerStore.Close()
+}
+
+// isCommitter may be overridden in unit tests
+var isCommitter = func() bool {
+	return roles.IsCommitter()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/storeprovider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/storeprovider.go
new file mode 100644
index 000000000..cbfd826d9
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/storeprovider.go
@@ -0,0 +1,94 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"sync"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	cb "github.com/hyperledger/fabric/protos/common"
+	olapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/dcas"
+	olstoreprovider "github.com/trustbloc/fabric-peer-ext/pkg/collections/offledger/storeprovider"
+	tdapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider"
+)
+
+// New returns a new store provider factory
+func New() *StoreProvider {
+	return &StoreProvider{
+		transientDataProvider: newTransientDataProvider(),
+		olProvider:            newOffLedgerProvider(),
+		stores:                make(map[string]*store),
+	}
+}
+
+// StoreProvider is a store provider that creates delegating stores.
+// A delegating store delegates requests to collection-specific store.
+// For example, transient data store, Off-ledger store, etc.
+type StoreProvider struct {
+	transientDataProvider tdapi.StoreProvider
+	olProvider            olapi.StoreProvider
+	stores                map[string]*store
+	sync.RWMutex
+}
+
+// StoreForChannel returns the store for the given channel
+func (sp *StoreProvider) StoreForChannel(channelID string) storeapi.Store {
+	sp.RLock()
+	defer sp.RUnlock()
+	return sp.stores[channelID]
+}
+
+// OpenStore opens the store for the given channel
+func (sp *StoreProvider) OpenStore(channelID string) (storeapi.Store, error) {
+	sp.Lock()
+	defer sp.Unlock()
+
+	store, ok := sp.stores[channelID]
+	if !ok {
+		tdataStore, err := sp.transientDataProvider.OpenStore(channelID)
+		if err != nil {
+			return nil, err
+		}
+		olStore, err := sp.olProvider.OpenStore(channelID)
+		if err != nil {
+			return nil, err
+		}
+		store = newDelegatingStore(channelID,
+			targetStores{
+				transientDataStore: tdataStore,
+				offLedgerStore:     olStore,
+			},
+		)
+		sp.stores[channelID] = store
+	}
+	return store, nil
+}
+
+// Close shuts down all of the stores
+func (sp *StoreProvider) Close() {
+	for _, s := range sp.stores {
+		s.Close()
+	}
+}
+
+// newTransientDataProvider may be overridden in unit tests
+var newTransientDataProvider = func() tdapi.StoreProvider {
+	return storeprovider.New()
+}
+
+// newOffLedgerProvider may be overridden in unit tests
+var newOffLedgerProvider = func() olapi.StoreProvider {
+	return olstoreprovider.New(
+		olstoreprovider.WithCollectionType(
+			cb.CollectionType_COL_DCAS,
+			olstoreprovider.WithDecorator(dcas.Decorator),
+			olstoreprovider.WithKeyDecorator(dcas.KeyDecorator),
+		),
+	)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/test_exports.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/test_exports.go
new file mode 100644
index 000000000..c026262a7
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/storeprovider/test_exports.go
@@ -0,0 +1,18 @@
+// +build testing
+
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	tdapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+)
+
+// SetNewTransientDataProvider sets the transient data provider for unit tests
+func SetNewTransientDataProvider(provider func() tdapi.StoreProvider) {
+	newTransientDataProvider = provider
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api/transientdata.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api/transientdata.go
new file mode 100644
index 000000000..3b5082699
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api/transientdata.go
@@ -0,0 +1,52 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// Store manages the storage of transient data.
+type Store interface {
+	// Persist stores the private write set of a transaction.
+	Persist(txID string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error
+
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+
+	// Close closes the store
+	Close()
+}
+
+// StoreProvider is an interface to open/close a provider
+type StoreProvider interface {
+	// OpenStore creates a handle to the transient data store for the given ledger ID
+	OpenStore(ledgerid string) (Store, error)
+
+	// Close cleans up the provider
+	Close()
+}
+
+// Retriever retrieves transient data
+type Retriever interface {
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+}
+
+// Provider provides transient data retrievers
+type Provider interface {
+	RetrieverForChannel(channel string) Retriever
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminationplan.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminationplan.go
new file mode 100644
index 000000000..2f11ccc1c
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminationplan.go
@@ -0,0 +1,92 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	protobuf "github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/common"
+	gdiscovery "github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/gossip"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	"github.com/pkg/errors"
+	"github.com/spf13/viper"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/discovery"
+)
+
+type gossipAdapter interface {
+	PeersOfChannel(common.ChainID) []gdiscovery.NetworkMember
+	SelfMembershipInfo() gdiscovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+// ComputeDisseminationPlan returns the dissemination plan for transient data
+func ComputeDisseminationPlan(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *protoext.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	logger.Debugf("Computing transient data dissemination plan for [%s:%s]", ns, rwSet.CollectionName)
+
+	disseminator := New(channelID, ns, rwSet.CollectionName, colAP, gossipAdapter)
+
+	kvRwSet := &kvrwset.KVRWSet{}
+	if err := protobuf.Unmarshal(rwSet.Rwset, kvRwSet); err != nil {
+		return nil, true, errors.WithMessage(err, "error unmarshalling KV read/write set for transient data")
+	}
+
+	var endorsers discovery.PeerGroup
+	for _, kvWrite := range kvRwSet.Writes {
+		if kvWrite.IsDelete {
+			continue
+		}
+		endorsersForKey, err := disseminator.ResolveEndorsers(kvWrite.Key)
+		if err != nil {
+			return nil, true, errors.WithMessage(err, "error resolving endorsers for transient data")
+		}
+
+		logger.Debugf("Endorsers for key [%s:%s:%s]: %s", ns, rwSet.CollectionName, kvWrite.Key, endorsersForKey)
+
+		for _, endorser := range endorsersForKey {
+			if endorser.Local {
+				logger.Debugf("Not adding local endorser for key [%s:%s:%s]", ns, rwSet.CollectionName, kvWrite.Key)
+				continue
+			}
+			endorsers = discovery.Merge(endorsers, endorser)
+		}
+	}
+
+	logger.Debugf("Endorsers for collection [%s:%s]: %s", ns, rwSet.CollectionName, endorsers)
+
+	routingFilter := func(member gdiscovery.NetworkMember) bool {
+		if endorsers.ContainsPeer(member.Endpoint) {
+			logger.Debugf("Peer [%s] is an endorser for [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+			return true
+		}
+
+		logger.Debugf("Peer [%s] is NOT an endorser for [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+		return false
+	}
+
+	sc := gossip.SendCriteria{
+		Timeout:    viper.GetDuration("peer.gossip.pvtData.pushAckTimeout"),
+		Channel:    common.ChainID(channelID),
+		MaxPeers:   colAP.MaximumPeerCount(),
+		MinAck:     colAP.RequiredPeerCount(),
+		IsEligible: routingFilter,
+	}
+
+	return []*dissemination.Plan{{
+		Criteria: sc,
+		Msg:      pvtDataMsg,
+	}}, true, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminator.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminator.go
new file mode 100644
index 000000000..93f4767ab
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination/disseminator.go
@@ -0,0 +1,135 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"hash/fnv"
+	"sort"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/discovery"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+var logger = flogging.MustGetLogger("transientdata")
+
+// Disseminator disseminates transient data to a deterministic set of endorsers
+type Disseminator struct {
+	*discovery.Discovery
+	namespace  string
+	collection string
+	policy     privdata.CollectionAccessPolicy
+}
+
+// New returns a new transient data disseminator
+func New(channelID, namespace, collection string, policy privdata.CollectionAccessPolicy, gossip gossipAdapter) *Disseminator {
+	return &Disseminator{
+		Discovery:  discovery.New(channelID, gossip),
+		namespace:  namespace,
+		collection: collection,
+		policy:     policy,
+	}
+}
+
+// ResolveEndorsers resolves to a set of endorsers to which transient data should be disseminated
+func (d *Disseminator) ResolveEndorsers(key string) (discovery.PeerGroup, error) {
+	h, err := getHash32(key)
+	if err != nil {
+		return nil, errors.WithMessage(err, "error computing int32 hash of key")
+	}
+
+	orgs := d.chooseOrgs(h)
+
+	logger.Debugf("[%s] Chosen orgs: %s", d.ChannelID(), orgs)
+
+	endorsers := d.chooseEndorsers(h, orgs)
+
+	logger.Debugf("[%s] Chosen endorsers from orgs %s: %s", d.ChannelID(), orgs, endorsers)
+	return endorsers, nil
+}
+
+func (d *Disseminator) chooseEndorsers(h uint32, orgs []string) discovery.PeerGroup {
+	var endorsers discovery.PeerGroup
+
+	for i := 0; i < d.policy.MaximumPeerCount(); i++ {
+		for _, org := range orgs {
+			if len(endorsers) == d.policy.MaximumPeerCount() {
+				// We have enough endorsers
+				break
+			}
+
+			// Get a sorted list of endorsers for the org
+			endorsersForOrg := d.getEndorsers(org).Sort()
+			if len(endorsersForOrg) == 0 {
+				logger.Debugf("[%s] There are no endorsers in org [%s]", d.ChannelID(), org)
+				continue
+			}
+
+			logger.Debugf("[%s] Endorsers for [%s]: %s", d.ChannelID(), org, endorsersForOrg)
+
+			// Deterministically choose an endorser
+			endorserForOrg := endorsersForOrg[(int(h)+i)%len(endorsersForOrg)]
+			if endorsers.Contains(endorserForOrg) {
+				logger.Debugf("[%s] Will not add endorser [%s] from org [%s] since it is already added", d.ChannelID(), endorserForOrg, org)
+				continue
+			}
+
+			endorsers = append(endorsers, endorserForOrg)
+		}
+	}
+	return endorsers
+}
+
+func (d *Disseminator) getEndorsers(mspID string) discovery.PeerGroup {
+	return d.GetMembers(func(m *discovery.Member) bool {
+		if m.MSPID != mspID {
+			logger.Debugf("[%s] Not adding peer [%s] as an endorser since it is not in org [%s]", d.ChannelID(), m.Endpoint, mspID)
+			return false
+		}
+		if !m.HasRole(roles.EndorserRole) {
+			logger.Debugf("[%s] Not adding peer [%s] as an endorser since it does not have the endorser role", d.ChannelID(), m.Endpoint)
+			return false
+		}
+		return true
+	})
+
+}
+
+func (d *Disseminator) chooseOrgs(h uint32) []string {
+	memberOrgs := d.policy.MemberOrgs()
+	numOrgs := min(d.policy.MaximumPeerCount(), len(memberOrgs))
+
+	// Copy and sort the orgs
+	var sortedOrgs []string
+	sortedOrgs = append(sortedOrgs, memberOrgs...)
+	sort.Strings(sortedOrgs)
+
+	var chosenOrgs []string
+	for i := 0; i < numOrgs; i++ {
+		chosenOrgs = append(chosenOrgs, sortedOrgs[(int(h)+i)%len(sortedOrgs)])
+	}
+
+	return chosenOrgs
+}
+
+func getHash32(key string) (uint32, error) {
+	h := fnv.New32a()
+	_, err := h.Write([]byte(key))
+	if err != nil {
+		return 0, err
+	}
+	return h.Sum32(), nil
+}
+
+func min(i, j int) int {
+	if i < j {
+		return i
+	}
+	return j
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/mocks/mockprovider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/mocks/mockprovider.go
new file mode 100644
index 000000000..5358ecca3
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/mocks/mockprovider.go
@@ -0,0 +1,40 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	tdataapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+)
+
+// TransientDataProvider is a mock transient data provider
+type TransientDataProvider struct {
+}
+
+// RetrieverForChannel returns a provider for the given channel
+func (p *TransientDataProvider) RetrieverForChannel(channel string) tdataapi.Retriever {
+	return &transientDataRetriever{}
+}
+
+type transientDataRetriever struct {
+}
+
+// GetTransientData returns the transientData
+func (m *transientDataRetriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return &storeapi.ExpiringValue{Value: []byte(key.Key)}, nil
+}
+
+// GetTransientDataMultipleKeys returns the data with multiple keys
+func (m *transientDataRetriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = &storeapi.ExpiringValue{Value: []byte(k)}
+	}
+	return values, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/policy/validator.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/policy/validator.go
new file mode 100644
index 000000000..fc5050c87
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/policy/validator.go
@@ -0,0 +1,39 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package policy
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+// ValidateConfig validates the Transient Data Collection configuration
+func ValidateConfig(config *common.StaticCollectionConfig) error {
+	if config.RequiredPeerCount <= 0 {
+		return errors.Errorf("collection-name: %s -- required peer count must be greater than 0", config.Name)
+	}
+
+	if config.RequiredPeerCount > config.MaximumPeerCount {
+		return errors.Errorf("collection-name: %s -- maximum peer count (%d) must be greater than or equal to required peer count (%d)", config.Name, config.MaximumPeerCount, config.RequiredPeerCount)
+	}
+	if config.TimeToLive == "" {
+		return errors.Errorf("collection-name: %s -- time to live must be specified", config.Name)
+	}
+
+	if config.BlockToLive != 0 {
+		return errors.Errorf("collection-name: %s -- block-to-live not supported", config.Name)
+	}
+
+	_, err := time.ParseDuration(config.TimeToLive)
+	if err != nil {
+		return errors.Errorf("collection-name: %s -- invalid time format for time to live: %s", config.Name, err)
+	}
+
+	return nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/retriever/transientdataretriever.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/retriever/transientdataretriever.go
new file mode 100644
index 000000000..b2ca340cf
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/retriever/transientdataretriever.go
@@ -0,0 +1,384 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	"context"
+	"sync"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/pkg/errors"
+	tdataapi "github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/dissemination"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/discovery"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/multirequest"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/requestmgr"
+)
+
+var logger = flogging.MustGetLogger("transientdata")
+
+type support interface {
+	Policy(channel, ns, collection string) (privdata.CollectionAccessPolicy, error)
+	BlockPublisher(channelID string) gossipapi.BlockPublisher
+}
+
+// Provider is a transient data provider.
+type Provider struct {
+	support
+	storeForChannel func(channelID string) tdataapi.Store
+	gossipAdapter   func() supportapi.GossipAdapter
+}
+
+// NewProvider returns a new transient data provider
+func NewProvider(storeProvider func(channelID string) tdataapi.Store, support support, gossipProvider func() supportapi.GossipAdapter) tdataapi.Provider {
+	return &Provider{
+		support:         support,
+		storeForChannel: storeProvider,
+		gossipAdapter:   gossipProvider,
+	}
+}
+
+// RetrieverForChannel returns the transient data dataRetriever for the given channel
+func (p *Provider) RetrieverForChannel(channelID string) tdataapi.Retriever {
+	r := &retriever{
+		support:       p.support,
+		gossipAdapter: p.gossipAdapter(),
+		store:         p.storeForChannel(channelID),
+		channelID:     channelID,
+		reqMgr:        requestmgr.Get(channelID),
+		resolvers:     make(map[collKey]resolver),
+	}
+
+	// Add a handler so that we can remove the resolver for a chaincode that has been upgraded
+	p.support.BlockPublisher(channelID).AddCCUpgradeHandler(func(blockNum uint64, txID string, chaincodeID string) error {
+		logger.Infof("[%s] Chaincode [%s] has been upgraded. Clearing resolver cache for chaincode.", channelID, chaincodeID)
+		r.removeResolvers(chaincodeID)
+		return nil
+	})
+
+	return r
+}
+
+// ResolveEndorsers resolves to a set of endorsers to which transient data should be disseminated
+type resolver interface {
+	ResolveEndorsers(key string) (discovery.PeerGroup, error)
+}
+
+type collKey struct {
+	ns   string
+	coll string
+}
+
+func newCollKey(ns, coll string) collKey {
+	return collKey{ns: ns, coll: coll}
+}
+
+type retriever struct {
+	support
+	channelID     string
+	gossipAdapter supportapi.GossipAdapter
+	store         tdataapi.Store
+	resolvers     map[collKey]resolver
+	lock          sync.RWMutex
+	reqMgr        requestmgr.RequestMgr
+}
+
+func (r *retriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	authorized, err := r.isAuthorized(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, err
+	}
+	if !authorized {
+		logger.Infof("[%s] This peer does not have access to the collection [%s:%s]", r.channelID, key.Namespace, key.Collection)
+		return nil, nil
+	}
+
+	endorsers, err := r.resolveEndorsers(key)
+	if err != nil {
+		return nil, errors.WithMessagef(err, "unable to get resolve endorsers for channel [%s] and [%s]", r.channelID, key)
+	}
+
+	logger.Debugf("[%s] Endorsers for [%s]: %s", r.channelID, key, endorsers)
+
+	if endorsers.ContainsLocal() {
+		value, ok, err := r.getTransientDataFromLocal(key)
+		if err != nil {
+			return nil, err
+		}
+		if ok {
+			return value, nil
+		}
+	}
+
+	return r.getTransientDataFromRemote(ctxt, key, endorsers.Remote())
+}
+
+type valueResp struct {
+	value *storeapi.ExpiringValue
+	err   error
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (r *retriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	if len(key.Keys) == 0 {
+		return nil, errors.New("at least one key must be specified")
+	}
+
+	var wg sync.WaitGroup
+	wg.Add(len(key.Keys))
+	var mutex sync.Mutex
+
+	// TODO: This can be optimized by sending one request to endorsers that have multiple keys, as opposed to one request per key.
+	responses := make(map[string]*valueResp)
+	for _, k := range key.Keys {
+		go func(key *storeapi.Key) {
+			cctxt, cancel := context.WithCancel(ctxt)
+			defer cancel()
+
+			value, err := r.GetTransientData(cctxt, key)
+			mutex.Lock()
+			responses[key.Key] = &valueResp{value: value, err: err}
+			logger.Debugf("Got response for [%s]: %s, Err: %s", key.Key, value, err)
+			mutex.Unlock()
+			wg.Done()
+		}(storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, k))
+	}
+
+	wg.Wait()
+
+	// Return the responses in the order of the requested keys
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		r, ok := responses[k]
+		if !ok {
+			return nil, errors.Errorf("no response for key [%s:%s:%s]", key.Namespace, key.Collection, k)
+		}
+		if r.err != nil {
+			return nil, r.err
+		}
+		values[i] = r.value
+	}
+
+	return values, nil
+}
+
+// resolveEndorsers returns the endorsers that (should) have the transient data
+func (r *retriever) resolveEndorsers(key *storeapi.Key) (discovery.PeerGroup, error) {
+	res, err := r.getResolver(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, errors.WithMessagef(err, "unable to get resolver for channel [%s] and [%s:%s]", r.channelID, key.Namespace, key.Collection)
+	}
+	return res.ResolveEndorsers(key.Key)
+}
+
+func (r *retriever) getTransientDataFromLocal(key *storeapi.Key) (*storeapi.ExpiringValue, bool, error) {
+	value, retrieveErr := r.store.GetTransientData(key)
+	if retrieveErr != nil {
+		logger.Debugf("[%s] Error getting transient data from local store for [%s]: %s", r.channelID, key, retrieveErr)
+		return nil, false, errors.WithMessagef(retrieveErr, "unable to get transient data for channel [%s] and [%s]", r.channelID, key)
+	}
+
+	if value != nil && len(value.Value) > 0 {
+		logger.Debugf("[%s] Got transient data from local store for [%s]", r.channelID, key)
+		return value, true, nil
+	}
+
+	logger.Debugf("[%s] nil transient data in local store for [%s]. Will try to pull from remote peer(s).", r.channelID, key)
+	return nil, false, nil
+}
+
+func (r *retriever) getTransientDataFromRemote(ctxt context.Context, key *storeapi.Key, endorsers discovery.PeerGroup) (*storeapi.ExpiringValue, error) {
+	cReq := multirequest.New()
+	for _, endorser := range endorsers {
+		logger.Debugf("Adding request to get transient data for [%s] from [%s] ...", key, endorser)
+		cReq.Add(endorser.String(), r.getTransientDataRequest(key, endorser))
+	}
+
+	response := cReq.Execute(ctxt)
+
+	if response.Values.IsEmpty() {
+		logger.Debugf("Got empty transient data response for [%s] ...", key)
+		return nil, nil
+	}
+
+	logger.Debugf("Got non-nil transient data response for [%s] ...", key)
+	return response.Values[0].(*storeapi.ExpiringValue), nil
+}
+
+func (r *retriever) getTransientDataRequest(key *storeapi.Key, endorser *discovery.Member) func(ctxt context.Context) (common.Values, error) {
+	return func(ctxt context.Context) (common.Values, error) {
+		return r.getTransientDataFromEndorser(ctxt, key, endorser)
+	}
+}
+
+func (r *retriever) getTransientDataFromEndorser(ctxt context.Context, key *storeapi.Key, endorser *discovery.Member) (common.Values, error) {
+	logger.Debugf("Getting transient data for [%s] from [%s] ...", key, endorser)
+
+	value, err := r.getTransientData(ctxt, key, endorser)
+	if err != nil {
+		if err == context.Canceled {
+			logger.Debugf("[%s] Request to get transient data from [%s] for [%s] was cancelled", r.channelID, endorser, key)
+		} else {
+			logger.Debugf("[%s] Error getting transient data from [%s] for [%s]: %s", r.channelID, endorser, key, err)
+		}
+		return common.Values{nil}, err
+	}
+
+	if value == nil {
+		logger.Debugf("[%s] Transient data not found on [%s] for [%s]", r.channelID, endorser, key)
+	} else {
+		logger.Debugf("[%s] Got transient data from [%s] for [%s]", r.channelID, endorser, key)
+	}
+
+	return common.Values{value}, nil
+}
+
+func (r *retriever) getResolver(ns, coll string) (resolver, error) {
+	key := newCollKey(ns, coll)
+
+	r.lock.RLock()
+	resolver, ok := r.resolvers[key]
+	r.lock.RUnlock()
+
+	if ok {
+		return resolver, nil
+	}
+
+	return r.getOrCreateResolver(key)
+}
+
+func (r *retriever) getOrCreateResolver(key collKey) (resolver, error) {
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	resolver, ok := r.resolvers[key]
+	if ok {
+		return resolver, nil
+	}
+
+	policy, err := r.Policy(r.channelID, key.ns, key.coll)
+	if err != nil {
+		return nil, err
+	}
+
+	resolver = dissemination.New(r.channelID, key.ns, key.coll, policy, r.gossipAdapter)
+
+	r.resolvers[key] = resolver
+
+	return resolver, nil
+}
+
+func (r *retriever) removeResolvers(ns string) {
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	for key := range r.resolvers {
+		if key.ns == ns {
+			logger.Debugf("[%s] Removing resolver [%s:%s] from cache", r.channelID, key.ns, key.coll)
+			delete(r.resolvers, key)
+		}
+	}
+}
+
+func (r *retriever) getTransientData(ctxt context.Context, key *storeapi.Key, endorsers ...*discovery.Member) (*storeapi.ExpiringValue, error) {
+	logger.Debugf("[%s] Sending Gossip request to %s for transient data for [%s]", r.channelID, endorsers, key)
+
+	req := r.reqMgr.NewRequest()
+
+	logger.Debugf("[%s] Creating Gossip request %d for transient data for [%s]", r.channelID, req.ID(), key)
+	msg := r.createCollDataRequestMsg(req, key)
+
+	logger.Debugf("[%s] Sending Gossip request %d for transient data for [%s]", r.channelID, req.ID(), key)
+	r.gossipAdapter.Send(msg, asRemotePeers(endorsers)...)
+
+	logger.Debugf("[%s] Waiting for response for %d for transient data for [%s]", r.channelID, req.ID(), key)
+	res, err := req.GetResponse(ctxt)
+	if err != nil {
+		return nil, err
+	}
+
+	logger.Debugf("[%s] Got response for %d for transient data for [%s]", r.channelID, req.ID(), key)
+	return r.findValue(res.Data, key)
+}
+
+func (r *retriever) findValue(data requestmgr.Elements, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	for _, e := range data {
+		if e.Namespace == key.Namespace && e.Collection == key.Collection && e.Key == key.Key {
+			logger.Debugf("[%s] Got response for transient data for [%s]", r.channelID, key)
+			if e.Value == nil {
+				return nil, nil
+			}
+			return &storeapi.ExpiringValue{Value: e.Value, Expiry: e.Expiry}, nil
+		}
+	}
+	return nil, errors.Errorf("expecting a response to a transient data request for [%s] but got a response for another key", key)
+}
+
+func (r *retriever) createCollDataRequestMsg(req requestmgr.Request, key *storeapi.Key) *gproto.GossipMessage {
+	return &gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(r.channelID),
+		Content: &gproto.GossipMessage_CollDataReq{
+			CollDataReq: &gproto.RemoteCollDataRequest{
+				Nonce: req.ID(),
+				Digests: []*gproto.CollDataDigest{
+					{
+						Namespace:      key.Namespace,
+						Collection:     key.Collection,
+						Key:            key.Key,
+						EndorsedAtTxID: key.EndorsedAtTxID,
+					},
+				},
+			},
+		},
+	}
+}
+
+// isAuthorized returns true if the local peer has access to the given collection
+func (r *retriever) isAuthorized(ns, coll string) (bool, error) {
+	policy, err := r.Policy(r.channelID, ns, coll)
+	if err != nil {
+		return false, errors.WithMessagef(err, "unable to get policy for [%s:%s]", ns, coll)
+	}
+
+	localMSPID, err := getLocalMSPID()
+	if err != nil {
+		return false, errors.WithMessagef(err, "unable to get local MSP ID")
+	}
+
+	for _, mspID := range policy.MemberOrgs() {
+		if mspID == localMSPID {
+			return true, nil
+		}
+	}
+
+	return false, nil
+}
+
+func asRemotePeers(members []*discovery.Member) []*comm.RemotePeer {
+	var peers []*comm.RemotePeer
+	for _, m := range members {
+		peers = append(peers, &comm.RemotePeer{
+			Endpoint: m.Endpoint,
+			PKIID:    m.PKIid,
+		})
+	}
+	return peers
+}
+
+// getLocalMSPID returns the MSP ID of the local peer. This variable may be overridden by unit tests.
+var getLocalMSPID = func() (string, error) {
+	return mspmgmt.GetLocalMSP().GetIdentifier()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/api/api.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/api/api.go
new file mode 100644
index 000000000..7c0917170
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/api/api.go
@@ -0,0 +1,30 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	"fmt"
+	"time"
+)
+
+// Key is a transient data key
+type Key struct {
+	Namespace  string
+	Collection string
+	Key        string
+}
+
+func (k Key) String() string {
+	return fmt.Sprintf("%s:%s:%s", k.Namespace, k.Collection, k.Key)
+}
+
+// Value is a transient data value
+type Value struct {
+	Value      []byte
+	TxID       string
+	ExpiryTime time.Time
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/cache/cache.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/cache/cache.go
new file mode 100644
index 000000000..2e9af2089
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/cache/cache.go
@@ -0,0 +1,150 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cache
+
+import (
+	"fmt"
+	"time"
+
+	"github.com/bluele/gcache"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+var logger = flogging.MustGetLogger("memtransientdatastore")
+
+// Cache is an in-memory key-value cache
+type Cache struct {
+	cache   gcache.Cache
+	ticker  *time.Ticker
+	dbstore transientDB
+}
+
+// transientDB - an interface for persisting and retrieving keys
+type transientDB interface {
+	AddKey(api.Key, *api.Value) error
+	DeleteExpiredKeys() error
+	GetKey(key api.Key) (*api.Value, error)
+}
+
+// New return a new in-memory key-value cache
+func New(size int, dbstore transientDB) *Cache {
+	c := &Cache{
+		ticker:  time.NewTicker(config.GetTransientDataExpiredIntervalTime()),
+		dbstore: dbstore,
+	}
+
+	c.cache = gcache.New(size).
+		LoaderExpireFunc(c.loadFromDB).
+		EvictedFunc(c.storeToDB).
+		ARC().Build()
+
+	// cleanup expired data in db
+	go c.periodicPurge()
+
+	return c
+}
+
+// Close closes the cache
+func (c *Cache) Close() {
+	c.cache.Purge()
+	c.ticker.Stop()
+}
+
+// Put adds the transient value for the given key.
+func (c *Cache) Put(key api.Key, value []byte, txID string) {
+	if err := c.cache.Set(key,
+		&api.Value{
+			Value: value,
+			TxID:  txID,
+		}); err != nil {
+		panic("Set must never return an error")
+	}
+}
+
+// PutWithExpire adds the transient value for the given key.
+func (c *Cache) PutWithExpire(key api.Key, value []byte, txID string, expiry time.Duration) {
+	if err := c.cache.SetWithExpire(key,
+		&api.Value{
+			Value:      value,
+			TxID:       txID,
+			ExpiryTime: time.Now().UTC().Add(expiry),
+		}, expiry); err != nil {
+		panic("Set must never return an error")
+	}
+}
+
+// Get returns the transient value for the given key
+func (c *Cache) Get(key api.Key) *api.Value {
+	value, err := c.cache.Get(key)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get must never return an error other than KeyNotFoundError err:%s", err))
+		}
+		return nil
+	}
+
+	return value.(*api.Value)
+}
+
+func (c *Cache) loadFromDB(key interface{}) (interface{}, *time.Duration, error) {
+	logger.Debugf("LoaderExpireFunc for key %s", key)
+	value, err := c.dbstore.GetKey(key.(api.Key))
+	if value == nil || err != nil {
+		if err != nil {
+			logger.Error(err.Error())
+		}
+		logger.Debugf("Key [%s] not found in DB", key)
+		return nil, nil, gcache.KeyNotFoundError
+	}
+	isExpired, diff := checkExpiryTime(value.ExpiryTime)
+	if isExpired {
+		logger.Debugf("Key [%s] from DB has expired", key)
+		return nil, nil, gcache.KeyNotFoundError
+	}
+	logger.Debugf("Loaded key [%s] from DB", key)
+	return value, &diff, nil
+}
+
+func (c *Cache) storeToDB(key, value interface{}) {
+	logger.Debugf("EvictedFunc for key %s", key)
+	if value != nil {
+		k := key.(api.Key)
+		v := value.(*api.Value)
+		isExpired, _ := checkExpiryTime(v.ExpiryTime)
+		if !isExpired {
+			dbstoreErr := c.dbstore.AddKey(k, v)
+			if dbstoreErr != nil {
+				logger.Error(dbstoreErr.Error())
+			} else {
+				logger.Debugf("Key [%s] offloaded to DB", key)
+			}
+		}
+	}
+}
+
+func (c *Cache) periodicPurge() {
+	for range c.ticker.C {
+		dbstoreErr := c.dbstore.DeleteExpiredKeys()
+		if dbstoreErr != nil {
+			logger.Error(dbstoreErr.Error())
+		}
+	}
+}
+
+func checkExpiryTime(expiryTime time.Time) (bool, time.Duration) {
+	if expiryTime.IsZero() {
+		return false, 0
+	}
+
+	timeNow := time.Now().UTC()
+	logger.Debugf("Checking expiration - Current time: %s, Expiry time: %s", timeNow, expiryTime)
+
+	diff := expiryTime.Sub(timeNow)
+	return diff <= 0, diff
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore.go
new file mode 100644
index 000000000..43a2d348c
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore.go
@@ -0,0 +1,130 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dbstore
+
+import (
+	"bytes"
+	"encoding/gob"
+	"fmt"
+	"strings"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/api"
+)
+
+var logger = flogging.MustGetLogger("transientdb")
+
+var compositeKeySep = "!"
+
+// DBStore holds the db handle and the db name
+type DBStore struct {
+	db     *leveldbhelper.DBHandle
+	dbName string
+}
+
+// newDBStore constructs an instance of db store
+func newDBStore(db *leveldbhelper.DBHandle, dbName string) *DBStore {
+	return &DBStore{db, dbName}
+}
+
+// AddKey add cache key to db
+func (s *DBStore) AddKey(key api.Key, value *api.Value) error {
+	encodeVal, err := encodeCacheVal(value)
+	if err != nil {
+		return errors.WithMessagef(err, "failed to encode transientdata value %s", value)
+	}
+	// put key in db
+	err = s.db.Put(encodeCacheKey(key, time.Time{}), encodeVal, true)
+	if err != nil {
+		return errors.Wrapf(err, "failed to save transientdata key %s in db", key)
+	}
+
+	if !value.ExpiryTime.IsZero() {
+		// put same previous key with prefix expiryTime so the clean up can remove all expired keys
+		err = s.db.Put(encodeCacheKey(key, value.ExpiryTime), []byte(""), true)
+		if err != nil {
+			return errors.Wrapf(err, "failed to save transientdata key %s in db", key)
+		}
+	}
+
+	return nil
+}
+
+// GetKey get cache key from db
+func (s *DBStore) GetKey(key api.Key) (*api.Value, error) {
+	logger.Debugf("load transientdata key %s from db", key)
+	value, err := s.db.Get(encodeCacheKey(key, time.Time{}))
+	if err != nil {
+		return nil, errors.Wrapf(err, "failed to load transientdata key %s from db", key)
+	}
+	if value != nil {
+		val, err := decodeCacheVal(value)
+		if err != nil {
+			return nil, errors.Wrapf(err, "failed to decode transientdata value %s", value)
+		}
+		return val, nil
+	}
+	return nil, nil
+}
+
+// DeleteExpiredKeys delete expired keys from db
+func (s *DBStore) DeleteExpiredKeys() error {
+	dbBatch := leveldbhelper.NewUpdateBatch()
+	itr := s.db.GetIterator(nil, []byte(fmt.Sprintf("%d%s", time.Now().UTC().UnixNano(), compositeKeySep)))
+	for itr.Next() {
+		key := string(itr.Key())
+		dbBatch.Delete([]byte(key))
+		dbBatch.Delete([]byte(key[strings.Index(key, compositeKeySep)+1:]))
+	}
+	if dbBatch.Len() > 0 {
+		err := s.db.WriteBatch(dbBatch, true)
+		if err != nil {
+			return errors.Errorf("failed to delete transient data keys %s in db %s", dbBatch.KVs, err.Error())
+		}
+		logger.Debugf("delete expired keys %s from db", dbBatch.KVs)
+	}
+
+	return nil
+}
+
+// Close db
+func (s *DBStore) Close() {
+}
+
+func encodeCacheKey(key api.Key, expiryTime time.Time) []byte {
+	var compositeKey []byte
+	if !expiryTime.IsZero() {
+		compositeKey = append(compositeKey, []byte(fmt.Sprintf("%d", expiryTime.UnixNano()))...)
+		compositeKey = append(compositeKey, compositeKeySep...)
+	}
+	compositeKey = append(compositeKey, []byte(key.Namespace)...)
+	compositeKey = append(compositeKey, compositeKeySep...)
+	compositeKey = append(compositeKey, []byte(key.Collection)...)
+	compositeKey = append(compositeKey, compositeKeySep...)
+	compositeKey = append(compositeKey, []byte(key.Key)...)
+	return compositeKey
+}
+
+func decodeCacheVal(b []byte) (*api.Value, error) {
+	decoder := gob.NewDecoder(bytes.NewBuffer(b))
+	var v *api.Value
+	if err := decoder.Decode(&v); err != nil {
+		return nil, err
+	}
+	return v, nil
+}
+
+func encodeCacheVal(v *api.Value) ([]byte, error) {
+	buf := bytes.NewBuffer(nil)
+	encoder := gob.NewEncoder(buf)
+	if err := encoder.Encode(v); err != nil {
+		return nil, err
+	}
+	return buf.Bytes(), nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go
new file mode 100644
index 000000000..1554d7ace
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go
@@ -0,0 +1,41 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dbstore
+
+import (
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+// DBProvider provides an handle to a transientdata db
+type DBProvider interface {
+	OpenDBStore(id string) (DBStore, error)
+	Close()
+}
+
+// LevelDBProvider provides an handle to a transientdata db
+type LevelDBProvider struct {
+	leveldbProvider *leveldbhelper.Provider
+}
+
+// NewDBProvider constructs new db provider
+func NewDBProvider() *LevelDBProvider {
+	dbPath := config.GetTransientDataLevelDBPath()
+	logger.Debugf("constructing DBProvider dbPath=%s", dbPath)
+	return &LevelDBProvider{leveldbhelper.NewProvider(&leveldbhelper.Conf{DBPath: dbPath})}
+
+}
+
+// OpenDBStore opens the db store
+func (p *LevelDBProvider) OpenDBStore(dbName string) (*DBStore, error) {
+	indexStore := p.leveldbProvider.GetDBHandle(dbName)
+	return newDBStore(indexStore, dbName), nil
+}
+
+// Close cleans up the Provider
+func (p *LevelDBProvider) Close() {
+	p.leveldbProvider.Close()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstore.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstore.go
new file mode 100644
index 000000000..49d05d2b0
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstore.go
@@ -0,0 +1,166 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/cache"
+)
+
+var logger = flogging.MustGetLogger("memtransientdatastore")
+
+type store struct {
+	channelID string
+	cache     *cache.Cache
+}
+
+type db interface {
+	AddKey(api.Key, *api.Value) error
+	DeleteExpiredKeys() error
+	GetKey(key api.Key) (*api.Value, error)
+}
+
+func newStore(channelID string, cacheSize int, transientDB db) *store {
+	logger.Debugf("[%s] Creating new store - cacheSize=%d", channelID, cacheSize)
+	return &store{
+		channelID: channelID,
+		cache:     cache.New(cacheSize, transientDB),
+	}
+}
+
+// Persist persists all transient data within the private data simulation results
+func (s *store) Persist(txID string, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	rwSet, err := rwsetutil.TxPvtRwSetFromProtoMsg(privateSimulationResultsWithConfig.PvtRwset)
+	if err != nil {
+		return errors.WithMessage(err, "error getting pvt RW set from bytes")
+	}
+
+	for _, nsRWSet := range rwSet.NsPvtRwSet {
+		for _, collRWSet := range nsRWSet.CollPvtRwSets {
+			if err := s.persistColl(txID, nsRWSet.NameSpace, privateSimulationResultsWithConfig.CollectionConfigs, collRWSet); err != nil {
+				return err
+			}
+		}
+	}
+
+	return nil
+}
+
+// GetTransientData returns the transient data for the given key
+func (s *store) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return s.getTransientData(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Key), nil
+}
+
+// GetTransientData returns the transient data for the given keys
+func (s *store) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return s.getTransientDataMultipleKeys(key), nil
+}
+
+// Close closes the transient data store
+func (s *store) Close() {
+	if s.cache != nil {
+		logger.Debugf("[%s] Closing cache", s.channelID)
+		s.cache.Close()
+		s.cache = nil
+	}
+}
+
+func (s *store) persistColl(txID string, ns string, collConfigPkgs map[string]*common.CollectionConfigPackage, collRWSet *rwsetutil.CollPvtRwSet) error {
+	config, exists := getCollectionConfig(collConfigPkgs, ns, collRWSet.CollectionName)
+	if !exists {
+		logger.Debugf("[%s] Config for collection [%s:%s] not found in config packages", s.channelID, ns, collRWSet.CollectionName)
+		return nil
+	}
+
+	ttl, err := time.ParseDuration(config.TimeToLive)
+	if err != nil {
+		// This shouldn't happen since the config was validated before being persisted
+		return errors.Wrapf(err, "error parsing time-to-live for collection [%s]", collRWSet.CollectionName)
+	}
+
+	logger.Debugf("[%s] Collection [%s:%s] is a transient data collection", s.channelID, ns, collRWSet.CollectionName)
+
+	for _, wSet := range collRWSet.KvRwSet.Writes {
+		s.persistKVWrite(txID, ns, collRWSet.CollectionName, wSet, ttl)
+	}
+
+	return nil
+}
+
+func (s *store) persistKVWrite(txID, ns, coll string, w *kvrwset.KVWrite, ttl time.Duration) {
+	if w.IsDelete {
+		logger.Debugf("[%s] Skipping key [%s] in collection [%s] in private data rw-set since it was deleted", s.channelID, w.Key, coll)
+		return
+	}
+
+	key := api.Key{
+		Namespace:  ns,
+		Collection: coll,
+		Key:        w.Key,
+	}
+
+	if s.cache.Get(key) != nil {
+		logger.Warningf("[%s] Attempt to update transient data key [%s] in collection [%s]", s.channelID, w.Key, coll)
+		return
+	}
+
+	s.cache.PutWithExpire(key, w.Value, txID, ttl)
+}
+
+func (s *store) getTransientData(txID, ns, coll, key string) *storeapi.ExpiringValue {
+	value := s.cache.Get(api.Key{Namespace: ns, Collection: coll, Key: key})
+	if value == nil {
+		logger.Debugf("[%s] Key [%s] not found in transient store", s.channelID, key)
+		return nil
+	}
+
+	// Check if the data was stored in the current transaction. If so, ignore it or else an endorsement mismatch may result.
+	if value.TxID == txID {
+		logger.Debugf("[%s] Key [%s] skipped since it was stored in the current transaction", s.channelID, key)
+		return nil
+	}
+
+	logger.Debugf("[%s] Key [%s] found in transient store", s.channelID, key)
+
+	return &storeapi.ExpiringValue{Value: value.Value, Expiry: value.ExpiryTime}
+}
+
+func (s *store) getTransientDataMultipleKeys(mkey *storeapi.MultiKey) storeapi.ExpiringValues {
+	var values storeapi.ExpiringValues
+	for _, key := range mkey.Keys {
+		value := s.getTransientData(mkey.EndorsedAtTxID, mkey.Namespace, mkey.Collection, key)
+		if value != nil {
+			values = append(values, value)
+		}
+	}
+	return values
+}
+
+func getCollectionConfig(collConfigPkgs map[string]*common.CollectionConfigPackage, namespace, collName string) (*common.StaticCollectionConfig, bool) {
+	collConfigPkg, ok := collConfigPkgs[namespace]
+	if !ok {
+		return nil, false
+	}
+
+	for _, collConfig := range collConfigPkg.Config {
+		transientConfig := collConfig.GetStaticCollectionConfig()
+		if transientConfig != nil && transientConfig.Type == common.CollectionType_COL_TRANSIENT && transientConfig.Name == collName {
+			return transientConfig, true
+		}
+	}
+
+	return nil, false
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstoreprovider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstoreprovider.go
new file mode 100644
index 000000000..7e38c6de2
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/tdstoreprovider.go
@@ -0,0 +1,67 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"sync"
+
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/api"
+	"github.com/trustbloc/fabric-peer-ext/pkg/collections/transientdata/storeprovider/store/dbstore"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+// New returns a new transient data store provider
+func New() *StoreProvider {
+	return &StoreProvider{
+		stores:     make(map[string]*store),
+		dbProvider: dbstore.NewDBProvider(),
+	}
+}
+
+// StoreProvider is a transient data store provider
+type StoreProvider struct {
+	stores     map[string]*store
+	dbProvider *dbstore.LevelDBProvider
+	sync.RWMutex
+}
+
+// StoreForChannel returns the transient data store for the given channel
+func (sp *StoreProvider) StoreForChannel(channelID string) api.Store {
+	sp.RLock()
+	defer sp.RUnlock()
+	return sp.stores[channelID]
+}
+
+// OpenStore opens the transient data store for the given channel
+func (sp *StoreProvider) OpenStore(channelID string) (api.Store, error) {
+	sp.Lock()
+	defer sp.Unlock()
+
+	_, ok := sp.stores[channelID]
+	if ok {
+		return nil, errors.Errorf("a store for channel [%s] already exists", channelID)
+	}
+
+	db, err := sp.dbProvider.OpenDBStore(channelID)
+	if err != nil {
+		return nil, err
+	}
+
+	store := newStore(channelID, config.GetTransientDataCacheSize(), db)
+	sp.stores[channelID] = store
+
+	return store, nil
+}
+
+// Close shuts down all of the stores
+func (sp *StoreProvider) Close() {
+	for _, s := range sp.stores {
+		s.Close()
+	}
+	sp.dbProvider.Close()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/common.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/common.go
new file mode 100644
index 000000000..1e338ab3b
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/common.go
@@ -0,0 +1,88 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"reflect"
+	"time"
+
+	"github.com/golang/protobuf/ptypes/timestamp"
+)
+
+// Values contains a slice of values
+type Values []interface{}
+
+// IsEmpty returns true if all of the values are nil
+func (v Values) IsEmpty() bool {
+	for _, value := range v {
+		if !IsNil(value) {
+			return false
+		}
+	}
+	return true
+}
+
+// AllSet returns true if all of the values are not nil
+func (v Values) AllSet() bool {
+	for _, value := range v {
+		if IsNil(value) {
+			return false
+		}
+	}
+	return true
+}
+
+// Merge merges this set of values with the given set
+// and returns the new set
+func (v Values) Merge(other Values) Values {
+	var max int
+	if len(other) < len(v) {
+		max = len(v)
+	} else {
+		max = len(other)
+	}
+
+	retVal := make(Values, max)
+	copy(retVal, v)
+
+	for i, o := range other {
+		if IsNil(retVal[i]) {
+			retVal[i] = o
+		}
+	}
+
+	return retVal
+}
+
+// IsNil returns true if the given value is nil
+func IsNil(p interface{}) bool {
+	if p == nil {
+		return true
+	}
+
+	v := reflect.ValueOf(p)
+
+	switch {
+	case v.Kind() == reflect.Ptr:
+		return v.IsNil()
+	case v.Kind() == reflect.Array || v.Kind() == reflect.Slice:
+		return v.Len() == 0
+	default:
+		return false
+	}
+}
+
+// ToTimestamp converts the Time into Timestamp
+func ToTimestamp(t time.Time) *timestamp.Timestamp {
+	now := time.Now().UTC()
+	return &(timestamp.Timestamp{Seconds: now.Unix()})
+}
+
+// FromTimestamp converts the Timestamp into Time
+func FromTimestamp(ts *timestamp.Timestamp) time.Time {
+	return time.Unix(ts.Seconds, 0)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/discovery.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/discovery.go
new file mode 100644
index 000000000..b139fefb0
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/discovery.go
@@ -0,0 +1,123 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"sync"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+var logger = flogging.MustGetLogger("discovery")
+
+// Discovery provides functions to retrieve info about the local peer and other peers in a given channel
+type Discovery struct {
+	channelID string
+	gossip    gossipAdapter
+	self      *Member
+	selfInit  sync.Once
+}
+
+// New returns a new Discovery
+func New(channelID string, gossip gossipAdapter) *Discovery {
+	return &Discovery{
+		channelID: channelID,
+		gossip:    gossip,
+	}
+}
+
+type filter func(m *Member) bool
+
+type gossipAdapter interface {
+	PeersOfChannel(common.ChainID) []discovery.NetworkMember
+	SelfMembershipInfo() discovery.NetworkMember
+	IdentityInfo() api.PeerIdentitySet
+}
+
+// Self returns the local peer
+func (r *Discovery) Self() *Member {
+	r.selfInit.Do(func() {
+		r.self = getSelf(r.channelID, r.gossip)
+	})
+	return r.self
+}
+
+// ChannelID returns the channel ID
+func (r *Discovery) ChannelID() string {
+	return r.channelID
+}
+
+// GetMembers returns members filtered by the given filter
+func (r *Discovery) GetMembers(accept filter) []*Member {
+	identityInfo := r.gossip.IdentityInfo()
+	mapByID := identityInfo.ByID()
+
+	var peers []*Member
+	for _, m := range r.gossip.PeersOfChannel(common.ChainID(r.channelID)) {
+		identity, ok := mapByID[string(m.PKIid)]
+		if !ok {
+			logger.Warningf("[%s] Not adding peer [%s] as a validator since unable to determine MSP ID from PKIID for [%s]", r.channelID, m.Endpoint)
+			continue
+		}
+
+		m := &Member{
+			NetworkMember: m,
+			MSPID:         string(identity.Organization),
+		}
+
+		if accept(m) {
+			peers = append(peers, m)
+		}
+	}
+
+	if accept(r.Self()) {
+		peers = append(peers, r.Self())
+	}
+
+	return peers
+}
+
+// GetMSPID gets the MSP id
+func (r *Discovery) GetMSPID(pkiID common.PKIidType) (string, bool) {
+	identityInfo := r.gossip.IdentityInfo()
+	mapByID := identityInfo.ByID()
+
+	identity, ok := mapByID[string(pkiID)]
+	if !ok {
+		return "", false
+	}
+	return string(identity.Organization), true
+}
+
+func getSelf(channelID string, gossip gossipAdapter) *Member {
+	self := gossip.SelfMembershipInfo()
+	self.Properties = &proto.Properties{
+		Roles: roles.AsString(),
+	}
+
+	identityInfo := gossip.IdentityInfo()
+	mapByID := identityInfo.ByID()
+
+	var mspID string
+	selfIdentity, ok := mapByID[string(self.PKIid)]
+	if ok {
+		mspID = string(selfIdentity.Organization)
+	} else {
+		logger.Warningf("[%s] Unable to determine MSP ID from PKIID for self", channelID)
+	}
+
+	return &Member{
+		NetworkMember: self,
+		MSPID:         mspID,
+		Local:         true,
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/member.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/member.go
new file mode 100644
index 000000000..8ef8cd8f8
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/member.go
@@ -0,0 +1,38 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+// Member wraps a NetworkMember and provides additional info
+type Member struct {
+	discovery.NetworkMember
+	ChannelID string
+	MSPID     string
+	Local     bool // Indicates whether this member is the local peer
+}
+
+func (m *Member) String() string {
+	return m.Endpoint
+}
+
+// Roles returns the roles of the peer
+func (m *Member) Roles() roles.Roles {
+	if m.Properties == nil {
+		logger.Debugf("[%s] Peer [%s] does not have any properties", m.ChannelID, m.Endpoint)
+		return nil
+	}
+	return roles.FromStrings(m.Properties.Roles...)
+}
+
+// HasRole returns true if the member has the given role
+func (m *Member) HasRole(role roles.Role) bool {
+	return m.Roles().Contains(role)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/peergroup.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/peergroup.go
new file mode 100644
index 000000000..944996f59
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/discovery/peergroup.go
@@ -0,0 +1,136 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"math/rand"
+	"sort"
+)
+
+// PeerGroup is a group of peers
+type PeerGroup []*Member
+
+// ContainsLocal return true if one of the peers in the group is the local peer
+func (g PeerGroup) ContainsLocal() bool {
+	for _, p := range g {
+		if p.Local {
+			return true
+		}
+	}
+	return false
+}
+
+func (g PeerGroup) String() string {
+	s := "["
+	for i, p := range g {
+		s += p.String()
+		if i+1 < len(g) {
+			s += ", "
+		}
+	}
+	s += "]"
+	return s
+}
+
+// Sort sorts the peer group by endpoint
+func (g PeerGroup) Sort() PeerGroup {
+	sort.Sort(g)
+	return g
+}
+
+// ContainsAll returns true if ALL of the peers within the given peer group are contained within this peer group
+func (g PeerGroup) ContainsAll(peerGroup PeerGroup) bool {
+	for _, p := range peerGroup {
+		if !g.Contains(p) {
+			return false
+		}
+	}
+	return true
+}
+
+// ContainsAny returns true if ANY of the peers within the given peer group are contained within this peer group
+func (g PeerGroup) ContainsAny(peerGroup PeerGroup) bool {
+	for _, p := range peerGroup {
+		if g.Contains(p) {
+			return true
+		}
+	}
+	return false
+}
+
+// Contains returns true if the given peer is contained within this peer group
+func (g PeerGroup) Contains(peer *Member) bool {
+	for _, p := range g {
+		if p.Endpoint == peer.Endpoint {
+			return true
+		}
+	}
+	return false
+}
+
+// ContainsPeer returns true if the given peer is contained within this peer group
+func (g PeerGroup) ContainsPeer(endpoint string) bool {
+	for _, p := range g {
+		if p.Endpoint == endpoint {
+			return true
+		}
+	}
+	return false
+}
+
+func (g PeerGroup) Len() int {
+	return len(g)
+}
+
+func (g PeerGroup) Less(i, j int) bool {
+	return g[i].Endpoint < g[j].Endpoint
+}
+
+func (g PeerGroup) Swap(i, j int) {
+	g[i], g[j] = g[j], g[i]
+}
+
+// Local returns the local peer from the group
+func (g PeerGroup) Local() (*Member, bool) {
+	for _, p := range g {
+		if p.Local {
+			return p, true
+		}
+	}
+	return nil, false
+}
+
+// Remote returns a PeerGroup subset that only includes remote peers
+func (g PeerGroup) Remote() PeerGroup {
+	var pg PeerGroup
+	for _, p := range g {
+		if !p.Local {
+			pg = append(pg, p)
+		}
+	}
+	return pg
+}
+
+// Shuffle returns a randomly shuffled PeerGroup
+func (g PeerGroup) Shuffle() PeerGroup {
+	var pg PeerGroup
+	for _, i := range rand.Perm(len(g)) {
+		pg = append(pg, g[i])
+	}
+
+	return pg
+}
+
+// Merge adds the given peers to the group if they don't already exist
+func Merge(g PeerGroup, peers ...*Member) PeerGroup {
+	for _, p := range peers {
+		if !g.Contains(p) {
+			g = append(g, p)
+		}
+	}
+	return g
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/multirequest/multirequest.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/multirequest/multirequest.go
new file mode 100644
index 000000000..9843e5814
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/multirequest/multirequest.go
@@ -0,0 +1,105 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package multirequest
+
+import (
+	"context"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common"
+)
+
+var logger = flogging.MustGetLogger("ext_multirequest")
+
+// Request is the request to execute
+type Request func(ctxt context.Context) (common.Values, error)
+
+type req struct {
+	id      string
+	execute Request
+}
+
+type res struct {
+	id     string
+	values common.Values
+	err    error
+}
+
+// Response contains the response for a given request ID
+type Response struct {
+	RequestID string
+	Values    common.Values
+}
+
+// MultiRequest executes multiple requests and returns the first, non-error response
+type MultiRequest struct {
+	requests []*req
+}
+
+// New returns a new MultiRequest
+func New() *MultiRequest {
+	return &MultiRequest{}
+}
+
+// Add adds a request function
+func (r *MultiRequest) Add(id string, execute Request) {
+	r.requests = append(r.requests, &req{id: id, execute: execute})
+}
+
+// Execute executes the requests concurrently and returns the responses.
+func (r *MultiRequest) Execute(ctxt context.Context) *Response {
+	respChan := make(chan *res, len(r.requests)+1)
+
+	cctxt, cancel := context.WithCancel(ctxt)
+	defer cancel()
+
+	for _, request := range r.requests {
+		go func(r *req) {
+			ccctxt, cancelReq := context.WithCancel(cctxt)
+			defer cancelReq()
+			values, err := r.execute(ccctxt)
+			respChan <- &res{id: r.id, values: values, err: err}
+		}(request)
+	}
+
+	resp := &Response{}
+	done := false
+
+	// Wait for all responses
+	for range r.requests {
+		response := <-respChan
+
+		if done {
+			continue
+		}
+
+		if handleResponse(response, resp) {
+			done = true
+			cancel()
+		}
+	}
+
+	return resp
+}
+
+func handleResponse(response *res, resp *Response) bool {
+	if response.err != nil {
+		logger.Debugf("Error response was received from [%s]: %s", response.id, response.err)
+		return false
+	}
+
+	resp.RequestID = response.id
+	resp.Values = resp.Values.Merge(response.values)
+
+	if response.values.AllSet() {
+		logger.Debugf("All values were received from [%s]", response.id)
+		return true
+	}
+
+	logger.Debugf("One or more values are missing in response from [%s]", response.id)
+	return false
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/requestmgr/requestmgr.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/requestmgr/requestmgr.go
new file mode 100644
index 000000000..25f69a1f8
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/requestmgr/requestmgr.go
@@ -0,0 +1,207 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package requestmgr
+
+import (
+	"context"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("ext_requestmgr")
+
+// Element contains transient data for a single key
+type Element struct {
+	Namespace  string
+	Collection string
+	Key        string
+	Value      []byte
+	Expiry     time.Time
+}
+
+// Elements is a slice of Element
+type Elements []*Element
+
+// Get returns the Element matching the given namespace, collection, and key.
+func (e Elements) Get(ns, coll, key string) (*Element, bool) {
+	for _, element := range e {
+		if element.Namespace == ns && element.Collection == coll && element.Key == key {
+			return element, true
+		}
+	}
+	return nil, false
+}
+
+// Response is the response from a remote peer for a collection of transient data keys
+type Response struct {
+	Endpoint  string   // The endpoint of the peer that sent the response
+	MSPID     string   // The MSP ID of the peer that sent the response
+	Signature []byte   // The signature of the peer that provided the data
+	Identity  []byte   // The identity of the peer that sent the response
+	Data      Elements // The transient data
+}
+
+// Request is an interface to get the response
+type Request interface {
+	ID() uint64
+	GetResponse(context context.Context) (*Response, error)
+	Cancel()
+}
+
+// RequestMgr is an interface to create a new request and to respond to the request
+type RequestMgr interface {
+	Respond(reqID uint64, response *Response)
+	NewRequest() Request
+}
+
+type requestMgr struct {
+	mutex sync.RWMutex
+	mgrs  map[string]*channelMgr
+}
+
+var mgr = newRequestMgr()
+
+// Get returns the RequestMgr for the given channel
+func Get(channelID string) RequestMgr {
+	return mgr.forChannel(channelID)
+}
+
+func newRequestMgr() *requestMgr {
+	return &requestMgr{
+		mgrs: make(map[string]*channelMgr),
+	}
+}
+
+func (m *requestMgr) forChannel(channelID string) RequestMgr {
+	m.mutex.RLock()
+	cm, ok := m.mgrs[channelID]
+	m.mutex.RUnlock()
+
+	if ok {
+		return cm
+	}
+
+	m.mutex.Lock()
+	defer m.mutex.Unlock()
+
+	cm = newChannelMgr(channelID)
+	m.mgrs[channelID] = cm
+	return cm
+}
+
+type channelMgr struct {
+	mutex         sync.RWMutex
+	channelID     string
+	requests      map[uint64]*request
+	nextRequestID uint64
+}
+
+func newChannelMgr(channelID string) *channelMgr {
+	logger.Debugf("[%s] Creating new channel request manager", channelID)
+	return &channelMgr{
+		channelID:     channelID,
+		requests:      make(map[uint64]*request),
+		nextRequestID: 1000000000,
+	}
+}
+
+func (c *channelMgr) NewRequest() Request {
+	c.mutex.Lock()
+	defer c.mutex.Unlock()
+
+	reqID := c.newRequestID()
+
+	logger.Debugf("[%s] Subscribing to transient data request %d", c.channelID, reqID)
+
+	s := newRequest(reqID, c.channelID, c.remove)
+	c.requests[reqID] = s
+
+	return s
+}
+
+func (c *channelMgr) Respond(reqID uint64, response *Response) {
+	c.mutex.RLock()
+	defer c.mutex.RUnlock()
+
+	s, ok := c.requests[reqID]
+	if !ok {
+		logger.Debugf("[%s] No transient data requests for %d", c.channelID, reqID)
+		return
+	}
+
+	logger.Debugf("[%s] Publishing transient data response %d", c.channelID, reqID)
+	s.respond(response)
+}
+
+func (c *channelMgr) remove(reqID uint64) {
+	c.mutex.Lock()
+	defer c.mutex.Unlock()
+
+	if _, ok := c.requests[reqID]; !ok {
+		return
+	}
+
+	delete(c.requests, reqID)
+	logger.Debugf("[%s] Unsubscribed from transient data request %d", c.channelID, reqID)
+}
+
+func (c *channelMgr) newRequestID() uint64 {
+	return atomic.AddUint64(&c.nextRequestID, 1)
+}
+
+type request struct {
+	reqID     uint64
+	channelID string
+	remove    func(reqID uint64)
+	respChan  chan *Response
+	done      bool
+}
+
+func newRequest(reqID uint64, channelID string, remove func(reqID uint64)) *request {
+	return &request{
+		reqID:     reqID,
+		channelID: channelID,
+		respChan:  make(chan *Response, 1),
+		remove:    remove,
+	}
+}
+
+func (r *request) ID() uint64 {
+	return r.reqID
+}
+
+func (r *request) GetResponse(ctxt context.Context) (*Response, error) {
+	if r.done {
+		return nil, errors.New("request has already completed")
+	}
+
+	logger.Debugf("[%s] Waiting for response to request %d", r.channelID, r.ID())
+
+	defer r.Cancel()
+
+	select {
+	case <-ctxt.Done():
+		logger.Debugf("[%s] Request %d was timed out or cancelled", r.channelID, r.ID())
+		return nil, ctxt.Err()
+	case item := <-r.respChan:
+		logger.Debugf("[%s] Got response for request %d", r.channelID, r.ID())
+		return item, nil
+	}
+}
+
+func (r *request) Cancel() {
+	r.remove(r.reqID)
+	r.done = true
+}
+
+func (r *request) respond(res *Response) {
+	r.respChan <- res
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/collconfigretriever.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/collconfigretriever.go
new file mode 100644
index 000000000..e5a191511
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/collconfigretriever.go
@@ -0,0 +1,206 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	"fmt"
+	"reflect"
+
+	"github.com/bluele/gcache"
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+type peerLedger interface {
+	// NewQueryExecutor gives handle to a query executor.
+	// A client can obtain more than one 'QueryExecutor's for parallel execution.
+	// Any synchronization should be performed at the implementation level if required
+	NewQueryExecutor() (ledger.QueryExecutor, error)
+}
+
+// CollectionConfigRetriever loads and caches collection configuration and policies
+type CollectionConfigRetriever struct {
+	channelID string
+	ledger    peerLedger
+	cache     gcache.Cache
+}
+
+type blockPublisher interface {
+	AddCCUpgradeHandler(handler gossipapi.ChaincodeUpgradeHandler)
+}
+
+// NewCollectionConfigRetriever returns a new collection configuration retriever
+func NewCollectionConfigRetriever(channelID string, ledger peerLedger, blockPublisher blockPublisher) *CollectionConfigRetriever {
+	r := &CollectionConfigRetriever{
+		channelID: channelID,
+		ledger:    ledger,
+	}
+
+	r.cache = gcache.New(0).Simple().LoaderFunc(
+		func(key interface{}) (interface{}, error) {
+			ccID := key.(string)
+			configs, err := r.loadConfigAndPolicy(ccID)
+			if err != nil {
+				logger.Debugf("Error loading collection configs for chaincode [%s]: %s", ccID, err)
+				return nil, err
+			}
+			return configs, nil
+		}).Build()
+
+	// Add a handler to remove the collection configs from cache when the chaincode is upgraded
+	blockPublisher.AddCCUpgradeHandler(func(blockNum uint64, txID string, chaincodeName string) error {
+		if r.cache.Remove(chaincodeName) {
+			logger.Infof("Chaincode [%s] was upgraded. Removed collection configs from cache.", chaincodeName)
+		}
+		return nil
+	})
+
+	return r
+}
+
+type cacheItem struct {
+	config *common.StaticCollectionConfig
+	policy privdata.CollectionAccessPolicy
+}
+
+type cacheItems []*cacheItem
+
+func (c cacheItems) get(coll string) (*cacheItem, error) {
+	for _, item := range c {
+		if item.config.Name == coll {
+			return item, nil
+		}
+	}
+	return nil, errors.Errorf("configuration not found for collection [%s]", coll)
+}
+
+func (c cacheItems) config(coll string) (*common.StaticCollectionConfig, error) {
+	item, err := c.get(coll)
+	if err != nil {
+		return nil, err
+	}
+	return item.config, nil
+}
+
+func (c cacheItems) policy(coll string) (privdata.CollectionAccessPolicy, error) {
+	item, err := c.get(coll)
+	if err != nil {
+		return nil, err
+	}
+	return item.policy, nil
+}
+
+// Config returns the configuration for the given collection
+func (s *CollectionConfigRetriever) Config(ns, coll string) (*common.StaticCollectionConfig, error) {
+	logger.Debugf("[%s] Retrieving collection configuration for chaincode [%s]", s.channelID, ns)
+	item, err := s.cache.Get(ns)
+	if err != nil {
+		return nil, err
+	}
+
+	configs, ok := item.(cacheItems)
+	if !ok {
+		panic(fmt.Sprintf("unexpected type in cache: %s", reflect.TypeOf(item)))
+	}
+
+	return configs.config(coll)
+}
+
+// Policy returns the collection access policy
+func (s *CollectionConfigRetriever) Policy(ns, coll string) (privdata.CollectionAccessPolicy, error) {
+	logger.Debugf("[%s] Retrieving collection policy for chaincode [%s]", s.channelID, ns)
+	item, err := s.cache.Get(ns)
+	if err != nil {
+		return nil, err
+	}
+
+	configs, ok := item.(cacheItems)
+	if !ok {
+		panic(fmt.Sprintf("unexpected type in cache: %s", reflect.TypeOf(item)))
+	}
+
+	return configs.policy(coll)
+}
+
+func (s *CollectionConfigRetriever) loadConfigAndPolicy(ns string) (cacheItems, error) {
+	configs, err := s.loadConfigs(ns)
+	if err != nil {
+		return nil, err
+	}
+
+	var items []*cacheItem
+	for _, config := range configs {
+		policy, err := s.loadPolicy(ns, config)
+		if err != nil {
+			return nil, err
+		}
+		items = append(items, &cacheItem{
+			config: config,
+			policy: policy,
+		})
+	}
+
+	return items, nil
+}
+
+func (s *CollectionConfigRetriever) loadConfigs(ns string) ([]*common.StaticCollectionConfig, error) {
+	logger.Debugf("[%s] Loading collection configs for chaincode [%s]", s.channelID, ns)
+
+	cpBytes, err := s.getCCPBytes(ns)
+	if err != nil {
+		return nil, errors.Wrapf(err, "error retrieving collection config for chaincode [%s]", ns)
+	}
+	if cpBytes == nil {
+		return nil, errors.Errorf("no collection config for chaincode [%s]", ns)
+	}
+
+	cp := &common.CollectionConfigPackage{}
+	err = proto.Unmarshal(cpBytes, cp)
+	if err != nil {
+		return nil, errors.Wrapf(err, "invalid collection configuration for [%s]", ns)
+	}
+
+	var configs []*common.StaticCollectionConfig
+	for _, collConfig := range cp.Config {
+		config := collConfig.GetStaticCollectionConfig()
+		logger.Debugf("[%s] Checking collection config for [%s:%+v]", s.channelID, ns, config)
+		if config == nil {
+			logger.Warningf("[%s] No config found for a collection in namespace [%s]", s.channelID, ns)
+			continue
+		}
+		configs = append(configs, config)
+	}
+
+	return configs, nil
+}
+
+func (s *CollectionConfigRetriever) loadPolicy(ns string, config *common.StaticCollectionConfig) (privdata.CollectionAccessPolicy, error) {
+	logger.Debugf("[%s] Loading collection policy for [%s:%s]", s.channelID, ns, config.Name)
+
+	colAP := &privdata.SimpleCollection{}
+	err := colAP.Setup(config, mspmgmt.GetIdentityDeserializer(s.channelID))
+	if err != nil {
+		return nil, errors.Wrapf(err, "error setting up collection policy %s", config.Name)
+	}
+
+	return colAP, nil
+}
+
+func (s *CollectionConfigRetriever) getCCPBytes(ns string) ([]byte, error) {
+	qe, err := s.ledger.NewQueryExecutor()
+	if err != nil {
+		return nil, err
+	}
+	defer qe.Done()
+
+	return qe.GetState("lscc", privdata.BuildCollectionKVSKey(ns))
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/support.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/support.go
new file mode 100644
index 000000000..cfd202991
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/common/support/support.go
@@ -0,0 +1,66 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	"github.com/bluele/gcache"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/protos/common"
+)
+
+var logger = flogging.MustGetLogger("ext_support")
+
+type ledgerProvider func(channelID string) ledger.PeerLedger
+type blockPublisherProvider func(channelID string) gossipapi.BlockPublisher
+
+// Support holds the ledger provider and the cache
+type Support struct {
+	getLedger              ledgerProvider
+	configRetrieverCache   gcache.Cache
+	blockPublisherProvider blockPublisherProvider
+}
+
+// New creates a new Support using the ledger provider
+func New(ledgerProvider ledgerProvider, blockPublisherProvider blockPublisherProvider) *Support {
+	s := &Support{
+		getLedger:              ledgerProvider,
+		blockPublisherProvider: blockPublisherProvider,
+	}
+	s.configRetrieverCache = gcache.New(0).Simple().LoaderFunc(
+		func(key interface{}) (interface{}, error) {
+			channelID := key.(string)
+			logger.Debugf("[%s] Creating collection config retriever", channelID)
+			return NewCollectionConfigRetriever(channelID, s.getLedger(channelID), blockPublisherProvider(channelID)), nil
+		}).Build()
+	return s
+}
+
+// Config returns the configuration for the given collection
+func (s *Support) Config(channelID, ns, coll string) (*common.StaticCollectionConfig, error) {
+	ccRetriever, err := s.configRetrieverCache.Get(channelID)
+	if err != nil {
+		return nil, err
+	}
+	return ccRetriever.(*CollectionConfigRetriever).Config(ns, coll)
+}
+
+// Policy returns the collection access policy for the given collection
+func (s *Support) Policy(channelID, ns, coll string) (privdata.CollectionAccessPolicy, error) {
+	ccRetriever, err := s.configRetrieverCache.Get(channelID)
+	if err != nil {
+		return nil, err
+	}
+	return ccRetriever.(*CollectionConfigRetriever).Policy(ns, coll)
+}
+
+// BlockPublisher returns the block publisher for the given channel
+func (s *Support) BlockPublisher(channelID string) gossipapi.BlockPublisher {
+	return s.blockPublisherProvider(channelID)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/config/config.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/config/config.go
new file mode 100644
index 000000000..94b496f2a
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/config/config.go
@@ -0,0 +1,145 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package config
+
+import (
+	"path/filepath"
+	"time"
+
+	"github.com/hyperledger/fabric/core/config"
+
+	"github.com/spf13/viper"
+)
+
+const (
+	confPeerFileSystemPath = "peer.fileSystemPath"
+	confLedgerDataPath     = "ledgersData"
+
+	confRoles            = "ledger.roles"
+	confPvtDataCacheSize = "ledger.blockchain.pvtDataStorage.cacheSize"
+
+	confTransientDataLeveldb             = "transientDataLeveldb"
+	confTransientDataCleanupIntervalTime = "coll.transientdata.cleanupExpired.Interval"
+	confTransientDataCacheSize           = "coll.transientdata.cacheSize"
+	confTransientDataPullTimeout         = "peer.gossip.transientData.pullTimeout"
+
+	confOLCollLeveldb              = "offLedgerLeveldb"
+	confOLCollCleanupIntervalTime  = "coll.offledger.cleanupExpired.Interval"
+	confOLCollMaxPeersForRetrieval = "coll.offledger.maxpeers"
+	confOLCollCacheSize            = "coll.offledger.cacheSize"
+	confOLCollPullTimeout          = "coll.offledger.gossip.pullTimeout"
+
+	confBlockPublisherBufferSize = "blockpublisher.buffersize"
+
+	defaultTransientDataCleanupIntervalTime = 5 * time.Second
+	defaultTransientDataCacheSize           = 100000
+	defaultTransientDataPullTimeout         = 5 * time.Second
+
+	defaultOLCollCleanupIntervalTime  = 5 * time.Second
+	defaultOLCollMaxPeersForRetrieval = 2
+	defaultOLCollCacheSize            = 10000
+	defaultOLCollPullTimeout          = 5 * time.Second
+
+	defaultBlockPublisherBufferSize = 100
+)
+
+// GetRoles returns the roles of the peer. Empty return value indicates that the peer has all roles.
+func GetRoles() string {
+	return viper.GetString(confRoles)
+}
+
+// GetPvtDataCacheSize returns the number of pvt data per block to keep the in the cache
+func GetPvtDataCacheSize() int {
+	pvtDataCacheSize := viper.GetInt(confPvtDataCacheSize)
+	if !viper.IsSet(confPvtDataCacheSize) {
+		pvtDataCacheSize = 10
+	}
+	return pvtDataCacheSize
+}
+
+// GetTransientDataLevelDBPath returns the filesystem path that is used to maintain the transient data level db
+func GetTransientDataLevelDBPath() string {
+	return filepath.Join(filepath.Clean(config.GetPath(confPeerFileSystemPath)), confTransientDataLeveldb)
+}
+
+// GetTransientDataExpiredIntervalTime is time when background routine check expired transient data in db to cleanup.
+func GetTransientDataExpiredIntervalTime() time.Duration {
+	timeout := viper.GetDuration(confTransientDataCleanupIntervalTime)
+	if timeout == 0 {
+		return defaultTransientDataCleanupIntervalTime
+	}
+	return timeout
+}
+
+// GetTransientDataCacheSize returns the size of the transient data cache
+func GetTransientDataCacheSize() int {
+	size := viper.GetInt(confTransientDataCacheSize)
+	if size <= 0 {
+		return defaultTransientDataCacheSize
+	}
+	return size
+}
+
+// GetOLCollLevelDBPath returns the filesystem path that is used to maintain the off-ledger level db
+func GetOLCollLevelDBPath() string {
+	return filepath.Join(filepath.Join(filepath.Clean(config.GetPath(confPeerFileSystemPath)), confLedgerDataPath), confOLCollLeveldb)
+}
+
+// GetOLCollExpirationCheckInterval is time when the background routine checks expired collection data in db to cleanup.
+func GetOLCollExpirationCheckInterval() time.Duration {
+	timeout := viper.GetDuration(confOLCollCleanupIntervalTime)
+	if timeout == 0 {
+		return defaultOLCollCleanupIntervalTime
+	}
+	return timeout
+}
+
+// GetTransientDataPullTimeout is the amount of time a peer waits for a response from another peer for transient data.
+func GetTransientDataPullTimeout() time.Duration {
+	timeout := viper.GetDuration(confTransientDataPullTimeout)
+	if timeout == 0 {
+		timeout = defaultTransientDataPullTimeout
+	}
+	return timeout
+}
+
+// GetBlockPublisherBufferSize returns the size of the block publisher channel buffer for various block events
+func GetBlockPublisherBufferSize() int {
+	size := viper.GetInt(confBlockPublisherBufferSize)
+	if size == 0 {
+		return defaultBlockPublisherBufferSize
+	}
+	return size
+}
+
+// GetOLCollMaxPeersForRetrieval returns the number of peers that should be messaged
+// to retrieve collection data that is not stored locally.
+func GetOLCollMaxPeersForRetrieval() int {
+	maxPeers := viper.GetInt(confOLCollMaxPeersForRetrieval)
+	if maxPeers <= 0 {
+		maxPeers = defaultOLCollMaxPeersForRetrieval
+	}
+	return maxPeers
+}
+
+// GetOLCollCacheSize returns the size of the off-ledger cache
+func GetOLCollCacheSize() int {
+	size := viper.GetInt(confOLCollCacheSize)
+	if size <= 0 {
+		return defaultOLCollCacheSize
+	}
+	return size
+}
+
+// GetOLCollPullTimeout is the amount of time a peer waits for a response from another peer for transient data.
+func GetOLCollPullTimeout() time.Duration {
+	timeout := viper.GetDuration(confOLCollPullTimeout)
+	if timeout == 0 {
+		timeout = defaultOLCollPullTimeout
+	}
+	return timeout
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/endorser/endorser.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/endorser/endorser.go
new file mode 100644
index 000000000..c14284662
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/endorser/endorser.go
@@ -0,0 +1,116 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package endorser
+
+import (
+	"github.com/bluele/gcache"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/extensions/endorser/api"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/support"
+)
+
+var endorserLogger = flogging.MustGetLogger("ext_endorser")
+
+type collConfigRetriever interface {
+	Config(ns, coll string) (*common.StaticCollectionConfig, error)
+}
+
+// CollRWSetFilter filters out all off-ledger (including transient data) read-write sets from the simulation results
+// so that they won't be included in the block.
+type CollRWSetFilter struct {
+	qepf                     api.QueryExecutorProviderFactory
+	bpp                      api.BlockPublisherProvider
+	collConfigRetrieverCache gcache.Cache
+}
+
+// NewCollRWSetFilter returns a new collection RW set filter
+func NewCollRWSetFilter(qepf api.QueryExecutorProviderFactory, bpp api.BlockPublisherProvider) *CollRWSetFilter {
+	return &CollRWSetFilter{
+		qepf: qepf,
+		bpp:  bpp,
+		collConfigRetrieverCache: gcache.New(0).LoaderFunc(func(chID interface{}) (interface{}, error) {
+			channelID := chID.(string)
+			return support.NewCollectionConfigRetriever(channelID, qepf.GetQueryExecutorProvider(channelID), bpp.ForChannel(channelID)), nil
+		}).Build(),
+	}
+}
+
+// Filter filters out all off-ledger (including transient data) read-write sets from the simulation results
+// so that they won't be included in the block.
+func (f *CollRWSetFilter) Filter(channelID string, pubSimulationResults *rwset.TxReadWriteSet) (*rwset.TxReadWriteSet, error) {
+	endorserLogger.Debugf("Filtering off-ledger collection types...")
+	filteredResults := &rwset.TxReadWriteSet{
+		DataModel: pubSimulationResults.DataModel,
+	}
+
+	// Filter out off-ledger collections from read/write sets
+	for _, rwSet := range pubSimulationResults.NsRwset {
+		endorserLogger.Debugf("Checking chaincode [%s] for off-ledger collection types...", rwSet.Namespace)
+
+		filteredRWSet, err := f.filterNamespace(channelID, rwSet)
+		if err != nil {
+			return nil, err
+		}
+
+		if len(filteredRWSet.Rwset) > 0 || len(filteredRWSet.CollectionHashedRwset) > 0 {
+			endorserLogger.Debugf("Adding rw-set for [%s]", rwSet.Namespace)
+			filteredResults.NsRwset = append(filteredResults.NsRwset, filteredRWSet)
+		} else {
+			endorserLogger.Debugf("Not adding rw-set for [%s] since everything has been filtered out", rwSet.Namespace)
+		}
+	}
+
+	return filteredResults, nil
+}
+
+func (f *CollRWSetFilter) filterNamespace(channelID string, nsRWSet *rwset.NsReadWriteSet) (*rwset.NsReadWriteSet, error) {
+	var filteredCollRWSets []*rwset.CollectionHashedReadWriteSet
+	for _, collRWSet := range nsRWSet.CollectionHashedRwset {
+		endorserLogger.Debugf("[%s] Checking collection [%s:%s] to see if it is an off-ledger type...", channelID, nsRWSet.Namespace, collRWSet.CollectionName)
+		offLedger, err := f.isOffLedger(channelID, nsRWSet.Namespace, collRWSet.CollectionName)
+		if err != nil {
+			return nil, err
+		}
+		if !offLedger {
+			endorserLogger.Debugf("[%s] ... adding hashed rw-set for collection [%s:%s] since it IS NOT an off-ledger type", channelID, nsRWSet.Namespace, collRWSet.CollectionName)
+			filteredCollRWSets = append(filteredCollRWSets, collRWSet)
+		} else {
+			endorserLogger.Debugf("[%s] ... removing hashed rw-set for collection [%s:%s] since it IS an off-ledger type", channelID, nsRWSet.Namespace, collRWSet.CollectionName)
+		}
+	}
+
+	return &rwset.NsReadWriteSet{
+		Namespace:             nsRWSet.Namespace,
+		Rwset:                 nsRWSet.Rwset,
+		CollectionHashedRwset: filteredCollRWSets,
+	}, nil
+}
+
+func (f *CollRWSetFilter) isOffLedger(channelID, ns, coll string) (bool, error) {
+	staticConfig, err := f.getConfigRetriever(channelID).Config(ns, coll)
+	if err != nil {
+		return false, err
+	}
+	return isCollOffLedger(staticConfig), nil
+}
+
+func (f *CollRWSetFilter) getConfigRetriever(channelID string) collConfigRetriever {
+	retriever, err := f.collConfigRetrieverCache.Get(channelID)
+	if err != nil {
+		// This should never happen
+		panic(err.Error())
+	}
+	return retriever.(collConfigRetriever)
+}
+
+func isCollOffLedger(collConfig *common.StaticCollectionConfig) bool {
+	return collConfig.Type == common.CollectionType_COL_TRANSIENT ||
+		collConfig.Type == common.CollectionType_COL_OFFLEDGER ||
+		collConfig.Type == common.CollectionType_COL_DCAS
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/blockpublisher/blockpublisher.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/blockpublisher/blockpublisher.go
new file mode 100644
index 000000000..90940365a
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/blockpublisher/blockpublisher.go
@@ -0,0 +1,514 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package blockpublisher
+
+import (
+	"sync"
+	"sync/atomic"
+
+	"github.com/bluele/gcache"
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	ledgerutil "github.com/hyperledger/fabric/core/ledger/util"
+	"github.com/hyperledger/fabric/extensions/gossip/api"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/hyperledger/fabric/protoutil"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+const (
+	lsccID       = "lscc"
+	upgradeEvent = "upgrade"
+)
+
+var logger = flogging.MustGetLogger("ext_blockpublisher")
+
+type write struct {
+	blockNum  uint64
+	txID      string
+	namespace string
+	w         *kvrwset.KVWrite
+}
+
+type read struct {
+	blockNum  uint64
+	txID      string
+	namespace string
+	r         *kvrwset.KVRead
+}
+
+type ccEvent struct {
+	blockNum uint64
+	txID     string
+	event    *pb.ChaincodeEvent
+}
+
+type configUpdate struct {
+	blockNum     uint64
+	configUpdate *cb.ConfigUpdate
+}
+
+// Provider maintains a cache of Block Publishers - one per channel
+type Provider struct {
+	cache gcache.Cache
+}
+
+// NewProvider returns a new block publisher provider
+func NewProvider() *Provider {
+	return &Provider{
+		cache: gcache.New(0).LoaderFunc(func(channelID interface{}) (interface{}, error) {
+			return New(channelID.(string)), nil
+		}).Build(),
+	}
+}
+
+// ForChannel returns the block publisher for the given channel
+func (p *Provider) ForChannel(channelID string) api.BlockPublisher {
+	publisher, err := p.cache.Get(channelID)
+	if err != nil {
+		// This should never happen
+		panic(err.Error())
+	}
+	return publisher.(*Publisher)
+}
+
+// Close closes all block publishers
+func (p *Provider) Close() {
+	for _, publisher := range p.cache.GetALL() {
+		publisher.(*Publisher).Close()
+	}
+}
+
+// Publisher traverses a block and publishes KV read, KV write, and chaincode events to registered handlers
+type Publisher struct {
+	channelID            string
+	writeHandlers        []api.WriteHandler
+	readHandlers         []api.ReadHandler
+	ccEventHandlers      []api.ChaincodeEventHandler
+	configUpdateHandlers []api.ConfigUpdateHandler
+	mutex                sync.RWMutex
+	blockChan            chan *cb.Block
+	wChan                chan *write
+	rChan                chan *read
+	ccEvtChan            chan *ccEvent
+	configUpdateChan     chan *configUpdate
+	doneChan             chan struct{}
+	closed               uint32
+}
+
+// New returns a new block Publisher for the given channel
+func New(channelID string) *Publisher {
+	bufferSize := config.GetBlockPublisherBufferSize()
+
+	p := &Publisher{
+		channelID:        channelID,
+		blockChan:        make(chan *cb.Block, bufferSize),
+		wChan:            make(chan *write, bufferSize),
+		rChan:            make(chan *read, bufferSize),
+		ccEvtChan:        make(chan *ccEvent, bufferSize),
+		configUpdateChan: make(chan *configUpdate, bufferSize),
+		doneChan:         make(chan struct{}),
+	}
+	go p.listen()
+	return p
+}
+
+// Close releases all resources associated with the Publisher. Calling this function
+// multiple times has no effect.
+func (p *Publisher) Close() {
+	if atomic.CompareAndSwapUint32(&p.closed, 0, 1) {
+		p.doneChan <- struct{}{}
+	} else {
+		logger.Debugf("[%s] Block Publisher already closed", p.channelID)
+	}
+}
+
+// AddConfigUpdateHandler adds a handler for config update events
+func (p *Publisher) AddConfigUpdateHandler(handler api.ConfigUpdateHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding config update", p.channelID)
+	p.configUpdateHandlers = append(p.configUpdateHandlers, handler)
+}
+
+// AddWriteHandler adds a new handler for KV writes
+func (p *Publisher) AddWriteHandler(handler api.WriteHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding write", p.channelID)
+	p.writeHandlers = append(p.writeHandlers, handler)
+}
+
+// AddReadHandler adds a new handler for KV reads
+func (p *Publisher) AddReadHandler(handler api.ReadHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding read", p.channelID)
+	p.readHandlers = append(p.readHandlers, handler)
+}
+
+// AddCCEventHandler adds a new handler for chaincode events
+func (p *Publisher) AddCCEventHandler(handler api.ChaincodeEventHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding chaincode event", p.channelID)
+	p.ccEventHandlers = append(p.ccEventHandlers, handler)
+}
+
+// AddCCUpgradeHandler adds a handler for chaincode upgrade events
+func (p *Publisher) AddCCUpgradeHandler(handler api.ChaincodeUpgradeHandler) {
+	logger.Debugf("[%s] Adding chaincode upgrade", p.channelID)
+	p.AddCCEventHandler(newChaincodeUpgradeHandler(handler))
+}
+
+// Publish publishes a block
+func (p *Publisher) Publish(block *cb.Block) {
+	newBlockEvent(p.channelID, block, p.wChan, p.rChan, p.ccEvtChan, p.configUpdateChan).publish()
+}
+
+func (p *Publisher) listen() {
+	for {
+		select {
+		case w := <-p.wChan:
+			p.handleWrite(w)
+		case r := <-p.rChan:
+			p.handleRead(r)
+		case ccEvt := <-p.ccEvtChan:
+			p.handleCCEvent(ccEvt)
+		case cu := <-p.configUpdateChan:
+			p.handleConfigUpdate(cu)
+		case <-p.doneChan:
+			logger.Debugf("[%s] Exiting block Publisher", p.channelID)
+			return
+		}
+	}
+}
+
+func (p *Publisher) handleRead(r *read) {
+	logger.Debugf("[%s] Handling read: [%s]", p.channelID, r)
+	for _, handleRead := range p.getReadHandlers() {
+		if err := handleRead(r.blockNum, p.channelID, r.txID, r.namespace, r.r); err != nil {
+			logger.Warningf("[%s] Error returned from KV read handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) handleWrite(w *write) {
+	logger.Debugf("[%s] Handling write: [%s]", p.channelID, w)
+	for _, handleWrite := range p.getWriteHandlers() {
+		if err := handleWrite(w.blockNum, p.channelID, w.txID, w.namespace, w.w); err != nil {
+			logger.Warningf("[%s] Error returned from KV write handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) handleCCEvent(event *ccEvent) {
+	logger.Debugf("[%s] Handling chaincode event: [%s]", p.channelID, event)
+	for _, handleCCEvent := range p.getCCEventHandlers() {
+		if err := handleCCEvent(event.blockNum, p.channelID, event.txID, event.event); err != nil {
+			logger.Warningf("[%s] Error returned from CC event handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) handleConfigUpdate(cu *configUpdate) {
+	logger.Debugf("[%s] Handling config update [%s]", p.channelID, cu)
+	for _, handleConfigUpdate := range p.getConfigUpdateHandlers() {
+		if err := handleConfigUpdate(cu.blockNum, cu.configUpdate); err != nil {
+			logger.Warningf("[%s] Error returned from config update handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) getReadHandlers() []api.ReadHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.ReadHandler, len(p.readHandlers))
+	copy(handlers, p.readHandlers)
+	return handlers
+}
+
+func (p *Publisher) getWriteHandlers() []api.WriteHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.WriteHandler, len(p.writeHandlers))
+	copy(handlers, p.writeHandlers)
+	return handlers
+}
+
+func (p *Publisher) getCCEventHandlers() []api.ChaincodeEventHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.ChaincodeEventHandler, len(p.ccEventHandlers))
+	copy(handlers, p.ccEventHandlers)
+	return handlers
+}
+
+func (p *Publisher) getConfigUpdateHandlers() []api.ConfigUpdateHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.ConfigUpdateHandler, len(p.configUpdateHandlers))
+	copy(handlers, p.configUpdateHandlers)
+	return handlers
+}
+
+type blockEvent struct {
+	channelID        string
+	block            *cb.Block
+	wChan            chan<- *write
+	rChan            chan<- *read
+	ccEvtChan        chan<- *ccEvent
+	configUpdateChan chan<- *configUpdate
+}
+
+func newBlockEvent(channelID string, block *cb.Block, wChan chan<- *write, rChan chan<- *read, ccEvtChan chan<- *ccEvent, configUpdateChan chan<- *configUpdate) *blockEvent {
+	return &blockEvent{
+		channelID:        channelID,
+		block:            block,
+		wChan:            wChan,
+		rChan:            rChan,
+		ccEvtChan:        ccEvtChan,
+		configUpdateChan: configUpdateChan,
+	}
+}
+
+func (p *blockEvent) publish() {
+	logger.Debugf("[%s] Publishing block #%d", p.channelID, p.block.Header.Number)
+	for i := range p.block.Data.Data {
+		envelope, err := protoutil.ExtractEnvelope(p.block, i)
+		if err != nil {
+			logger.Warningf("[%s] Error extracting envelope at index %d in block %d: %s", p.channelID, i, p.block.Header.Number, err)
+		} else {
+			err = p.visitEnvelope(i, envelope)
+			if err != nil {
+				logger.Warningf("[%s] Error checking envelope at index %d in block %d: %s", p.channelID, i, p.block.Header.Number, err)
+			}
+		}
+	}
+}
+
+func (p *blockEvent) visitEnvelope(i int, envelope *cb.Envelope) error {
+	payload, err := protoutil.ExtractPayload(envelope)
+	if err != nil {
+		return err
+	}
+
+	chdr, err := protoutil.UnmarshalChannelHeader(payload.Header.ChannelHeader)
+	if err != nil {
+		return err
+	}
+
+	if cb.HeaderType(chdr.Type) == cb.HeaderType_ENDORSER_TRANSACTION {
+		txFilter := ledgerutil.TxValidationFlags(p.block.Metadata.Metadata[cb.BlockMetadataIndex_TRANSACTIONS_FILTER])
+		code := txFilter.Flag(i)
+		if code != pb.TxValidationCode_VALID {
+			logger.Debugf("[%s] Transaction at index %d in block %d is not valid. Status code: %s", p.channelID, i, p.block.Header.Number, code)
+			return nil
+		}
+		tx, err := protoutil.GetTransaction(payload.Data)
+		if err != nil {
+			return err
+		}
+		newTxEvent(p.channelID, p.block.Header.Number, chdr.TxId, tx, p.wChan, p.rChan, p.ccEvtChan).publish()
+		return nil
+	}
+
+	if cb.HeaderType(chdr.Type) == cb.HeaderType_CONFIG_UPDATE {
+		envelope := &cb.ConfigUpdateEnvelope{}
+		if err := proto.Unmarshal(payload.Data, envelope); err != nil {
+			return err
+		}
+		newConfigUpdateEvent(p.channelID, p.block.Header.Number, envelope, p.configUpdateChan).publish()
+		return nil
+	}
+
+	return nil
+}
+
+type txEvent struct {
+	channelID string
+	blockNum  uint64
+	txID      string
+	tx        *pb.Transaction
+	wChan     chan<- *write
+	rChan     chan<- *read
+	ccEvtChan chan<- *ccEvent
+}
+
+func newTxEvent(channelID string, blockNum uint64, txID string, tx *pb.Transaction, wChan chan<- *write, rChan chan<- *read, ccEvtChan chan<- *ccEvent) *txEvent {
+	return &txEvent{
+		channelID: channelID,
+		blockNum:  blockNum,
+		txID:      txID,
+		tx:        tx,
+		wChan:     wChan,
+		rChan:     rChan,
+		ccEvtChan: ccEvtChan,
+	}
+}
+
+func (p *txEvent) publish() {
+	logger.Debugf("[%s] Publishing Tx %s in block #%d", p.channelID, p.txID, p.blockNum)
+	for i, action := range p.tx.Actions {
+		err := p.visitTXAction(action)
+		if err != nil {
+			logger.Warningf("[%s] Error checking TxAction at index %d: %s", p.channelID, i, err)
+		}
+	}
+}
+
+func (p *txEvent) visitTXAction(action *pb.TransactionAction) error {
+	chaPayload, err := protoutil.GetChaincodeActionPayload(action.Payload)
+	if err != nil {
+		return err
+	}
+	return p.visitChaincodeActionPayload(chaPayload)
+}
+
+func (p *txEvent) visitChaincodeActionPayload(chaPayload *pb.ChaincodeActionPayload) error {
+	cpp := &pb.ChaincodeProposalPayload{}
+	err := proto.Unmarshal(chaPayload.ChaincodeProposalPayload, cpp)
+	if err != nil {
+		return err
+	}
+
+	return p.visitAction(chaPayload.Action)
+}
+
+func (p *txEvent) visitAction(action *pb.ChaincodeEndorsedAction) error {
+	prp := &pb.ProposalResponsePayload{}
+	err := proto.Unmarshal(action.ProposalResponsePayload, prp)
+	if err != nil {
+		return err
+	}
+	return p.visitProposalResponsePayload(prp)
+}
+
+func (p *txEvent) visitProposalResponsePayload(prp *pb.ProposalResponsePayload) error {
+	chaincodeAction := &pb.ChaincodeAction{}
+	err := proto.Unmarshal(prp.Extension, chaincodeAction)
+	if err != nil {
+		return err
+	}
+	return p.visitChaincodeAction(chaincodeAction)
+}
+
+func (p *txEvent) visitChaincodeAction(chaincodeAction *pb.ChaincodeAction) error {
+	if len(chaincodeAction.Results) > 0 {
+		txRWSet := &rwsetutil.TxRwSet{}
+		if err := txRWSet.FromProtoBytes(chaincodeAction.Results); err != nil {
+			return err
+		}
+		p.visitTxReadWriteSet(txRWSet)
+	}
+
+	if len(chaincodeAction.Events) > 0 {
+		evt := &pb.ChaincodeEvent{}
+		if err := proto.Unmarshal(chaincodeAction.Events, evt); err != nil {
+			logger.Warningf("[%s] Invalid chaincode event for chaincode [%s]", p.channelID, chaincodeAction.ChaincodeId)
+			return errors.WithMessagef(err, "invalid chaincode event for chaincode [%s]", chaincodeAction.ChaincodeId)
+		}
+		p.ccEvtChan <- &ccEvent{
+			blockNum: p.blockNum,
+			txID:     p.txID,
+			event:    evt,
+		}
+	}
+
+	return nil
+}
+
+func (p *txEvent) visitTxReadWriteSet(txRWSet *rwsetutil.TxRwSet) {
+	for _, nsRWSet := range txRWSet.NsRwSets {
+		p.visitNsReadWriteSet(nsRWSet)
+	}
+}
+
+func (p *txEvent) visitNsReadWriteSet(nsRWSet *rwsetutil.NsRwSet) {
+	for _, r := range nsRWSet.KvRwSet.Reads {
+		p.rChan <- &read{
+			blockNum:  p.blockNum,
+			txID:      p.txID,
+			namespace: nsRWSet.NameSpace,
+			r:         r,
+		}
+	}
+	for _, w := range nsRWSet.KvRwSet.Writes {
+		p.wChan <- &write{
+			blockNum:  p.blockNum,
+			txID:      p.txID,
+			namespace: nsRWSet.NameSpace,
+			w:         w,
+		}
+	}
+}
+
+type configUpdateEvent struct {
+	channelID        string
+	blockNum         uint64
+	envelope         *cb.ConfigUpdateEnvelope
+	configUpdateChan chan<- *configUpdate
+}
+
+func newConfigUpdateEvent(channelID string, blockNum uint64, envelope *cb.ConfigUpdateEnvelope, configUpdateChan chan<- *configUpdate) *configUpdateEvent {
+	return &configUpdateEvent{
+		channelID:        channelID,
+		blockNum:         blockNum,
+		envelope:         envelope,
+		configUpdateChan: configUpdateChan,
+	}
+}
+
+func (p *configUpdateEvent) publish() {
+	cu := &cb.ConfigUpdate{}
+	if err := proto.Unmarshal(p.envelope.ConfigUpdate, cu); err != nil {
+		logger.Warningf("[%s] Error unmarshalling config update: %s", p.channelID, err)
+		return
+	}
+
+	logger.Debugf("[%s] Publishing Config Update [%s] in block #%d", p.channelID, cu, p.blockNum)
+
+	p.configUpdateChan <- &configUpdate{
+		blockNum:     p.blockNum,
+		configUpdate: cu,
+	}
+}
+
+func newChaincodeUpgradeHandler(handleUpgrade api.ChaincodeUpgradeHandler) api.ChaincodeEventHandler {
+	return func(blockNum uint64, channelID string, txID string, event *pb.ChaincodeEvent) error {
+		logger.Debugf("[%s] Handling chaincode event: %s", channelID, event)
+		if event.ChaincodeId != lsccID {
+			logger.Debugf("[%s] Chaincode event is not from 'lscc'", channelID)
+			return nil
+		}
+		if event.EventName != upgradeEvent {
+			logger.Debugf("[%s] Chaincode event from 'lscc' is not an upgrade event", channelID)
+			return nil
+		}
+
+		ccData := &pb.LifecycleEvent{}
+		err := proto.Unmarshal(event.Payload, ccData)
+		if err != nil {
+			return errors.WithMessage(err, "error unmarshalling chaincode upgrade event")
+		}
+
+		logger.Debugf("[%s] Handling chaincode upgrade of chaincode [%s]", channelID, ccData.ChaincodeName)
+		return handleUpgrade(blockNum, txID, ccData.ChaincodeName)
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/dispatcher/dispatcher.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/dispatcher/dispatcher.go
new file mode 100644
index 000000000..f409b7ec1
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/gossip/dispatcher/dispatcher.go
@@ -0,0 +1,268 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dispatcher
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/extensions/collections/api/store"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	extgossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	ledgerconfig "github.com/hyperledger/fabric/extensions/roles"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	gdiscovery "github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	cb "github.com/hyperledger/fabric/protos/common"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/discovery"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common/requestmgr"
+	supp "github.com/trustbloc/fabric-peer-ext/pkg/common/support"
+	"go.uber.org/zap/zapcore"
+)
+
+var logger = flogging.MustGetLogger("kevlar_gossip_state")
+
+type gossipAdapter interface {
+	PeersOfChannel(gcommon.ChainID) []gdiscovery.NetworkMember
+	SelfMembershipInfo() gdiscovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+type blockPublisher interface {
+	AddCCUpgradeHandler(handler extgossipapi.ChaincodeUpgradeHandler)
+}
+
+type ccRetriever interface {
+	Config(ns, coll string) (*cb.StaticCollectionConfig, error)
+	Policy(ns, coll string) (privdata.CollectionAccessPolicy, error)
+}
+
+// isEndorser should only be overridden for unit testing
+var isEndorser = func() bool {
+	return ledgerconfig.IsEndorser()
+}
+
+// New returns a new Gossip message dispatcher
+func New(
+	channelID string,
+	dataStore storeapi.Store,
+	gossipAdapter gossipAdapter,
+	ledger ledger.PeerLedger,
+	blockPublisher blockPublisher) *Dispatcher {
+	return &Dispatcher{
+		ccRetriever: supp.NewCollectionConfigRetriever(channelID, ledger, blockPublisher),
+		channelID:   channelID,
+		reqMgr:      requestmgr.Get(channelID),
+		dataStore:   dataStore,
+		discovery:   discovery.New(channelID, gossipAdapter),
+	}
+}
+
+// Dispatcher is a Gossip message dispatcher
+type Dispatcher struct {
+	ccRetriever
+	channelID string
+	reqMgr    requestmgr.RequestMgr
+	dataStore storeapi.Store
+	discovery *discovery.Discovery
+}
+
+// Dispatch handles the message and returns true if the message was handled; false if the message is unrecognized
+func (s *Dispatcher) Dispatch(msg protoext.ReceivedMessage) bool {
+	switch {
+	case msg.GetGossipMessage().GetCollDataReq() != nil:
+		logger.Debug("Handling collection data request message")
+		s.handleDataRequest(msg)
+		return true
+	case msg.GetGossipMessage().GetCollDataRes() != nil:
+		logger.Debug("Handling collection data response message")
+		s.handleDataResponse(msg)
+		return true
+	default:
+		logger.Debug("Not handling msg")
+		return false
+	}
+}
+
+func (s *Dispatcher) handleDataRequest(msg protoext.ReceivedMessage) {
+	if logger.IsEnabledFor(zapcore.DebugLevel) {
+		logger.Debugf("[ENTER] -> handleDataRequest")
+		defer logger.Debug("[EXIT] ->  handleDataRequest")
+	}
+
+	if !isEndorser() {
+		logger.Warningf("Non-endorser should not be receiving collection data request messages")
+		return
+	}
+
+	req := msg.GetGossipMessage().GetCollDataReq()
+	if len(req.Digests) == 0 {
+		logger.Warning("Got nil digests in CollDataRequestMsg")
+		return
+	}
+
+	reqMSPID, ok := s.discovery.GetMSPID(msg.GetConnectionInfo().ID)
+	if !ok {
+		logger.Warningf("Unable to get MSP ID from PKI ID of remote endpoint [%s]", msg.GetConnectionInfo().Endpoint)
+		return
+	}
+
+	responses, err := s.getRequestData(reqMSPID, req)
+	if err != nil {
+		logger.Warningf("[%s] Error processing request for data: %s", s.channelID, err.Error())
+		return
+	}
+
+	logger.Debugf("[%s] Responding with collection data for request %d", s.channelID, req.Nonce)
+
+	msg.Respond(&gproto.GossipMessage{
+		// Copy nonce field from the request, so it will be possible to match response
+		Nonce:   msg.GetGossipMessage().Nonce,
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(s.channelID),
+		Content: &gproto.GossipMessage_CollDataRes{
+			CollDataRes: &gproto.RemoteCollDataResponse{
+				Nonce:    req.Nonce,
+				Elements: responses,
+			},
+		},
+	})
+}
+
+func (s *Dispatcher) handleDataResponse(msg protoext.ReceivedMessage) {
+	if logger.IsEnabledFor(zapcore.DebugLevel) {
+		logger.Debug("[ENTER] -> handleDataResponse")
+		defer logger.Debug("[EXIT] ->  handleDataResponse")
+	}
+
+	mspID, ok := s.discovery.GetMSPID(msg.GetConnectionInfo().ID)
+	if !ok {
+		logger.Errorf("Unable to get MSP ID from PKI ID")
+		return
+	}
+
+	res := msg.GetGossipMessage().GetCollDataRes()
+
+	s.reqMgr.Respond(
+		res.Nonce,
+		&requestmgr.Response{
+			Endpoint: msg.GetConnectionInfo().Endpoint,
+			MSPID:    mspID,
+			// FIXME: Should the message be signed?
+			//Signature:   element.Signature,
+			//Identity:    element.Identity,
+			Data: s.getResponseData(res),
+		},
+	)
+}
+
+func (s *Dispatcher) getRequestData(reqMSPID string, req *gproto.RemoteCollDataRequest) ([]*gproto.CollDataElement, error) {
+	var responses []*gproto.CollDataElement
+	for _, digest := range req.Digests {
+		if digest == nil {
+			return nil, errors.New("got nil digest in CollDataRequestMsg")
+		}
+		e, err := s.getRequestDataElement(reqMSPID, digest)
+		if err != nil {
+			return nil, err
+		}
+		responses = append(responses, e)
+	}
+	return responses, nil
+}
+
+func (s *Dispatcher) getRequestDataElement(reqMSPID string, digest *gproto.CollDataDigest) (*gproto.CollDataElement, error) {
+	key := store.NewKey(digest.EndorsedAtTxID, digest.Namespace, digest.Collection, digest.Key)
+
+	logger.Debugf("[%s] Getting data for key [%s]", s.channelID, key)
+	value, err := s.getDataForKey(key)
+	if err != nil {
+		return nil, errors.WithMessagef(err, "error getting data for [%s]", key)
+	}
+
+	e := &gproto.CollDataElement{
+		Digest: digest,
+	}
+
+	authorized, err := s.isAuthorized(reqMSPID, digest.Namespace, digest.Collection)
+	if err != nil {
+		return nil, err
+	}
+
+	if !authorized {
+		logger.Infof("[%s] Requesting MSP [%s] is not authorized to read data for [%s]", s.channelID, reqMSPID, key)
+	} else if value != nil {
+		e.Value = value.Value
+		e.ExpiryTime = common.ToTimestamp(value.Expiry)
+	}
+
+	return e, nil
+}
+
+func (s *Dispatcher) getResponseData(res *gproto.RemoteCollDataResponse) []*requestmgr.Element {
+	var elements []*requestmgr.Element
+	for _, e := range res.Elements {
+		d := e.Digest
+		logger.Debugf("[%s] Coll data response for request %d - [%s:%s:%s] received", s.channelID, res.Nonce, d.Namespace, d.Collection, d.Key)
+
+		element := &requestmgr.Element{
+			Namespace:  d.Namespace,
+			Collection: d.Collection,
+			Key:        d.Key,
+			Value:      e.Value,
+		}
+
+		if e.ExpiryTime != nil {
+			element.Expiry = time.Unix(e.ExpiryTime.Seconds, 0)
+		}
+		elements = append(elements, element)
+	}
+	return elements
+}
+
+func (s *Dispatcher) getDataForKey(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	logger.Debugf("[%s] Getting config for [%s:%s]", s.channelID, key.Namespace, key.Collection)
+	config, err := s.Config(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, err
+	}
+
+	switch config.Type {
+	case cb.CollectionType_COL_TRANSIENT:
+		logger.Debugf("[%s] Getting transient data for key [%s]", s.channelID, key)
+		return s.dataStore.GetTransientData(key)
+	case cb.CollectionType_COL_DCAS:
+		fallthrough
+	case cb.CollectionType_COL_OFFLEDGER:
+		logger.Debugf("[%s] Getting off-ledger data for key [%s]", s.channelID, key)
+		return s.dataStore.GetData(key)
+	default:
+		return nil, errors.Errorf("unsupported collection type: [%s]", config.Type)
+	}
+}
+
+// isAuthorized determines whether the given MSP ID is authorized to read data from the given collection
+func (s *Dispatcher) isAuthorized(mspID string, ns, coll string) (bool, error) {
+	policy, err := s.Policy(ns, coll)
+	if err != nil {
+		return false, errors.WithMessagef(err, "unable to get policy for collection [%s:%s]", ns, coll)
+	}
+
+	for _, memberMSPID := range policy.MemberOrgs() {
+		if memberMSPID == mspID {
+			return true, nil
+		}
+	}
+
+	return false, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/couchdoc_conv.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/couchdoc_conv.go
new file mode 100644
index 000000000..ad44d7008
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/couchdoc_conv.go
@@ -0,0 +1,142 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package idstore
+
+import (
+	"bytes"
+	"encoding/json"
+	"fmt"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+const (
+	idField                    = "_id"
+	underConstructionLedgerKey = "under_construction"
+	ledgerKeyPrefix            = "ledger_"
+	metadataKey                = "metadata"
+	blockAttachmentName        = "genesis_block"
+	inventoryTypeField         = "type"
+	inventoryTypeIndexName     = "by_type"
+	inventoryTypeIndexDoc      = "indexMetadataInventory"
+	inventoryNameLedgerIDField = "ledger_id"
+	typeLedgerName             = "ledger"
+)
+
+const inventoryTypeIndexDef = `
+	{
+		"index": {
+			"fields": ["` + inventoryTypeField + `"]
+		},
+		"name": "` + inventoryTypeIndexName + `",
+		"ddoc": "` + inventoryTypeIndexDoc + `",
+		"type": "json"
+	}`
+
+type jsonValue map[string]interface{}
+
+func (v jsonValue) toBytes() ([]byte, error) {
+	return json.Marshal(v)
+}
+
+func ledgerToCouchDoc(ledgerID string, gb *common.Block) (*couchdb.CouchDoc, error) {
+	jsonMap := make(jsonValue)
+
+	jsonMap[idField] = ledgerIDToKey(ledgerID)
+	jsonMap[inventoryTypeField] = typeLedgerName
+	jsonMap[inventoryNameLedgerIDField] = ledgerID
+
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	attachment, err := blockToAttachment(gb)
+	if err != nil {
+		return nil, err
+	}
+
+	attachments := append([]*couchdb.AttachmentInfo{}, attachment)
+	couchDoc.Attachments = attachments
+
+	return &couchDoc, nil
+}
+
+func createMetadataDoc(constructionLedger string) (*couchdb.CouchDoc, error) {
+	jsonMap := make(jsonValue)
+
+	jsonMap[idField] = metadataKey
+	jsonMap[underConstructionLedgerKey] = constructionLedger
+
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	return &couchDoc, nil
+}
+
+func ledgerIDToKey(ledgerID string) string {
+	return fmt.Sprintf(ledgerKeyPrefix+"%s", ledgerID)
+}
+
+func blockToAttachment(block *common.Block) (*couchdb.AttachmentInfo, error) {
+	blockBytes, err := proto.Marshal(block)
+	if err != nil {
+		return nil, errors.Wrap(err, "marshaling block failed")
+	}
+
+	attachment := &couchdb.AttachmentInfo{}
+	attachment.AttachmentBytes = blockBytes
+	attachment.ContentType = "application/octet-stream"
+	attachment.Name = blockAttachmentName
+
+	return attachment, nil
+}
+
+func couchDocToJSON(doc *couchdb.CouchDoc) (jsonValue, error) {
+	return couchValueToJSON(doc.JSONValue)
+}
+
+func couchValueToJSON(value []byte) (jsonValue, error) {
+	// create a generic map unmarshal the json
+	jsonResult := make(map[string]interface{})
+	decoder := json.NewDecoder(bytes.NewBuffer(value))
+	decoder.UseNumber()
+
+	err := decoder.Decode(&jsonResult)
+	if err != nil {
+		return nil, errors.Wrap(err, "result from DB is not JSON encoded")
+	}
+
+	return jsonResult, nil
+}
+
+func queryInventory(db *couchdb.CouchDatabase, inventoryType string) ([]*couchdb.QueryResult, error) {
+	const queryFmt = `
+	{
+		"selector": {
+			"` + inventoryTypeField + `": {
+				"$eq": "%s"
+			}
+		},
+		"use_index": ["_design/` + inventoryTypeIndexDoc + `", "` + inventoryTypeIndexName + `"]
+	}`
+
+	results, _, err := db.QueryDocuments(fmt.Sprintf(queryFmt, inventoryType))
+	if err != nil {
+		return nil, err
+	}
+	return results, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/store_impl.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/store_impl.go
new file mode 100644
index 000000000..36e701799
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/store_impl.go
@@ -0,0 +1,269 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package idstore
+
+import (
+	"fmt"
+
+	"github.com/hyperledger/fabric/core/ledger"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/idstore"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+var logger = flogging.MustGetLogger("idstore")
+
+const (
+	systemID      = "fabric_system_"
+	inventoryName = "inventory"
+)
+
+//Store contain couchdb instance
+type Store struct {
+	db               *couchdb.CouchDatabase
+	couchMetadataRev string
+}
+
+//OpenIDStore return id store
+func OpenIDStore(path string, ledgerconfig *ledger.Config) idstore.IDStore {
+	couchInstance, err := createCouchInstance(ledgerconfig)
+	if err != nil {
+		logger.Errorf("create couchdb instance failed %s", err.Error())
+		return nil
+	}
+
+	inventoryDBName := couchdb.ConstructBlockchainDBName(systemID, inventoryName)
+	if roles.IsCommitter() {
+		return newCommitterStore(couchInstance, inventoryDBName)
+	}
+	s, err := newStore(couchInstance, inventoryDBName)
+	if err != nil {
+		logger.Error(err.Error())
+		return nil
+	}
+	return s
+}
+
+func newStore(couchInstance *couchdb.CouchInstance, dbName string) (idstore.IDStore, error) {
+	db, err := couchdb.NewCouchDatabase(couchInstance, dbName)
+	if err != nil {
+		return nil, errors.WithMessagef(err, "create new couchdb database called [%s] failed", dbName)
+	}
+
+	dbExists, err := db.ExistsWithRetry()
+	if err != nil {
+		return nil, errors.WithMessagef(err, "check couchdb [%s] exist failed", dbName)
+	}
+	if !dbExists {
+		return nil, errors.New(fmt.Sprintf("DB not found: [%s]", dbName))
+	}
+
+	indexExists, err := db.IndexDesignDocExistsWithRetry(inventoryTypeIndexDoc)
+	if err != nil {
+		return nil, errors.WithMessagef(err, "check couchdb [%s] index exist failed", dbName)
+
+	}
+	if !indexExists {
+		return nil, errors.New(fmt.Sprintf("DB index not found: [%s]", db.DBName))
+	}
+
+	s := Store{db, ""}
+	return &s, nil
+}
+
+func newCommitterStore(couchInstance *couchdb.CouchInstance, dbName string) idstore.IDStore {
+	db, err := couchdb.CreateCouchDatabase(couchInstance, dbName)
+	if err != nil {
+		logger.Errorf("create new couchdb database failed %s", err.Error())
+		return nil
+	}
+
+	err = createIndices(db)
+	if err != nil {
+		logger.Errorf("create couchdb index failed %s", err.Error())
+		return nil
+	}
+
+	s := Store{db, ""}
+
+	return &s
+}
+
+func createIndices(db *couchdb.CouchDatabase) error {
+	err := db.CreateNewIndexWithRetry(inventoryTypeIndexDef, inventoryTypeIndexDoc)
+	if err != nil {
+		return errors.WithMessagef(err, "creation of inventory metadata index failed for [%s]", db.DBName)
+	}
+	return nil
+}
+
+func createCouchInstance(ledgerconfig *ledger.Config) (*couchdb.CouchInstance, error) {
+	logger.Debugf("constructing CouchDB block storage provider")
+	couchDBConfig := ledgerconfig.StateDB.CouchDB
+	couchInstance, err := couchdb.CreateCouchInstance(couchDBConfig, &disabled.Provider{})
+	if err != nil {
+		return nil, errors.WithMessage(err, "obtaining CouchDB instance failed")
+	}
+
+	return couchInstance, nil
+}
+
+//SetUnderConstructionFlag set under construction flag
+func (s *Store) SetUnderConstructionFlag(ledgerID string) error {
+	doc, err := createMetadataDoc(ledgerID)
+	if err != nil {
+		return err
+	}
+
+	rev, err := s.db.SaveDoc(metadataKey, s.couchMetadataRev, doc)
+	if err != nil {
+		return errors.WithMessage(err, "update of metadata in CouchDB failed")
+	}
+
+	s.couchMetadataRev = rev
+
+	logger.Debugf("updated metadata in CouchDB inventory [%s]", rev)
+	return nil
+}
+
+//UnsetUnderConstructionFlag unset under construction flag
+func (s *Store) UnsetUnderConstructionFlag() error {
+	doc, err := createMetadataDoc("")
+	if err != nil {
+		return err
+	}
+
+	rev, err := s.db.SaveDoc(metadataKey, s.couchMetadataRev, doc)
+	if err != nil {
+		return errors.WithMessage(err, "update of metadata in CouchDB failed")
+	}
+
+	s.couchMetadataRev = rev
+
+	logger.Debugf("updated metadata in CouchDB inventory [%s]", rev)
+	return nil
+}
+
+//GetUnderConstructionFlag get under construction flag
+func (s *Store) GetUnderConstructionFlag() (string, error) {
+	doc, _, err := s.db.ReadDoc(metadataKey)
+	if err != nil {
+		return "", errors.WithMessage(err, "retrieval of metadata from CouchDB inventory failed")
+	}
+
+	// if metadata does not exist, assume that there is nothing under construction.
+	if doc == nil {
+		return "", nil
+	}
+
+	metadata, err := couchDocToJSON(doc)
+	if err != nil {
+		return "", errors.WithMessage(err, "metadata in CouchDB inventory is invalid")
+	}
+
+	constructionLedgerUT := metadata[underConstructionLedgerKey]
+	constructionLedger, ok := constructionLedgerUT.(string)
+	if !ok {
+		return "", errors.New("metadata under construction key in CouchDB inventory is invalid")
+	}
+
+	return constructionLedger, nil
+}
+
+//CreateLedgerID create ledger id
+func (s *Store) CreateLedgerID(ledgerID string, gb *common.Block) error {
+	exists, err := s.LedgerIDExists(ledgerID)
+	if err != nil {
+		return err
+	}
+
+	if exists {
+		return errors.Errorf("ledger already exists [%s]", ledgerID)
+	}
+
+	doc, err := ledgerToCouchDoc(ledgerID, gb)
+	if err != nil {
+		return err
+	}
+
+	rev, err := s.db.BatchUpdateDocuments([]*couchdb.CouchDoc{doc})
+	if err != nil {
+		return errors.WithMessagef(err, "creation of ledger failed [%s]", ledgerID)
+	}
+
+	err = s.UnsetUnderConstructionFlag()
+	if err != nil {
+		return err
+	}
+
+	logger.Debugf("created ledger in CouchDB inventory [%s, %s]", ledgerID, rev)
+	return nil
+}
+
+//LedgerIDExists check ledger id exists
+func (s *Store) LedgerIDExists(ledgerID string) (bool, error) {
+	doc, _, err := s.db.ReadDoc(ledgerIDToKey(ledgerID))
+	if err != nil {
+		return false, err
+	}
+
+	exists := doc != nil
+	return exists, nil
+}
+
+//GetLedgeIDValue get ledger id value
+func (s *Store) GetLedgeIDValue(ledgerID string) ([]byte, error) {
+	doc, _, err := s.db.ReadDoc(ledgerIDToKey(ledgerID))
+	if err != nil {
+		return nil, err
+	}
+	for _, v := range doc.Attachments {
+		if v.Name == blockAttachmentName {
+			return v.AttachmentBytes, nil
+		}
+	}
+	return nil, nil
+}
+
+//GetAllLedgerIds get all ledger ids
+func (s *Store) GetAllLedgerIds() ([]string, error) {
+	results, err := queryInventory(s.db, typeLedgerName)
+	if err != nil {
+		return nil, err
+	}
+
+	ledgers := make([]string, 0)
+	for _, r := range results {
+		ledgerJSON, err := couchValueToJSON(r.Value)
+		if err != nil {
+			return nil, err
+		}
+
+		ledgerIDUT, ok := ledgerJSON[inventoryNameLedgerIDField]
+		if !ok {
+			return nil, errors.Errorf("ledger inventory document is invalid [%s]", r.ID)
+		}
+
+		ledgerID, ok := ledgerIDUT.(string)
+		if !ok {
+			return nil, errors.Errorf("ledger inventory document value is invalid [%s]", r.ID)
+		}
+
+		ledgers = append(ledgers, ledgerID)
+	}
+
+	return ledgers, nil
+}
+
+//Close the store
+func (s *Store) Close() {
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/test_exports.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/test_exports.go
new file mode 100644
index 000000000..3b6a06941
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/idstore/test_exports.go
@@ -0,0 +1,59 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package idstore
+
+import (
+	"os"
+	"testing"
+
+	"github.com/trustbloc/fabric-peer-ext/pkg/testutil"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/idstore"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+)
+
+// StoreEnv provides the  store env for testing
+type StoreEnv struct {
+	t             testing.TB
+	TestStore     idstore.IDStore
+	ledgerid      string
+	couchDBConfig *couchdb.Config
+}
+
+// NewTestStoreEnv construct a StoreEnv for testing
+func NewTestStoreEnv(t *testing.T, ledgerid string, couchDBConfig *couchdb.Config) *StoreEnv {
+	removeStorePath()
+	testStore := OpenIDStore(ledgerid, testutil.TestLedgerConf())
+	s := &StoreEnv{t, testStore, ledgerid, couchDBConfig}
+	return s
+}
+
+//Cleanup env test
+func (env *StoreEnv) Cleanup(ledgerid string) {
+	//create a new connection
+	couchInstance, err := couchdb.CreateCouchInstance(env.couchDBConfig, &disabled.Provider{})
+	if err != nil {
+		panic(err.Error())
+	}
+	pvtDataStoreDBName := couchdb.ConstructBlockchainDBName(systemID, inventoryName)
+	db := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: pvtDataStoreDBName}
+	//drop the test database
+	if _, err := db.DropDatabase(); err != nil {
+		panic(err.Error())
+	}
+	env.TestStore.Close()
+
+	removeStorePath()
+}
+
+func removeStorePath() {
+	dbPath := testutil.TestLedgerConf().PrivateData.StorePath
+	if err := os.RemoveAll(dbPath); err != nil {
+		panic(err.Error())
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockaccesspolicy.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockaccesspolicy.go
new file mode 100644
index 000000000..63925f199
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockaccesspolicy.go
@@ -0,0 +1,61 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/protoutil"
+)
+
+// MockAccessPolicy implements a mock CollectionAccessPolicy
+type MockAccessPolicy struct {
+	ReqPeerCount int
+	MaxPeerCount int
+	Orgs         []string
+	OnlyRead     bool
+	OnlyWrite    bool
+	Filter       privdata.Filter
+}
+
+// AccessFilter returns a member filter function for a collection
+func (m *MockAccessPolicy) AccessFilter() privdata.Filter {
+	if m.Filter == nil {
+		return func(protoutil.SignedData) bool { return true }
+	}
+	return m.Filter
+}
+
+// RequiredPeerCount The minimum number of peers private data will be sent to upon
+// endorsement. The endorsement would fail if dissemination to at least
+// this number of peers is not achieved.
+func (m *MockAccessPolicy) RequiredPeerCount() int {
+	return m.ReqPeerCount
+}
+
+// MaximumPeerCount The maximum number of peers that private data will be sent to
+// upon endorsement. This number has to be bigger than RequiredPeerCount().
+func (m *MockAccessPolicy) MaximumPeerCount() int {
+	return m.MaxPeerCount
+}
+
+// MemberOrgs returns the collection's members as MSP IDs. This serves as
+// a human-readable way of quickly identifying who is part of a collection.
+func (m *MockAccessPolicy) MemberOrgs() []string {
+	return m.Orgs
+}
+
+// IsMemberOnlyRead returns a true if only collection members can read
+// the private data
+func (m *MockAccessPolicy) IsMemberOnlyRead() bool {
+	return m.OnlyRead
+}
+
+// IsMemberOnlyWrite returns a true if only collection members can write
+// the private data
+func (m *MockAccessPolicy) IsMemberOnlyWrite() bool {
+	return m.OnlyWrite
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockbuilder.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockbuilder.go
new file mode 100644
index 000000000..fce275ff8
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockbuilder.go
@@ -0,0 +1,367 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"crypto/sha256"
+	"time"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/golang/protobuf/ptypes/timestamp"
+	cutil "github.com/hyperledger/fabric/common/util"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	ledger_util "github.com/hyperledger/fabric/core/ledger/util"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/hyperledger/fabric/protoutil"
+)
+
+// BlockBuilder builds a mock Block
+type BlockBuilder struct {
+	channelID    string
+	blockNum     uint64
+	previousHash []byte
+	configUpdate *ConfigUpdateBuilder
+	transactions []*TxBuilder
+}
+
+// NewBlockBuilder returns a new mock BlockBuilder
+func NewBlockBuilder(channelID string, blockNum uint64) *BlockBuilder {
+	return &BlockBuilder{
+		channelID: channelID,
+		blockNum:  blockNum,
+	}
+}
+
+// Build builds the block
+func (b *BlockBuilder) Build() *cb.Block {
+	block := &cb.Block{}
+	block.Header = &cb.BlockHeader{}
+	block.Header.Number = b.blockNum
+	block.Header.PreviousHash = b.previousHash
+	block.Data = &cb.BlockData{}
+
+	var metadataContents [][]byte
+	for i := 0; i < len(cb.BlockMetadataIndex_name); i++ {
+		metadataContents = append(metadataContents, []byte{})
+	}
+	block.Metadata = &cb.BlockMetadata{Metadata: metadataContents}
+
+	if b.configUpdate != nil {
+		block.Data.Data = append(block.Data.Data, b.configUpdate.Build())
+	} else {
+		var txValidationCodes []uint8
+		for _, tx := range b.transactions {
+			txBytes, txValidationCode := tx.Build()
+			block.Data.Data = append(block.Data.Data, txBytes)
+			txValidationCodes = append(txValidationCodes, uint8(txValidationCode))
+		}
+		txsfltr := ledger_util.NewTxValidationFlags(len(block.Data.Data))
+		for i := 0; i < len(block.Data.Data); i++ {
+			txsfltr[i] = txValidationCodes[i]
+		}
+		block.Metadata.Metadata[cb.BlockMetadataIndex_TRANSACTIONS_FILTER] = txsfltr
+	}
+
+	blockbytes := cutil.ConcatenateBytes(block.Data.Data...)
+	block.Header.DataHash = computeSHA256(blockbytes)
+
+	return block
+}
+
+// ConfigUpdate adds a config update
+func (b *BlockBuilder) ConfigUpdate() *ConfigUpdateBuilder {
+	if len(b.transactions) > 0 {
+		panic("Cannot mix config updates with endorsement transactions")
+	}
+	cb := NewConfigUpdateBuilder(b.channelID)
+	b.configUpdate = cb
+	return cb
+}
+
+// ConfigUpdateBuilder builds a mock config update envelope
+type ConfigUpdateBuilder struct {
+	channelID string
+}
+
+// NewConfigUpdateBuilder returns a new mock ConfigUpdateBuilder
+func NewConfigUpdateBuilder(channelID string) *ConfigUpdateBuilder {
+	return &ConfigUpdateBuilder{
+		channelID: channelID,
+	}
+}
+
+// Build builds a config update envelope
+func (b *ConfigUpdateBuilder) Build() []byte {
+	chdr := &cb.ChannelHeader{
+		Type:    int32(cb.HeaderType_CONFIG_UPDATE),
+		Version: 1,
+		Timestamp: &timestamp.Timestamp{
+			Seconds: time.Now().Unix(),
+			Nanos:   0,
+		},
+		ChannelId: b.channelID}
+	hdr := &cb.Header{ChannelHeader: protoutil.MarshalOrPanic(chdr)}
+	payload := &cb.Payload{Header: hdr}
+
+	env := &cb.Envelope{}
+
+	var err error
+	env.Payload, err = protoutil.GetBytesPayload(payload)
+	if err != nil {
+		panic(err.Error())
+	}
+	ebytes, err := protoutil.GetBytesEnvelope(env)
+	if err != nil {
+		panic(err.Error())
+	}
+
+	return ebytes
+}
+
+// Transaction adds a new transaction
+func (b *BlockBuilder) Transaction(txID string, validationCode pb.TxValidationCode) *TxBuilder {
+	if b.configUpdate != nil {
+		panic("Cannot mix config updates with endorsement transactions")
+	}
+	tx := NewTxBuilder(b.channelID, txID, validationCode)
+	b.transactions = append(b.transactions, tx)
+	return tx
+}
+
+// TxBuilder builds a mock Transaction
+type TxBuilder struct {
+	channelID        string
+	txID             string
+	validationCode   pb.TxValidationCode
+	chaincodeActions []*ChaincodeActionBuilder
+}
+
+// NewTxBuilder returns a new mock TxBuilder
+func NewTxBuilder(channelID, txID string, validationCode pb.TxValidationCode) *TxBuilder {
+	return &TxBuilder{
+		channelID:      channelID,
+		txID:           txID,
+		validationCode: validationCode,
+	}
+}
+
+// Build builds a transaction
+func (b *TxBuilder) Build() ([]byte, pb.TxValidationCode) {
+	chdr := &cb.ChannelHeader{
+		Type:    int32(cb.HeaderType_ENDORSER_TRANSACTION),
+		Version: 1,
+		Timestamp: &timestamp.Timestamp{
+			Seconds: time.Now().Unix(),
+			Nanos:   0,
+		},
+		ChannelId: b.channelID,
+		TxId:      b.txID}
+	hdr := &cb.Header{ChannelHeader: protoutil.MarshalOrPanic(chdr)}
+	payload := &cb.Payload{Header: hdr}
+
+	env := &cb.Envelope{}
+
+	tx := &pb.Transaction{}
+
+	for _, ccAction := range b.chaincodeActions {
+		tx.Actions = append(tx.Actions, &pb.TransactionAction{
+			Payload: ccAction.Build(),
+		})
+	}
+
+	var err error
+	payload.Data, err = protoutil.GetBytesTransaction(tx)
+	if err != nil {
+		panic(err.Error())
+	}
+	env.Payload, err = protoutil.GetBytesPayload(payload)
+	if err != nil {
+		panic(err.Error())
+	}
+	ebytes, err := protoutil.GetBytesEnvelope(env)
+	if err != nil {
+		panic(err.Error())
+	}
+
+	return ebytes, b.validationCode
+}
+
+// ChaincodeAction adds a chaincode action to the transaction
+func (b *TxBuilder) ChaincodeAction(ccID string) *ChaincodeActionBuilder {
+	cc := NewChaincodeActionBuilder(ccID, b.txID)
+	b.chaincodeActions = append(b.chaincodeActions, cc)
+	return cc
+}
+
+// ChaincodeActionBuilder builds a mock Chaincode Action
+type ChaincodeActionBuilder struct {
+	ccID         string
+	txID         string
+	response     *pb.Response
+	ccEvent      *pb.ChaincodeEvent
+	nsRWSet      *NamespaceRWSetBuilder
+	collNSRWSets []*NamespaceRWSetBuilder
+}
+
+// NewChaincodeActionBuilder returns a new ChaincodeActionBuilder
+func NewChaincodeActionBuilder(ccID, txID string) *ChaincodeActionBuilder {
+	return &ChaincodeActionBuilder{
+		ccID:    ccID,
+		txID:    txID,
+		nsRWSet: NewNamespaceRWSetBuilder(ccID),
+	}
+}
+
+// Build builds the chaincode action
+func (b *ChaincodeActionBuilder) Build() []byte {
+	ccID := &pb.ChaincodeID{
+		Name: b.ccID,
+	}
+
+	var ccEventBytes []byte
+	if b.ccEvent != nil {
+		var err error
+		ccEventBytes, err = proto.Marshal(b.ccEvent)
+		if err != nil {
+			panic(err.Error())
+		}
+	}
+
+	txRWSet := &rwsetutil.TxRwSet{}
+	txRWSet.NsRwSets = append(txRWSet.NsRwSets, b.nsRWSet.Build())
+	for _, collRWSet := range b.collNSRWSets {
+		txRWSet.NsRwSets = append(txRWSet.NsRwSets, collRWSet.Build())
+	}
+
+	nsRWSetBytes, err := txRWSet.ToProtoBytes()
+	if err != nil {
+		panic(err.Error())
+	}
+
+	proposalResponsePayload, err := protoutil.GetBytesProposalResponsePayload(
+		[]byte("proposal_hash"), b.response, nsRWSetBytes, ccEventBytes, ccID)
+	if err != nil {
+		panic(err.Error())
+	}
+
+	ccaPayload := &pb.ChaincodeActionPayload{
+		Action: &pb.ChaincodeEndorsedAction{
+			ProposalResponsePayload: proposalResponsePayload,
+		},
+	}
+
+	payload, err := protoutil.GetBytesChaincodeActionPayload(ccaPayload)
+	if err != nil {
+		panic(err.Error())
+	}
+	return payload
+}
+
+// Response sets the chaincode response
+func (b *ChaincodeActionBuilder) Response(response *pb.Response) *ChaincodeActionBuilder {
+	b.response = response
+	return b
+}
+
+// Read adds a KV read to the read/write set
+func (b *ChaincodeActionBuilder) Read(key string, version *kvrwset.Version) *ChaincodeActionBuilder {
+	b.nsRWSet.Read(key, version)
+	return b
+}
+
+// Write adds a KV write to the read/write set
+func (b *ChaincodeActionBuilder) Write(key string, value []byte) *ChaincodeActionBuilder {
+	b.nsRWSet.Write(key, value)
+	return b
+}
+
+// Delete adds a KV write (with delete=true) to the read/write set
+func (b *ChaincodeActionBuilder) Delete(key string) *ChaincodeActionBuilder {
+	b.nsRWSet.Delete(key)
+	return b
+}
+
+// ChaincodeEvent adds a chaincode event to the chaincode action
+func (b *ChaincodeActionBuilder) ChaincodeEvent(eventName string, payload []byte) *ChaincodeActionBuilder {
+	b.ccEvent = &pb.ChaincodeEvent{
+		ChaincodeId: b.ccID,
+		TxId:        b.txID,
+		EventName:   eventName,
+		Payload:     payload,
+	}
+	return b
+}
+
+// Collection starts a new collection read/write set
+func (b *ChaincodeActionBuilder) Collection(coll string) *NamespaceRWSetBuilder {
+	nsRWSetBuilder := NewNamespaceRWSetBuilder(b.ccID + "~" + coll)
+	b.collNSRWSets = append(b.collNSRWSets, nsRWSetBuilder)
+	return nsRWSetBuilder
+}
+
+// NamespaceRWSetBuilder builds a mock read/write set for a given namespace
+type NamespaceRWSetBuilder struct {
+	namespace string
+	reads     []*kvrwset.KVRead
+	writes    []*kvrwset.KVWrite
+}
+
+// NewNamespaceRWSetBuilder returns a new namespace read/write set builder
+func NewNamespaceRWSetBuilder(ns string) *NamespaceRWSetBuilder {
+	return &NamespaceRWSetBuilder{
+		namespace: ns,
+	}
+}
+
+// Build builds a namespace read/write set
+func (b *NamespaceRWSetBuilder) Build() *rwsetutil.NsRwSet {
+	return &rwsetutil.NsRwSet{
+		NameSpace: b.namespace,
+		KvRwSet: &kvrwset.KVRWSet{
+			Reads:  b.reads,
+			Writes: b.writes,
+		},
+	}
+}
+
+// Read adds a KV read to the read/write set
+func (b *NamespaceRWSetBuilder) Read(key string, version *kvrwset.Version) *NamespaceRWSetBuilder {
+	b.reads = append(b.reads, &kvrwset.KVRead{
+		Key:     key,
+		Version: version,
+	})
+	return b
+}
+
+// Write adds a KV write to the read/write set
+func (b *NamespaceRWSetBuilder) Write(key string, value []byte) *NamespaceRWSetBuilder {
+	b.writes = append(b.writes, &kvrwset.KVWrite{
+		Key:   key,
+		Value: value,
+	})
+	return b
+}
+
+// Delete adds a KV write (with delete=true) to the read/write set
+func (b *NamespaceRWSetBuilder) Delete(key string) *NamespaceRWSetBuilder {
+	b.writes = append(b.writes, &kvrwset.KVWrite{
+		Key:      key,
+		IsDelete: true,
+	})
+	return b
+}
+
+func computeSHA256(data []byte) (hash []byte) {
+	h := sha256.New()
+	_, err := h.Write(data)
+	if err != nil {
+		panic("unable to create digest")
+	}
+	return h.Sum(nil)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockhandler.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockhandler.go
new file mode 100644
index 000000000..b094acb7b
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockhandler.go
@@ -0,0 +1,91 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"sync/atomic"
+
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+)
+
+// MockBlockHandler is a mock block handler
+type MockBlockHandler struct {
+	numReads           int32
+	numWrites          int32
+	numCCEvents        int32
+	numCCUpgradeEvents int32
+	numConfigUpdates   int32
+	err                error
+}
+
+// NewMockBlockHandler returns a mock Block Handler
+func NewMockBlockHandler() *MockBlockHandler {
+	return &MockBlockHandler{}
+}
+
+// WithError sets an error
+func (m *MockBlockHandler) WithError(err error) *MockBlockHandler {
+	m.err = err
+	return m
+}
+
+// NumReads returns the number of reads handled
+func (m *MockBlockHandler) NumReads() int {
+	return int(atomic.LoadInt32(&m.numReads))
+}
+
+// NumWrites returns the number of writes handled
+func (m *MockBlockHandler) NumWrites() int {
+	return int(atomic.LoadInt32(&m.numWrites))
+}
+
+// NumCCEvents returns the number of chaincode events handled
+func (m *MockBlockHandler) NumCCEvents() int {
+	return int(atomic.LoadInt32(&m.numCCEvents))
+}
+
+// NumCCUpgradeEvents returns the number of chaincode upgrades handled
+func (m *MockBlockHandler) NumCCUpgradeEvents() int {
+	return int(atomic.LoadInt32(&m.numCCUpgradeEvents))
+}
+
+// NumConfigUpdates returns the number of configuration updates handled
+func (m *MockBlockHandler) NumConfigUpdates() int {
+	return int(atomic.LoadInt32(&m.numConfigUpdates))
+}
+
+// HandleRead handles a read event by incrementing the read counter
+func (m *MockBlockHandler) HandleRead(blockNum uint64, channelID string, txID string, namespace string, kvRead *kvrwset.KVRead) error {
+	atomic.AddInt32(&m.numReads, 1)
+	return m.err
+}
+
+// HandleWrite handles a write event by incrementing the write counter
+func (m *MockBlockHandler) HandleWrite(blockNum uint64, channelID string, txID string, namespace string, kvWrite *kvrwset.KVWrite) error {
+	atomic.AddInt32(&m.numWrites, 1)
+	return m.err
+}
+
+// HandleChaincodeEvent handle a chaincode event by incrementing the CC event counter
+func (m *MockBlockHandler) HandleChaincodeEvent(blockNum uint64, channelID string, txID string, event *pb.ChaincodeEvent) error {
+	atomic.AddInt32(&m.numCCEvents, 1)
+	return m.err
+}
+
+// HandleChaincodeUpgradeEvent handles a chaincode upgrade event by incrementing the chaincode upgrade counter
+func (m *MockBlockHandler) HandleChaincodeUpgradeEvent(blockNum uint64, txID string, chaincodeName string) error {
+	atomic.AddInt32(&m.numCCUpgradeEvents, 1)
+	return m.err
+}
+
+// HandleConfigUpdate handles a config update by incrementing the config update counter
+func (m *MockBlockHandler) HandleConfigUpdate(blockNum uint64, configUpdate *cb.ConfigUpdate) error {
+	atomic.AddInt32(&m.numConfigUpdates, 1)
+	return m.err
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockpublisher.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockpublisher.go
new file mode 100644
index 000000000..e9d33b65d
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockblockpublisher.go
@@ -0,0 +1,56 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/protos/common"
+)
+
+// MockBlockPublisher is a mock block publisher
+type MockBlockPublisher struct {
+	HandleUpgrade      gossipapi.ChaincodeUpgradeHandler
+	HandleConfigUpdate gossipapi.ConfigUpdateHandler
+	HandleWrite        gossipapi.WriteHandler
+	HandleRead         gossipapi.ReadHandler
+	HandleCCEvent      gossipapi.ChaincodeEventHandler
+}
+
+// NewBlockPublisher returns a mock block publisher
+func NewBlockPublisher() *MockBlockPublisher {
+	return &MockBlockPublisher{}
+}
+
+// AddCCUpgradeHandler adds a chaincode upgrade handler
+func (m *MockBlockPublisher) AddCCUpgradeHandler(handler gossipapi.ChaincodeUpgradeHandler) {
+	m.HandleUpgrade = handler
+}
+
+// AddConfigUpdateHandler adds a config update handler
+func (m *MockBlockPublisher) AddConfigUpdateHandler(handler gossipapi.ConfigUpdateHandler) {
+	m.HandleConfigUpdate = handler
+}
+
+// AddWriteHandler adds a write handler
+func (m *MockBlockPublisher) AddWriteHandler(handler gossipapi.WriteHandler) {
+	m.HandleWrite = handler
+}
+
+// AddReadHandler adds a read handler
+func (m *MockBlockPublisher) AddReadHandler(handler gossipapi.ReadHandler) {
+	m.HandleRead = handler
+}
+
+// AddCCEventHandler adds a chaincode event handler
+func (m *MockBlockPublisher) AddCCEventHandler(handler gossipapi.ChaincodeEventHandler) {
+	m.HandleCCEvent = handler
+}
+
+// Publish is not implemented and panics if invoked
+func (m *MockBlockPublisher) Publish(block *common.Block) {
+	panic("not implemented")
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdataprovider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdataprovider.go
new file mode 100644
index 000000000..a90e5ad5f
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdataprovider.go
@@ -0,0 +1,91 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+)
+
+// DataProvider is a mock transient data provider
+type DataProvider struct {
+	data map[storeapi.Key]*storeapi.ExpiringValue
+	err  error
+}
+
+// NewDataProvider returns a new Data Provider
+func NewDataProvider() *DataProvider {
+	return &DataProvider{
+		data: make(map[storeapi.Key]*storeapi.ExpiringValue),
+	}
+}
+
+// WithData sets the data to be returned by the retriever
+func (p *DataProvider) WithData(key *storeapi.Key, value *storeapi.ExpiringValue) *DataProvider {
+	p.data[*key] = value
+	return p
+}
+
+// WithError sets the error to be returned by the retriever
+func (p *DataProvider) WithError(err error) *DataProvider {
+	p.err = err
+	return p
+}
+
+// RetrieverForChannel returns the retriever for the given channel
+func (p *DataProvider) RetrieverForChannel(channel string) storeapi.Retriever {
+	return &dataRetriever{
+		err:  p.err,
+		data: p.data,
+	}
+}
+
+type dataRetriever struct {
+	err  error
+	data map[storeapi.Key]*storeapi.ExpiringValue
+}
+
+// GetTransientData returns the transient data for the given context and key
+func (m *dataRetriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	if m.err != nil {
+		return nil, m.err
+	}
+	return m.data[*key], nil
+}
+
+// GetTransientDataMultipleKeys returns the transient data with multiple keys for the given context and key
+func (m *dataRetriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	if m.err != nil {
+		return nil, m.err
+	}
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = m.data[*storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, k)]
+	}
+	return values, nil
+}
+
+// GetData returns the data for the given context and key
+func (m *dataRetriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	if m.err != nil {
+		return nil, m.err
+	}
+	return m.data[*key], nil
+}
+
+// GetDataMultipleKeys returns the  data with multiple keys for the given context and key
+func (m *dataRetriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	if m.err != nil {
+		return nil, m.err
+	}
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = m.data[*storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, k)]
+	}
+	return values, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdatastore.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdatastore.go
new file mode 100644
index 000000000..7410c46d0
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockdatastore.go
@@ -0,0 +1,101 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// DataStore implements a mock data store
+type DataStore struct {
+	transientData map[storeapi.Key]*storeapi.ExpiringValue
+	olData        map[storeapi.Key]*storeapi.ExpiringValue
+	err           error
+}
+
+// NewDataStore returns a mock transient data store
+func NewDataStore() *DataStore {
+	return &DataStore{
+		transientData: make(map[storeapi.Key]*storeapi.ExpiringValue),
+		olData:        make(map[storeapi.Key]*storeapi.ExpiringValue),
+	}
+}
+
+// TransientData sets the transient data for the given key
+func (m *DataStore) TransientData(key *storeapi.Key, value *storeapi.ExpiringValue) *DataStore {
+	m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return m
+}
+
+// Data sets the data for the given key
+func (m *DataStore) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *DataStore {
+	m.olData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return m
+}
+
+// Error sets an err
+func (m *DataStore) Error(err error) *DataStore {
+	m.err = err
+	return m
+}
+
+// Persist stores the private write set of a transaction along with the collection config
+// in the transient store based on txid and the block height the private data was received at
+func (m *DataStore) Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error {
+	return m.err
+}
+
+// GetTransientData gets the value for the given transient data item
+func (m *DataStore) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}], m.err
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (m *DataStore) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	var values storeapi.ExpiringValues
+	for _, k := range key.Keys {
+		value, err := m.GetTransientData(&storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: k})
+		if err != nil {
+			return nil, err
+		}
+		values = append(values, value)
+	}
+	return values, m.err
+}
+
+// PutData stores the key/value
+func (m *DataStore) PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error {
+	if m.err != nil {
+		return m.err
+	}
+	m.olData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return nil
+}
+
+// GetData gets the value for the given DCAS item
+func (m *DataStore) GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return m.olData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}], m.err
+}
+
+// GetDataMultipleKeys gets the values for the multiple DCAS items in a single call
+func (m *DataStore) GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	var values storeapi.ExpiringValues
+	for _, k := range key.Keys {
+		value, err := m.GetData(&storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: k})
+		if err != nil {
+			return nil, err
+		}
+		values = append(values, value)
+	}
+	return values, m.err
+}
+
+// Close closes the store
+func (m *DataStore) Close() {
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipadapter.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipadapter.go
new file mode 100644
index 000000000..a337d0379
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipadapter.go
@@ -0,0 +1,96 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	"github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	gossipproto "github.com/hyperledger/fabric/protos/gossip"
+)
+
+// MessageHandler defines a function that handles a gossip message.
+type MessageHandler func(msg *gossipproto.GossipMessage)
+
+// MockGossipAdapter is the gossip adapter
+type MockGossipAdapter struct {
+	self        discovery.NetworkMember
+	members     []discovery.NetworkMember
+	identitySet gossipapi.PeerIdentitySet
+	handler     MessageHandler
+}
+
+// NewMockGossipAdapter returns the adapter
+func NewMockGossipAdapter() *MockGossipAdapter {
+	return &MockGossipAdapter{}
+}
+
+// Self discovers a network member
+func (m *MockGossipAdapter) Self(mspID string, self discovery.NetworkMember) *MockGossipAdapter {
+	m.self = self
+	m.identitySet = append(m.identitySet, gossipapi.PeerIdentityInfo{
+		PKIId:        self.PKIid,
+		Organization: []byte(mspID),
+	})
+	return m
+}
+
+// Member adds the network member
+func (m *MockGossipAdapter) Member(mspID string, member discovery.NetworkMember) *MockGossipAdapter {
+	m.members = append(m.members, member)
+	m.identitySet = append(m.identitySet, gossipapi.PeerIdentityInfo{
+		PKIId:        member.PKIid,
+		Organization: []byte(mspID),
+	})
+	return m
+}
+
+// MemberWithNoPKIID appends the member
+func (m *MockGossipAdapter) MemberWithNoPKIID(mspID string, member discovery.NetworkMember) *MockGossipAdapter {
+	m.members = append(m.members, member)
+	return m
+}
+
+// MessageHandler sets the handler
+func (m *MockGossipAdapter) MessageHandler(handler MessageHandler) *MockGossipAdapter {
+	m.handler = handler
+	return m
+}
+
+// PeersOfChannel returns the members
+func (m *MockGossipAdapter) PeersOfChannel(common.ChainID) []discovery.NetworkMember {
+	return m.members
+}
+
+// SelfMembershipInfo returns self
+func (m *MockGossipAdapter) SelfMembershipInfo() discovery.NetworkMember {
+	return m.self
+}
+
+// IdentityInfo returns the identitySet of this adapter
+func (m *MockGossipAdapter) IdentityInfo() gossipapi.PeerIdentitySet {
+	return m.identitySet
+}
+
+// Send sends a message to remote peers
+func (m *MockGossipAdapter) Send(msg *gossipproto.GossipMessage, peers ...*comm.RemotePeer) {
+	if m.handler != nil {
+		go m.handler(msg)
+	}
+}
+
+// NewMember creates a new network member
+func NewMember(endpoint string, pkiID []byte, roles ...string) discovery.NetworkMember {
+	return discovery.NetworkMember{
+		Endpoint: endpoint,
+		PKIid:    pkiID,
+		Properties: &gossipproto.Properties{
+			Roles: roles,
+		},
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipmsg.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipmsg.go
new file mode 100644
index 000000000..565b430f2
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockgossipmsg.go
@@ -0,0 +1,130 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/trustbloc/fabric-peer-ext/pkg/common"
+)
+
+// KeyVal stores a key and a value
+type KeyVal struct {
+	*store.Key
+	*store.ExpiringValue
+}
+
+// NewKeyValue creates a new KeyVal
+func NewKeyValue(key *store.Key, value *store.ExpiringValue) *KeyVal {
+	return &KeyVal{
+		Key:           key,
+		ExpiringValue: value,
+	}
+}
+
+// NewCollDataReqMsg returns a mock collection data request message
+func NewCollDataReqMsg(channelID string, reqID uint64, keys ...*store.Key) *protoext.SignedGossipMessage {
+	var digests []*gproto.CollDataDigest
+	for _, key := range keys {
+		digests = append(digests, &gproto.CollDataDigest{
+			Namespace:      key.Namespace,
+			Collection:     key.Collection,
+			Key:            key.Key,
+			EndorsedAtTxID: key.EndorsedAtTxID,
+		})
+	}
+
+	msg, _ := protoext.NoopSign(&gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(channelID),
+		Content: &gproto.GossipMessage_CollDataReq{
+			CollDataReq: &gproto.RemoteCollDataRequest{
+				Nonce:   reqID,
+				Digests: digests,
+			},
+		},
+	})
+	return msg
+}
+
+// NewCollDataResMsg returns a mock collection data response message
+func NewCollDataResMsg(channelID string, reqID uint64, keyVals ...*KeyVal) *protoext.SignedGossipMessage {
+	var elements []*gproto.CollDataElement
+	for _, kv := range keyVals {
+		elements = append(elements, &gproto.CollDataElement{
+			Digest: &gproto.CollDataDigest{
+				Namespace:      kv.Namespace,
+				Collection:     kv.Collection,
+				Key:            kv.Key.Key,
+				EndorsedAtTxID: kv.EndorsedAtTxID,
+			},
+			Value:      kv.Value,
+			ExpiryTime: common.ToTimestamp(kv.Expiry),
+		})
+	}
+
+	msg, _ := protoext.NoopSign(&gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(channelID),
+		Content: &gproto.GossipMessage_CollDataRes{
+			CollDataRes: &gproto.RemoteCollDataResponse{
+				Nonce:    reqID,
+				Elements: elements,
+			},
+		},
+	})
+	return msg
+}
+
+// NewDataMsg returns a mock data message
+func NewDataMsg(channelID string) *protoext.SignedGossipMessage {
+	msg, _ := protoext.NoopSign(&gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(channelID),
+		Content: &gproto.GossipMessage_DataMsg{},
+	})
+	return msg
+}
+
+// MockReceivedMessage mocks the Gossip received message
+type MockReceivedMessage struct {
+	Message   *protoext.SignedGossipMessage
+	RespondTo func(msg *gproto.GossipMessage)
+	Member    discovery.NetworkMember
+}
+
+// Respond responds to the given request
+func (m *MockReceivedMessage) Respond(msg *gproto.GossipMessage) {
+	if m.RespondTo != nil {
+		m.RespondTo(msg)
+	}
+}
+
+// GetGossipMessage returns the mock signed gossip message
+func (m *MockReceivedMessage) GetGossipMessage() *protoext.SignedGossipMessage {
+	return m.Message
+}
+
+// GetSourceEnvelope is not implemented
+func (m *MockReceivedMessage) GetSourceEnvelope() *gproto.Envelope {
+	panic("not implemented")
+}
+
+// GetConnectionInfo returns the connection information of the source of the message
+func (m *MockReceivedMessage) GetConnectionInfo() *protoext.ConnectionInfo {
+	return &protoext.ConnectionInfo{
+		ID:       m.Member.PKIid,
+		Endpoint: m.Member.Endpoint,
+	}
+}
+
+// Ack is a noop
+func (m *MockReceivedMessage) Ack(err error) {
+	// Nothing to do
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockledger.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockledger.go
new file mode 100644
index 000000000..366caf27f
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockledger.go
@@ -0,0 +1,107 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"github.com/hyperledger/fabric/common/ledger"
+	ledger2 "github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/peer"
+)
+
+// Ledger is a struct which is used to retrieve data using query
+type Ledger struct {
+	QueryExecutor  *QueryExecutor
+	TxSimulator    *TxSimulator
+	BlockchainInfo *common.BlockchainInfo
+	Error          error
+	BcInfoError    error
+}
+
+// GetConfigHistoryRetriever returns the config history retriever
+func (m *Ledger) GetConfigHistoryRetriever() (ledger2.ConfigHistoryRetriever, error) {
+	panic("not implemented")
+}
+
+// GetBlockchainInfo returns the block chain info
+func (m *Ledger) GetBlockchainInfo() (*common.BlockchainInfo, error) {
+	return m.BlockchainInfo, m.BcInfoError
+}
+
+// GetBlockByNumber returns the block by number
+func (m *Ledger) GetBlockByNumber(blockNumber uint64) (*common.Block, error) {
+	panic("not implemented")
+}
+
+// GetBlocksIterator returns the block iterator
+func (m *Ledger) GetBlocksIterator(startBlockNumber uint64) (ledger.ResultsIterator, error) {
+	panic("not implemented")
+}
+
+// Close closes the ledger
+func (m *Ledger) Close() {
+}
+
+// GetTransactionByID gets the transaction by id
+func (m *Ledger) GetTransactionByID(txID string) (*peer.ProcessedTransaction, error) {
+	panic("not implemented")
+}
+
+// GetBlockByHash returns the block by hash
+func (m *Ledger) GetBlockByHash(blockHash []byte) (*common.Block, error) {
+	panic("not implemented")
+}
+
+// GetBlockByTxID gets the block by transaction id
+func (m *Ledger) GetBlockByTxID(txID string) (*common.Block, error) {
+	panic("not implemented")
+}
+
+// GetTxValidationCodeByTxID gets the validation code
+func (m *Ledger) GetTxValidationCodeByTxID(txID string) (peer.TxValidationCode, error) {
+	panic("not implemented")
+}
+
+// NewTxSimulator returns the transaction simulator
+func (m *Ledger) NewTxSimulator(txid string) (ledger2.TxSimulator, error) {
+	return m.TxSimulator, m.Error
+}
+
+// NewQueryExecutor returns the query executor
+func (m *Ledger) NewQueryExecutor() (ledger2.QueryExecutor, error) {
+	return m.QueryExecutor, m.Error
+}
+
+// NewHistoryQueryExecutor returns the history query executor
+func (m *Ledger) NewHistoryQueryExecutor() (ledger2.HistoryQueryExecutor, error) {
+	panic("not implemented")
+}
+
+// GetPvtDataAndBlockByNum gets private data and block by block number
+func (m *Ledger) GetPvtDataAndBlockByNum(blockNum uint64, filter ledger2.PvtNsCollFilter) (*ledger2.BlockAndPvtData, error) {
+	panic("not implemented")
+}
+
+// GetPvtDataByNum gets private data by number
+func (m *Ledger) GetPvtDataByNum(blockNum uint64, filter ledger2.PvtNsCollFilter) ([]*ledger2.TxPvtData, error) {
+	panic("not implemented")
+}
+
+// CommitWithPvtData commits the private data
+func (m *Ledger) CommitWithPvtData(blockAndPvtdata *ledger2.BlockAndPvtData) error {
+	panic("not implemented")
+}
+
+// CommitPvtDataOfOldBlocks commits the private data of old blocks
+func (m *Ledger) CommitPvtDataOfOldBlocks(blockPvtData []*ledger2.BlockPvtData) ([]*ledger2.PvtdataHashMismatch, error) {
+	panic("not implemented")
+}
+
+// GetMissingPvtDataTracker returns the private data tracker
+func (m *Ledger) GetMissingPvtDataTracker() (ledger2.MissingPvtDataTracker, error) {
+	panic("not implemented")
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockqueryexecutor.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockqueryexecutor.go
new file mode 100644
index 000000000..5ee75fdbf
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockqueryexecutor.go
@@ -0,0 +1,124 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"fmt"
+
+	commonledger "github.com/hyperledger/fabric/common/ledger"
+	"github.com/hyperledger/fabric/core/ledger"
+)
+
+// QueryExecutor is a mock query executor
+type QueryExecutor struct {
+	State map[string]map[string][]byte
+	Error error
+}
+
+// NewQueryExecutor returns a new mock query executor
+func NewQueryExecutor(state map[string]map[string][]byte) *QueryExecutor {
+	return &QueryExecutor{
+		State: state,
+	}
+}
+
+// WithError injects an error to the mock executor
+func (m *QueryExecutor) WithError(err error) *QueryExecutor {
+	m.Error = err
+	return m
+}
+
+// GetState returns the mock state for the given namespace and key
+func (m *QueryExecutor) GetState(namespace string, key string) ([]byte, error) {
+	if m.Error != nil {
+		return nil, m.Error
+	}
+
+	ns := m.State[namespace]
+	if ns == nil {
+		return nil, fmt.Errorf("Could not retrieve namespace %s", namespace)
+	}
+
+	return ns[key], nil
+}
+
+// GetStateMultipleKeys returns the mock state for the given namespace and keys
+func (m *QueryExecutor) GetStateMultipleKeys(namespace string, keys []string) ([][]byte, error) {
+	values := make([][]byte, len(keys))
+	for i, k := range keys {
+		v, err := m.GetState(namespace, k)
+		if err != nil {
+			return nil, err
+		}
+		values[i] = v
+	}
+	return values, nil
+}
+
+// GetStateRangeScanIterator is not currently implemented and will panic if called
+func (m *QueryExecutor) GetStateRangeScanIterator(namespace string, startKey string, endKey string) (commonledger.ResultsIterator, error) {
+	panic("not implemented")
+}
+
+// GetStateRangeScanIteratorWithMetadata is not currently implemented and will panic if called
+func (m *QueryExecutor) GetStateRangeScanIteratorWithMetadata(namespace string, startKey, endKey string, metadata map[string]interface{}) (ledger.QueryResultsIterator, error) {
+	panic("not implemented")
+}
+
+// ExecuteQuery is not currently implemented and will panic if called
+func (m *QueryExecutor) ExecuteQuery(namespace, query string) (commonledger.ResultsIterator, error) {
+	panic("not implemented")
+}
+
+// ExecuteQueryWithMetadata is not currently implemented and will panic if called
+func (m *QueryExecutor) ExecuteQueryWithMetadata(namespace, query string, metadata map[string]interface{}) (ledger.QueryResultsIterator, error) {
+	panic("not implemented")
+}
+
+// GetPrivateData returns the private data for the given namespace, collection, and key
+func (m *QueryExecutor) GetPrivateData(namespace, collection, key string) ([]byte, error) {
+	return m.GetState(namespace+"$"+collection, key)
+}
+
+// GetPrivateDataHash is not currently implemented and will panic if called
+func (m *QueryExecutor) GetPrivateDataHash(namespace, collection, key string) ([]byte, error) {
+	panic("not implemented")
+}
+
+// GetPrivateDataMetadataByHash is not currently implemented and will panic if called
+func (m *QueryExecutor) GetPrivateDataMetadataByHash(namespace, collection string, keyhash []byte) (map[string][]byte, error) {
+	panic("not implemented")
+}
+
+// GetPrivateDataMultipleKeys returns the private data for the given namespace, collection, and keys
+func (m *QueryExecutor) GetPrivateDataMultipleKeys(namespace, collection string, keys []string) ([][]byte, error) {
+	return m.GetStateMultipleKeys(namespace+"$"+collection, keys)
+}
+
+// GetPrivateDataRangeScanIterator is not currently implemented and will panic if called
+func (m *QueryExecutor) GetPrivateDataRangeScanIterator(namespace, collection, startKey, endKey string) (commonledger.ResultsIterator, error) {
+	panic("not implemented")
+}
+
+// ExecuteQueryOnPrivateData is not currently implemented and will panic if called
+func (m *QueryExecutor) ExecuteQueryOnPrivateData(namespace, collection, query string) (commonledger.ResultsIterator, error) {
+	panic("not implemented")
+}
+
+// Done does nothing
+func (m *QueryExecutor) Done() {
+}
+
+// GetStateMetadata is not currently implemented and will panic if called
+func (m *QueryExecutor) GetStateMetadata(namespace, key string) (map[string][]byte, error) {
+	panic("not implemented")
+}
+
+// GetPrivateDataMetadata is not currently implemented and will panic if called
+func (m *QueryExecutor) GetPrivateDataMetadata(namespace, collection, key string) (map[string][]byte, error) {
+	panic("not implemented")
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockrwsetbuilder.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockrwsetbuilder.go
new file mode 100644
index 000000000..45cc43579
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mockrwsetbuilder.go
@@ -0,0 +1,345 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/cauthdsl"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	"github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// ReadWriteSetBuilder is a utility that builds a TxReadWriteSet for unit testing
+type ReadWriteSetBuilder struct {
+	namespaces []*NamespaceBuilder
+}
+
+// NewReadWriteSetBuilder returns a new ReadWriteSetBuilder
+func NewReadWriteSetBuilder() *ReadWriteSetBuilder {
+	return &ReadWriteSetBuilder{}
+}
+
+// Namespace returns a new NamespaceBuilder
+func (b *ReadWriteSetBuilder) Namespace(name string) *NamespaceBuilder {
+	ns := NewNamespaceBuilder(name)
+	b.namespaces = append(b.namespaces, ns)
+	return ns
+}
+
+// Build builds the read-write sets
+func (b *ReadWriteSetBuilder) Build() *rwset.TxReadWriteSet {
+	txRWSet := &rwset.TxReadWriteSet{
+		DataModel: rwset.TxReadWriteSet_KV,
+	}
+
+	for _, ns := range b.namespaces {
+		txRWSet.NsRwset = append(txRWSet.NsRwset,
+			&rwset.NsReadWriteSet{
+				Namespace:             ns.name,
+				Rwset:                 ns.BuildNSReadWriteSets(),
+				CollectionHashedRwset: ns.BuildCollectionHashedRWSets(),
+			},
+		)
+	}
+
+	return txRWSet
+}
+
+// PvtReadWriteSetBuilder is a utility that builds a TxPvtReadWriteSetWithConfigInfo for unit testing
+type PvtReadWriteSetBuilder struct {
+	namespaces []*NamespaceBuilder
+}
+
+// NewPvtReadWriteSetBuilder returns a new PvtReadWriteSetBuilder
+func NewPvtReadWriteSetBuilder() *PvtReadWriteSetBuilder {
+	return &PvtReadWriteSetBuilder{}
+}
+
+// Namespace returns a new NamespaceBuilder
+func (b *PvtReadWriteSetBuilder) Namespace(name string) *NamespaceBuilder {
+	ns := NewNamespaceBuilder(name)
+	b.namespaces = append(b.namespaces, ns)
+	return ns
+}
+
+// Build builds a TxPvtReadWriteSetWithConfigInfo
+func (b *PvtReadWriteSetBuilder) Build() *transientstore.TxPvtReadWriteSetWithConfigInfo {
+	return &transientstore.TxPvtReadWriteSetWithConfigInfo{
+		PvtRwset:          b.BuildReadWriteSet(),
+		CollectionConfigs: b.BuildCollectionConfigs(),
+	}
+}
+
+// BuildReadWriteSet builds the private read-write sets
+func (b *PvtReadWriteSetBuilder) BuildReadWriteSet() *rwset.TxPvtReadWriteSet {
+	pvtWriteSet := &rwset.TxPvtReadWriteSet{
+		DataModel: rwset.TxReadWriteSet_KV,
+	}
+
+	for _, ns := range b.namespaces {
+		pvtWriteSet.NsPvtRwset = append(pvtWriteSet.NsPvtRwset,
+			&rwset.NsPvtReadWriteSet{
+				Namespace:          ns.name,
+				CollectionPvtRwset: ns.BuildReadWriteSets(),
+			},
+		)
+	}
+
+	return pvtWriteSet
+}
+
+// BuildCollectionConfigs builds the collection config package
+func (b *PvtReadWriteSetBuilder) BuildCollectionConfigs() map[string]*common.CollectionConfigPackage {
+	configs := make(map[string]*common.CollectionConfigPackage)
+	for _, ns := range b.namespaces {
+		configs[ns.name] = ns.BuildCollectionConfig()
+	}
+	return configs
+}
+
+// NamespaceBuilder is a utility that builds a CollectionPvtReadWriteSet and CollectionConfigPackage for unit testing
+type NamespaceBuilder struct {
+	name        string
+	reads       map[string]*kvrwset.Version
+	writes      map[string][]byte
+	collections []*CollectionBuilder
+	marshalErr  bool
+}
+
+// NewNamespaceBuilder returns a new namespace builder
+func NewNamespaceBuilder(name string) *NamespaceBuilder {
+	return &NamespaceBuilder{
+		name:   name,
+		reads:  make(map[string]*kvrwset.Version),
+		writes: make(map[string][]byte),
+	}
+}
+
+// Read adds a new read to the namespace
+func (b *NamespaceBuilder) Read(key string, blockNum uint64, txIdx uint64) *NamespaceBuilder {
+	b.reads[key] = &kvrwset.Version{BlockNum: blockNum, TxNum: txIdx}
+	return b
+}
+
+// Write adds a new write to the namespace
+func (b *NamespaceBuilder) Write(key string, value []byte) *NamespaceBuilder {
+	b.writes[key] = value
+	return b
+}
+
+// Delete adds a new write with 'IsDelete=true' to the namespace
+func (b *NamespaceBuilder) Delete(key string) *NamespaceBuilder {
+	b.writes[key] = nil
+	return b
+}
+
+// Collection adds a new collection
+func (b *NamespaceBuilder) Collection(name string) *CollectionBuilder {
+	cb := NewPvtReadWriteSetCollectionBuilder(name)
+	b.collections = append(b.collections, cb)
+	return cb
+}
+
+// WithMarshalError simulates a marshalling error
+func (b *NamespaceBuilder) WithMarshalError() *NamespaceBuilder {
+	b.marshalErr = true
+	return b
+}
+
+// BuildReadWriteSets builds the collection read-write sets for the namespace
+func (b *NamespaceBuilder) BuildReadWriteSets() []*rwset.CollectionPvtReadWriteSet {
+	var rwSets []*rwset.CollectionPvtReadWriteSet
+	for _, coll := range b.collections {
+		rwSets = append(rwSets, coll.Build())
+	}
+	return rwSets
+}
+
+// BuildNSReadWriteSets builds the read-write sets
+func (b *NamespaceBuilder) BuildNSReadWriteSets() []byte {
+	kvRWSet := &kvrwset.KVRWSet{}
+	for key, version := range b.reads {
+		kvRWSet.Reads = append(kvRWSet.Reads, &kvrwset.KVRead{Key: key, Version: version})
+	}
+	for key, value := range b.writes {
+		kvRWSet.Writes = append(kvRWSet.Writes, &kvrwset.KVWrite{Key: key, Value: value, IsDelete: value == nil})
+	}
+
+	if b.marshalErr {
+		return []byte("invalid proto buf")
+	}
+
+	bytes, err := proto.Marshal(kvRWSet)
+	if err != nil {
+		panic(err.Error())
+	}
+	return bytes
+}
+
+// BuildCollectionHashedRWSets builds the collection-hashed read-write sets
+func (b *NamespaceBuilder) BuildCollectionHashedRWSets() []*rwset.CollectionHashedReadWriteSet {
+	var collHashedRWSets []*rwset.CollectionHashedReadWriteSet
+	for _, coll := range b.collections {
+		collHashedRWSets = append(collHashedRWSets, &rwset.CollectionHashedReadWriteSet{
+			CollectionName: coll.name,
+			HashedRwset:    []byte("hashed-rw-set"),
+			PvtRwsetHash:   []byte("pvt-rw-set-hash"),
+		})
+	}
+	return collHashedRWSets
+}
+
+// BuildCollectionConfig builds the collection config package for the namespace
+func (b *NamespaceBuilder) BuildCollectionConfig() *common.CollectionConfigPackage {
+	cp := &common.CollectionConfigPackage{}
+	for _, coll := range b.collections {
+		config := coll.buildConfig()
+		cp.Config = append(cp.Config, config)
+	}
+	return cp
+}
+
+// CollectionBuilder is a utility that builds a CollectionConfig and private data read/write sets for unit testing
+type CollectionBuilder struct {
+	name              string
+	reads             map[string]*kvrwset.Version
+	writes            map[string][]byte
+	policy            string
+	requiredPeerCount int32
+	maximumPeerCount  int32
+	blocksToLive      uint64
+	collType          common.CollectionType
+	marshalErr        bool
+	ttl               string
+}
+
+// NewPvtReadWriteSetCollectionBuilder returns a new private read-write set collection builder
+func NewPvtReadWriteSetCollectionBuilder(name string) *CollectionBuilder {
+	return &CollectionBuilder{
+		name:   name,
+		reads:  make(map[string]*kvrwset.Version),
+		writes: make(map[string][]byte),
+	}
+}
+
+// Read adds a new read to the collection
+func (c *CollectionBuilder) Read(key string, blockNum uint64, txIdx uint64) *CollectionBuilder {
+	c.reads[key] = &kvrwset.Version{BlockNum: blockNum, TxNum: txIdx}
+	return c
+}
+
+// Write adds a new write to the collection
+func (c *CollectionBuilder) Write(key string, value []byte) *CollectionBuilder {
+	c.writes[key] = value
+	return c
+}
+
+// Delete adds a new write with 'IsDelete=true' to the collection
+func (c *CollectionBuilder) Delete(key string) *CollectionBuilder {
+	c.writes[key] = nil
+	return c
+}
+
+// TransientConfig sets the transient collection config
+func (c *CollectionBuilder) TransientConfig(policy string, requiredPeerCount, maximumPeerCount int32, ttl string) *CollectionBuilder {
+	c.policy = policy
+	c.requiredPeerCount = requiredPeerCount
+	c.maximumPeerCount = maximumPeerCount
+	c.collType = common.CollectionType_COL_TRANSIENT
+	c.ttl = ttl
+	return c
+}
+
+// StaticConfig sets the static collection config
+func (c *CollectionBuilder) StaticConfig(policy string, requiredPeerCount, maximumPeerCount int32, btl uint64) *CollectionBuilder {
+	c.policy = policy
+	c.requiredPeerCount = requiredPeerCount
+	c.maximumPeerCount = maximumPeerCount
+	c.blocksToLive = btl
+	return c
+}
+
+// OffLedgerConfig sets the off-ledger collection config
+func (c *CollectionBuilder) OffLedgerConfig(policy string, requiredPeerCount, maximumPeerCount int32, ttl string) *CollectionBuilder {
+	c.policy = policy
+	c.requiredPeerCount = requiredPeerCount
+	c.maximumPeerCount = maximumPeerCount
+	c.collType = common.CollectionType_COL_OFFLEDGER
+	c.ttl = ttl
+	return c
+}
+
+// DCASConfig sets the DCAS collection config
+func (c *CollectionBuilder) DCASConfig(policy string, requiredPeerCount, maximumPeerCount int32, ttl string) *CollectionBuilder {
+	c.policy = policy
+	c.requiredPeerCount = requiredPeerCount
+	c.maximumPeerCount = maximumPeerCount
+	c.collType = common.CollectionType_COL_DCAS
+	c.ttl = ttl
+	return c
+}
+
+// WithMarshalError simulates a marshalling error
+func (c *CollectionBuilder) WithMarshalError() *CollectionBuilder {
+	c.marshalErr = true
+	return c
+}
+
+// Build builds the collection private read-write set
+func (c *CollectionBuilder) Build() *rwset.CollectionPvtReadWriteSet {
+	return &rwset.CollectionPvtReadWriteSet{
+		CollectionName: c.name,
+		Rwset:          c.buildReadWriteSet(),
+	}
+}
+
+func (c *CollectionBuilder) buildReadWriteSet() []byte {
+	kvRWSet := &kvrwset.KVRWSet{}
+	for key, version := range c.reads {
+		kvRWSet.Reads = append(kvRWSet.Reads, &kvrwset.KVRead{Key: key, Version: version})
+	}
+	for key, value := range c.writes {
+		kvRWSet.Writes = append(kvRWSet.Writes, &kvrwset.KVWrite{Key: key, Value: value, IsDelete: value == nil})
+	}
+
+	if c.marshalErr {
+		return []byte("invalid proto buf")
+	}
+
+	bytes, err := proto.Marshal(kvRWSet)
+	if err != nil {
+		panic(err.Error())
+	}
+	return bytes
+}
+
+func (c *CollectionBuilder) buildConfig() *common.CollectionConfig {
+	signaturePolicyEnvelope, err := cauthdsl.FromString(c.policy)
+	if err != nil {
+		panic(err.Error())
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Type: c.collType,
+				Name: c.name,
+				MemberOrgsPolicy: &common.CollectionPolicyConfig{
+					Payload: &common.CollectionPolicyConfig_SignaturePolicy{
+						SignaturePolicy: signaturePolicyEnvelope,
+					},
+				},
+				RequiredPeerCount: c.requiredPeerCount,
+				MaximumPeerCount:  c.maximumPeerCount,
+				BlockToLive:       c.blocksToLive,
+				TimeToLive:        c.ttl,
+			},
+		},
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocksupport.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocksupport.go
new file mode 100644
index 000000000..2f1b99468
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocksupport.go
@@ -0,0 +1,61 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"github.com/hyperledger/fabric/core/common/privdata"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+// MockSupport is a holder of policy, config and error
+type MockSupport struct {
+	CollPolicy  privdata.CollectionAccessPolicy
+	CollConfigs []*cb.StaticCollectionConfig
+	Err         error
+	Publisher   *MockBlockPublisher
+}
+
+// NewMockSupport returns a new MockSupport
+func NewMockSupport() *MockSupport {
+	return &MockSupport{
+		Publisher: NewBlockPublisher(),
+	}
+}
+
+// CollectionPolicy sets the collection access policy for the given collection
+func (s *MockSupport) CollectionPolicy(collPolicy privdata.CollectionAccessPolicy) *MockSupport {
+	s.CollPolicy = collPolicy
+	return s
+}
+
+// CollectionConfig sets the collection config for the given collection
+func (s *MockSupport) CollectionConfig(collConfig *cb.StaticCollectionConfig) *MockSupport {
+	s.CollConfigs = append(s.CollConfigs, collConfig)
+	return s
+}
+
+// Policy returns the collection access policy for the given collection
+func (s *MockSupport) Policy(channelID, ns, coll string) (privdata.CollectionAccessPolicy, error) {
+	return s.CollPolicy, s.Err
+}
+
+// Config returns the collection config for the given collection
+func (s *MockSupport) Config(channelID, ns, coll string) (*cb.StaticCollectionConfig, error) {
+	for _, config := range s.CollConfigs {
+		if config.Name == coll {
+			return config, nil
+		}
+	}
+	return nil, errors.Errorf("config not found for collection: %s", coll)
+}
+
+// BlockPublisher returns a mock block publisher for the given channel
+func (s *MockSupport) BlockPublisher(channelID string) gossipapi.BlockPublisher {
+	return s.Publisher
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocktxsim.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocktxsim.go
new file mode 100644
index 000000000..26eec58d8
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/mocks/mocktxsim.go
@@ -0,0 +1,80 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	ledgermocks "github.com/hyperledger/fabric/common/mocks/ledger"
+	"github.com/hyperledger/fabric/core/ledger"
+)
+
+// TxSimulator implements a mock transaction simulator
+type TxSimulator struct {
+	ledgermocks.MockQueryExecutor
+	SimulationResults *ledger.TxSimulationResults
+	Error             error
+	SimError          error
+}
+
+// SetState is not currently implemented and will panic if called
+func (m *TxSimulator) SetState(namespace string, key string, value []byte) error {
+	panic("not implemented")
+}
+
+// DeleteState is not currently implemented and will panic if called
+func (m *TxSimulator) DeleteState(namespace string, key string) error {
+	panic("not implemented")
+}
+
+// SetStateMultipleKeys is not currently implemented and will panic if called
+func (m *TxSimulator) SetStateMultipleKeys(namespace string, kvs map[string][]byte) error {
+	panic("not implemented")
+}
+
+// ExecuteUpdate is not currently implemented and will panic if called
+func (m *TxSimulator) ExecuteUpdate(query string) error {
+	panic("not implemented")
+}
+
+// GetTxSimulationResults returns the mock simulation results
+func (m *TxSimulator) GetTxSimulationResults() (*ledger.TxSimulationResults, error) {
+	return m.SimulationResults, m.SimError
+}
+
+// DeletePrivateData is not currently implemented and will panic if called
+func (m *TxSimulator) DeletePrivateData(namespace, collection, key string) error {
+	panic("not implemented")
+}
+
+// SetPrivateData is not currently implemented and will panic if called
+func (m *TxSimulator) SetPrivateData(namespace, collection, key string, value []byte) error {
+	panic("not implemented")
+}
+
+// SetPrivateDataMultipleKeys currently does nothing except return the mock error (if any)
+func (m *TxSimulator) SetPrivateDataMultipleKeys(namespace, collection string, kvs map[string][]byte) error {
+	return m.Error
+}
+
+// SetStateMetadata is not currently implemented and will panic if called
+func (m *TxSimulator) SetStateMetadata(namespace, key string, metadata map[string][]byte) error {
+	panic("not implemented")
+}
+
+// DeleteStateMetadata is not currently implemented and will panic if called
+func (m *TxSimulator) DeleteStateMetadata(namespace, key string) error {
+	panic("not implemented")
+}
+
+// SetPrivateDataMetadata is not currently implemented and will panic if called
+func (m *TxSimulator) SetPrivateDataMetadata(namespace, collection, key string, metadata map[string][]byte) error {
+	panic("not implemented")
+}
+
+// DeletePrivateDataMetadata is not currently implemented and will panic if called
+func (m *TxSimulator) DeletePrivateDataMetadata(namespace, collection, key string) error {
+	panic("not implemented")
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/store_impl.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/store_impl.go
new file mode 100644
index 000000000..fd42f469b
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/store_impl.go
@@ -0,0 +1,285 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cachedpvtdatastore
+
+import (
+	"fmt"
+
+	"github.com/pkg/errors"
+
+	"github.com/bluele/gcache"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+	"github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+)
+
+var logger = flogging.MustGetLogger("cachedpvtdatastore")
+
+type provider struct {
+}
+
+type store struct {
+	ledgerid           string
+	btlPolicy          pvtdatapolicy.BTLPolicy
+	cache              gcache.Cache
+	lastCommittedBlock uint64
+	pendingPvtData     *pendingPvtData
+	isEmpty            bool
+}
+
+type pendingPvtData struct {
+	batchPending bool
+	dataEntries  []*common.DataEntry
+}
+
+//////// Provider functions  /////////////
+//////////////////////////////////////////
+
+// NewProvider instantiates a private data storage provider backed by cache
+func NewProvider() pvtdatastorage.Provider {
+	logger.Debugf("constructing cached private data storage provider")
+	return &provider{}
+}
+
+// OpenStore returns a handle to a store
+func (p *provider) OpenStore(ledgerid string) (pvtdatastorage.Store, error) {
+	s := &store{cache: gcache.New(config.GetPvtDataCacheSize()).ARC().Build(), ledgerid: ledgerid,
+		pendingPvtData:     &pendingPvtData{batchPending: false},
+		isEmpty:            true,
+		lastCommittedBlock: 0,
+	}
+
+	logger.Debugf("Pvtdata cache store opened. Initial state: isEmpty [%t], lastCommittedBlock [%d]",
+		s.isEmpty, s.lastCommittedBlock)
+
+	return s, nil
+}
+
+// Close closes the store
+func (p *provider) Close() {
+}
+
+//////// store functions  ////////////////
+//////////////////////////////////////////
+
+func (s *store) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
+	s.btlPolicy = btlPolicy
+}
+
+// Prepare implements the function in the interface `Store`
+func (s *store) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData, missingPvtData ledger.TxMissingPvtDataMap) error {
+	if !roles.IsCommitter() {
+		panic("calling Prepare on a peer that is not a committer")
+	}
+
+	if s.pendingPvtData.batchPending {
+		return pvtdatastorage.NewErrIllegalCall(`A pending batch exists as as result of last invoke to "Prepare" call. Invoke "Commit" or "Rollback" on the pending batch before invoking "Prepare" function`)
+	}
+
+	expectedBlockNum := s.nextBlockNum()
+	if expectedBlockNum != blockNum {
+		return pvtdatastorage.NewErrIllegalCall(fmt.Sprintf("Expected block number=%d, received block number=%d", expectedBlockNum, blockNum))
+	}
+
+	storeEntries, err := common.PrepareStoreEntries(blockNum, pvtData, s.btlPolicy, missingPvtData)
+	if err != nil {
+		return err
+	}
+
+	s.pendingPvtData = &pendingPvtData{batchPending: true}
+	if len(storeEntries.DataEntries) > 0 {
+		s.pendingPvtData.dataEntries = storeEntries.DataEntries
+	}
+	logger.Debugf("Saved %d private data write sets for block [%d]", len(pvtData), blockNum)
+	return nil
+}
+
+// Commit implements the function in the interface `Store`
+func (s *store) Commit() error {
+	if !roles.IsCommitter() {
+		panic("calling Commit on a peer that is not a committer")
+	}
+
+	committingBlockNum := s.nextBlockNum()
+	logger.Debugf("Committing private data for block [%d]", committingBlockNum)
+
+	if s.pendingPvtData.dataEntries != nil {
+		err := s.cache.Set(committingBlockNum, s.pendingPvtData.dataEntries)
+		if err != nil {
+			return errors.WithMessage(err, fmt.Sprintf("writing private data to cache failed [%d]", committingBlockNum))
+		}
+	}
+
+	s.pendingPvtData = &pendingPvtData{batchPending: false}
+	s.isEmpty = false
+	s.lastCommittedBlock = committingBlockNum
+
+	logger.Debugf("Committed private data for block [%d]", committingBlockNum)
+	return nil
+}
+
+// Rollback implements the function in the interface `Store`
+func (s *store) Rollback() error {
+	if !roles.IsCommitter() {
+		panic("calling Rollback on a peer that is not a committer")
+	}
+
+	s.pendingPvtData = &pendingPvtData{batchPending: false}
+	return nil
+}
+
+// CommitPvtDataOfOldBlocks implements the function in the interface `Store`
+func (s *store) CommitPvtDataOfOldBlocks(blocksPvtData map[uint64][]*ledger.TxPvtData) error {
+	return errors.New("not supported")
+}
+
+// GetLastUpdatedOldBlocksPvtData implements the function in the interface `Store`
+func (s *store) GetLastUpdatedOldBlocksPvtData() (map[uint64][]*ledger.TxPvtData, error) {
+	return nil, errors.New("not supported")
+}
+
+// ResetLastUpdatedOldBlocksList implements the function in the interface `Store`
+func (s *store) ResetLastUpdatedOldBlocksList() error {
+	return errors.New("not supported")
+}
+
+// GetPvtDataByBlockNum implements the function in the interface `Store`.
+// If the store is empty or the last committed block number is smaller then the
+// requested block number, an 'ErrOutOfRange' is thrown
+func (s *store) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	logger.Debugf("Get private data for block [%d], filter=%#v", blockNum, filter)
+	if s.isEmpty {
+		return nil, pvtdatastorage.NewErrOutOfRange("The store is empty")
+	}
+	if blockNum > s.lastCommittedBlock {
+		return nil, pvtdatastorage.NewErrOutOfRange(fmt.Sprintf("Last committed block=%d, block requested=%d", s.lastCommittedBlock, blockNum))
+	}
+
+	value, err := s.cache.Get(blockNum)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get must never return an error other than KeyNotFoundError err:%s", err))
+		}
+		return nil, nil
+	}
+
+	dataEntries := value.([]*common.DataEntry)
+
+	return s.getBlockPvtData(dataEntries, filter, blockNum)
+
+}
+
+// ProcessCollsEligibilityEnabled implements the function in the interface `Store`
+func (s *store) ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string) error {
+	return errors.New("not supported")
+}
+
+//GetMissingPvtDataInfoForMostRecentBlocks implements the function in the interface `Store`
+func (s *store) GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int) (ledger.MissingPvtDataInfo, error) {
+	return nil, errors.New("not supported")
+}
+
+// LastCommittedBlockHeight implements the function in the interface `Store`
+func (s *store) LastCommittedBlockHeight() (uint64, error) {
+	if s.isEmpty {
+		return 0, nil
+	}
+	return s.lastCommittedBlock + 1, nil
+}
+
+// HasPendingBatch implements the function in the interface `Store`
+func (s *store) HasPendingBatch() (bool, error) {
+	return s.pendingPvtData.batchPending, nil
+}
+
+// IsEmpty implements the function in the interface `Store`
+func (s *store) IsEmpty() (bool, error) {
+	return s.isEmpty, nil
+}
+
+// InitLastCommittedBlock implements the function in the interface `Store`
+func (s *store) InitLastCommittedBlock(blockNum uint64) error {
+	if !(s.isEmpty && !s.pendingPvtData.batchPending) {
+		return pvtdatastorage.NewErrIllegalCall("The private data store is not empty. InitLastCommittedBlock() function call is not allowed")
+	}
+	s.isEmpty = false
+	s.lastCommittedBlock = blockNum
+
+	logger.Debugf("InitLastCommittedBlock set to block [%d]", blockNum)
+	return nil
+}
+
+// Shutdown implements the function in the interface `Store`
+func (s *store) Shutdown() {
+	// do nothing
+}
+
+func v11RetrievePvtdata(dataEntries []*common.DataEntry, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	var blkPvtData []*ledger.TxPvtData
+	for _, dataEntry := range dataEntries {
+		value, err := common.EncodeDataValue(dataEntry.Value)
+		if err != nil {
+			return nil, err
+		}
+		pvtDatum, err := common.V11DecodeKV(common.EncodeDataKey(dataEntry.Key), value, filter)
+		if err != nil {
+			return nil, err
+		}
+		blkPvtData = append(blkPvtData, pvtDatum)
+	}
+	return blkPvtData, nil
+}
+
+func (s *store) nextBlockNum() uint64 {
+	if s.isEmpty {
+		return 0
+	}
+	return s.lastCommittedBlock + 1
+}
+
+func (s *store) getBlockPvtData(dataEntries []*common.DataEntry, filter ledger.PvtNsCollFilter, blockNum uint64) ([]*ledger.TxPvtData, error) {
+	var blockPvtdata []*ledger.TxPvtData
+	var currentTxNum uint64
+	var currentTxWsetAssember *common.TxPvtdataAssembler
+	firstItr := true
+
+	for _, dataEntry := range dataEntries {
+		dataKeyBytes := common.EncodeDataKey(dataEntry.Key)
+		if common.V11Format(dataKeyBytes) {
+			return v11RetrievePvtdata(dataEntries, filter)
+		}
+		expired, err := common.IsExpired(dataEntry.Key.NsCollBlk, s.btlPolicy, s.lastCommittedBlock)
+		if err != nil {
+			return nil, err
+		}
+		if expired || !common.PassesFilter(dataEntry.Key, filter) {
+			continue
+		}
+
+		if firstItr {
+			currentTxNum = dataEntry.Key.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+			firstItr = false
+		}
+
+		if dataEntry.Key.TxNum != currentTxNum {
+			blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+			currentTxNum = dataEntry.Key.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+		}
+		currentTxWsetAssember.Add(dataEntry.Key.Ns, dataEntry.Value)
+	}
+	if currentTxWsetAssember != nil {
+		blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+	}
+	return blockPvtdata, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/test_exports.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/test_exports.go
new file mode 100644
index 000000000..32554b243
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore/test_exports.go
@@ -0,0 +1,46 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cachedpvtdatastore
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/stretchr/testify/require"
+)
+
+// StoreEnv provides the  store env for testing
+type StoreEnv struct {
+	t                 testing.TB
+	TestStoreProvider pvtdatastorage.Provider
+	TestStore         pvtdatastorage.Store
+	ledgerid          string
+	btlPolicy         pvtdatapolicy.BTLPolicy
+}
+
+// NewTestStoreEnv construct a StoreEnv for testing
+func NewTestStoreEnv(t *testing.T, ledgerid string, btlPolicy pvtdatapolicy.BTLPolicy) *StoreEnv {
+	req := require.New(t)
+	testStoreProvider := NewProvider()
+	testStore, err := testStoreProvider.OpenStore(ledgerid)
+	req.NoError(err)
+	testStore.Init(btlPolicy)
+	s := &StoreEnv{t, testStoreProvider, testStore, ledgerid, btlPolicy}
+	return s
+}
+
+// CloseAndReopen closes and opens the store provider
+func (env *StoreEnv) CloseAndReopen() {
+	var err error
+	env.TestStoreProvider.Close()
+	env.TestStoreProvider = NewProvider()
+	require.NoError(env.t, err)
+	env.TestStore, err = env.TestStoreProvider.OpenStore(env.ledgerid)
+	env.TestStore.Init(env.btlPolicy)
+	require.NoError(env.t, err)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go
new file mode 100644
index 000000000..24a3166c7
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go
@@ -0,0 +1,277 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"strconv"
+
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common"
+)
+
+const (
+	idField                       = "_id"
+	revField                      = "_rev"
+	dataField                     = "data"
+	expiryField                   = "expiry"
+	expiringBlkNumsField          = "expiringBlkNums"
+	expiringBlockNumbersIndexName = "by_expiring_block_numbers"
+	expiringBlockNumbersIndexDoc  = "indexExpiringBlockNumbers"
+	blockKeyPrefix                = ""
+	lastCommittedBlockID          = "lastCommittedBlock"
+	lastCommittedBlockData        = "data"
+)
+
+type blockPvtDataResponse struct {
+	ID              string            `json:"_id"`
+	Rev             string            `json:"_rev"`
+	Data            map[string][]byte `json:"data"`
+	Expiry          map[string][]byte `json:"expiry"`
+	Deleted         bool              `json:"_deleted"`
+	ExpiringBlkNums []string          `json:"expiringBlkNums"`
+}
+
+type lastCommittedBlockResponse struct {
+	ID   string `json:"_id"`
+	Rev  string `json:"_rev"`
+	Data string `json:"data"`
+}
+
+const expiringBlockNumbersIndexDef = `
+	{
+		"index": {
+			"fields": ["` + expiringBlkNumsField + `"]
+		},
+		"name": "` + expiringBlockNumbersIndexName + `",
+		"ddoc": "` + expiringBlockNumbersIndexDoc + `",
+		"type": "json"
+	}`
+
+type jsonValue map[string]interface{}
+
+func (v jsonValue) toBytes() ([]byte, error) {
+	return json.Marshal(v)
+}
+
+func createPvtDataCouchDoc(storeEntries *common.StoreEntries, blockNumber uint64, rev string) (*couchdb.CouchDoc, error) {
+	if len(storeEntries.DataEntries) <= 0 && len(storeEntries.ExpiryEntries) <= 0 {
+		return nil, nil
+	}
+	jsonMap := make(jsonValue)
+	jsonMap[idField] = blockNumberToKey(blockNumber)
+
+	if rev != "" {
+		jsonMap[revField] = rev
+	}
+
+	dataEntriesJSON, err := dataEntriesToJSONValue(storeEntries.DataEntries)
+	if err != nil {
+		return nil, err
+	}
+	jsonMap[dataField] = dataEntriesJSON
+
+	expiryEntriesJSON, expiringBlkNums, err := expiryEntriesToJSONValue(storeEntries.ExpiryEntries)
+	if err != nil {
+		return nil, err
+	}
+	jsonMap[expiryField] = expiryEntriesJSON
+
+	jsonMap[expiringBlkNumsField] = expiringBlkNums
+
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	return &couchDoc, nil
+
+}
+
+func createLastCommittedBlockDoc(committingBlockNum uint64, rev string) (*couchdb.CouchDoc, error) {
+	jsonMap := make(jsonValue)
+	jsonMap[idField] = lastCommittedBlockID
+	if rev != "" {
+		jsonMap[revField] = rev
+	}
+	jsonMap[lastCommittedBlockData] = strconv.FormatUint(committingBlockNum, 10)
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	return &couchDoc, nil
+
+}
+
+func lookupLastBlock(db *couchdb.CouchDatabase) (uint64, string, error) {
+	v, _, err := db.ReadDoc(lastCommittedBlockID)
+	if err != nil {
+		return 0, "", err
+	}
+	if v != nil {
+		var lastBlockResponse lastCommittedBlockResponse
+		if err = json.Unmarshal(v.JSONValue, &lastBlockResponse); err != nil {
+			return 0, "", err
+		}
+		lastBlockNum, err := strconv.ParseInt(lastBlockResponse.Data, 10, 64)
+		if err != nil {
+			return 0, "", err
+		}
+		return uint64(lastBlockNum), lastBlockResponse.Rev, nil
+	}
+	return 0, "", nil
+}
+
+func dataEntriesToJSONValue(dataEntries []*common.DataEntry) (jsonValue, error) {
+	data := make(jsonValue)
+
+	for _, dataEntry := range dataEntries {
+		keyBytes := common.EncodeDataKey(dataEntry.Key)
+		valBytes, err := common.EncodeDataValue(dataEntry.Value)
+		if err != nil {
+			return nil, err
+		}
+
+		keyBytesHex := hex.EncodeToString(keyBytes)
+		data[keyBytesHex] = valBytes
+	}
+
+	return data, nil
+}
+
+func expiryEntriesToJSONValue(expiryEntries []*common.ExpiryEntry) (jsonValue, []string, error) {
+	data := make(jsonValue)
+	expiringBlkNums := make([]string, 0)
+
+	for _, expEntry := range expiryEntries {
+		keyBytes := common.EncodeExpiryKey(expEntry.Key)
+		valBytes, err := common.EncodeExpiryValue(expEntry.Value)
+		if err != nil {
+			return nil, nil, err
+		}
+		expiringBlkNums = append(expiringBlkNums, blockNumberToKey(expEntry.Key.ExpiringBlk))
+		keyBytesHex := hex.EncodeToString(keyBytes)
+		data[keyBytesHex] = valBytes
+	}
+
+	return data, expiringBlkNums, nil
+}
+
+func createPvtDataCouchDB(couchInstance *couchdb.CouchInstance, dbName string) (*couchdb.CouchDatabase, error) {
+	db, err := couchdb.CreateCouchDatabase(couchInstance, dbName)
+	if err != nil {
+		return nil, err
+	}
+	err = db.CreateNewIndexWithRetry(expiringBlockNumbersIndexDef, expiringBlockNumbersIndexDoc)
+	if err != nil {
+		return nil, errors.WithMessage(err, "creation of purge block number index failed")
+	}
+	return db, err
+}
+
+func getPvtDataCouchInstance(couchInstance *couchdb.CouchInstance, dbName string) (*couchdb.CouchDatabase, error) {
+	db, err := couchdb.NewCouchDatabase(couchInstance, dbName)
+	if err != nil {
+		return nil, err
+	}
+
+	dbExists, err := db.ExistsWithRetry()
+	if err != nil {
+		return nil, err
+	}
+	if !dbExists {
+		return nil, errors.Errorf("DB not found: [%s]", db.DBName)
+	}
+
+	indexExists, err := db.IndexDesignDocExistsWithRetry(expiringBlockNumbersIndexDoc)
+	if err != nil {
+		return nil, err
+	}
+	if !indexExists {
+		return nil, errors.Errorf("DB index not found: [%s]", db.DBName)
+	}
+	return db, nil
+}
+
+func retrieveBlockPvtData(db *couchdb.CouchDatabase, id string) (*blockPvtDataResponse, error) {
+	doc, _, err := db.ReadDoc(id)
+	if err != nil {
+		return nil, err
+	}
+
+	if doc == nil {
+		return nil, NewErrNotFoundInIndex()
+	}
+
+	var blockPvtData blockPvtDataResponse
+	err = json.Unmarshal(doc.JSONValue, &blockPvtData)
+	if err != nil {
+		return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+	}
+
+	return &blockPvtData, nil
+}
+
+func retrieveBlockExpiryData(db *couchdb.CouchDatabase, id string) ([]*blockPvtDataResponse, error) {
+	const queryFmt = `
+	{
+		"selector": {
+			"` + expiringBlkNumsField + `": {
+				"$elemMatch": {
+					"$lte": "%s"
+				}
+			}
+		},
+		"use_index": ["_design/` + expiringBlockNumbersIndexDoc + `", "` + expiringBlockNumbersIndexName + `"]
+	}`
+
+	results, _, err := db.QueryDocuments(fmt.Sprintf(queryFmt, id))
+	if err != nil {
+		return nil, err
+	}
+
+	if len(results) == 0 {
+		return nil, nil
+	}
+
+	var responses []*blockPvtDataResponse
+	for _, result := range results {
+		var blockPvtData blockPvtDataResponse
+		err = json.Unmarshal(result.Value, &blockPvtData)
+		if err != nil {
+			return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+		}
+		responses = append(responses, &blockPvtData)
+	}
+
+	return responses, nil
+}
+
+func blockNumberToKey(blockNum uint64) string {
+	return fmt.Sprintf("%064s", blockKeyPrefix+strconv.FormatUint(blockNum, 10))
+}
+
+// NotFoundInIndexErr is used to indicate missing entry in the index
+type NotFoundInIndexErr struct {
+}
+
+// NewErrNotFoundInIndex creates an missing entry in the index error
+func NewErrNotFoundInIndex() *NotFoundInIndexErr {
+	return &NotFoundInIndexErr{}
+}
+
+func (err *NotFoundInIndexErr) Error() string {
+	return "Entry not found in index"
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/store_impl.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/store_impl.go
new file mode 100644
index 000000000..083e87059
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/store_impl.go
@@ -0,0 +1,851 @@
+/*
+Copyright IBM Corp, SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"sort"
+	"strings"
+	"sync"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common"
+	"github.com/trustbloc/fabric-peer-ext/pkg/roles"
+	"github.com/willf/bitset"
+)
+
+var logger = flogging.MustGetLogger("cdbpvtdatastore")
+
+const (
+	pvtDataStoreName = "pvtdata"
+)
+
+type provider struct {
+	couchInstance            *couchdb.CouchInstance
+	missingKeysIndexProvider *leveldbhelper.Provider
+	ledgerConfig             *ledger.Config
+}
+
+type store struct {
+	ledgerid           string
+	btlPolicy          pvtdatapolicy.BTLPolicy
+	db                 *couchdb.CouchDatabase
+	lastCommittedBlock uint64
+	purgerLock         *sync.Mutex
+	pendingPvtData     *pendingPvtData
+	collElgProc        *common.CollElgProc
+	// missing keys db
+	missingKeysIndexDB *leveldbhelper.DBHandle
+	isEmpty            bool
+	// After committing the pvtdata of old blocks,
+	// the `isLastUpdatedOldBlocksSet` is set to true.
+	// Once the stateDB is updated with these pvtdata,
+	// the `isLastUpdatedOldBlocksSet` is set to false.
+	// isLastUpdatedOldBlocksSet is mainly used during the
+	// recovery process. During the peer startup, if the
+	// isLastUpdatedOldBlocksSet is set to true, the pvtdata
+	// in the stateDB needs to be updated before finishing the
+	// recovery operation.
+	isLastUpdatedOldBlocksSet bool
+	ledgerConfig              *ledger.Config
+}
+
+type pendingPvtData struct {
+	PvtDataDoc         *couchdb.CouchDoc `json:"pvtDataDoc"`
+	MissingDataEntries map[string]string `json:"missingDataEntries"`
+	BatchPending       bool              `json:"batchPending"`
+}
+
+// lastUpdatedOldBlocksList keeps the list of last updated blocks
+// and is stored as the value of lastUpdatedOldBlocksKey (defined in kv_encoding.go)
+type lastUpdatedOldBlocksList []uint64
+
+//////// Provider functions  /////////////
+//////////////////////////////////////////
+
+// NewProvider instantiates a private data storage provider backed by CouchDB
+func NewProvider(conf *ledger.PrivateData, ledgerconfig *ledger.Config) (pvtdatastorage.Provider, error) {
+	logger.Debugf("constructing CouchDB private data storage provider")
+	couchDBConfig := ledgerconfig.StateDB.CouchDB
+
+	return newProviderWithDBDef(couchDBConfig, conf, ledgerconfig)
+}
+
+func newProviderWithDBDef(couchDBConfig *couchdb.Config, conf *ledger.PrivateData, ledgerconfig *ledger.Config) (pvtdatastorage.Provider, error) {
+	couchInstance, err := couchdb.CreateCouchInstance(couchDBConfig, &disabled.Provider{})
+	if err != nil {
+		return nil, errors.WithMessage(err, "obtaining CouchDB instance failed")
+	}
+
+	dbPath := conf.StorePath
+	missingKeysIndexProvider := leveldbhelper.NewProvider(&leveldbhelper.Conf{DBPath: dbPath})
+
+	return &provider{couchInstance, missingKeysIndexProvider, ledgerconfig}, nil
+}
+
+// OpenStore returns a handle to a store
+func (p *provider) OpenStore(ledgerid string) (pvtdatastorage.Store, error) {
+	// Create couchdb
+	pvtDataStoreDBName := couchdb.ConstructBlockchainDBName(strings.ToLower(ledgerid), pvtDataStoreName)
+	var db *couchdb.CouchDatabase
+	var err error
+	if roles.IsCommitter() {
+		db, err = createPvtDataCouchDB(p.couchInstance, pvtDataStoreDBName)
+		if err != nil {
+			return nil, err
+		}
+		// Create missing pvt keys index in leveldb
+		missingKeysIndexDB := p.missingKeysIndexProvider.GetDBHandle(ledgerid)
+
+		purgerLock := &sync.Mutex{}
+		s := &store{db: db, ledgerid: ledgerid,
+			collElgProc:        common.NewCollElgProc(purgerLock, missingKeysIndexDB, p.ledgerConfig),
+			purgerLock:         purgerLock,
+			missingKeysIndexDB: missingKeysIndexDB,
+			pendingPvtData:     &pendingPvtData{BatchPending: false},
+			ledgerConfig:       p.ledgerConfig,
+		}
+
+		if errInitState := s.initState(); errInitState != nil {
+			return nil, errInitState
+		}
+		s.collElgProc.LaunchCollElgProc()
+
+		logger.Debugf("Pvtdata store opened. Initial state: isEmpty [%t], lastCommittedBlock [%d]",
+			s.isEmpty, s.lastCommittedBlock)
+
+		return s, nil
+	}
+
+	db, err = getPvtDataCouchInstance(p.couchInstance, pvtDataStoreDBName)
+	if err != nil {
+		return nil, err
+	}
+	s := &store{db: db, ledgerid: ledgerid,
+		pendingPvtData: &pendingPvtData{BatchPending: false},
+	}
+	lastCommittedBlock, _, err := lookupLastBlock(db)
+	if err != nil {
+		return nil, err
+	}
+	s.isEmpty = true
+	if lastCommittedBlock != 0 {
+		s.lastCommittedBlock = lastCommittedBlock
+		s.isEmpty = false
+	}
+	return s, nil
+
+}
+
+// Close closes the store
+func (p *provider) Close() {
+	p.missingKeysIndexProvider.Close()
+}
+
+//////// store functions  ////////////////
+//////////////////////////////////////////
+
+func (s *store) initState() error {
+	var blist lastUpdatedOldBlocksList
+	lastCommittedBlock, _, err := lookupLastBlock(s.db)
+	if err != nil {
+		return err
+	}
+	s.isEmpty = true
+	if lastCommittedBlock != 0 {
+		s.lastCommittedBlock = lastCommittedBlock
+		s.isEmpty = false
+	}
+
+	if s.pendingPvtData, err = s.hasPendingCommit(); err != nil {
+		return err
+	}
+
+	if blist, err = common.GetLastUpdatedOldBlocksList(s.missingKeysIndexDB); err != nil {
+		return err
+	}
+	if len(blist) > 0 {
+		s.isLastUpdatedOldBlocksSet = true
+	} // false if not set
+
+	return nil
+}
+
+func (s *store) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
+	s.btlPolicy = btlPolicy
+}
+
+// Prepare implements the function in the interface `Store`
+func (s *store) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData, missingPvtData ledger.TxMissingPvtDataMap) error {
+	if !roles.IsCommitter() {
+		panic("calling Prepare on a peer that is not a committer")
+	}
+
+	if s.pendingPvtData.BatchPending {
+		return pvtdatastorage.NewErrIllegalCall(`A pending batch exists as as result of last invoke to "Prepare" call. Invoke "Commit" or "Rollback" on the pending batch before invoking "Prepare" function`)
+	}
+
+	expectedBlockNum := s.nextBlockNum()
+	if expectedBlockNum != blockNum {
+		return pvtdatastorage.NewErrIllegalCall(fmt.Sprintf("Expected block number=%d, received block number=%d", expectedBlockNum, blockNum))
+	}
+
+	storeEntries, err := common.PrepareStoreEntries(blockNum, pvtData, s.btlPolicy, missingPvtData)
+	if err != nil {
+		return err
+	}
+
+	pvtDataDoc, err := createPvtDataCouchDoc(storeEntries, blockNum, "")
+	if err != nil {
+		return err
+	}
+
+	s.pendingPvtData = &pendingPvtData{BatchPending: true}
+	if pvtDataDoc != nil || len(storeEntries.MissingDataEntries) > 0 {
+		s.pendingPvtData.MissingDataEntries, err = s.perparePendingMissingDataEntries(storeEntries.MissingDataEntries)
+		if err != nil {
+			return err
+		}
+		s.pendingPvtData.PvtDataDoc = pvtDataDoc
+		if err := s.savePendingKey(); err != nil {
+			return err
+		}
+
+	}
+	logger.Debugf("Saved %d private data write sets for block [%d]", len(pvtData), blockNum)
+	return nil
+}
+
+// Commit implements the function in the interface `Store`
+func (s *store) Commit() error {
+	if !roles.IsCommitter() {
+		panic("calling Commit on a peer that is not a committer")
+	}
+
+	if !s.pendingPvtData.BatchPending {
+		return pvtdatastorage.NewErrIllegalCall("No pending batch to commit")
+	}
+
+	committingBlockNum := s.nextBlockNum()
+	logger.Debugf("Committing private data for block [%d]", committingBlockNum)
+
+	var docs []*couchdb.CouchDoc
+	if s.pendingPvtData.PvtDataDoc != nil {
+		docs = append(docs, s.pendingPvtData.PvtDataDoc)
+	}
+
+	lastCommittedBlockDoc, err := s.prepareLastCommittedBlockDoc(committingBlockNum)
+	if err != nil {
+		return err
+	}
+	docs = append(docs, lastCommittedBlockDoc)
+
+	_, err = s.db.BatchUpdateDocuments(docs)
+	if err != nil {
+		return errors.WithMessage(err, fmt.Sprintf("writing private data to CouchDB failed [%d]", committingBlockNum))
+	}
+
+	batch := leveldbhelper.NewUpdateBatch()
+	if len(s.pendingPvtData.MissingDataEntries) > 0 {
+		for missingDataKey, missingDataValue := range s.pendingPvtData.MissingDataEntries {
+			batch.Put([]byte(missingDataKey), []byte(missingDataValue))
+		}
+		if err := s.missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+			return err
+		}
+	}
+
+	batch.Delete(common.PendingCommitKey)
+	if err := s.missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+
+	s.pendingPvtData = &pendingPvtData{BatchPending: false}
+	s.isEmpty = false
+	s.lastCommittedBlock = committingBlockNum
+
+	logger.Debugf("Committed private data for block [%d]", committingBlockNum)
+	s.performPurgeIfScheduled(committingBlockNum)
+	return nil
+}
+
+func (s *store) prepareLastCommittedBlockDoc(committingBlockNum uint64) (*couchdb.CouchDoc, error) {
+	_, rev, err := lookupLastBlock(s.db)
+	if err != nil {
+		return nil, err
+	}
+	lastCommittedBlockDoc, err := createLastCommittedBlockDoc(committingBlockNum, rev)
+	if err != nil {
+		return nil, err
+	}
+	return lastCommittedBlockDoc, nil
+}
+
+// Rollback implements the function in the interface `Store`
+func (s *store) Rollback() error {
+	if !roles.IsCommitter() {
+		panic("calling Rollback on a peer that is not a committer")
+	}
+
+	if !s.pendingPvtData.BatchPending {
+		return pvtdatastorage.NewErrIllegalCall("No pending batch to rollback")
+	}
+
+	s.pendingPvtData = &pendingPvtData{BatchPending: false}
+	if err := s.missingKeysIndexDB.Delete(common.PendingCommitKey, true); err != nil {
+		return err
+	}
+	return nil
+}
+
+// CommitPvtDataOfOldBlocks commits the pvtData (i.e., previously missing data) of old blocks.
+// The parameter `blocksPvtData` refers a list of old block's pvtdata which are missing in the pvtstore.
+// Given a list of old block's pvtData, `CommitPvtDataOfOldBlocks` performs the following four
+// operations
+// (1) construct dataEntries for all pvtData
+// (2) construct update entries (i.e., dataEntries, expiryEntries, missingDataEntries, and
+//     lastUpdatedOldBlocksList) from the above created data entries
+// (3) create a db update batch from the update entries
+// (4) commit the update entries to the pvtStore
+func (s *store) CommitPvtDataOfOldBlocks(blocksPvtData map[uint64][]*ledger.TxPvtData) error {
+	if s.isLastUpdatedOldBlocksSet {
+		return pvtdatastorage.NewErrIllegalCall(`The lastUpdatedOldBlocksList is set. It means that the
+		stateDB may not be in sync with the pvtStore`)
+	}
+
+	batch := leveldbhelper.NewUpdateBatch()
+	docs := make([]*couchdb.CouchDoc, 0)
+	// create a list of blocks' pvtData which are being stored. If this list is
+	// found during the recovery, the stateDB may not be in sync with the pvtData
+	// and needs recovery. In a normal flow, once the stateDB is synced, the
+	// block list would be deleted.
+	updatedBlksListMap := make(map[uint64]bool)
+	// (1) construct dataEntries for all pvtData
+	entries := common.ConstructDataEntriesFromBlocksPvtData(blocksPvtData)
+
+	for blockNum, value := range entries {
+		// (2) construct update entries (i.e., dataEntries, expiryEntries, missingDataEntries) from the above created data entries
+		logger.Debugf("Constructing pvtdatastore entries for pvtData of [%d] old blocks", len(blocksPvtData))
+		updateEntries, err := common.ConstructUpdateEntriesFromDataEntries(value, s.btlPolicy, s.getExpiryDataOfExpiryKey, s.getBitmapOfMissingDataKey)
+		if err != nil {
+			return err
+		}
+		// (3) create a db update batch from the update entries
+		logger.Debug("Constructing update batch from pvtdatastore entries")
+		batch, err = common.ConstructUpdateBatchFromUpdateEntries(updateEntries, batch)
+		if err != nil {
+			return err
+		}
+		pvtDataDoc, err := s.preparePvtDataDoc(blockNum, updateEntries)
+		if err != nil {
+			return err
+		}
+		if pvtDataDoc != nil {
+			docs = append(docs, pvtDataDoc)
+		}
+		updatedBlksListMap[blockNum] = true
+	}
+	if err := s.addLastUpdatedOldBlocksList(batch, updatedBlksListMap); err != nil {
+		return err
+	}
+	// (4) commit the update entries to the pvtStore
+	logger.Debug("Committing the update batch to pvtdatastore")
+	if _, err := s.db.BatchUpdateDocuments(docs); err != nil {
+		return err
+	}
+	if err := s.missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+	s.isLastUpdatedOldBlocksSet = true
+
+	return nil
+}
+
+// GetLastUpdatedOldBlocksPvtData implements the function in the interface `Store`
+func (s *store) GetLastUpdatedOldBlocksPvtData() (map[uint64][]*ledger.TxPvtData, error) {
+	if !s.isLastUpdatedOldBlocksSet {
+		return nil, nil
+	}
+
+	updatedBlksList, err := common.GetLastUpdatedOldBlocksList(s.missingKeysIndexDB)
+	if err != nil {
+		return nil, err
+	}
+
+	blksPvtData := make(map[uint64][]*ledger.TxPvtData)
+	for _, blkNum := range updatedBlksList {
+		if blksPvtData[blkNum], err = s.GetPvtDataByBlockNum(blkNum, nil); err != nil {
+			return nil, err
+		}
+	}
+	return blksPvtData, nil
+}
+
+// ResetLastUpdatedOldBlocksList implements the function in the interface `Store`
+func (s *store) ResetLastUpdatedOldBlocksList() error {
+	if err := common.ResetLastUpdatedOldBlocksList(s.missingKeysIndexDB); err != nil {
+		return err
+	}
+	s.isLastUpdatedOldBlocksSet = false
+	return nil
+}
+
+// GetPvtDataByBlockNum implements the function in the interface `Store`.
+// If the store is empty or the last committed block number is smaller then the
+// requested block number, an 'ErrOutOfRange' is thrown
+func (s *store) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	logger.Debugf("Get private data for block [%d], filter=%#v", blockNum, filter)
+
+	if err := s.checkLastCommittedBlock(blockNum); err != nil {
+		return nil, err
+	}
+
+	blockPvtDataResponse, err := retrieveBlockPvtData(s.db, blockNumberToKey(blockNum))
+	if err != nil {
+		_, ok := err.(*NotFoundInIndexErr)
+		if ok {
+			return nil, nil
+		}
+		return nil, err
+	}
+
+	var sortedKeys []string
+	for key := range blockPvtDataResponse.Data {
+		sortedKeys = append(sortedKeys, key)
+	}
+	sort.Strings(sortedKeys)
+
+	return s.getBlockPvtData(blockPvtDataResponse.Data, filter, blockNum, sortedKeys)
+
+}
+
+func (s *store) checkLastCommittedBlock(blockNum uint64) error {
+	if roles.IsCommitter() {
+		if s.isEmpty {
+			return pvtdatastorage.NewErrOutOfRange("The store is empty")
+		}
+		if blockNum > s.lastCommittedBlock {
+			return pvtdatastorage.NewErrOutOfRange(fmt.Sprintf("Last committed block=%d, block requested=%d", s.lastCommittedBlock, blockNum))
+		}
+	} else {
+		lastCommittedBlock, _, err := lookupLastBlock(s.db)
+		if err != nil {
+			return err
+		}
+		if lastCommittedBlock == 0 {
+			return pvtdatastorage.NewErrOutOfRange("The store is empty")
+		}
+		if blockNum > lastCommittedBlock {
+			return pvtdatastorage.NewErrOutOfRange(fmt.Sprintf("Last committed block=%d, block requested=%d", s.lastCommittedBlock, blockNum))
+		}
+	}
+	return nil
+}
+
+// ProcessCollsEligibilityEnabled implements the function in the interface `Store`
+func (s *store) ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string) error {
+	return common.ProcessCollsEligibilityEnabled(committingBlk, nsCollMap, s.collElgProc, s.missingKeysIndexDB)
+}
+
+// LastCommittedBlockHeight implements the function in the interface `Store`
+func (s *store) LastCommittedBlockHeight() (uint64, error) {
+	if s.isEmpty {
+		return 0, nil
+	}
+	return s.lastCommittedBlock + 1, nil
+}
+
+// HasPendingBatch implements the function in the interface `Store`
+func (s *store) HasPendingBatch() (bool, error) {
+	return s.pendingPvtData.BatchPending, nil
+}
+
+// IsEmpty implements the function in the interface `Store`
+func (s *store) IsEmpty() (bool, error) {
+	return s.isEmpty, nil
+}
+
+// Shutdown implements the function in the interface `Store`
+func (s *store) Shutdown() {
+	// do nothing
+}
+
+func (s *store) preparePvtDataDoc(blockNum uint64, updateEntries *common.EntriesForPvtDataOfOldBlocks) (*couchdb.CouchDoc, error) {
+	dataEntries, expiryEntries, rev, err := s.retrieveBlockPvtEntries(blockNum)
+	if err != nil {
+		return nil, err
+	}
+	pvtDataDoc, err := createPvtDataCouchDoc(s.prepareStoreEntries(updateEntries, dataEntries, expiryEntries), blockNum, rev)
+	if err != nil {
+		return nil, err
+	}
+	return pvtDataDoc, nil
+}
+
+func (s *store) retrieveBlockPvtEntries(blockNum uint64) ([]*common.DataEntry, []*common.ExpiryEntry, string, error) {
+	rev := ""
+	var dataEntries []*common.DataEntry
+	var expiryEntries []*common.ExpiryEntry
+	blockPvtDataResponse, err := retrieveBlockPvtData(s.db, blockNumberToKey(blockNum))
+	if err != nil {
+		_, ok := err.(*NotFoundInIndexErr)
+		if ok {
+			return nil, nil, "", nil
+		}
+		return nil, nil, "", err
+	}
+
+	if blockPvtDataResponse != nil {
+		rev = blockPvtDataResponse.Rev
+		for key := range blockPvtDataResponse.Data {
+			dataKeyBytes, errDecodeString := hex.DecodeString(key)
+			if errDecodeString != nil {
+				return nil, nil, "", errDecodeString
+			}
+			dataKey := common.DecodeDatakey(dataKeyBytes)
+			dataValue, err := common.DecodeDataValue(blockPvtDataResponse.Data[key])
+			if err != nil {
+				return nil, nil, "", err
+			}
+			dataEntries = append(dataEntries, &common.DataEntry{Key: dataKey, Value: dataValue})
+		}
+		for key := range blockPvtDataResponse.Expiry {
+			expiryKeyBytes, err := hex.DecodeString(key)
+			if err != nil {
+				return nil, nil, "", err
+			}
+			expiryKey := common.DecodeExpiryKey(expiryKeyBytes)
+			expiryValue, err := common.DecodeExpiryValue(blockPvtDataResponse.Expiry[key])
+			if err != nil {
+				return nil, nil, "", err
+			}
+			expiryEntries = append(expiryEntries, &common.ExpiryEntry{Key: expiryKey, Value: expiryValue})
+		}
+	}
+	return dataEntries, expiryEntries, rev, nil
+}
+
+func (s *store) addLastUpdatedOldBlocksList(batch *leveldbhelper.UpdateBatch, updatedBlksListMap map[uint64]bool) error {
+	var updatedBlksList lastUpdatedOldBlocksList
+	for blkNum := range updatedBlksListMap {
+		updatedBlksList = append(updatedBlksList, blkNum)
+	}
+
+	// better to store as sorted list
+	sort.SliceStable(updatedBlksList, func(i, j int) bool {
+		return updatedBlksList[i] < updatedBlksList[j]
+	})
+
+	buf := proto.NewBuffer(nil)
+	if err := buf.EncodeVarint(uint64(len(updatedBlksList))); err != nil {
+		return err
+	}
+	for _, blkNum := range updatedBlksList {
+		if err := buf.EncodeVarint(blkNum); err != nil {
+			return err
+		}
+	}
+
+	batch.Put(common.LastUpdatedOldBlocksKey, buf.Bytes())
+	return nil
+}
+
+func (s *store) getBlockPvtData(results map[string][]byte, filter ledger.PvtNsCollFilter, blockNum uint64, sortedKeys []string) ([]*ledger.TxPvtData, error) {
+	var blockPvtdata []*ledger.TxPvtData
+	var currentTxNum uint64
+	var currentTxWsetAssember *common.TxPvtdataAssembler
+	firstItr := true
+
+	for _, key := range sortedKeys {
+		dataKeyBytes, err := hex.DecodeString(key)
+		if err != nil {
+			return nil, err
+		}
+		if common.V11Format(dataKeyBytes) {
+			return v11RetrievePvtdata(results, filter)
+		}
+		dataValueBytes := results[key]
+		dataKey := common.DecodeDatakey(dataKeyBytes)
+		expired, err := s.checkIsExpired(dataKey, filter, s.lastCommittedBlock)
+		if err != nil {
+			return nil, err
+		}
+		if expired {
+			continue
+		}
+
+		dataValue, err := common.DecodeDataValue(dataValueBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		if firstItr {
+			currentTxNum = dataKey.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+			firstItr = false
+		}
+
+		if dataKey.TxNum != currentTxNum {
+			blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+			currentTxNum = dataKey.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+		}
+		currentTxWsetAssember.Add(dataKey.Ns, dataValue)
+	}
+	if currentTxWsetAssember != nil {
+		blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+	}
+	return blockPvtdata, nil
+}
+
+func (s *store) checkIsExpired(dataKey *common.DataKey, filter ledger.PvtNsCollFilter, lastCommittedBlock uint64) (bool, error) {
+	expired, err := common.IsExpired(dataKey.NsCollBlk, s.btlPolicy, lastCommittedBlock)
+	if err != nil {
+		return false, err
+	}
+	if expired || !common.PassesFilter(dataKey, filter) {
+		return true, nil
+	}
+	return false, nil
+}
+
+// InitLastCommittedBlock implements the function in the interface `Store`
+func (s *store) InitLastCommittedBlock(blockNum uint64) error {
+	if !(s.isEmpty && !s.pendingPvtData.BatchPending) {
+		return pvtdatastorage.NewErrIllegalCall("The private data store is not empty. InitLastCommittedBlock() function call is not allowed")
+	}
+	s.isEmpty = false
+	s.lastCommittedBlock = blockNum
+
+	_, rev, err := lookupLastBlock(s.db)
+	if err != nil {
+		return err
+	}
+	lastCommittedBlockDoc, err := createLastCommittedBlockDoc(s.lastCommittedBlock, rev)
+	if err != nil {
+		return err
+	}
+	_, err = s.db.BatchUpdateDocuments([]*couchdb.CouchDoc{lastCommittedBlockDoc})
+	if err != nil {
+		return err
+	}
+
+	logger.Debugf("InitLastCommittedBlock set to block [%d]", blockNum)
+	return nil
+}
+
+//GetMissingPvtDataInfoForMostRecentBlocks implements the function in the interface `Store`
+func (s *store) GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int) (ledger.MissingPvtDataInfo, error) {
+	return common.GetMissingPvtDataInfoForMostRecentBlocks(maxBlock, s.lastCommittedBlock, s.btlPolicy, s.missingKeysIndexDB)
+}
+
+func v11RetrievePvtdata(pvtDataResults map[string][]byte, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	var blkPvtData []*ledger.TxPvtData
+	for key, val := range pvtDataResults {
+		pvtDatum, err := common.V11DecodeKV([]byte(key), val, filter)
+		if err != nil {
+			return nil, err
+		}
+		blkPvtData = append(blkPvtData, pvtDatum)
+	}
+	return blkPvtData, nil
+}
+
+func (s *store) getExpiryDataOfExpiryKey(expiryKey *common.ExpiryKey) (*common.ExpiryData, error) {
+	var expiryEntriesMap map[string][]byte
+	var err error
+	if expiryEntriesMap, err = s.getExpiryEntriesDB(expiryKey.CommittingBlk); err != nil {
+		return nil, err
+	}
+	v := expiryEntriesMap[hex.EncodeToString(common.EncodeExpiryKey(expiryKey))]
+	if v == nil {
+		return nil, nil
+	}
+	return common.DecodeExpiryValue(v)
+}
+
+func (s *store) getExpiryEntriesDB(blockNum uint64) (map[string][]byte, error) {
+	blockPvtData, err := retrieveBlockPvtData(s.db, blockNumberToKey(blockNum))
+	if err != nil {
+		return nil, err
+	}
+	return blockPvtData.Expiry, nil
+}
+
+func (s *store) getBitmapOfMissingDataKey(missingDataKey *common.MissingDataKey) (*bitset.BitSet, error) {
+	var v []byte
+	var err error
+	if v, err = s.missingKeysIndexDB.Get(common.EncodeMissingDataKey(missingDataKey)); err != nil {
+		return nil, err
+	}
+	if v == nil {
+		return nil, nil
+	}
+	return common.DecodeMissingDataValue(v)
+}
+
+func (s *store) prepareStoreEntries(updateEntries *common.EntriesForPvtDataOfOldBlocks, dataEntries []*common.DataEntry, expiryEntries []*common.ExpiryEntry) *common.StoreEntries {
+	if dataEntries == nil {
+		dataEntries = make([]*common.DataEntry, 0)
+	}
+	if expiryEntries == nil {
+		expiryEntries = make([]*common.ExpiryEntry, 0)
+	}
+	for k, v := range updateEntries.DataEntries {
+		k := k
+		dataEntries = append(dataEntries, &common.DataEntry{Key: &k, Value: v})
+	}
+	for k, v := range updateEntries.ExpiryEntries {
+		k := k
+		expiryEntries = append(expiryEntries, &common.ExpiryEntry{Key: &k, Value: v})
+	}
+	return &common.StoreEntries{DataEntries: dataEntries, ExpiryEntries: expiryEntries}
+}
+
+func (s *store) hasPendingCommit() (*pendingPvtData, error) {
+	var v []byte
+	var err error
+	if v, err = s.missingKeysIndexDB.Get(common.PendingCommitKey); err != nil {
+		return nil, err
+	}
+	if v != nil {
+		var pPvtData pendingPvtData
+		if err := json.Unmarshal(v, &pPvtData); err != nil {
+			return nil, err
+		}
+		return &pPvtData, nil
+	}
+	return &pendingPvtData{BatchPending: false}, nil
+
+}
+
+func (s *store) savePendingKey() error {
+	bytes, err := json.Marshal(s.pendingPvtData)
+	if err != nil {
+		return err
+	}
+	if err := s.missingKeysIndexDB.Put(common.PendingCommitKey, bytes, true); err != nil {
+		return err
+	}
+	return nil
+}
+
+func (s *store) perparePendingMissingDataEntries(mssingDataEntries map[common.MissingDataKey]*bitset.BitSet) (map[string]string, error) {
+	pendingMissingDataEntries := make(map[string]string)
+	for missingDataKey, missingDataValue := range mssingDataEntries {
+		missingDataKey := missingDataKey
+		keyBytes := common.EncodeMissingDataKey(&missingDataKey)
+		valBytes, err := common.EncodeMissingDataValue(missingDataValue)
+		if err != nil {
+			return nil, err
+		}
+		pendingMissingDataEntries[string(keyBytes)] = string(valBytes)
+	}
+	return pendingMissingDataEntries, nil
+}
+
+func (s *store) nextBlockNum() uint64 {
+	if s.isEmpty {
+		return 0
+	}
+	return s.lastCommittedBlock + 1
+}
+
+func (s *store) performPurgeIfScheduled(latestCommittedBlk uint64) {
+	if latestCommittedBlk%uint64(s.ledgerConfig.PrivateData.PurgeInterval) != 0 {
+		return
+	}
+	go func() {
+		s.purgerLock.Lock()
+		logger.Debugf("Purger started: Purging expired private data till block number [%d]", latestCommittedBlk)
+		defer s.purgerLock.Unlock()
+		err := s.purgeExpiredData(latestCommittedBlk)
+		if err != nil {
+			logger.Warningf("Could not purge data from pvtdata store:%s", err)
+		}
+		logger.Debug("Purger finished")
+	}()
+}
+
+func (s *store) purgeExpiredData(maxBlkNum uint64) error {
+	pvtData, err := retrieveBlockExpiryData(s.db, blockNumberToKey(maxBlkNum))
+	if err != nil {
+		return err
+	}
+	if len(pvtData) == 0 {
+		return nil
+	}
+
+	docs, batch, err := s.prepareExpiredData(pvtData, maxBlkNum)
+	if err != nil {
+		return err
+	}
+	if len(docs) > 0 {
+		_, err := s.db.BatchUpdateDocuments(docs)
+		if err != nil {
+			return errors.WithMessage(err, fmt.Sprintf("BatchUpdateDocuments failed for [%d] documents", len(docs)))
+		}
+	}
+	if err := s.missingKeysIndexDB.WriteBatch(batch, false); err != nil {
+		return err
+	}
+
+	logger.Infof("[%s] - Entries purged from private data storage till block number [%d]", s.ledgerid, maxBlkNum)
+	return nil
+}
+
+func (s *store) prepareExpiredData(pvtData []*blockPvtDataResponse, maxBlkNum uint64) ([]*couchdb.CouchDoc, *leveldbhelper.UpdateBatch, error) {
+	batch := leveldbhelper.NewUpdateBatch()
+	var docs []*couchdb.CouchDoc
+	for _, data := range pvtData {
+		expBlkNums := make([]string, 0)
+		for key, value := range data.Expiry {
+			expiryKeyBytes, err := hex.DecodeString(key)
+			if err != nil {
+				return nil, nil, err
+			}
+			expiryKey := common.DecodeExpiryKey(expiryKeyBytes)
+			if expiryKey.ExpiringBlk <= maxBlkNum {
+				expiryValue, err := common.DecodeExpiryValue(value)
+				if err != nil {
+					return nil, nil, err
+				}
+				dataKeys, missingDataKeys := common.DeriveKeys(&common.ExpiryEntry{Key: expiryKey, Value: expiryValue})
+				for _, dataKey := range dataKeys {
+					delete(data.Data, hex.EncodeToString(common.EncodeDataKey(dataKey)))
+				}
+				for _, missingDataKey := range missingDataKeys {
+					batch.Delete(common.EncodeMissingDataKey(missingDataKey))
+				}
+			} else {
+				expBlkNums = append(expBlkNums, blockNumberToKey(expiryKey.ExpiringBlk))
+			}
+		}
+		if len(data.Data) == 0 {
+			data.Deleted = true
+		}
+		data.ExpiringBlkNums = expBlkNums
+		jsonBytes, err := json.Marshal(data)
+		if err != nil {
+			return nil, nil, err
+		}
+		docs = append(docs, &couchdb.CouchDoc{JSONValue: jsonBytes})
+	}
+
+	return docs, batch, nil
+
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/test_exports.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/test_exports.go
new file mode 100644
index 000000000..86ffe459b
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore/test_exports.go
@@ -0,0 +1,79 @@
+/*
+Copyright IBM Corp, SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"os"
+	"testing"
+
+	"github.com/trustbloc/fabric-peer-ext/pkg/testutil"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/stretchr/testify/require"
+)
+
+// StoreEnv provides the  store env for testing
+type StoreEnv struct {
+	t                 testing.TB
+	TestStoreProvider pvtdatastorage.Provider
+	TestStore         pvtdatastorage.Store
+	ledgerid          string
+	btlPolicy         pvtdatapolicy.BTLPolicy
+	couchDBConfig     *couchdb.Config
+}
+
+// NewTestStoreEnv construct a StoreEnv for testing
+func NewTestStoreEnv(t *testing.T, ledgerid string, btlPolicy pvtdatapolicy.BTLPolicy, couchDBConfig *couchdb.Config) *StoreEnv {
+	removeStorePath()
+	req := require.New(t)
+	conf := testutil.TestLedgerConf().PrivateData
+	testStoreProvider, err := NewProvider(conf, testutil.TestLedgerConf())
+	req.NoError(err)
+	testStore, err := testStoreProvider.OpenStore(ledgerid)
+	req.NoError(err)
+	testStore.Init(btlPolicy)
+	s := &StoreEnv{t, testStoreProvider, testStore, ledgerid, btlPolicy, couchDBConfig}
+	return s
+}
+
+// CloseAndReopen closes and opens the store provider
+func (env *StoreEnv) CloseAndReopen() {
+	var err error
+	env.TestStoreProvider.Close()
+	conf := testutil.TestLedgerConf().PrivateData
+	env.TestStoreProvider, err = NewProvider(conf, testutil.TestLedgerConf())
+	require.NoError(env.t, err)
+	env.TestStore, err = env.TestStoreProvider.OpenStore(env.ledgerid)
+	env.TestStore.Init(env.btlPolicy)
+	require.NoError(env.t, err)
+}
+
+//Cleanup env test
+func (env *StoreEnv) Cleanup(ledgerid string) {
+	//create a new connection
+	couchInstance, err := couchdb.CreateCouchInstance(env.couchDBConfig, &disabled.Provider{})
+	if err != nil {
+		panic(err.Error())
+	}
+	pvtDataStoreDBName := couchdb.ConstructBlockchainDBName(ledgerid, pvtDataStoreName)
+	db := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: pvtDataStoreDBName}
+	//drop the test database
+	if _, err := db.DropDatabase(); err != nil {
+		panic(err.Error())
+	}
+	removeStorePath()
+}
+
+func removeStorePath() {
+	dbPath := testutil.TestLedgerConf().PrivateData.StorePath
+	if err := os.RemoveAll(dbPath); err != nil {
+		panic(err.Error())
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/collelgproc.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/collelgproc.go
new file mode 100644
index 000000000..24ac3a125
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/collelgproc.go
@@ -0,0 +1,142 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"sync"
+	"time"
+
+	"github.com/hyperledger/fabric/core/ledger"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+)
+
+// todo add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/store_imp.go
+// todo below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+var logger = flogging.MustGetLogger("collelgproc")
+
+type CollElgProc struct {
+	notification, procComplete chan bool
+	purgerLock                 *sync.Mutex
+	db                         *leveldbhelper.DBHandle
+	ledgerconfig               *ledger.Config
+}
+
+func NewCollElgProc(purgerLock *sync.Mutex, missingKeysIndexDB *leveldbhelper.DBHandle, ledgerconfig *ledger.Config) *CollElgProc {
+
+	return &CollElgProc{
+		notification: make(chan bool, 1),
+		procComplete: make(chan bool, 1),
+		purgerLock:   purgerLock,
+		db:           missingKeysIndexDB,
+		ledgerconfig: ledgerconfig,
+	}
+}
+
+func (c *CollElgProc) notify() {
+	select {
+	case c.notification <- true:
+		logger.Debugf("Signaled to collection eligibility processing routine")
+	default: //noop
+		logger.Debugf("Previous signal still pending. Skipping new signal")
+	}
+}
+
+func (c *CollElgProc) waitForNotification() {
+	<-c.notification
+}
+
+func (c *CollElgProc) done() {
+	select {
+	case c.procComplete <- true:
+	default:
+	}
+}
+
+func (c *CollElgProc) WaitForDone() {
+	<-c.procComplete
+}
+
+func (c *CollElgProc) LaunchCollElgProc() {
+	maxBatchSize := c.ledgerconfig.PrivateData.MaxBatchSize
+	batchesInterval := c.ledgerconfig.PrivateData.BatchesInterval
+	go func() {
+		c.processCollElgEvents(maxBatchSize, batchesInterval) // process collection eligibility events when store is opened - in case there is an unprocessed events from previous run
+		for {
+			logger.Debugf("Waiting for collection eligibility event")
+			c.waitForNotification()
+			c.processCollElgEvents(maxBatchSize, batchesInterval)
+			c.done()
+		}
+	}()
+}
+
+func (c *CollElgProc) processCollElgEvents(maxBatchSize, batchesInterval int) {
+	logger.Debugf("Starting to process collection eligibility events")
+	c.purgerLock.Lock()
+	defer c.purgerLock.Unlock()
+	collElgStartKey, collElgEndKey := createRangeScanKeysForCollElg()
+	eventItr := c.db.GetIterator(collElgStartKey, collElgEndKey)
+	defer eventItr.Release()
+	batch := leveldbhelper.NewUpdateBatch()
+	totalEntriesConverted := 0
+
+	for eventItr.Next() {
+		collElgKey, collElgVal := eventItr.Key(), eventItr.Value()
+		blkNum := decodeCollElgKey(collElgKey)
+		CollElgInfo, err := decodeCollElgVal(collElgVal)
+		logger.Debugf("Processing collection eligibility event [blkNum=%d], CollElgInfo=%s", blkNum, CollElgInfo)
+		if err != nil {
+			logger.Errorf("This error is not expected %s", err)
+			continue
+		}
+		for ns, colls := range CollElgInfo.NsCollMap {
+			var coll string
+			for _, coll = range colls.Entries {
+				logger.Infof("Converting missing data entries from ineligible to eligible for [ns=%s, coll=%s]", ns, coll)
+				startKey, endKey := createRangeScanKeysForIneligibleMissingData(blkNum, ns, coll)
+				collItr := c.db.GetIterator(startKey, endKey)
+				collEntriesConverted := 0
+
+				for collItr.Next() { // each entry
+					originalKey, originalVal := collItr.Key(), collItr.Value()
+					modifiedKey := decodeMissingDataKey(originalKey)
+					modifiedKey.IsEligible = true
+					batch.Delete(originalKey)
+					copyVal := make([]byte, len(originalVal))
+					copy(copyVal, originalVal)
+					batch.Put(EncodeMissingDataKey(modifiedKey), copyVal)
+					collEntriesConverted++
+					if batch.Len() > maxBatchSize {
+						if err := c.db.WriteBatch(batch, true); err != nil {
+							logger.Error(err.Error())
+						}
+						batch = leveldbhelper.NewUpdateBatch()
+						sleepTime := time.Duration(batchesInterval)
+						logger.Infof("Going to sleep for %d milliseconds between batches. Entries for [ns=%s, coll=%s] converted so far = %d",
+							sleepTime, ns, coll, collEntriesConverted)
+						c.purgerLock.Unlock()
+						time.Sleep(sleepTime * time.Millisecond)
+						c.purgerLock.Lock()
+					}
+				} // entry loop
+
+				collItr.Release()
+				logger.Infof("Converted all [%d] entries for [ns=%s, coll=%s]", collEntriesConverted, ns, coll)
+				totalEntriesConverted += collEntriesConverted
+			} // coll loop
+		} // ns loop
+		batch.Delete(collElgKey) // delete the collection eligibility event key as well
+	} // event loop
+
+	if err := c.db.WriteBatch(batch, true); err != nil {
+		logger.Error(err.Error())
+	}
+	logger.Debugf("Converted [%d] inelligible mising data entries to elligible", totalEntriesConverted)
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/helper.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/helper.go
new file mode 100644
index 000000000..43ff69720
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/helper.go
@@ -0,0 +1,235 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"math"
+
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/willf/bitset"
+)
+
+// todo add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/helper.go
+// todo below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+func PrepareStoreEntries(blockNum uint64, pvtData []*ledger.TxPvtData, btlPolicy pvtdatapolicy.BTLPolicy,
+	missingPvtData ledger.TxMissingPvtDataMap) (*StoreEntries, error) {
+	dataEntries := prepareDataEntries(blockNum, pvtData)
+
+	missingDataEntries := prepareMissingDataEntries(blockNum, missingPvtData)
+
+	expiryEntries, err := prepareExpiryEntries(blockNum, dataEntries, missingDataEntries, btlPolicy)
+	if err != nil {
+		return nil, err
+	}
+
+	return &StoreEntries{
+		DataEntries:        dataEntries,
+		ExpiryEntries:      expiryEntries,
+		MissingDataEntries: missingDataEntries}, nil
+}
+
+func prepareDataEntries(blockNum uint64, pvtData []*ledger.TxPvtData) []*DataEntry {
+	var dataEntries []*DataEntry
+	for _, txPvtdata := range pvtData {
+		for _, nsPvtdata := range txPvtdata.WriteSet.NsPvtRwset {
+			for _, collPvtdata := range nsPvtdata.CollectionPvtRwset {
+				txnum := txPvtdata.SeqInBlock
+				ns := nsPvtdata.Namespace
+				coll := collPvtdata.CollectionName
+				dataKey := &DataKey{NsCollBlk: NsCollBlk{Ns: ns, Coll: coll, BlkNum: blockNum}, TxNum: txnum}
+				dataEntries = append(dataEntries, &DataEntry{Key: dataKey, Value: collPvtdata})
+			}
+		}
+	}
+	return dataEntries
+}
+
+func prepareMissingDataEntries(committingBlk uint64, missingPvtData ledger.TxMissingPvtDataMap) map[MissingDataKey]*bitset.BitSet {
+	missingDataEntries := make(map[MissingDataKey]*bitset.BitSet)
+
+	for txNum, missingData := range missingPvtData {
+		for _, nsColl := range missingData {
+			key := MissingDataKey{NsCollBlk: NsCollBlk{Ns: nsColl.Namespace, Coll: nsColl.Collection, BlkNum: committingBlk},
+				IsEligible: nsColl.IsEligible}
+
+			if _, ok := missingDataEntries[key]; !ok {
+				missingDataEntries[key] = &bitset.BitSet{}
+			}
+			bitmap := missingDataEntries[key]
+
+			bitmap.Set(uint(txNum))
+		}
+	}
+
+	return missingDataEntries
+}
+
+// prepareExpiryEntries returns expiry entries for both private data which is present in the committingBlk
+// and missing private.
+func prepareExpiryEntries(committingBlk uint64, dataEntries []*DataEntry, missingDataEntries map[MissingDataKey]*bitset.BitSet,
+	btlPolicy pvtdatapolicy.BTLPolicy) ([]*ExpiryEntry, error) {
+
+	var expiryEntries []*ExpiryEntry
+	mapByExpiringBlk := make(map[uint64]*ExpiryData)
+
+	// 1. prepare expiryData for non-missing data
+	for _, dataEntry := range dataEntries {
+		prepareExpiryEntriesForPresentData(mapByExpiringBlk, dataEntry.Key, btlPolicy)
+
+	}
+
+	// 2. prepare expiryData for missing data
+	for missingDataKey := range missingDataEntries {
+		prepareExpiryEntriesForMissingData(mapByExpiringBlk, &missingDataKey, btlPolicy)
+
+	}
+
+	for expiryBlk, expiryData := range mapByExpiringBlk {
+		expiryKey := &ExpiryKey{ExpiringBlk: expiryBlk, CommittingBlk: committingBlk}
+		expiryEntries = append(expiryEntries, &ExpiryEntry{Key: expiryKey, Value: expiryData})
+	}
+
+	return expiryEntries, nil
+}
+
+// prepareExpiryDataForPresentData creates expiryData for non-missing pvt data
+func prepareExpiryEntriesForPresentData(mapByExpiringBlk map[uint64]*ExpiryData, dataKey *DataKey, btlPolicy pvtdatapolicy.BTLPolicy) error {
+	expiringBlk, err := btlPolicy.GetExpiringBlock(dataKey.Ns, dataKey.Coll, dataKey.BlkNum)
+	if err != nil {
+		return err
+	}
+	if neverExpires(expiringBlk) {
+		return nil
+	}
+
+	expiryData := getOrCreateExpiryData(mapByExpiringBlk, expiringBlk)
+
+	expiryData.AddPresentData(dataKey.Ns, dataKey.Coll, dataKey.TxNum)
+	return nil
+}
+
+// prepareExpiryDataForMissingData creates expiryData for missing pvt data
+func prepareExpiryEntriesForMissingData(mapByExpiringBlk map[uint64]*ExpiryData, missingKey *MissingDataKey, btlPolicy pvtdatapolicy.BTLPolicy) error {
+	expiringBlk, err := btlPolicy.GetExpiringBlock(missingKey.Ns, missingKey.Coll, missingKey.BlkNum)
+	if err != nil {
+		return err
+	}
+	if neverExpires(expiringBlk) {
+		return nil
+	}
+
+	expiryData := getOrCreateExpiryData(mapByExpiringBlk, expiringBlk)
+
+	expiryData.AddMissingData(missingKey.Ns, missingKey.Coll)
+	return nil
+}
+
+func getOrCreateExpiryData(mapByExpiringBlk map[uint64]*ExpiryData, expiringBlk uint64) *ExpiryData {
+	expiryData, ok := mapByExpiringBlk[expiringBlk]
+	if !ok {
+		expiryData = NewExpiryData()
+		mapByExpiringBlk[expiringBlk] = expiryData
+	}
+	return expiryData
+}
+
+func PassesFilter(dataKey *DataKey, filter ledger.PvtNsCollFilter) bool {
+	return filter == nil || filter.Has(dataKey.Ns, dataKey.Coll)
+}
+
+func IsExpired(key NsCollBlk, btl pvtdatapolicy.BTLPolicy, latestBlkNum uint64) (bool, error) {
+	expiringBlk, err := btl.GetExpiringBlock(key.Ns, key.Coll, key.BlkNum)
+	if err != nil {
+		return false, err
+	}
+
+	return latestBlkNum >= expiringBlk, nil
+}
+
+// DeriveKeys constructs dataKeys and missingDataKey from an expiryEntry
+func DeriveKeys(expiryEntry *ExpiryEntry) (dataKeys []*DataKey, missingDataKeys []*MissingDataKey) {
+	for ns, colls := range expiryEntry.Value.Map {
+		// 1. constructs dataKeys of expired existing pvt data
+		for coll, txNums := range colls.Map {
+			for _, txNum := range txNums.List {
+				dataKeys = append(dataKeys,
+					&DataKey{NsCollBlk{ns, coll, expiryEntry.Key.CommittingBlk}, txNum})
+			}
+		}
+		// 2. constructs missingDataKeys of expired missing pvt data
+		for coll := range colls.MissingDataMap {
+			// one key for eligible entries and another for ieligible entries
+			missingDataKeys = append(missingDataKeys,
+				&MissingDataKey{NsCollBlk{ns, coll, expiryEntry.Key.CommittingBlk}, true})
+			missingDataKeys = append(missingDataKeys,
+				&MissingDataKey{NsCollBlk{ns, coll, expiryEntry.Key.CommittingBlk}, false})
+
+		}
+	}
+	return
+}
+
+func newCollElgInfo(nsCollMap map[string][]string) *pvtdatastorage.CollElgInfo {
+	m := &pvtdatastorage.CollElgInfo{NsCollMap: map[string]*pvtdatastorage.CollNames{}}
+	for ns, colls := range nsCollMap {
+		collNames, ok := m.NsCollMap[ns]
+		if !ok {
+			collNames = &pvtdatastorage.CollNames{}
+			m.NsCollMap[ns] = collNames
+		}
+		collNames.Entries = colls
+	}
+	return m
+}
+
+type TxPvtdataAssembler struct {
+	blockNum, txNum uint64
+	txWset          *rwset.TxPvtReadWriteSet
+	currentNsWSet   *rwset.NsPvtReadWriteSet
+	firstCall       bool
+}
+
+func NewTxPvtdataAssembler(blockNum, txNum uint64) *TxPvtdataAssembler {
+	return &TxPvtdataAssembler{blockNum, txNum, &rwset.TxPvtReadWriteSet{}, nil, true}
+}
+
+func (a *TxPvtdataAssembler) Add(ns string, collPvtWset *rwset.CollectionPvtReadWriteSet) {
+	// start a NsWset
+	if a.firstCall {
+		a.currentNsWSet = &rwset.NsPvtReadWriteSet{Namespace: ns}
+		a.firstCall = false
+	}
+
+	// if a new ns started, add the existing NsWset to TxWset and start a new one
+	if a.currentNsWSet.Namespace != ns {
+		a.txWset.NsPvtRwset = append(a.txWset.NsPvtRwset, a.currentNsWSet)
+		a.currentNsWSet = &rwset.NsPvtReadWriteSet{Namespace: ns}
+	}
+	// add the collWset to the current NsWset
+	a.currentNsWSet.CollectionPvtRwset = append(a.currentNsWSet.CollectionPvtRwset, collPvtWset)
+}
+
+func (a *TxPvtdataAssembler) done() {
+	if a.currentNsWSet != nil {
+		a.txWset.NsPvtRwset = append(a.txWset.NsPvtRwset, a.currentNsWSet)
+	}
+	a.currentNsWSet = nil
+}
+
+func (a *TxPvtdataAssembler) GetTxPvtdata() *ledger.TxPvtData {
+	a.done()
+	return &ledger.TxPvtData{SeqInBlock: a.txNum, WriteSet: a.txWset}
+}
+
+func neverExpires(expiringBlkNum uint64) bool {
+	return expiringBlkNum == math.MaxUint64
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/kv_encoding.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/kv_encoding.go
new file mode 100644
index 000000000..db7cfd320
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/kv_encoding.go
@@ -0,0 +1,192 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"bytes"
+	"math"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/pkg/errors"
+	"github.com/willf/bitset"
+)
+
+// todo add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/kv_encoding.go
+// todo below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+var (
+	PendingCommitKey               = []byte{0}
+	pvtDataKeyPrefix               = []byte{2}
+	expiryKeyPrefix                = []byte{3}
+	eligibleMissingDataKeyPrefix   = []byte{4}
+	ineligibleMissingDataKeyPrefix = []byte{5}
+	collElgKeyPrefix               = []byte{6}
+	LastUpdatedOldBlocksKey        = []byte{7}
+	lastCommittedBlockKey          = []byte{8}
+	nilByte                        = byte(0)
+)
+
+func EncodeDataKey(key *DataKey) []byte {
+	dataKeyBytes := append(pvtDataKeyPrefix, version.NewHeight(key.BlkNum, key.TxNum).ToBytes()...)
+	dataKeyBytes = append(dataKeyBytes, []byte(key.Ns)...)
+	dataKeyBytes = append(dataKeyBytes, nilByte)
+	return append(dataKeyBytes, []byte(key.Coll)...)
+}
+
+func EncodeDataValue(collData *rwset.CollectionPvtReadWriteSet) ([]byte, error) {
+	return proto.Marshal(collData)
+}
+
+func EncodeExpiryKey(expiryKey *ExpiryKey) []byte {
+	// reusing version encoding scheme here
+	return append(expiryKeyPrefix, version.NewHeight(expiryKey.ExpiringBlk, expiryKey.CommittingBlk).ToBytes()...)
+}
+
+func DecodeExpiryKey(expiryKeyBytes []byte) *ExpiryKey {
+	height, _ := version.NewHeightFromBytes(expiryKeyBytes[1:])
+	return &ExpiryKey{ExpiringBlk: height.BlockNum, CommittingBlk: height.TxNum}
+}
+
+func EncodeExpiryValue(expiryData *ExpiryData) ([]byte, error) {
+	return proto.Marshal(expiryData)
+}
+
+func DecodeExpiryValue(expiryValueBytes []byte) (*ExpiryData, error) {
+	expiryData := &ExpiryData{}
+	err := proto.Unmarshal(expiryValueBytes, expiryData)
+	return expiryData, err
+}
+
+func DecodeDatakey(datakeyBytes []byte) *DataKey {
+	v, n := version.NewHeightFromBytes(datakeyBytes[1:])
+	blkNum := v.BlockNum
+	tranNum := v.TxNum
+	remainingBytes := datakeyBytes[n+1:]
+	nilByteIndex := bytes.IndexByte(remainingBytes, nilByte)
+	ns := string(remainingBytes[:nilByteIndex])
+	coll := string(remainingBytes[nilByteIndex+1:])
+	return &DataKey{NsCollBlk: NsCollBlk{Ns: ns, Coll: coll, BlkNum: blkNum}, TxNum: tranNum}
+}
+
+func DecodeDataValue(datavalueBytes []byte) (*rwset.CollectionPvtReadWriteSet, error) {
+	collPvtdata := &rwset.CollectionPvtReadWriteSet{}
+	err := proto.Unmarshal(datavalueBytes, collPvtdata)
+	return collPvtdata, err
+}
+
+func EncodeMissingDataKey(key *MissingDataKey) []byte {
+	if key.IsEligible {
+		keyBytes := append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(key.BlkNum)...)
+		keyBytes = append(keyBytes, []byte(key.Ns)...)
+		keyBytes = append(keyBytes, nilByte)
+		return append(keyBytes, []byte(key.Coll)...)
+	}
+
+	keyBytes := append(ineligibleMissingDataKeyPrefix, []byte(key.Ns)...)
+	keyBytes = append(keyBytes, nilByte)
+	keyBytes = append(keyBytes, []byte(key.Coll)...)
+	keyBytes = append(keyBytes, nilByte)
+	return append(keyBytes, []byte(util.EncodeReverseOrderVarUint64(key.BlkNum))...)
+}
+
+func decodeMissingDataKey(keyBytes []byte) *MissingDataKey {
+	key := &MissingDataKey{NsCollBlk: NsCollBlk{}}
+	if keyBytes[0] == eligibleMissingDataKeyPrefix[0] {
+		blkNum, numBytesConsumed := util.DecodeReverseOrderVarUint64(keyBytes[1:])
+
+		splittedKey := bytes.Split(keyBytes[numBytesConsumed+1:], []byte{nilByte})
+		key.Ns = string(splittedKey[0])
+		key.Coll = string(splittedKey[1])
+		key.BlkNum = blkNum
+		key.IsEligible = true
+		return key
+	}
+
+	splittedKey := bytes.SplitN(keyBytes[1:], []byte{nilByte}, 3) //encoded bytes for blknum may contain empty bytes
+	key.Ns = string(splittedKey[0])
+	key.Coll = string(splittedKey[1])
+	key.BlkNum, _ = util.DecodeReverseOrderVarUint64(splittedKey[2])
+	key.IsEligible = false
+	return key
+}
+
+func EncodeMissingDataValue(bitmap *bitset.BitSet) ([]byte, error) {
+	return bitmap.MarshalBinary()
+}
+
+func DecodeMissingDataValue(bitmapBytes []byte) (*bitset.BitSet, error) {
+	bitmap := &bitset.BitSet{}
+	if err := bitmap.UnmarshalBinary(bitmapBytes); err != nil {
+		return nil, err
+	}
+	return bitmap, nil
+}
+
+func encodeCollElgKey(blkNum uint64) []byte {
+	return append(collElgKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum)...)
+}
+
+func decodeCollElgKey(b []byte) uint64 {
+	blkNum, _ := util.DecodeReverseOrderVarUint64(b[1:])
+	return blkNum
+}
+
+func encodeCollElgVal(m *pvtdatastorage.CollElgInfo) ([]byte, error) {
+	return proto.Marshal(m)
+}
+
+func decodeCollElgVal(b []byte) (*pvtdatastorage.CollElgInfo, error) {
+	m := &pvtdatastorage.CollElgInfo{}
+	if err := proto.Unmarshal(b, m); err != nil {
+		return nil, errors.WithStack(err)
+	}
+	return m, nil
+}
+
+func createRangeScanKeysForIneligibleMissingData(maxBlkNum uint64, ns, coll string) (startKey, endKey []byte) {
+	startKey = EncodeMissingDataKey(
+		&MissingDataKey{
+			NsCollBlk:  NsCollBlk{Ns: ns, Coll: coll, BlkNum: maxBlkNum},
+			IsEligible: false,
+		},
+	)
+	endKey = EncodeMissingDataKey(
+		&MissingDataKey{
+			NsCollBlk:  NsCollBlk{Ns: ns, Coll: coll, BlkNum: 0},
+			IsEligible: false,
+		},
+	)
+	return
+}
+
+func createRangeScanKeysForEligibleMissingDataEntries(blkNum uint64) (startKey, endKey []byte) {
+	startKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum)...)
+	endKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(0)...)
+
+	return startKey, endKey
+}
+
+func createRangeScanKeysForCollElg() (startKey, endKey []byte) {
+	return encodeCollElgKey(math.MaxUint64),
+		encodeCollElgKey(0)
+}
+
+func datakeyRange(blockNum uint64) (startKey, endKey []byte) {
+	startKey = append(pvtDataKeyPrefix, version.NewHeight(blockNum, 0).ToBytes()...)
+	endKey = append(pvtDataKeyPrefix, version.NewHeight(blockNum, math.MaxUint64).ToBytes()...)
+	return
+}
+
+func eligibleMissingdatakeyRange(blkNum uint64) (startKey, endKey []byte) {
+	startKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum)...)
+	endKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum-1)...)
+	return
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/store.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/store.go
new file mode 100644
index 000000000..bbc1433e2
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/store.go
@@ -0,0 +1,404 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"sync/atomic"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/willf/bitset"
+)
+
+// todo add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/store_imp.go
+// todo below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+type DataEntry struct {
+	Key   *DataKey
+	Value *rwset.CollectionPvtReadWriteSet
+}
+
+type ExpiryEntry struct {
+	Key   *ExpiryKey
+	Value *ExpiryData
+}
+
+type ExpiryKey struct {
+	ExpiringBlk   uint64
+	CommittingBlk uint64
+}
+
+type NsCollBlk struct {
+	Ns, Coll string
+	BlkNum   uint64
+}
+
+type DataKey struct {
+	NsCollBlk
+	TxNum uint64
+}
+
+type MissingDataKey struct {
+	NsCollBlk
+	IsEligible bool
+}
+
+type StoreEntries struct {
+	DataEntries        []*DataEntry
+	ExpiryEntries      []*ExpiryEntry
+	MissingDataEntries map[MissingDataKey]*bitset.BitSet
+}
+
+type EntriesForPvtDataOfOldBlocks struct {
+	// for each <ns, coll, blkNum, txNum>, store the dataEntry, i.e., pvtData
+	DataEntries map[DataKey]*rwset.CollectionPvtReadWriteSet
+	// store the retrieved (& updated) expiryData in expiryEntries
+	ExpiryEntries map[ExpiryKey]*ExpiryData
+	// for each <ns, coll, blkNum>, store the retrieved (& updated) bitmap in the missingDataEntries
+	MissingDataEntries map[NsCollBlk]*bitset.BitSet
+}
+
+func (updateEntries *EntriesForPvtDataOfOldBlocks) AddDataEntry(dataEntry *DataEntry) {
+	dataKey := DataKey{NsCollBlk: dataEntry.Key.NsCollBlk, TxNum: dataEntry.Key.TxNum}
+	updateEntries.DataEntries[dataKey] = dataEntry.Value
+}
+
+func (updateEntries *EntriesForPvtDataOfOldBlocks) UpdateAndAddExpiryEntry(expiryEntry *ExpiryEntry, dataKey *DataKey) {
+	txNum := dataKey.TxNum
+	nsCollBlk := dataKey.NsCollBlk
+	// update
+	expiryEntry.Value.AddPresentData(nsCollBlk.Ns, nsCollBlk.Coll, txNum)
+	// we cannot delete entries from MissingDataMap as
+	// we keep only one entry per missing <ns-col>
+	// irrespective of the number of txNum.
+
+	// add
+	expiryKey := ExpiryKey{expiryEntry.Key.ExpiringBlk, expiryEntry.Key.CommittingBlk}
+	updateEntries.ExpiryEntries[expiryKey] = expiryEntry.Value
+}
+
+func (updateEntries *EntriesForPvtDataOfOldBlocks) UpdateAndAddMissingDataEntry(missingData *bitset.BitSet, dataKey *DataKey) {
+
+	txNum := dataKey.TxNum
+	nsCollBlk := dataKey.NsCollBlk
+	// update
+	missingData.Clear(uint(txNum))
+	// add
+	updateEntries.MissingDataEntries[nsCollBlk] = missingData
+}
+
+type ExpiryData pvtdatastorage.ExpiryData
+
+func NewExpiryData() *ExpiryData {
+	return &ExpiryData{Map: make(map[string]*pvtdatastorage.Collections)}
+}
+
+func (e *ExpiryData) getOrCreateCollections(ns string) *pvtdatastorage.Collections {
+	collections, ok := e.Map[ns]
+	if !ok {
+		collections = &pvtdatastorage.Collections{
+			Map:            make(map[string]*pvtdatastorage.TxNums),
+			MissingDataMap: make(map[string]bool)}
+		e.Map[ns] = collections
+	} else {
+		// due to protobuf encoding/decoding, the previously
+		// initialized map could be a nil now due to 0 length.
+		// Hence, we need to reinitialize the map.
+		if collections.Map == nil {
+			collections.Map = make(map[string]*pvtdatastorage.TxNums)
+		}
+		if collections.MissingDataMap == nil {
+			collections.MissingDataMap = make(map[string]bool)
+		}
+	}
+	return collections
+}
+
+func (e *ExpiryData) AddPresentData(ns, coll string, txNum uint64) {
+	collections := e.getOrCreateCollections(ns)
+
+	txNums, ok := collections.Map[coll]
+	if !ok {
+		txNums = &pvtdatastorage.TxNums{}
+		collections.Map[coll] = txNums
+	}
+	txNums.List = append(txNums.List, txNum)
+}
+
+func (e *ExpiryData) AddMissingData(ns, coll string) {
+	collections := e.getOrCreateCollections(ns)
+	collections.MissingDataMap[coll] = true
+}
+
+func (e *ExpiryData) Reset() {
+	*e = ExpiryData{}
+}
+func (e *ExpiryData) String() string {
+	return proto.CompactTextString(e)
+}
+
+func (*ExpiryData) ProtoMessage() {
+}
+
+func ConstructDataEntriesFromBlocksPvtData(blocksPvtData map[uint64][]*ledger.TxPvtData) map[uint64][]*DataEntry {
+	// construct dataEntries for all pvtData
+	dataEntries := make(map[uint64][]*DataEntry)
+	for blkNum, pvtData := range blocksPvtData {
+		// prepare the dataEntries for the pvtData
+		dataEntries[blkNum] = prepareDataEntries(blkNum, pvtData)
+	}
+	return dataEntries
+}
+
+func ConstructUpdateEntriesFromDataEntries(dataEntries []*DataEntry, btlPolicy pvtdatapolicy.BTLPolicy,
+	getExpiryDataOfExpiryKey func(*ExpiryKey) (*ExpiryData, error), getBitmapOfMissingDataKey func(*MissingDataKey) (*bitset.BitSet, error)) (*EntriesForPvtDataOfOldBlocks, error) {
+	updateEntries := &EntriesForPvtDataOfOldBlocks{
+		DataEntries:        make(map[DataKey]*rwset.CollectionPvtReadWriteSet),
+		ExpiryEntries:      make(map[ExpiryKey]*ExpiryData),
+		MissingDataEntries: make(map[NsCollBlk]*bitset.BitSet)}
+
+	// for each data entry, first, get the expiryData and missingData from the pvtStore.
+	// Second, update the expiryData and missingData as per the data entry. Finally, add
+	// the data entry along with the updated expiryData and missingData to the update entries
+	for _, dataEntry := range dataEntries {
+		// get the expiryBlk number to construct the expiryKey
+		expiryKey, err := constructExpiryKeyFromDataEntry(dataEntry, btlPolicy)
+		if err != nil {
+			return nil, err
+		}
+
+		// get the existing expiryData ntry
+		var expiryData *ExpiryData
+		if !neverExpires(expiryKey.ExpiringBlk) {
+			if expiryData, err = getExpiryDataFromUpdateEntriesOrStore(updateEntries, expiryKey, getExpiryDataOfExpiryKey); err != nil {
+				return nil, err
+			}
+			if expiryData == nil {
+				// data entry is already expired
+				// and purged (a rare scenario)
+				continue
+			}
+		}
+
+		// get the existing missingData entry
+		var missingData *bitset.BitSet
+		nsCollBlk := dataEntry.Key.NsCollBlk
+		if missingData, err = getMissingDataFromUpdateEntriesOrStore(updateEntries, nsCollBlk, getBitmapOfMissingDataKey); err != nil {
+			return nil, err
+		}
+		if missingData == nil {
+			// data entry is already expired
+			// and purged (a rare scenario)
+			continue
+		}
+
+		updateEntries.AddDataEntry(dataEntry)
+		if expiryData != nil { // would be nill for the never expiring entry
+			expiryEntry := &ExpiryEntry{Key: &expiryKey, Value: expiryData}
+			updateEntries.UpdateAndAddExpiryEntry(expiryEntry, dataEntry.Key)
+		}
+		updateEntries.UpdateAndAddMissingDataEntry(missingData, dataEntry.Key)
+	}
+	return updateEntries, nil
+}
+
+func ConstructUpdateBatchFromUpdateEntries(updateEntries *EntriesForPvtDataOfOldBlocks, batch *leveldbhelper.UpdateBatch) (*leveldbhelper.UpdateBatch, error) {
+	// add the following four types of entries to the update batch: (1) updated missing data entries
+
+	// (1) add updated missingData to the batch
+	if err := addUpdatedMissingDataEntriesToUpdateBatch(batch, updateEntries); err != nil {
+		return nil, err
+	}
+
+	return batch, nil
+}
+
+func constructExpiryKeyFromDataEntry(dataEntry *DataEntry, btlPolicy pvtdatapolicy.BTLPolicy) (ExpiryKey, error) {
+	// get the expiryBlk number to construct the expiryKey
+	nsCollBlk := dataEntry.Key.NsCollBlk
+	expiringBlk, err := btlPolicy.GetExpiringBlock(nsCollBlk.Ns, nsCollBlk.Coll, nsCollBlk.BlkNum)
+	if err != nil {
+		return ExpiryKey{}, err
+	}
+	return ExpiryKey{ExpiringBlk: expiringBlk, CommittingBlk: nsCollBlk.BlkNum}, nil
+}
+
+func getExpiryDataFromUpdateEntriesOrStore(updateEntries *EntriesForPvtDataOfOldBlocks, expiryKey ExpiryKey, getExpiryDataOfExpiryKey func(*ExpiryKey) (*ExpiryData, error)) (*ExpiryData, error) {
+	expiryData, ok := updateEntries.ExpiryEntries[expiryKey]
+	if !ok {
+		var err error
+		expiryData, err = getExpiryDataOfExpiryKey(&expiryKey)
+		if err != nil {
+			return nil, err
+		}
+	}
+	return expiryData, nil
+}
+
+func getMissingDataFromUpdateEntriesOrStore(updateEntries *EntriesForPvtDataOfOldBlocks, nsCollBlk NsCollBlk, getBitmapOfMissingDataKey func(*MissingDataKey) (*bitset.BitSet, error)) (*bitset.BitSet, error) {
+	missingData, ok := updateEntries.MissingDataEntries[nsCollBlk]
+	if !ok {
+		var err error
+		missingDataKey := &MissingDataKey{NsCollBlk: nsCollBlk, IsEligible: true}
+		missingData, err = getBitmapOfMissingDataKey(missingDataKey)
+		if err != nil {
+			return nil, err
+		}
+	}
+	return missingData, nil
+}
+
+func addUpdatedMissingDataEntriesToUpdateBatch(batch *leveldbhelper.UpdateBatch, entries *EntriesForPvtDataOfOldBlocks) error {
+	var keyBytes, valBytes []byte
+	var err error
+	for nsCollBlk, missingData := range entries.MissingDataEntries {
+		keyBytes = EncodeMissingDataKey(&MissingDataKey{nsCollBlk, true})
+		// if the missingData is empty, we need to delete the missingDataKey
+		if missingData.None() {
+			batch.Delete(keyBytes)
+			continue
+		}
+		if valBytes, err = EncodeMissingDataValue(missingData); err != nil {
+			return err
+		}
+		batch.Put(keyBytes, valBytes)
+	}
+	return nil
+}
+
+func GetLastUpdatedOldBlocksList(missingKeysIndexDB *leveldbhelper.DBHandle) ([]uint64, error) {
+	var v []byte
+	var err error
+	if v, err = missingKeysIndexDB.Get(LastUpdatedOldBlocksKey); err != nil {
+		return nil, err
+	}
+	if v == nil {
+		return nil, nil
+	}
+
+	var updatedBlksList []uint64
+	buf := proto.NewBuffer(v)
+	numBlks, err := buf.DecodeVarint()
+	if err != nil {
+		return nil, err
+	}
+	for i := 0; i < int(numBlks); i++ {
+		blkNum, err := buf.DecodeVarint()
+		if err != nil {
+			return nil, err
+		}
+		updatedBlksList = append(updatedBlksList, blkNum)
+	}
+	return updatedBlksList, nil
+}
+
+func ResetLastUpdatedOldBlocksList(missingKeysIndexDB *leveldbhelper.DBHandle) error {
+	batch := leveldbhelper.NewUpdateBatch()
+	batch.Delete(LastUpdatedOldBlocksKey)
+	if err := missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+	return nil
+}
+
+// GetMissingPvtDataInfoForMostRecentBlocks
+func GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int, lastCommittedBlk uint64, btlPolicy pvtdatapolicy.BTLPolicy, missingKeysIndexDB *leveldbhelper.DBHandle) (ledger.MissingPvtDataInfo, error) {
+	// we assume that this function would be called by the gossip only after processing the
+	// last retrieved missing pvtdata info and committing the same.
+	if maxBlock < 1 {
+		return nil, nil
+	}
+
+	missingPvtDataInfo := make(ledger.MissingPvtDataInfo)
+	numberOfBlockProcessed := 0
+	lastProcessedBlock := uint64(0)
+	isMaxBlockLimitReached := false
+	// as we are not acquiring a read lock, new blocks can get committed while we
+	// construct the MissingPvtDataInfo. As a result, lastCommittedBlock can get
+	// changed. To ensure consistency, we atomically load the lastCommittedBlock value
+	lastCommittedBlock := atomic.LoadUint64(&lastCommittedBlk)
+
+	startKey, endKey := createRangeScanKeysForEligibleMissingDataEntries(lastCommittedBlock)
+	dbItr := missingKeysIndexDB.GetIterator(startKey, endKey)
+	defer dbItr.Release()
+
+	for dbItr.Next() {
+		missingDataKeyBytes := dbItr.Key()
+		missingDataKey := decodeMissingDataKey(missingDataKeyBytes)
+
+		if isMaxBlockLimitReached && (missingDataKey.BlkNum != lastProcessedBlock) {
+			// esnures that exactly maxBlock number
+			// of blocks' entries are processed
+			break
+		}
+
+		// check whether the entry is expired. If so, move to the next item.
+		// As we may use the old lastCommittedBlock value, there is a possibility that
+		// this missing data is actually expired but we may get the stale information.
+		// Though it may leads to extra work of pulling the expired data, it will not
+		// affect the correctness. Further, as we try to fetch the most recent missing
+		// data (less possibility of expiring now), such scenario would be rare. In the
+		// best case, we can load the latest lastCommittedBlock value here atomically to
+		// make this scenario very rare.
+		lastCommittedBlock = atomic.LoadUint64(&lastCommittedBlk)
+		expired, err := IsExpired(missingDataKey.NsCollBlk, btlPolicy, lastCommittedBlock)
+		if err != nil {
+			return nil, err
+		}
+		if expired {
+			continue
+		}
+
+		// check for an existing entry for the blkNum in the MissingPvtDataInfo.
+		// If no such entry exists, create one. Also, keep track of the number of
+		// processed block due to maxBlock limit.
+		if _, ok := missingPvtDataInfo[missingDataKey.BlkNum]; !ok {
+			numberOfBlockProcessed++
+			if numberOfBlockProcessed == maxBlock {
+				isMaxBlockLimitReached = true
+				// as there can be more than one entry for this block,
+				// we cannot `break` here
+				lastProcessedBlock = missingDataKey.BlkNum
+			}
+		}
+
+		valueBytes := dbItr.Value()
+		bitmap, err := DecodeMissingDataValue(valueBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		// for each transaction which misses private data, make an entry in missingBlockPvtDataInfo
+		for index, isSet := bitmap.NextSet(0); isSet; index, isSet = bitmap.NextSet(index + 1) {
+			txNum := uint64(index)
+			missingPvtDataInfo.Add(missingDataKey.BlkNum, txNum, missingDataKey.Ns, missingDataKey.Coll)
+		}
+	}
+
+	return missingPvtDataInfo, nil
+}
+
+// ProcessCollsEligibilityEnabled
+func ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string, collElgProcSync *CollElgProc, missingKeysIndexDB *leveldbhelper.DBHandle) error {
+	key := encodeCollElgKey(committingBlk)
+	m := newCollElgInfo(nsCollMap)
+	val, err := encodeCollElgVal(m)
+	if err != nil {
+		return err
+	}
+	batch := leveldbhelper.NewUpdateBatch()
+	batch.Put(key, val)
+	if err = missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+	collElgProcSync.notify()
+	return nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/v11.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/v11.go
new file mode 100644
index 000000000..804729f4d
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/common/v11.go
@@ -0,0 +1,78 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+)
+
+// todo add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/v11.go
+// todo below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+type blkTranNumKey []byte
+
+func V11Format(datakeyBytes []byte) bool {
+	_, n := version.NewHeightFromBytes(datakeyBytes[1:])
+	remainingBytes := datakeyBytes[n+1:]
+	return len(remainingBytes) == 0
+}
+
+func v11DecodePK(key blkTranNumKey) (blockNum uint64, tranNum uint64) {
+	height, _ := version.NewHeightFromBytes(key[1:])
+	return height.BlockNum, height.TxNum
+}
+
+func v11DecodePvtRwSet(encodedBytes []byte) (*rwset.TxPvtReadWriteSet, error) {
+	writeset := &rwset.TxPvtReadWriteSet{}
+	return writeset, proto.Unmarshal(encodedBytes, writeset)
+}
+
+func V11DecodeKV(k, v []byte, filter ledger.PvtNsCollFilter) (*ledger.TxPvtData, error) {
+	_, tNum := v11DecodePK(k)
+	var pvtWSet *rwset.TxPvtReadWriteSet
+	var err error
+	if pvtWSet, err = v11DecodePvtRwSet(v); err != nil {
+		return nil, err
+	}
+	filteredWSet := v11TrimPvtWSet(pvtWSet, filter)
+	return &ledger.TxPvtData{SeqInBlock: tNum, WriteSet: filteredWSet}, nil
+}
+
+func v11TrimPvtWSet(pvtWSet *rwset.TxPvtReadWriteSet, filter ledger.PvtNsCollFilter) *rwset.TxPvtReadWriteSet {
+	if filter == nil {
+		return pvtWSet
+	}
+
+	var filteredNsRwSet []*rwset.NsPvtReadWriteSet
+	for _, ns := range pvtWSet.NsPvtRwset {
+		var filteredCollRwSet []*rwset.CollectionPvtReadWriteSet
+		for _, coll := range ns.CollectionPvtRwset {
+			if filter.Has(ns.Namespace, coll.CollectionName) {
+				filteredCollRwSet = append(filteredCollRwSet, coll)
+			}
+		}
+		if filteredCollRwSet != nil {
+			filteredNsRwSet = append(filteredNsRwSet,
+				&rwset.NsPvtReadWriteSet{
+					Namespace:          ns.Namespace,
+					CollectionPvtRwset: filteredCollRwSet,
+				},
+			)
+		}
+	}
+	var filteredTxPvtRwSet *rwset.TxPvtReadWriteSet
+	if filteredNsRwSet != nil {
+		filteredTxPvtRwSet = &rwset.TxPvtReadWriteSet{
+			DataModel:  pvtWSet.GetDataModel(),
+			NsPvtRwset: filteredNsRwSet,
+		}
+	}
+	return filteredTxPvtRwSet
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/store_impl.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/store_impl.go
new file mode 100644
index 000000000..aea54516c
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/store_impl.go
@@ -0,0 +1,206 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cachedpvtdatastore"
+	cdbpvtdatastore "github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/cdbpvtdatastore"
+)
+
+//////// Provider functions  /////////////
+//////////////////////////////////////////
+
+// PvtDataProvider encapsulates the storage and cache providers in addition to the missing data index provider
+type PvtDataProvider struct {
+	storageProvider pvtdatastorage.Provider
+	cacheProvider   pvtdatastorage.Provider
+}
+
+// NewProvider creates a new PvtDataStoreProvider that combines a cache provider and a backing storage provider
+func NewProvider(conf *ledger.PrivateData, ledgerconfig *ledger.Config) *PvtDataProvider {
+	// create couchdb pvt date store provider
+	storageProvider, err := cdbpvtdatastore.NewProvider(conf, ledgerconfig)
+	if err != nil {
+		panic(err)
+	}
+	// create cache pvt date store provider
+	cacheProvider := cachedpvtdatastore.NewProvider()
+
+	p := PvtDataProvider{
+		storageProvider: storageProvider,
+		cacheProvider:   cacheProvider,
+	}
+	return &p
+}
+
+// OpenStore creates a pvt data store instance for the given ledger ID
+func (c *PvtDataProvider) OpenStore(ledgerID string) (pvtdatastorage.Store, error) {
+	pvtDataStore, err := c.storageProvider.OpenStore(ledgerID)
+	if err != nil {
+		return nil, err
+	}
+	cachePvtDataStore, err := c.cacheProvider.OpenStore(ledgerID)
+	if err != nil {
+		return nil, err
+	}
+
+	return newPvtDataStore(pvtDataStore, cachePvtDataStore)
+}
+
+// Close cleans up the Provider
+func (c *PvtDataProvider) Close() {
+	c.storageProvider.Close()
+	c.cacheProvider.Close()
+
+}
+
+type pvtDataStore struct {
+	pvtDataDBStore    pvtdatastorage.Store
+	cachePvtDataStore pvtdatastorage.Store
+}
+
+func newPvtDataStore(pvtDataDBStore pvtdatastorage.Store, cachePvtDataStore pvtdatastorage.Store) (*pvtDataStore, error) {
+	isEmpty, err := pvtDataDBStore.IsEmpty()
+	if err != nil {
+		return nil, err
+	}
+	// InitLastCommittedBlock for cache if pvtdata storage not empty
+	if !isEmpty {
+		lastCommittedBlockHeight, err := pvtDataDBStore.LastCommittedBlockHeight()
+		if err != nil {
+			return nil, err
+		}
+		err = cachePvtDataStore.InitLastCommittedBlock(lastCommittedBlockHeight - 1)
+		if err != nil {
+			return nil, err
+		}
+	}
+	c := pvtDataStore{
+		pvtDataDBStore:    pvtDataDBStore,
+		cachePvtDataStore: cachePvtDataStore,
+	}
+	return &c, nil
+}
+
+//////// store functions  ////////////////
+//////////////////////////////////////////
+func (c *pvtDataStore) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
+	c.cachePvtDataStore.Init(btlPolicy)
+	c.pvtDataDBStore.Init(btlPolicy)
+}
+
+// Prepare pvt data in cache and send pvt data to background prepare/commit go routine
+func (c *pvtDataStore) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData, pvtMissingDataMap ledger.TxMissingPvtDataMap) error {
+	// Prepare data in cache
+	err := c.cachePvtDataStore.Prepare(blockNum, pvtData, pvtMissingDataMap)
+	if err != nil {
+		return err
+	}
+	// Prepare data in storage
+	return c.pvtDataDBStore.Prepare(blockNum, pvtData, pvtMissingDataMap)
+}
+
+// Commit pvt data in cache and call background pvtDataWriter go routine to commit data
+func (c *pvtDataStore) Commit() error {
+	// Commit data in cache
+	err := c.cachePvtDataStore.Commit()
+	if err != nil {
+		return err
+	}
+	// Commit data in storage
+	return c.pvtDataDBStore.Commit()
+}
+
+//InitLastCommittedBlock initialize last committed block
+func (c *pvtDataStore) InitLastCommittedBlock(blockNum uint64) error {
+	// InitLastCommittedBlock data in cache
+	err := c.cachePvtDataStore.InitLastCommittedBlock(blockNum)
+	if err != nil {
+		return err
+	}
+	// InitLastCommittedBlock data in storage
+	return c.pvtDataDBStore.InitLastCommittedBlock(blockNum)
+}
+
+//GetPvtDataByBlockNum implements the function in the interface `Store`
+func (c *pvtDataStore) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	result, err := c.cachePvtDataStore.GetPvtDataByBlockNum(blockNum, filter)
+	if err != nil {
+		return nil, err
+	}
+	if len(result) > 0 {
+		return result, nil
+	}
+
+	// data is not in cache will try to get it from storage
+	return c.pvtDataDBStore.GetPvtDataByBlockNum(blockNum, filter)
+}
+
+//HasPendingBatch implements the function in the interface `Store`
+func (c *pvtDataStore) HasPendingBatch() (bool, error) {
+	return c.pvtDataDBStore.HasPendingBatch()
+}
+
+//LastCommittedBlockHeight implements the function in the interface `Store`
+func (c *pvtDataStore) LastCommittedBlockHeight() (uint64, error) {
+	return c.pvtDataDBStore.LastCommittedBlockHeight()
+}
+
+//IsEmpty implements the function in the interface `Store`
+func (c *pvtDataStore) IsEmpty() (bool, error) {
+	return c.pvtDataDBStore.IsEmpty()
+}
+
+// Rollback pvt data in cache and call background pvtDataWriter go routine to rollback data
+func (c *pvtDataStore) Rollback() error {
+	// Rollback data in cache
+	err := c.cachePvtDataStore.Rollback()
+	if err != nil {
+		return err
+	}
+	// Rollback data in storage
+	return c.pvtDataDBStore.Rollback()
+}
+
+//Shutdown implements the function in the interface `Store`
+func (c *pvtDataStore) Shutdown() {
+	c.cachePvtDataStore.Shutdown()
+	c.pvtDataDBStore.Shutdown()
+}
+
+//GetMissingPvtDataInfoForMostRecentBlocks implements the function in the interface `Store`
+func (c *pvtDataStore) GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int) (ledger.MissingPvtDataInfo, error) {
+	return c.pvtDataDBStore.GetMissingPvtDataInfoForMostRecentBlocks(maxBlock)
+}
+
+//ProcessCollsEligibilityEnabled implements the function in the interface `Store`
+func (c *pvtDataStore) ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string) error {
+	return c.pvtDataDBStore.ProcessCollsEligibilityEnabled(committingBlk, nsCollMap)
+}
+
+//CommitPvtDataOfOldBlocks implements the function in the interface `Store`
+func (c *pvtDataStore) CommitPvtDataOfOldBlocks(blocksPvtData map[uint64][]*ledger.TxPvtData) error {
+	err := c.pvtDataDBStore.CommitPvtDataOfOldBlocks(blocksPvtData)
+	if err != nil {
+		return errors.WithMessage(err, "CommitPvtDataOfOldBlocks in store failed")
+	}
+	return nil
+}
+
+//GetLastUpdatedOldBlocksPvtData implements the function in the interface `Store`
+func (c *pvtDataStore) GetLastUpdatedOldBlocksPvtData() (map[uint64][]*ledger.TxPvtData, error) {
+	return c.pvtDataDBStore.GetLastUpdatedOldBlocksPvtData()
+}
+
+//ResetLastUpdatedOldBlocksList implements the function in the interface `Store`
+func (c *pvtDataStore) ResetLastUpdatedOldBlocksList() error {
+	return c.pvtDataDBStore.ResetLastUpdatedOldBlocksList()
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/test_exports.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/test_exports.go
new file mode 100644
index 000000000..b8ce7c965
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/pvtdatastorage/test_exports.go
@@ -0,0 +1,84 @@
+/*
+Copyright IBM Corp, SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"os"
+	"testing"
+
+	"github.com/trustbloc/fabric-peer-ext/pkg/testutil"
+
+	"github.com/hyperledger/fabric/core/ledger"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/stretchr/testify/require"
+)
+
+// StoreEnv provides the  store env for testing
+type StoreEnv struct {
+	t                 testing.TB
+	TestStoreProvider pvtdatastorage.Provider
+	TestStore         pvtdatastorage.Store
+	ledgerid          string
+	btlPolicy         pvtdatapolicy.BTLPolicy
+	couchDBConfig     *couchdb.Config
+}
+
+// NewTestStoreEnv construct a StoreEnv for testing
+func NewTestStoreEnv(t *testing.T, ledgerid string, btlPolicy pvtdatapolicy.BTLPolicy, couchDBConfig *couchdb.Config) *StoreEnv {
+	removeStorePath()
+	req := require.New(t)
+	conf := testutil.TestLedgerConf().PrivateData
+	testStoreProvider := NewProvider(conf, testutil.TestLedgerConf())
+	testStore, err := testStoreProvider.OpenStore(ledgerid)
+	req.NoError(err)
+	testStore.Init(btlPolicy)
+	s := &StoreEnv{t, testStoreProvider, testStore, ledgerid, btlPolicy, couchDBConfig}
+	return s
+}
+
+// CloseAndReopen closes and opens the store provider
+func (env *StoreEnv) CloseAndReopen() {
+	var err error
+	env.TestStoreProvider.Close()
+	conf := &ledger.PrivateData{
+		StorePath:     testutil.TestLedgerConf().PrivateData.StorePath,
+		PurgeInterval: 1,
+	}
+	env.TestStoreProvider = NewProvider(conf, testutil.TestLedgerConf())
+	env.TestStore, err = env.TestStoreProvider.OpenStore(env.ledgerid)
+	env.TestStore.Init(env.btlPolicy)
+	require.NoError(env.t, err)
+}
+
+//Cleanup env test
+func (env *StoreEnv) Cleanup(ledgerid string) {
+	//create a new connection
+	couchInstance, err := couchdb.CreateCouchInstance(env.couchDBConfig, &disabled.Provider{})
+	if err != nil {
+		panic(err.Error())
+	}
+	pvtDataStoreDBName := couchdb.ConstructBlockchainDBName(ledgerid, "pvtdata")
+	db := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: pvtDataStoreDBName}
+	//drop the test database
+	if _, err := db.DropDatabase(); err != nil {
+		panic(err.Error())
+	}
+	env.TestStore.Shutdown()
+
+	removeStorePath()
+}
+
+func removeStorePath() {
+	dbPath := testutil.TestLedgerConf().PrivateData.StorePath
+	if err := os.RemoveAll(dbPath); err != nil {
+		panic(err.Error())
+	}
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/roles.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/roles.go
new file mode 100644
index 000000000..fe540a440
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/roles.go
@@ -0,0 +1,123 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package roles
+
+import (
+	"strings"
+	"sync"
+
+	"github.com/trustbloc/fabric-peer-ext/pkg/config"
+)
+
+const (
+	// CommitterRole indicates that the peer commits data to the ledger
+	CommitterRole Role = "committer"
+	// EndorserRole indicates that the peer endorses transaction proposals
+	EndorserRole Role = "endorser"
+	// ValidatorRole indicates that the peer validates the block
+	ValidatorRole Role = "validator"
+)
+
+// Role is the role of the peer
+type Role string
+
+// Roles is a set of peer roles
+type Roles []Role
+
+// New creates Roles from the given slice of roles
+func New(r ...Role) Roles {
+	return Roles(r)
+}
+
+// FromStrings creates Roles from the given slice of strings
+func FromStrings(r ...string) Roles {
+	rls := make(Roles, len(r))
+	for i, s := range r {
+		rls[i] = Role(strings.ToLower(s))
+	}
+	return rls
+}
+
+// Contains return true if the given role is included in the set
+func (r Roles) Contains(role Role) bool {
+	if len(r) == 0 {
+		// Return true by default in order to be backward compatible
+		return true
+	}
+	for _, r := range r {
+		if r == role {
+			return true
+		}
+	}
+	return false
+}
+
+var initOnce sync.Once
+var roles map[Role]struct{}
+
+// HasRole returns true if the peer has the given role
+func HasRole(role Role) bool {
+	initOnce.Do(func() {
+		roles = getRoles()
+	})
+
+	if len(roles) == 0 {
+		// No roles were explicitly set, therefore the peer is assumed to have all roles.
+		return true
+	}
+
+	_, ok := roles[role]
+	return ok
+}
+
+// IsCommitter returns true if the peer is a committer, otherwise the peer does not commit to the DB
+func IsCommitter() bool {
+	return HasRole(CommitterRole)
+}
+
+// IsEndorser returns true if the peer is an endorser
+func IsEndorser() bool {
+	return HasRole(EndorserRole)
+}
+
+// IsValidator returns true if the peer is a validator
+func IsValidator() bool {
+	return HasRole(ValidatorRole)
+}
+
+// GetRoles returns the roles for the peer
+func GetRoles() []Role {
+	var ret []Role
+	for role := range roles {
+		ret = append(ret, role)
+	}
+	return ret
+}
+
+// AsString returns the roles for the peer
+func AsString() []string {
+	var ret []string
+	for role := range roles {
+		ret = append(ret, string(role))
+	}
+	return ret
+}
+
+func getRoles() map[Role]struct{} {
+	exists := struct{}{}
+	strRoles := config.GetRoles()
+	if strRoles == "" {
+		// The peer has all roles by default
+		return map[Role]struct{}{}
+	}
+	rolesMap := make(map[Role]struct{})
+	for _, r := range strings.Split(strRoles, ",") {
+		r = strings.ToLower(strings.TrimSpace(r))
+		rolesMap[Role(r)] = exists
+	}
+	return rolesMap
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/test_exports.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/test_exports.go
new file mode 100644
index 000000000..27057b162
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/roles/test_exports.go
@@ -0,0 +1,14 @@
+// +build testing
+
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package roles
+
+//SetRoles used for unit test
+func SetRoles(rolesValue map[Role]struct{}) {
+	roles = rolesValue
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/testutil/ext_test_env.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/testutil/ext_test_env.go
new file mode 100644
index 000000000..c197fd0b9
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/testutil/ext_test_env.go
@@ -0,0 +1,159 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package testutil
+
+import (
+	"fmt"
+	"os"
+	"path/filepath"
+	"time"
+
+	coreconfig "github.com/hyperledger/fabric/core/config"
+	"github.com/hyperledger/fabric/core/ledger"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/integration/runner"
+	"github.com/spf13/viper"
+)
+
+var logger = flogging.MustGetLogger("testutil")
+
+//SetupExtTestEnv creates new couchdb instance for test
+//returns couchdbd address, cleanup and stop function handle.
+func SetupExtTestEnv() (addr string, cleanup func(string), stop func()) {
+	externalCouch, set := os.LookupEnv("COUCHDB_ADDR")
+	if set {
+		return externalCouch, func(string) {}, func() {}
+	}
+
+	couchDB := &runner.CouchDB{}
+	couchDB.Image = "couchdb:2.2.0"
+	if err := couchDB.Start(); err != nil {
+		panic(fmt.Errorf("failed to start couchDB: %s", err))
+	}
+
+	oldAddr := viper.GetString("ledger.state.couchDBConfig.couchDBAddress")
+
+	//update config
+	updateConfig(couchDB.Address())
+
+	return couchDB.Address(),
+		func(name string) {
+			cleanupCouchDB(name)
+		}, func() {
+			//reset viper cdb config
+			updateConfig(oldAddr)
+			if err := couchDB.Stop(); err != nil {
+				panic(err.Error())
+			}
+		}
+}
+
+func cleanupCouchDB(name string) {
+	couchDBConfig := TestLedgerConf().StateDB.CouchDB
+	couchInstance, _ := couchdb.CreateCouchInstance(couchDBConfig, &disabled.Provider{})
+
+	blkdb := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: fmt.Sprintf("%s$$blocks_", name)}
+	pvtdb := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: fmt.Sprintf("%s$$pvtdata_", name)}
+	txndb := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: fmt.Sprintf("%s$$transactions_", name)}
+
+	//drop the test databases
+	_, err := blkdb.DropDatabase()
+	if err != nil {
+		logger.Warnf("Failed to drop db: %s, cause:%s", blkdb.DBName, err)
+	}
+	_, err = pvtdb.DropDatabase()
+	if err != nil {
+		logger.Warnf("Failed to drop db: %s, cause:%s", pvtdb.DBName, err)
+	}
+	_, err = txndb.DropDatabase()
+	if err != nil {
+		logger.Warnf("Failed to drop db: %s, cause:%s", txndb.DBName, err)
+	}
+}
+
+//updateConfig updates 'couchAddress' in config
+func updateConfig(couchAddress string) {
+
+	viper.Set("ledger.state.stateDatabase", "CouchDB")
+	viper.Set("ledger.state.couchDBConfig.couchDBAddress", couchAddress)
+	// Replace with correct username/password such as
+	// admin/admin if user security is enabled on couchdb.
+	viper.Set("ledger.state.couchDBConfig.username", "")
+	viper.Set("ledger.state.couchDBConfig.password", "")
+	viper.Set("ledger.state.couchDBConfig.maxRetries", 1)
+	viper.Set("ledger.state.couchDBConfig.maxRetriesOnStartup", 1)
+	viper.Set("ledger.state.couchDBConfig.requestTimeout", time.Second*35)
+	viper.Set("ledger.state.couchDBConfig.createGlobalChangesDB", false)
+
+}
+
+// TestLedgerConf return the ledger configs
+func TestLedgerConf() *ledger.Config {
+	// set defaults
+	warmAfterNBlocks := 1
+	if viper.IsSet("ledger.state.couchDBConfig.warmIndexesAfterNBlocks") {
+		warmAfterNBlocks = viper.GetInt("ledger.state.couchDBConfig.warmIndexesAfterNBlocks")
+	}
+	internalQueryLimit := 1000
+	if viper.IsSet("ledger.state.couchDBConfig.internalQueryLimit") {
+		internalQueryLimit = viper.GetInt("ledger.state.couchDBConfig.internalQueryLimit")
+	}
+	maxBatchUpdateSize := 500
+	if viper.IsSet("ledger.state.couchDBConfig.maxBatchUpdateSize") {
+		maxBatchUpdateSize = viper.GetInt("ledger.state.couchDBConfig.maxBatchUpdateSize")
+	}
+	collElgProcMaxDbBatchSize := 5000
+	if viper.IsSet("ledger.pvtdataStore.collElgProcMaxDbBatchSize") {
+		collElgProcMaxDbBatchSize = viper.GetInt("ledger.pvtdataStore.collElgProcMaxDbBatchSize")
+	}
+	collElgProcDbBatchesInterval := 1000
+	if viper.IsSet("ledger.pvtdataStore.collElgProcDbBatchesInterval") {
+		collElgProcDbBatchesInterval = viper.GetInt("ledger.pvtdataStore.collElgProcDbBatchesInterval")
+	}
+	purgeInterval := 100
+	if viper.IsSet("ledger.pvtdataStore.purgeInterval") {
+		purgeInterval = viper.GetInt("ledger.pvtdataStore.purgeInterval")
+	}
+
+	rootFSPath := filepath.Join(coreconfig.GetPath("peer.fileSystemPath"), "ledgersData")
+	conf := &ledger.Config{
+		RootFSPath: rootFSPath,
+		StateDB: &ledger.StateDB{
+			StateDatabase: viper.GetString("ledger.state.stateDatabase"),
+			LevelDBPath:   filepath.Join(rootFSPath, "stateLeveldb"),
+			CouchDB:       &couchdb.Config{},
+		},
+		PrivateData: &ledger.PrivateData{
+			StorePath:       filepath.Join(rootFSPath, "pvtdataStore"),
+			MaxBatchSize:    collElgProcMaxDbBatchSize,
+			BatchesInterval: collElgProcDbBatchesInterval,
+			PurgeInterval:   purgeInterval,
+		},
+		HistoryDB: &ledger.HistoryDB{
+			Enabled: viper.GetBool("ledger.history.enableHistoryDatabase"),
+		},
+	}
+
+	conf.StateDB.CouchDB = &couchdb.Config{
+		Address:                 viper.GetString("ledger.state.couchDBConfig.couchDBAddress"),
+		Username:                viper.GetString("ledger.state.couchDBConfig.username"),
+		Password:                viper.GetString("ledger.state.couchDBConfig.password"),
+		MaxRetries:              viper.GetInt("ledger.state.couchDBConfig.maxRetries"),
+		MaxRetriesOnStartup:     viper.GetInt("ledger.state.couchDBConfig.maxRetriesOnStartup"),
+		RequestTimeout:          viper.GetDuration("ledger.state.couchDBConfig.requestTimeout"),
+		InternalQueryLimit:      internalQueryLimit,
+		MaxBatchUpdateSize:      maxBatchUpdateSize,
+		WarmIndexesAfterNBlocks: warmAfterNBlocks,
+		CreateGlobalChangesDB:   viper.GetBool("ledger.state.couchDBConfig.createGlobalChangesDB"),
+		RedoLogPath:             filepath.Join(rootFSPath, "couchdbRedoLogs"),
+	}
+
+	return conf
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/common/common_store_helper.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/common/common_store_helper.go
new file mode 100644
index 000000000..65ef598f8
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/common/common_store_helper.go
@@ -0,0 +1,159 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"bytes"
+	"errors"
+
+	"github.com/hyperledger/fabric/common/ledger/util"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+)
+
+// TODO add pinning script to include copied code into this file, original file from fabric is found in fabric/core/transientstore/store_helper.go
+// TODO below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+var (
+	prwsetPrefix             = []byte("P")[0] // key prefix for storing private write set in transient store.
+	purgeIndexByHeightPrefix = []byte("H")[0] // key prefix for storing index on private write set using received at block height.
+	//purgeIndexByTxidPrefix   = []byte("T")[0] // key prefix for storing index on private write set using txid
+	compositeKeySep = byte(0x00)
+)
+
+// CreateCompositeKeyForPvtRWSet creates a key for storing private write set
+// in the transient store. The structure of the key is <prwsetPrefix>~txid~uuid~blockHeight.
+// TODO add pinning script to expose this function
+func CreateCompositeKeyForPvtRWSet(txid string, uuid string, blockHeight uint64) []byte {
+	var compositeKey []byte
+	compositeKey = append(compositeKey, prwsetPrefix)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, createCompositeKeyWithoutPrefixForTxid(txid, uuid, blockHeight)...)
+
+	return compositeKey
+}
+
+// createCompositeKeyWithoutPrefixForTxid creates a composite key of structure txid~uuid~blockHeight.
+func createCompositeKeyWithoutPrefixForTxid(txid string, uuid string, blockHeight uint64) []byte {
+	var compositeKey []byte
+	compositeKey = append(compositeKey, []byte(txid)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, []byte(uuid)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, util.EncodeOrderPreservingVarUint64(blockHeight)...)
+
+	return compositeKey
+}
+
+// CreateCompositeKeyForPurgeIndexByHeight creates a key to index private write set based on
+// received at block height such that purge based on block height can be achieved. The structure
+// of the key is <purgeIndexByHeightPrefix>~blockHeight~txid~uuid.
+// TODO add pinning script to expose this function
+func CreateCompositeKeyForPurgeIndexByHeight(blockHeight uint64, txid string, uuid string) []byte {
+	var compositeKey []byte
+	compositeKey = append(compositeKey, purgeIndexByHeightPrefix)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, util.EncodeOrderPreservingVarUint64(blockHeight)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, []byte(txid)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, []byte(uuid)...)
+
+	return compositeKey
+}
+
+// SplitCompositeKeyOfPvtRWSet splits the compositeKey (<prwsetPrefix>~txid~uuid~blockHeight)
+// into uuid and blockHeight.
+// TODO add pinning script to expose this function
+func SplitCompositeKeyOfPvtRWSet(compositeKey []byte) (uuid string, blockHeight uint64) {
+	return splitCompositeKeyWithoutPrefixForTxid(compositeKey[2:])
+}
+
+// SplitCompositeKeyOfPurgeIndexByHeight splits the compositeKey (<purgeIndexByHeightPrefix>~blockHeight~txid~uuid)
+// into txid, uuid and blockHeight.
+// TODO add pinning script to expose this function
+func SplitCompositeKeyOfPurgeIndexByHeight(compositeKey []byte) (txid string, uuid string, blockHeight uint64) {
+	var n int
+	blockHeight, n = util.DecodeOrderPreservingVarUint64(compositeKey[2:])
+	splits := bytes.Split(compositeKey[n+3:], []byte{compositeKeySep})
+	txid = string(splits[0])
+	uuid = string(splits[1])
+	return
+}
+
+// splitCompositeKeyWithoutPrefixForTxid splits the composite key txid~uuid~blockHeight into
+// uuid and blockHeight
+func splitCompositeKeyWithoutPrefixForTxid(compositeKey []byte) (uuid string, blockHeight uint64) {
+	// skip txid as all functions which requires split of composite key already has it
+	firstSepIndex := bytes.IndexByte(compositeKey, compositeKeySep)
+	secondSepIndex := firstSepIndex + bytes.IndexByte(compositeKey[firstSepIndex+1:], compositeKeySep) + 1
+	uuid = string(compositeKey[firstSepIndex+1 : secondSepIndex])
+	blockHeight, _ = util.DecodeOrderPreservingVarUint64(compositeKey[secondSepIndex+1:])
+	return
+}
+
+// TrimPvtWSet returns a `TxPvtReadWriteSet` that retains only list of 'ns/collections' supplied in the filter
+// A nil filter does not filter any results and returns the original `pvtWSet` as is
+// TODO add pinning script to expose this function
+func TrimPvtWSet(pvtWSet *rwset.TxPvtReadWriteSet, filter ledger.PvtNsCollFilter) *rwset.TxPvtReadWriteSet {
+	if filter == nil {
+		return pvtWSet
+	}
+
+	var filteredNsRwSet []*rwset.NsPvtReadWriteSet
+	for _, ns := range pvtWSet.NsPvtRwset {
+		var filteredCollRwSet []*rwset.CollectionPvtReadWriteSet
+		for _, coll := range ns.CollectionPvtRwset {
+			if filter.Has(ns.Namespace, coll.CollectionName) {
+				filteredCollRwSet = append(filteredCollRwSet, coll)
+			}
+		}
+		if filteredCollRwSet != nil {
+			filteredNsRwSet = append(filteredNsRwSet,
+				&rwset.NsPvtReadWriteSet{
+					Namespace:          ns.Namespace,
+					CollectionPvtRwset: filteredCollRwSet,
+				},
+			)
+		}
+	}
+	var filteredTxPvtRwSet *rwset.TxPvtReadWriteSet
+	if filteredNsRwSet != nil {
+		filteredTxPvtRwSet = &rwset.TxPvtReadWriteSet{
+			DataModel:  pvtWSet.GetDataModel(),
+			NsPvtRwset: filteredNsRwSet,
+		}
+	}
+	return filteredTxPvtRwSet
+}
+
+// TrimPvtCollectionConfigs returns a map of `CollectionConfigPackage` with configs retained only for config types 'staticCollectionConfig' supplied in the filter
+// A nil filter does not set Config to any collectionConfigPackage and returns a map with empty configs for each `configs` element
+// TODO add pinning script to expose this function and add below comment
+func TrimPvtCollectionConfigs(configs map[string]*common.CollectionConfigPackage,
+	filter ledger.PvtNsCollFilter) (map[string]*common.CollectionConfigPackage, error) {
+	if filter == nil {
+		return configs, nil
+	}
+	result := make(map[string]*common.CollectionConfigPackage)
+
+	for ns, pkg := range configs {
+		result[ns] = &common.CollectionConfigPackage{}
+		for _, colConf := range pkg.GetConfig() {
+			switch cconf := colConf.Payload.(type) {
+			case *common.CollectionConfig_StaticCollectionConfig:
+				if filter.Has(ns, cconf.StaticCollectionConfig.Name) {
+					result[ns].Config = append(result[ns].Config, colConf)
+				}
+			default:
+				return nil, errors.New("unexpected collection type")
+			}
+		}
+	}
+	return result, nil
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store.go
new file mode 100644
index 000000000..ad40e4c23
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store.go
@@ -0,0 +1,471 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"encoding/base64"
+	"encoding/hex"
+	"fmt"
+	"sort"
+
+	"github.com/bluele/gcache"
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/util"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/transientstore"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+	"github.com/trustbloc/fabric-peer-ext/pkg/transientstore/common"
+)
+
+var nilByte = byte('\x00')
+
+// ErrStoreEmpty is used to indicate that there are no entries in transient store
+var ErrStoreEmpty = errors.New("Transient store is empty")
+
+// Store manages the storage of private write sets for a ledgerId.
+// Ideally, a ledger can remove the data from this storage when it is committed to
+// the permanent storage or the pruning of some data items is enforced by the policy
+// the internal storage mechanism used in this specific store is gcache of type 'simple'
+// cache to allow 'unlimited' growth of data and avoid eviction due to size. The assumption
+// is the ledger will purge data from this storage once transactions are committed or deemed invalid.
+
+type store struct {
+	// cache contains a key of txid and a value represented as a map of composite key/TxRWSet values (real data store)
+	cache gcache.Cache
+	// blockHeightCache contains a key of blockHeight and value represented as a slice of txids
+	blockHeightCache gcache.Cache
+	// txidCache contains a key of txid and value represented as a slice of blockHeights
+	txidCache gcache.Cache
+}
+
+func newStore() *store {
+	s := &store{}
+	s.cache = gcache.New(0).LoaderFunc(loadPvtRWSetMap).Build()
+	s.blockHeightCache = gcache.New(0).LoaderFunc(loadBlockHeight).Build()
+	s.txidCache = gcache.New(0).LoaderFunc(loadTxid).Build()
+	return s
+}
+
+// Persist stores the private write set of a transaction in the transient store
+// based on txid and the block height the private data was received at
+func (s *store) Persist(txid string, blockHeight uint64, privateSimulationResults *rwset.TxPvtReadWriteSet) error {
+	logger.Debugf("Persisting private data to transient store for txid [%s] at block height [%d]", txid, blockHeight)
+
+	uuid := util.GenerateUUID()
+	compositeKeyPvtRWSet := common.CreateCompositeKeyForPvtRWSet(txid, uuid, blockHeight)
+	privateSimulationResultsBytes, err := proto.Marshal(privateSimulationResults)
+	if err != nil {
+		return err
+	}
+
+	s.setTxPvtWRSetToCache(txid, compositeKeyPvtRWSet, privateSimulationResultsBytes)
+
+	s.setTxidToBlockHeightCache(txid, blockHeight)
+
+	s.updateTxidCache(txid, blockHeight)
+	return nil
+}
+
+// PersistWithConfig stores the private write set of a transaction along with the collection config
+// in the transient store based on txid and the block height the private data was received at
+func (s *store) PersistWithConfig(txid string, blockHeight uint64, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	if privateSimulationResultsWithConfig != nil {
+		logger.Debugf("Persisting private data to transient store for txid [%s] at block height [%d] with [%d] config(s)", txid, blockHeight, len(privateSimulationResultsWithConfig.CollectionConfigs))
+	} else {
+		logger.Debugf("Persisting private data to transient store for txid [%s] at block height [%d] with nil config", txid, blockHeight)
+	}
+
+	uuid := util.GenerateUUID()
+	compositeKeyPvtRWSet := common.CreateCompositeKeyForPvtRWSet(txid, uuid, blockHeight)
+	privateSimulationResultsWithConfigBytes, err := proto.Marshal(privateSimulationResultsWithConfig)
+	if err != nil {
+		return err
+	}
+
+	// emulating original Fabric's new proto (post v1.2) by appending nilByte
+	// TODO remove this when Fabric stops appending nilByte
+	privateSimulationResultsWithConfigBytes = append([]byte{nilByte}, privateSimulationResultsWithConfigBytes...)
+
+	s.setTxPvtWRSetToCache(txid, compositeKeyPvtRWSet, privateSimulationResultsWithConfigBytes)
+
+	s.setTxidToBlockHeightCache(txid, blockHeight)
+
+	s.updateTxidCache(txid, blockHeight)
+	return nil
+}
+
+func (s *store) updateTxidCache(txid string, blockHeight uint64) {
+	value, err := s.txidCache.Get(txid)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+	}
+	uintVal := value.(*blockHeightsSlice)
+	if found, _ := uintVal.findBlockHeightEntryInSlice(blockHeight); !found {
+		uintVal.add(blockHeight)
+	}
+
+	err = s.txidCache.Set(txid, uintVal)
+	if err != nil {
+		panic(fmt.Sprintf("Storing blockheight '%d' for txid key '%s' in transientstore cache must never fail, err:%s", blockHeight, txid, err))
+	}
+}
+
+func (s *store) getTxPvtRWSetFromCache(txid string) *pvtRWSetMap {
+	value, err := s.cache.Get(txid)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+	}
+
+	return value.(*pvtRWSetMap)
+}
+
+func (s *store) getTxidsFromBlockHeightCache(blockHeight uint64) *txidsSlice {
+	blockHeightValue, err := s.blockHeightCache.Get(blockHeight)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+	}
+	return blockHeightValue.(*txidsSlice)
+}
+
+func (s *store) setTxPvtWRSetToCache(txid string, compositeKeyPvtRWSet, privSimulationResults []byte) {
+	txPvtRWSetMap := s.getTxPvtRWSetFromCache(txid)
+	k := hex.EncodeToString(compositeKeyPvtRWSet)
+	v := base64.StdEncoding.EncodeToString(privSimulationResults)
+	txPvtRWSetMap.set(k, v)
+
+	err := s.cache.Set(txid, txPvtRWSetMap)
+	if err != nil {
+		panic(fmt.Sprintf("Set to cache must never return an error, got error:%s", err))
+	}
+}
+
+func (s *store) setTxidToBlockHeightCache(txid string, blockHeight uint64) {
+	blockHeightTxids := s.getTxidsFromBlockHeightCache(blockHeight)
+	found, _ := blockHeightTxids.findTxidEntryInSlice(txid)
+	if !found {
+		blockHeightTxids.add(txid)
+		err := s.blockHeightCache.Set(blockHeight, blockHeightTxids)
+		if err != nil {
+			panic(fmt.Sprintf("Set to cache must never return an error, got error:%s", err))
+		}
+	}
+}
+
+// GetTxPvtRWSetByTxid returns an iterator due to the fact that the txid may have multiple private
+// write sets persisted from different endorsers (via Gossip)
+func (s *store) GetTxPvtRWSetByTxid(txid string, filter ledger.PvtNsCollFilter) (transientstore.RWSetScanner, error) {
+	logger.Debugf("Calling GetTxPvtRWSetByTxid on transient store for txid [%s]", txid)
+	var results []keyValue
+
+	val, err := s.cache.Get(txid)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+		// return empty results
+		return &RwsetScanner{filter: filter, results: []keyValue{}}, nil
+	}
+
+	pvtRWsm := val.(*pvtRWSetMap)
+	pvtRWsm.mu.RLock()
+	defer pvtRWsm.mu.RUnlock()
+	for key, value := range pvtRWsm.m {
+		results = append(results, keyValue{key: key, value: value})
+	}
+
+	return &RwsetScanner{filter: filter, results: results}, nil
+}
+
+// GetMinTransientBlkHt returns the lowest block height remaining in transient store
+func (s *store) GetMinTransientBlkHt() (uint64, error) {
+	var minTransientBlkHt uint64
+	val := s.blockHeightCache.GetALL()
+
+	for key := range val {
+		k := key.(uint64)
+		if minTransientBlkHt == 0 || k < minTransientBlkHt {
+			minTransientBlkHt = k
+		}
+	}
+	logger.Debugf("Called GetMinTransientBlkHt on transient store, min block height is: %d", minTransientBlkHt)
+	if minTransientBlkHt == 0 { // mimic Fabric's transientstore with leveldb -> return an error
+		return 0, ErrStoreEmpty
+	}
+	return minTransientBlkHt, nil
+}
+
+// PurgeByTxids removes private write sets of a given set of transactions from the
+// transient store
+func (s *store) PurgeByTxids(txids []string) error {
+	logger.Debugf("Calling PurgeByTxids on transient store for txids [%v]", txids)
+	s.purgeTxPvtRWSetCacheByTxids(txids)
+	return s.purgeBlockHeightCacheByTxids(txids)
+}
+
+// Shutdown noop for in memory storage
+func (s *store) Shutdown() {
+
+}
+
+func (s *store) purgeTxPvtRWSetCacheByTxids(txids []string) {
+	for _, txID := range txids {
+		s.cache.Remove(txID)
+	}
+}
+
+func (s *store) purgeTxRWSetCacheByBlockHeight(txids []string, maxBlockNumToRetain uint64) error {
+	for _, txID := range txids {
+		txMap := s.getTxPvtRWSetFromCache(txID)
+		for _, txK := range txMap.keys() {
+			hexKey, err := hex.DecodeString(txK)
+			if err != nil {
+				return err
+			}
+			_, blkHeight := common.SplitCompositeKeyOfPvtRWSet(hexKey)
+			if blkHeight < maxBlockNumToRetain {
+				txMap.delete(txK)
+			}
+		}
+		if txMap.length() == 0 {
+			s.cache.Remove(txID)
+		}
+
+	}
+	return nil
+}
+
+func (s *store) purgeBlockHeightCacheByTxids(txids []string) error {
+	sort.Strings(txids)
+	var blkHeightTxids *txidsSlice
+	var blkHeightKeys []uint64
+	// step 1 fetch block heights for txids
+	blkHeightKeys, err := s.getBlockHeightKeysFromTxidCache(txids)
+	if err != nil {
+		return err
+	}
+	// step 2 remove txids from blockHeightCache
+	for _, blkHgtKey := range blkHeightKeys {
+		value, err := s.blockHeightCache.Get(blkHgtKey)
+		if err != nil {
+			if err == gcache.KeyNotFoundError {
+				continue
+			}
+			return err
+		}
+		blkHeightTxids = value.(*txidsSlice)
+		for _, txID := range txids {
+			for { // ensure to remove duplicates
+				if isFound, i := blkHeightTxids.findTxidEntryInSlice(txID); isFound {
+					blkHeightTxids.removeTxidEntryAtIndex(i)
+				} else {
+					break
+				}
+			}
+		}
+
+		if blkHeightTxids.length() == 0 {
+			s.blockHeightCache.Remove(blkHgtKey)
+		} else {
+			err := s.blockHeightCache.Set(blkHgtKey, blkHeightTxids)
+			if err != nil {
+				return err
+			}
+		}
+	}
+
+	// step 3 remove blockHeights from txidCache
+	return s.purgeTxidsCacheByBlockHeight(txids, blkHeightKeys)
+}
+
+func (s *store) getBlockHeightKeysFromTxidCache(txids []string) ([]uint64, error) {
+	var blkHeightKeys []uint64
+	for _, t := range txids {
+		blkHgts, err := s.txidCache.Get(t)
+		if err != nil {
+			if err == gcache.KeyNotFoundError {
+				continue
+			}
+			return nil, err
+		}
+		if blkHgts.(*blockHeightsSlice).length() > 0 {
+			blkHeightKeys = append(blkHeightKeys, blkHgts.(*blockHeightsSlice).getBlockHeights()...)
+		}
+	}
+	blkHeightKeys = sliceUniqueUint64(blkHeightKeys)
+	return blkHeightKeys, nil
+}
+
+// PurgeByHeight will remove all ReadWriteSets with block height below maxBlockNumToRetain
+func (s *store) PurgeByHeight(maxBlockNumToRetain uint64) error {
+	logger.Debugf("Calling PurgeByHeight on transient store for maxBlockNumToRetain [%d]", maxBlockNumToRetain)
+	txIDs := make([]string, 0)
+	blkHgts := make([]uint64, 0)
+	for key, value := range s.blockHeightCache.GetALL() {
+		k := key.(uint64)
+		if k < maxBlockNumToRetain {
+			txIDs = append(txIDs, value.(*txidsSlice).getTxids()...)
+			blkHgts = append(blkHgts, k)
+			s.blockHeightCache.Remove(k)
+		}
+	}
+	txIDs = sliceUniqueString(txIDs)
+	blkHgts = sliceUniqueUint64(blkHgts)
+	err := s.purgeTxRWSetCacheByBlockHeight(txIDs, maxBlockNumToRetain)
+	if err != nil {
+		return err
+	}
+	return s.purgeTxidsCacheByBlockHeight(txIDs, blkHgts)
+
+}
+
+func (s *store) purgeTxidsCacheByBlockHeight(txids []string, blockHeights []uint64) error {
+	for _, txid := range txids {
+		blkHgt, err := s.txidCache.Get(txid)
+		if err != nil {
+			if err == gcache.KeyNotFoundError {
+				continue
+			}
+			return err
+		}
+		blkHgtSliceByTxid := blkHgt.(*blockHeightsSlice)
+
+		for _, b := range blockHeights {
+			if isFound, i := blkHgtSliceByTxid.findBlockHeightEntryInSlice(b); isFound {
+				blkHgtSliceByTxid.removeBlockHeightEntryAtIndex(i)
+			}
+		}
+
+		if blkHgtSliceByTxid.length() == 0 {
+			s.txidCache.Remove(txid)
+		}
+	}
+	return nil
+}
+
+type keyValue struct {
+	key   string
+	value string
+}
+
+// RwsetScanner provides an iterator for EndorserPvtSimulationResults from transientstore
+type RwsetScanner struct {
+	filter  ledger.PvtNsCollFilter
+	results []keyValue
+	next    int
+}
+
+// Next moves the iterator to the next key/value pair.
+// It returns whether the iterator is exhausted.
+// TODO: Once the related gossip changes are made as per FAB-5096, remove this function
+func (scanner *RwsetScanner) Next() (*transientstore.EndorserPvtSimulationResults, error) {
+	kv, ok := scanner.nextKV()
+	if !ok {
+		return nil, nil
+	}
+
+	keyBytes, err := hex.DecodeString(kv.key)
+	if err != nil {
+		return nil, err
+	}
+	_, blockHeight := common.SplitCompositeKeyOfPvtRWSet(keyBytes)
+	logger.Debugf("scanner next blockHeight %d", blockHeight)
+	txPvtRWSet := &rwset.TxPvtReadWriteSet{}
+	valueBytes, err := base64.StdEncoding.DecodeString(kv.value)
+	if err != nil {
+		return nil, errors.Wrapf(err, "error from DecodeString for transientDataField")
+	}
+
+	if err := proto.Unmarshal(valueBytes, txPvtRWSet); err != nil {
+		return nil, err
+	}
+	filteredTxPvtRWSet := common.TrimPvtWSet(txPvtRWSet, scanner.filter)
+	logger.Debugf("scanner next filteredTxPvtRWSet %v", filteredTxPvtRWSet)
+
+	return &transientstore.EndorserPvtSimulationResults{
+		ReceivedAtBlockHeight: blockHeight,
+		PvtSimulationResults:  filteredTxPvtRWSet,
+	}, nil
+
+}
+
+// NextWithConfig moves the iterator to the next key/value pair with configs.
+// It returns whether the iterator is exhausted.
+// TODO: Once the related gossip changes are made as per FAB-5096, rename this function to Next
+func (scanner *RwsetScanner) NextWithConfig() (*transientstore.EndorserPvtSimulationResultsWithConfig, error) {
+	kv, ok := scanner.nextKV()
+	if !ok {
+		return nil, nil
+	}
+
+	keyBytes, err := hex.DecodeString(kv.key)
+	if err != nil {
+		return nil, err
+	}
+	_, blockHeight := common.SplitCompositeKeyOfPvtRWSet(keyBytes)
+	logger.Debugf("scanner NextWithConfig blockHeight %d", blockHeight)
+
+	valueBytes, err := base64.StdEncoding.DecodeString(kv.value)
+	if err != nil {
+		return nil, errors.Wrapf(err, "error from DecodeString for transientDataField")
+	}
+
+	txPvtRWSet := &rwset.TxPvtReadWriteSet{}
+	var filteredTxPvtRWSet *rwset.TxPvtReadWriteSet
+	txPvtRWSetWithConfig := &pb.TxPvtReadWriteSetWithConfigInfo{}
+
+	if valueBytes[0] == nilByte {
+		// new proto, i.e., TxPvtReadWriteSetWithConfigInfo
+		if er := proto.Unmarshal(valueBytes[1:], txPvtRWSetWithConfig); er != nil {
+			return nil, er
+		}
+
+		logger.Debugf("scanner NextWithConfig txPvtRWSetWithConfig %v", txPvtRWSetWithConfig)
+
+		filteredTxPvtRWSet = common.TrimPvtWSet(txPvtRWSetWithConfig.GetPvtRwset(), scanner.filter)
+		logger.Debugf("scanner NextWithConfig filteredTxPvtRWSet %v", filteredTxPvtRWSet)
+		configs, err := common.TrimPvtCollectionConfigs(txPvtRWSetWithConfig.CollectionConfigs, scanner.filter)
+		if err != nil {
+			return nil, err
+		}
+		logger.Debugf("scanner NextWithConfig configs %v", configs)
+		txPvtRWSetWithConfig.CollectionConfigs = configs
+	} else {
+		// old proto, i.e., TxPvtReadWriteSet
+		if e := proto.Unmarshal(valueBytes, txPvtRWSet); e != nil {
+			return nil, e
+		}
+		filteredTxPvtRWSet = common.TrimPvtWSet(txPvtRWSet, scanner.filter)
+	}
+
+	txPvtRWSetWithConfig.PvtRwset = filteredTxPvtRWSet
+
+	return &transientstore.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          blockHeight,
+		PvtSimulationResultsWithConfig: txPvtRWSetWithConfig,
+	}, nil
+}
+
+func (scanner *RwsetScanner) nextKV() (keyValue, bool) {
+	i := scanner.next
+	if i >= len(scanner.results) {
+		return keyValue{}, false
+	}
+	scanner.next++
+	return scanner.results[i], true
+}
+
+// Close releases resource held by the iterator
+func (scanner *RwsetScanner) Close() {
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store_helper.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store_helper.go
new file mode 100644
index 000000000..04a3532d8
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/store_helper.go
@@ -0,0 +1,190 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"sort"
+	"sync"
+)
+
+// loadPvtRWSetMap is a loader function of store.cache
+func loadPvtRWSetMap(key interface{}) (interface{}, error) {
+	return &pvtRWSetMap{m: map[string]string{}}, nil
+}
+
+// pvtRWSetMap represents a cached element in store.cache
+type pvtRWSetMap struct {
+	mu sync.RWMutex
+	m  map[string]string
+}
+
+//func (p *pvtRWSetMap) get(k string) string {
+//	p.mu.RLock()
+//	defer p.mu.RUnlock()
+//
+//	return p.m[k]
+//}
+
+func (p *pvtRWSetMap) set(k string, v string) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m[k] = v
+}
+
+func (p *pvtRWSetMap) length() int {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return len(p.m)
+}
+
+func (p *pvtRWSetMap) delete(k string) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	delete(p.m, k)
+}
+
+func (p *pvtRWSetMap) keys() []string {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	keys := make([]string, 0, len(p.m))
+	for k := range p.m {
+		keys = append(keys, k)
+	}
+	return keys
+}
+
+// loadBlockHeight is a loader function of store.blockHeightCache
+func loadBlockHeight(key interface{}) (interface{}, error) {
+	return &txidsSlice{m: []string{}}, nil
+}
+
+// txidsSlice represents a cached element in store.blockHeightCache
+type txidsSlice struct {
+	mu sync.RWMutex
+	m  []string
+}
+
+func (p *txidsSlice) add(v string) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m = append(p.m, v)
+	sort.Strings(p.m) // ensures txids are sorted to help sort.search call in findTxidEntryInSlice below
+}
+
+// findTxidEntryInSlice will search for txid in txidsSlice
+func (p *txidsSlice) findTxidEntryInSlice(txid string) (bool, int) {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+	i := sort.Search(len(p.m), func(i int) bool { return p.m[i] >= txid }) //  equivalent to sort.SearchStrings()
+	return i < len(p.m) && p.m[i] == txid, i
+}
+
+// removeTxidEntryAtIndex removes an entry at index in the txidsSlice
+func (p *txidsSlice) removeTxidEntryAtIndex(index int) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+	p.m = append(p.m[:index], p.m[index+1:]...)
+}
+
+func (p *txidsSlice) length() int {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return len(p.m)
+}
+
+func (p *txidsSlice) getTxids() []string {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return p.m
+}
+
+// loadTxid is a loader function of store.txidCache
+func loadTxid(key interface{}) (interface{}, error) {
+	return &blockHeightsSlice{m: []uint64{}}, nil
+}
+
+// blockHeightsSlice represents a cached element in store.txidCache
+type blockHeightsSlice struct {
+	mu sync.RWMutex
+	m  []uint64
+}
+
+func (p *blockHeightsSlice) add(v uint64) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m = append(p.m, v)
+	sort.Slice(p.m, func(i, j int) bool { return p.m[i] < p.m[j] }) // ensures blockHeights are sorted to help sort.search call in findBlockHeightEntryInSlice below
+}
+
+// findBlockHeightEntryInSlice will search for a blockheight (uint64) entry in blockHeightsSlice
+func (p *blockHeightsSlice) findBlockHeightEntryInSlice(blockHeight uint64) (bool, int) {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	i := sort.Search(len(p.m), func(i int) bool { return p.m[i] >= blockHeight })
+	return i < len(p.m) && p.m[i] == blockHeight, i
+}
+
+// removeBlockHeightEntryAtIndex removes an entry at index in blockHeightsSlice
+func (p *blockHeightsSlice) removeBlockHeightEntryAtIndex(index int) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m = append(p.m[:index], p.m[index+1:]...)
+}
+
+func (p *blockHeightsSlice) length() int {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return len(p.m)
+}
+
+func (p *blockHeightsSlice) getBlockHeights() []uint64 {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return p.m
+}
+
+// sliceUniqueString will strip out any duplicate entries from a slice s (of type string)
+func sliceUniqueString(s []string) []string {
+	seen := make(map[string]struct{}, len(s))
+	j := 0
+	for _, v := range s {
+		if _, ok := seen[v]; ok {
+			continue
+		}
+		seen[v] = struct{}{}
+		s[j] = v
+		j++
+	}
+	return s[:j]
+}
+
+// sliceUniqueUint64 will strip out any duplicate entries from a slice s (of type uint64)
+func sliceUniqueUint64(s []uint64) []uint64 {
+	seen := make(map[uint64]struct{}, len(s))
+	j := 0
+	for _, v := range s {
+		if _, ok := seen[v]; ok {
+			continue
+		}
+		seen[v] = struct{}{}
+		s[j] = v
+		j++
+	}
+	return s[:j]
+}
diff --git a/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/storeprovider.go b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/storeprovider.go
new file mode 100644
index 000000000..098dd6faf
--- /dev/null
+++ b/vendor/github.com/trustbloc/fabric-peer-ext/pkg/transientstore/storeprovider.go
@@ -0,0 +1,33 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/transientstore"
+)
+
+var logger = flogging.MustGetLogger("transientstore")
+
+// Provider represents a trasientstore provider
+type Provider struct {
+}
+
+// NewStoreProvider instantiates a transient data storage provider backed by Memory
+func NewStoreProvider() *Provider {
+	logger.Debugf("constructing transient mem data storage provider")
+	return &Provider{}
+}
+
+// OpenStore creates a handle to the transient data store for the given ledger ID
+func (p *Provider) OpenStore(ledgerid string) (transientstore.Store, error) {
+	return newStore(), nil
+}
+
+// Close cleans up the provider
+func (p *Provider) Close() {
+}
diff --git a/vendor/google.golang.org/grpc/backoff.go b/vendor/google.golang.org/grpc/backoff.go
index fa31565fd..97c6e2568 100644
--- a/vendor/google.golang.org/grpc/backoff.go
+++ b/vendor/google.golang.org/grpc/backoff.go
@@ -17,7 +17,7 @@
  */

 // See internal/backoff package for the backoff implementation. This file is
-// kept for the exported types and API backward compatility.
+// kept for the exported types and API backward compatibility.

 package grpc

diff --git a/vendor/google.golang.org/grpc/balancer.go b/vendor/google.golang.org/grpc/balancer.go
index 5aeb646d1..a78e702ba 100644
--- a/vendor/google.golang.org/grpc/balancer.go
+++ b/vendor/google.golang.org/grpc/balancer.go
@@ -19,10 +19,10 @@
 package grpc

 import (
+	"context"
 	"net"
 	"sync"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/credentials"
 	"google.golang.org/grpc/grpclog"
diff --git a/vendor/google.golang.org/grpc/balancer/balancer.go b/vendor/google.golang.org/grpc/balancer/balancer.go
index 069feb1e7..67518de9a 100644
--- a/vendor/google.golang.org/grpc/balancer/balancer.go
+++ b/vendor/google.golang.org/grpc/balancer/balancer.go
@@ -21,13 +21,15 @@
 package balancer

 import (
+	"context"
 	"errors"
 	"net"
 	"strings"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/connectivity"
 	"google.golang.org/grpc/credentials"
+	"google.golang.org/grpc/internal"
+	"google.golang.org/grpc/metadata"
 	"google.golang.org/grpc/resolver"
 )

@@ -46,8 +48,20 @@ func Register(b Builder) {
 	m[strings.ToLower(b.Name())] = b
 }

+// unregisterForTesting deletes the balancer with the given name from the
+// balancer map.
+//
+// This function is not thread-safe.
+func unregisterForTesting(name string) {
+	delete(m, name)
+}
+
+func init() {
+	internal.BalancerUnregister = unregisterForTesting
+}
+
 // Get returns the resolver builder registered with the given name.
-// Note that the compare is done in a case-insenstive fashion.
+// Note that the compare is done in a case-insensitive fashion.
 // If no builder is register with the name, nil will be returned.
 func Get(name string) Builder {
 	if b, ok := m[strings.ToLower(name)]; ok {
@@ -88,7 +102,15 @@ type SubConn interface {
 }

 // NewSubConnOptions contains options to create new SubConn.
-type NewSubConnOptions struct{}
+type NewSubConnOptions struct {
+	// CredsBundle is the credentials bundle that will be used in the created
+	// SubConn. If it's nil, the original creds from grpc DialOptions will be
+	// used.
+	CredsBundle credentials.Bundle
+	// HealthCheckEnabled indicates whether health check service should be
+	// enabled on this SubConn
+	HealthCheckEnabled bool
+}

 // ClientConn represents a gRPC ClientConn.
 //
@@ -105,7 +127,7 @@ type ClientConn interface {
 	// The SubConn will be shutdown.
 	RemoveSubConn(SubConn)

-	// UpdateBalancerState is called by balancer to nofity gRPC that some internal
+	// UpdateBalancerState is called by balancer to notify gRPC that some internal
 	// state in balancer has changed.
 	//
 	// gRPC will update the connectivity state of the ClientConn, and will call pick
@@ -125,6 +147,8 @@ type BuildOptions struct {
 	// use to dial to a remote load balancer server. The Balancer implementations
 	// can ignore this if it does not need to talk to another party securely.
 	DialCreds credentials.TransportCredentials
+	// CredsBundle is the credentials bundle that the Balancer can use.
+	CredsBundle credentials.Bundle
 	// Dialer is the custom dialer the Balancer implementation can use to dial
 	// to a remote load balancer server. The Balancer implementations
 	// can ignore this if it doesn't need to talk to remote balancer.
@@ -147,12 +171,17 @@ type PickOptions struct {
 	// FullMethodName is the method name that NewClientStream() is called
 	// with. The canonical format is /service/Method.
 	FullMethodName string
+	// Header contains the metadata from the RPC's client header.  The metadata
+	// should not be modified; make a copy first if needed.
+	Header metadata.MD
 }

 // DoneInfo contains additional information for done.
 type DoneInfo struct {
 	// Err is the rpc error the RPC finished with. It could be nil.
 	Err error
+	// Trailer contains the metadata from the RPC's trailer, if present.
+	Trailer metadata.MD
 	// BytesSent indicates if any bytes have been sent to the server.
 	BytesSent bool
 	// BytesReceived indicates if any byte has been received from the server.
@@ -198,9 +227,10 @@ type Picker interface {
 	// - Else (error is other non-nil error):
 	//   - The RPC will fail with unavailable error.
 	//
-	// The returned done() function will be called once the rpc has finished, with the
-	// final status of that RPC.
-	// done may be nil if balancer doesn't care about the RPC status.
+	// The returned done() function will be called once the rpc has finished,
+	// with the final status of that RPC.  If the SubConn returned is not a
+	// valid SubConn type, done may not be called.  done may be nil if balancer
+	// doesn't care about the RPC status.
 	Pick(ctx context.Context, opts PickOptions) (conn SubConn, done func(DoneInfo), err error)
 }

diff --git a/vendor/google.golang.org/grpc/balancer/base/balancer.go b/vendor/google.golang.org/grpc/balancer/base/balancer.go
index 23d13511b..245785e7a 100644
--- a/vendor/google.golang.org/grpc/balancer/base/balancer.go
+++ b/vendor/google.golang.org/grpc/balancer/base/balancer.go
@@ -19,7 +19,8 @@
 package base

 import (
-	"golang.org/x/net/context"
+	"context"
+
 	"google.golang.org/grpc/balancer"
 	"google.golang.org/grpc/connectivity"
 	"google.golang.org/grpc/grpclog"
@@ -29,6 +30,7 @@ import (
 type baseBuilder struct {
 	name          string
 	pickerBuilder PickerBuilder
+	config        Config
 }

 func (bb *baseBuilder) Build(cc balancer.ClientConn, opt balancer.BuildOptions) balancer.Balancer {
@@ -38,11 +40,12 @@ func (bb *baseBuilder) Build(cc balancer.ClientConn, opt balancer.BuildOptions)

 		subConns: make(map[resolver.Address]balancer.SubConn),
 		scStates: make(map[balancer.SubConn]connectivity.State),
-		csEvltr:  &connectivityStateEvaluator{},
+		csEvltr:  &balancer.ConnectivityStateEvaluator{},
 		// Initialize picker to a picker that always return
 		// ErrNoSubConnAvailable, because when state of a SubConn changes, we
 		// may call UpdateBalancerState with this picker.
 		picker: NewErrPicker(balancer.ErrNoSubConnAvailable),
+		config: bb.config,
 	}
 }

@@ -54,12 +57,13 @@ type baseBalancer struct {
 	cc            balancer.ClientConn
 	pickerBuilder PickerBuilder

-	csEvltr *connectivityStateEvaluator
+	csEvltr *balancer.ConnectivityStateEvaluator
 	state   connectivity.State

 	subConns map[resolver.Address]balancer.SubConn
 	scStates map[balancer.SubConn]connectivity.State
 	picker   balancer.Picker
+	config   Config
 }

 func (b *baseBalancer) HandleResolvedAddrs(addrs []resolver.Address, err error) {
@@ -74,7 +78,7 @@ func (b *baseBalancer) HandleResolvedAddrs(addrs []resolver.Address, err error)
 		addrsSet[a] = struct{}{}
 		if _, ok := b.subConns[a]; !ok {
 			// a is a new address (not existing in b.subConns).
-			sc, err := b.cc.NewSubConn([]resolver.Address{a}, balancer.NewSubConnOptions{})
+			sc, err := b.cc.NewSubConn([]resolver.Address{a}, balancer.NewSubConnOptions{HealthCheckEnabled: b.config.HealthCheck})
 			if err != nil {
 				grpclog.Warningf("base.baseBalancer: failed to create new SubConn: %v", err)
 				continue
@@ -133,7 +137,7 @@ func (b *baseBalancer) HandleSubConnStateChange(sc balancer.SubConn, s connectiv
 	}

 	oldAggrState := b.state
-	b.state = b.csEvltr.recordTransition(oldS, s)
+	b.state = b.csEvltr.RecordTransition(oldS, s)

 	// Regenerate picker when one of the following happens:
 	//  - this sc became ready from not-ready
@@ -165,44 +169,3 @@ type errPicker struct {
 func (p *errPicker) Pick(ctx context.Context, opts balancer.PickOptions) (balancer.SubConn, func(balancer.DoneInfo), error) {
 	return nil, nil, p.err
 }
-
-// connectivityStateEvaluator gets updated by addrConns when their
-// states transition, based on which it evaluates the state of
-// ClientConn.
-type connectivityStateEvaluator struct {
-	numReady            uint64 // Number of addrConns in ready state.
-	numConnecting       uint64 // Number of addrConns in connecting state.
-	numTransientFailure uint64 // Number of addrConns in transientFailure.
-}
-
-// recordTransition records state change happening in every subConn and based on
-// that it evaluates what aggregated state should be.
-// It can only transition between Ready, Connecting and TransientFailure. Other states,
-// Idle and Shutdown are transitioned into by ClientConn; in the beginning of the connection
-// before any subConn is created ClientConn is in idle state. In the end when ClientConn
-// closes it is in Shutdown state.
-//
-// recordTransition should only be called synchronously from the same goroutine.
-func (cse *connectivityStateEvaluator) recordTransition(oldState, newState connectivity.State) connectivity.State {
-	// Update counters.
-	for idx, state := range []connectivity.State{oldState, newState} {
-		updateVal := 2*uint64(idx) - 1 // -1 for oldState and +1 for new.
-		switch state {
-		case connectivity.Ready:
-			cse.numReady += updateVal
-		case connectivity.Connecting:
-			cse.numConnecting += updateVal
-		case connectivity.TransientFailure:
-			cse.numTransientFailure += updateVal
-		}
-	}
-
-	// Evaluate.
-	if cse.numReady > 0 {
-		return connectivity.Ready
-	}
-	if cse.numConnecting > 0 {
-		return connectivity.Connecting
-	}
-	return connectivity.TransientFailure
-}
diff --git a/vendor/google.golang.org/grpc/balancer/base/base.go b/vendor/google.golang.org/grpc/balancer/base/base.go
index 012ace2f2..34b1f2994 100644
--- a/vendor/google.golang.org/grpc/balancer/base/base.go
+++ b/vendor/google.golang.org/grpc/balancer/base/base.go
@@ -45,8 +45,20 @@ type PickerBuilder interface {
 // NewBalancerBuilder returns a balancer builder. The balancers
 // built by this builder will use the picker builder to build pickers.
 func NewBalancerBuilder(name string, pb PickerBuilder) balancer.Builder {
+	return NewBalancerBuilderWithConfig(name, pb, Config{})
+}
+
+// Config contains the config info about the base balancer builder.
+type Config struct {
+	// HealthCheck indicates whether health checking should be enabled for this specific balancer.
+	HealthCheck bool
+}
+
+// NewBalancerBuilderWithConfig returns a base balancer builder configured by the provided config.
+func NewBalancerBuilderWithConfig(name string, pb PickerBuilder, config Config) balancer.Builder {
 	return &baseBuilder{
 		name:          name,
 		pickerBuilder: pb,
+		config:        config,
 	}
 }
diff --git a/vendor/google.golang.org/grpc/balancer/roundrobin/roundrobin.go b/vendor/google.golang.org/grpc/balancer/roundrobin/roundrobin.go
index 2eda0a1c2..29f7a4ddd 100644
--- a/vendor/google.golang.org/grpc/balancer/roundrobin/roundrobin.go
+++ b/vendor/google.golang.org/grpc/balancer/roundrobin/roundrobin.go
@@ -22,12 +22,13 @@
 package roundrobin

 import (
+	"context"
 	"sync"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/balancer"
 	"google.golang.org/grpc/balancer/base"
 	"google.golang.org/grpc/grpclog"
+	"google.golang.org/grpc/internal/grpcrand"
 	"google.golang.org/grpc/resolver"
 )

@@ -36,7 +37,7 @@ const Name = "round_robin"

 // newBuilder creates a new roundrobin balancer builder.
 func newBuilder() balancer.Builder {
-	return base.NewBalancerBuilder(Name, &rrPickerBuilder{})
+	return base.NewBalancerBuilderWithConfig(Name, &rrPickerBuilder{}, base.Config{HealthCheck: true})
 }

 func init() {
@@ -47,12 +48,19 @@ type rrPickerBuilder struct{}

 func (*rrPickerBuilder) Build(readySCs map[resolver.Address]balancer.SubConn) balancer.Picker {
 	grpclog.Infof("roundrobinPicker: newPicker called with readySCs: %v", readySCs)
+	if len(readySCs) == 0 {
+		return base.NewErrPicker(balancer.ErrNoSubConnAvailable)
+	}
 	var scs []balancer.SubConn
 	for _, sc := range readySCs {
 		scs = append(scs, sc)
 	}
 	return &rrPicker{
 		subConns: scs,
+		// Start at a random index, as the same RR balancer rebuilds a new
+		// picker when SubConn states change, and we don't want to apply excess
+		// load to the first server in the list.
+		next: grpcrand.Intn(len(scs)),
 	}
 }

@@ -67,10 +75,6 @@ type rrPicker struct {
 }

 func (p *rrPicker) Pick(ctx context.Context, opts balancer.PickOptions) (balancer.SubConn, func(balancer.DoneInfo), error) {
-	if len(p.subConns) <= 0 {
-		return nil, nil, balancer.ErrNoSubConnAvailable
-	}
-
 	p.mu.Lock()
 	sc := p.subConns[p.next]
 	p.next = (p.next + 1) % len(p.subConns)
diff --git a/vendor/google.golang.org/grpc/balancer_conn_wrappers.go b/vendor/google.golang.org/grpc/balancer_conn_wrappers.go
index c23f81706..7233ade29 100644
--- a/vendor/google.golang.org/grpc/balancer_conn_wrappers.go
+++ b/vendor/google.golang.org/grpc/balancer_conn_wrappers.go
@@ -178,6 +178,28 @@ func (ccb *ccBalancerWrapper) handleSubConnStateChange(sc balancer.SubConn, s co
 }

 func (ccb *ccBalancerWrapper) handleResolvedAddrs(addrs []resolver.Address, err error) {
+	if ccb.cc.curBalancerName != grpclbName {
+		var containsGRPCLB bool
+		for _, a := range addrs {
+			if a.Type == resolver.GRPCLB {
+				containsGRPCLB = true
+				break
+			}
+		}
+		if containsGRPCLB {
+			// The current balancer is not grpclb, but addresses contain grpclb
+			// address. This means we failed to switch to grpclb, most likely
+			// because grpclb is not registered. Filter out all grpclb addresses
+			// from addrs before sending to balancer.
+			tempAddrs := make([]resolver.Address, 0, len(addrs))
+			for _, a := range addrs {
+				if a.Type != resolver.GRPCLB {
+					tempAddrs = append(tempAddrs, a)
+				}
+			}
+			addrs = tempAddrs
+		}
+	}
 	select {
 	case <-ccb.resolverUpdateCh:
 	default:
@@ -197,7 +219,7 @@ func (ccb *ccBalancerWrapper) NewSubConn(addrs []resolver.Address, opts balancer
 	if ccb.subConns == nil {
 		return nil, fmt.Errorf("grpc: ClientConn balancer wrapper was closed")
 	}
-	ac, err := ccb.cc.newAddrConn(addrs)
+	ac, err := ccb.cc.newAddrConn(addrs, opts)
 	if err != nil {
 		return nil, err
 	}
@@ -229,8 +251,13 @@ func (ccb *ccBalancerWrapper) UpdateBalancerState(s connectivity.State, p balanc
 	if ccb.subConns == nil {
 		return
 	}
-	ccb.cc.csMgr.updateState(s)
+	// Update picker before updating state.  Even though the ordering here does
+	// not matter, it can lead to multiple calls of Pick in the common start-up
+	// case where we wait for ready and then perform an RPC.  If the picker is
+	// updated later, we could call the "connecting" picker when the state is
+	// updated, and then call the "ready" picker after the picker gets updated.
 	ccb.cc.blockingpicker.updatePicker(p)
+	ccb.cc.csMgr.updateState(s)
 }

 func (ccb *ccBalancerWrapper) ResolveNow(o resolver.ResolveNowOption) {
@@ -257,6 +284,7 @@ func (acbw *acBalancerWrapper) UpdateAddresses(addrs []resolver.Address) {
 	}
 	if !acbw.ac.tryUpdateAddrs(addrs) {
 		cc := acbw.ac.cc
+		opts := acbw.ac.scopts
 		acbw.ac.mu.Lock()
 		// Set old ac.acbw to nil so the Shutdown state update will be ignored
 		// by balancer.
@@ -272,7 +300,7 @@ func (acbw *acBalancerWrapper) UpdateAddresses(addrs []resolver.Address) {
 			return
 		}

-		ac, err := cc.newAddrConn(addrs)
+		ac, err := cc.newAddrConn(addrs, opts)
 		if err != nil {
 			grpclog.Warningf("acBalancerWrapper: UpdateAddresses: failed to newAddrConn: %v", err)
 			return
diff --git a/vendor/google.golang.org/grpc/balancer_v1_wrapper.go b/vendor/google.golang.org/grpc/balancer_v1_wrapper.go
index e0ce32cfb..29bda6353 100644
--- a/vendor/google.golang.org/grpc/balancer_v1_wrapper.go
+++ b/vendor/google.golang.org/grpc/balancer_v1_wrapper.go
@@ -19,16 +19,14 @@
 package grpc

 import (
+	"context"
 	"strings"
 	"sync"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/balancer"
-	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/connectivity"
 	"google.golang.org/grpc/grpclog"
 	"google.golang.org/grpc/resolver"
-	"google.golang.org/grpc/status"
 )

 type balancerWrapperBuilder struct {
@@ -283,9 +281,8 @@ func (bw *balancerWrapper) Close() {
 }

 // The picker is the balancerWrapper itself.
-// Pick should never return ErrNoSubConnAvailable.
 // It either blocks or returns error, consistent with v1 balancer Get().
-func (bw *balancerWrapper) Pick(ctx context.Context, opts balancer.PickOptions) (balancer.SubConn, func(balancer.DoneInfo), error) {
+func (bw *balancerWrapper) Pick(ctx context.Context, opts balancer.PickOptions) (sc balancer.SubConn, done func(balancer.DoneInfo), err error) {
 	failfast := true // Default failfast is true.
 	if ss, ok := rpcInfoFromContext(ctx); ok {
 		failfast = ss.failfast
@@ -294,35 +291,51 @@ func (bw *balancerWrapper) Pick(ctx context.Context, opts balancer.PickOptions)
 	if err != nil {
 		return nil, nil, err
 	}
-	var done func(balancer.DoneInfo)
 	if p != nil {
-		done = func(i balancer.DoneInfo) { p() }
+		done = func(balancer.DoneInfo) { p() }
+		defer func() {
+			if err != nil {
+				p()
+			}
+		}()
 	}
-	var sc balancer.SubConn
+
 	bw.mu.Lock()
 	defer bw.mu.Unlock()
 	if bw.pickfirst {
 		// Get the first sc in conns.
-		for _, sc = range bw.conns {
-			break
-		}
-	} else {
-		var ok bool
-		sc, ok = bw.conns[resolver.Address{
-			Addr:       a.Addr,
-			Type:       resolver.Backend,
-			ServerName: "",
-			Metadata:   a.Metadata,
-		}]
-		if !ok && failfast {
-			return nil, nil, status.Errorf(codes.Unavailable, "there is no connection available")
-		}
-		if s, ok := bw.connSt[sc]; failfast && (!ok || s.s != connectivity.Ready) {
-			// If the returned sc is not ready and RPC is failfast,
-			// return error, and this RPC will fail.
-			return nil, nil, status.Errorf(codes.Unavailable, "there is no connection available")
+		for _, sc := range bw.conns {
+			return sc, done, nil
 		}
+		return nil, nil, balancer.ErrNoSubConnAvailable
+	}
+	sc, ok1 := bw.conns[resolver.Address{
+		Addr:       a.Addr,
+		Type:       resolver.Backend,
+		ServerName: "",
+		Metadata:   a.Metadata,
+	}]
+	s, ok2 := bw.connSt[sc]
+	if !ok1 || !ok2 {
+		// This can only happen due to a race where Get() returned an address
+		// that was subsequently removed by Notify.  In this case we should
+		// retry always.
+		return nil, nil, balancer.ErrNoSubConnAvailable
+	}
+	switch s.s {
+	case connectivity.Ready, connectivity.Idle:
+		return sc, done, nil
+	case connectivity.Shutdown, connectivity.TransientFailure:
+		// If the returned sc has been shut down or is in transient failure,
+		// return error, and this RPC will fail or wait for another picker (if
+		// non-failfast).
+		return nil, nil, balancer.ErrTransientFailure
+	default:
+		// For other states (connecting or unknown), the v1 balancer would
+		// traditionally wait until ready and then issue the RPC.  Returning
+		// ErrNoSubConnAvailable will be a slight improvement in that it will
+		// allow the balancer to choose another address in case others are
+		// connected.
+		return nil, nil, balancer.ErrNoSubConnAvailable
 	}
-
-	return sc, done, nil
 }
diff --git a/vendor/google.golang.org/grpc/binarylog/grpc_binarylog_v1/binarylog.pb.go b/vendor/google.golang.org/grpc/binarylog/grpc_binarylog_v1/binarylog.pb.go
new file mode 100644
index 000000000..f393bb661
--- /dev/null
+++ b/vendor/google.golang.org/grpc/binarylog/grpc_binarylog_v1/binarylog.pb.go
@@ -0,0 +1,900 @@
+// Code generated by protoc-gen-go. DO NOT EDIT.
+// source: grpc/binarylog/grpc_binarylog_v1/binarylog.proto
+
+package grpc_binarylog_v1 // import "google.golang.org/grpc/binarylog/grpc_binarylog_v1"
+
+import proto "github.com/golang/protobuf/proto"
+import fmt "fmt"
+import math "math"
+import duration "github.com/golang/protobuf/ptypes/duration"
+import timestamp "github.com/golang/protobuf/ptypes/timestamp"
+
+// Reference imports to suppress errors if they are not otherwise used.
+var _ = proto.Marshal
+var _ = fmt.Errorf
+var _ = math.Inf
+
+// This is a compile-time assertion to ensure that this generated file
+// is compatible with the proto package it is being compiled against.
+// A compilation error at this line likely means your copy of the
+// proto package needs to be updated.
+const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package
+
+// Enumerates the type of event
+// Note the terminology is different from the RPC semantics
+// definition, but the same meaning is expressed here.
+type GrpcLogEntry_EventType int32
+
+const (
+	GrpcLogEntry_EVENT_TYPE_UNKNOWN GrpcLogEntry_EventType = 0
+	// Header sent from client to server
+	GrpcLogEntry_EVENT_TYPE_CLIENT_HEADER GrpcLogEntry_EventType = 1
+	// Header sent from server to client
+	GrpcLogEntry_EVENT_TYPE_SERVER_HEADER GrpcLogEntry_EventType = 2
+	// Message sent from client to server
+	GrpcLogEntry_EVENT_TYPE_CLIENT_MESSAGE GrpcLogEntry_EventType = 3
+	// Message sent from server to client
+	GrpcLogEntry_EVENT_TYPE_SERVER_MESSAGE GrpcLogEntry_EventType = 4
+	// A signal that client is done sending
+	GrpcLogEntry_EVENT_TYPE_CLIENT_HALF_CLOSE GrpcLogEntry_EventType = 5
+	// Trailer indicates the end of the RPC.
+	// On client side, this event means a trailer was either received
+	// from the network or the gRPC library locally generated a status
+	// to inform the application about a failure.
+	// On server side, this event means the server application requested
+	// to send a trailer. Note: EVENT_TYPE_CANCEL may still arrive after
+	// this due to races on server side.
+	GrpcLogEntry_EVENT_TYPE_SERVER_TRAILER GrpcLogEntry_EventType = 6
+	// A signal that the RPC is cancelled. On client side, this
+	// indicates the client application requests a cancellation.
+	// On server side, this indicates that cancellation was detected.
+	// Note: This marks the end of the RPC. Events may arrive after
+	// this due to races. For example, on client side a trailer
+	// may arrive even though the application requested to cancel the RPC.
+	GrpcLogEntry_EVENT_TYPE_CANCEL GrpcLogEntry_EventType = 7
+)
+
+var GrpcLogEntry_EventType_name = map[int32]string{
+	0: "EVENT_TYPE_UNKNOWN",
+	1: "EVENT_TYPE_CLIENT_HEADER",
+	2: "EVENT_TYPE_SERVER_HEADER",
+	3: "EVENT_TYPE_CLIENT_MESSAGE",
+	4: "EVENT_TYPE_SERVER_MESSAGE",
+	5: "EVENT_TYPE_CLIENT_HALF_CLOSE",
+	6: "EVENT_TYPE_SERVER_TRAILER",
+	7: "EVENT_TYPE_CANCEL",
+}
+var GrpcLogEntry_EventType_value = map[string]int32{
+	"EVENT_TYPE_UNKNOWN":           0,
+	"EVENT_TYPE_CLIENT_HEADER":     1,
+	"EVENT_TYPE_SERVER_HEADER":     2,
+	"EVENT_TYPE_CLIENT_MESSAGE":    3,
+	"EVENT_TYPE_SERVER_MESSAGE":    4,
+	"EVENT_TYPE_CLIENT_HALF_CLOSE": 5,
+	"EVENT_TYPE_SERVER_TRAILER":    6,
+	"EVENT_TYPE_CANCEL":            7,
+}
+
+func (x GrpcLogEntry_EventType) String() string {
+	return proto.EnumName(GrpcLogEntry_EventType_name, int32(x))
+}
+func (GrpcLogEntry_EventType) EnumDescriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{0, 0}
+}
+
+// Enumerates the entity that generates the log entry
+type GrpcLogEntry_Logger int32
+
+const (
+	GrpcLogEntry_LOGGER_UNKNOWN GrpcLogEntry_Logger = 0
+	GrpcLogEntry_LOGGER_CLIENT  GrpcLogEntry_Logger = 1
+	GrpcLogEntry_LOGGER_SERVER  GrpcLogEntry_Logger = 2
+)
+
+var GrpcLogEntry_Logger_name = map[int32]string{
+	0: "LOGGER_UNKNOWN",
+	1: "LOGGER_CLIENT",
+	2: "LOGGER_SERVER",
+}
+var GrpcLogEntry_Logger_value = map[string]int32{
+	"LOGGER_UNKNOWN": 0,
+	"LOGGER_CLIENT":  1,
+	"LOGGER_SERVER":  2,
+}
+
+func (x GrpcLogEntry_Logger) String() string {
+	return proto.EnumName(GrpcLogEntry_Logger_name, int32(x))
+}
+func (GrpcLogEntry_Logger) EnumDescriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{0, 1}
+}
+
+type Address_Type int32
+
+const (
+	Address_TYPE_UNKNOWN Address_Type = 0
+	// address is in 1.2.3.4 form
+	Address_TYPE_IPV4 Address_Type = 1
+	// address is in IPv6 canonical form (RFC5952 section 4)
+	// The scope is NOT included in the address string.
+	Address_TYPE_IPV6 Address_Type = 2
+	// address is UDS string
+	Address_TYPE_UNIX Address_Type = 3
+)
+
+var Address_Type_name = map[int32]string{
+	0: "TYPE_UNKNOWN",
+	1: "TYPE_IPV4",
+	2: "TYPE_IPV6",
+	3: "TYPE_UNIX",
+}
+var Address_Type_value = map[string]int32{
+	"TYPE_UNKNOWN": 0,
+	"TYPE_IPV4":    1,
+	"TYPE_IPV6":    2,
+	"TYPE_UNIX":    3,
+}
+
+func (x Address_Type) String() string {
+	return proto.EnumName(Address_Type_name, int32(x))
+}
+func (Address_Type) EnumDescriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{7, 0}
+}
+
+// Log entry we store in binary logs
+type GrpcLogEntry struct {
+	// The timestamp of the binary log message
+	Timestamp *timestamp.Timestamp `protobuf:"bytes,1,opt,name=timestamp,proto3" json:"timestamp,omitempty"`
+	// Uniquely identifies a call. The value must not be 0 in order to disambiguate
+	// from an unset value.
+	// Each call may have several log entries, they will all have the same call_id.
+	// Nothing is guaranteed about their value other than they are unique across
+	// different RPCs in the same gRPC process.
+	CallId uint64 `protobuf:"varint,2,opt,name=call_id,json=callId,proto3" json:"call_id,omitempty"`
+	// The entry sequence id for this call. The first GrpcLogEntry has a
+	// value of 1, to disambiguate from an unset value. The purpose of
+	// this field is to detect missing entries in environments where
+	// durability or ordering is not guaranteed.
+	SequenceIdWithinCall uint64                 `protobuf:"varint,3,opt,name=sequence_id_within_call,json=sequenceIdWithinCall,proto3" json:"sequence_id_within_call,omitempty"`
+	Type                 GrpcLogEntry_EventType `protobuf:"varint,4,opt,name=type,proto3,enum=grpc.binarylog.v1.GrpcLogEntry_EventType" json:"type,omitempty"`
+	Logger               GrpcLogEntry_Logger    `protobuf:"varint,5,opt,name=logger,proto3,enum=grpc.binarylog.v1.GrpcLogEntry_Logger" json:"logger,omitempty"`
+	// The logger uses one of the following fields to record the payload,
+	// according to the type of the log entry.
+	//
+	// Types that are valid to be assigned to Payload:
+	//	*GrpcLogEntry_ClientHeader
+	//	*GrpcLogEntry_ServerHeader
+	//	*GrpcLogEntry_Message
+	//	*GrpcLogEntry_Trailer
+	Payload isGrpcLogEntry_Payload `protobuf_oneof:"payload"`
+	// true if payload does not represent the full message or metadata.
+	PayloadTruncated bool `protobuf:"varint,10,opt,name=payload_truncated,json=payloadTruncated,proto3" json:"payload_truncated,omitempty"`
+	// Peer address information, will only be recorded on the first
+	// incoming event. On client side, peer is logged on
+	// EVENT_TYPE_SERVER_HEADER normally or EVENT_TYPE_SERVER_TRAILER in
+	// the case of trailers-only. On server side, peer is always
+	// logged on EVENT_TYPE_CLIENT_HEADER.
+	Peer                 *Address `protobuf:"bytes,11,opt,name=peer,proto3" json:"peer,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *GrpcLogEntry) Reset()         { *m = GrpcLogEntry{} }
+func (m *GrpcLogEntry) String() string { return proto.CompactTextString(m) }
+func (*GrpcLogEntry) ProtoMessage()    {}
+func (*GrpcLogEntry) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{0}
+}
+func (m *GrpcLogEntry) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_GrpcLogEntry.Unmarshal(m, b)
+}
+func (m *GrpcLogEntry) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_GrpcLogEntry.Marshal(b, m, deterministic)
+}
+func (dst *GrpcLogEntry) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_GrpcLogEntry.Merge(dst, src)
+}
+func (m *GrpcLogEntry) XXX_Size() int {
+	return xxx_messageInfo_GrpcLogEntry.Size(m)
+}
+func (m *GrpcLogEntry) XXX_DiscardUnknown() {
+	xxx_messageInfo_GrpcLogEntry.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_GrpcLogEntry proto.InternalMessageInfo
+
+func (m *GrpcLogEntry) GetTimestamp() *timestamp.Timestamp {
+	if m != nil {
+		return m.Timestamp
+	}
+	return nil
+}
+
+func (m *GrpcLogEntry) GetCallId() uint64 {
+	if m != nil {
+		return m.CallId
+	}
+	return 0
+}
+
+func (m *GrpcLogEntry) GetSequenceIdWithinCall() uint64 {
+	if m != nil {
+		return m.SequenceIdWithinCall
+	}
+	return 0
+}
+
+func (m *GrpcLogEntry) GetType() GrpcLogEntry_EventType {
+	if m != nil {
+		return m.Type
+	}
+	return GrpcLogEntry_EVENT_TYPE_UNKNOWN
+}
+
+func (m *GrpcLogEntry) GetLogger() GrpcLogEntry_Logger {
+	if m != nil {
+		return m.Logger
+	}
+	return GrpcLogEntry_LOGGER_UNKNOWN
+}
+
+type isGrpcLogEntry_Payload interface {
+	isGrpcLogEntry_Payload()
+}
+
+type GrpcLogEntry_ClientHeader struct {
+	ClientHeader *ClientHeader `protobuf:"bytes,6,opt,name=client_header,json=clientHeader,proto3,oneof"`
+}
+
+type GrpcLogEntry_ServerHeader struct {
+	ServerHeader *ServerHeader `protobuf:"bytes,7,opt,name=server_header,json=serverHeader,proto3,oneof"`
+}
+
+type GrpcLogEntry_Message struct {
+	Message *Message `protobuf:"bytes,8,opt,name=message,proto3,oneof"`
+}
+
+type GrpcLogEntry_Trailer struct {
+	Trailer *Trailer `protobuf:"bytes,9,opt,name=trailer,proto3,oneof"`
+}
+
+func (*GrpcLogEntry_ClientHeader) isGrpcLogEntry_Payload() {}
+
+func (*GrpcLogEntry_ServerHeader) isGrpcLogEntry_Payload() {}
+
+func (*GrpcLogEntry_Message) isGrpcLogEntry_Payload() {}
+
+func (*GrpcLogEntry_Trailer) isGrpcLogEntry_Payload() {}
+
+func (m *GrpcLogEntry) GetPayload() isGrpcLogEntry_Payload {
+	if m != nil {
+		return m.Payload
+	}
+	return nil
+}
+
+func (m *GrpcLogEntry) GetClientHeader() *ClientHeader {
+	if x, ok := m.GetPayload().(*GrpcLogEntry_ClientHeader); ok {
+		return x.ClientHeader
+	}
+	return nil
+}
+
+func (m *GrpcLogEntry) GetServerHeader() *ServerHeader {
+	if x, ok := m.GetPayload().(*GrpcLogEntry_ServerHeader); ok {
+		return x.ServerHeader
+	}
+	return nil
+}
+
+func (m *GrpcLogEntry) GetMessage() *Message {
+	if x, ok := m.GetPayload().(*GrpcLogEntry_Message); ok {
+		return x.Message
+	}
+	return nil
+}
+
+func (m *GrpcLogEntry) GetTrailer() *Trailer {
+	if x, ok := m.GetPayload().(*GrpcLogEntry_Trailer); ok {
+		return x.Trailer
+	}
+	return nil
+}
+
+func (m *GrpcLogEntry) GetPayloadTruncated() bool {
+	if m != nil {
+		return m.PayloadTruncated
+	}
+	return false
+}
+
+func (m *GrpcLogEntry) GetPeer() *Address {
+	if m != nil {
+		return m.Peer
+	}
+	return nil
+}
+
+// XXX_OneofFuncs is for the internal use of the proto package.
+func (*GrpcLogEntry) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer) error, func(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error), func(msg proto.Message) (n int), []interface{}) {
+	return _GrpcLogEntry_OneofMarshaler, _GrpcLogEntry_OneofUnmarshaler, _GrpcLogEntry_OneofSizer, []interface{}{
+		(*GrpcLogEntry_ClientHeader)(nil),
+		(*GrpcLogEntry_ServerHeader)(nil),
+		(*GrpcLogEntry_Message)(nil),
+		(*GrpcLogEntry_Trailer)(nil),
+	}
+}
+
+func _GrpcLogEntry_OneofMarshaler(msg proto.Message, b *proto.Buffer) error {
+	m := msg.(*GrpcLogEntry)
+	// payload
+	switch x := m.Payload.(type) {
+	case *GrpcLogEntry_ClientHeader:
+		b.EncodeVarint(6<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.ClientHeader); err != nil {
+			return err
+		}
+	case *GrpcLogEntry_ServerHeader:
+		b.EncodeVarint(7<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.ServerHeader); err != nil {
+			return err
+		}
+	case *GrpcLogEntry_Message:
+		b.EncodeVarint(8<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.Message); err != nil {
+			return err
+		}
+	case *GrpcLogEntry_Trailer:
+		b.EncodeVarint(9<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.Trailer); err != nil {
+			return err
+		}
+	case nil:
+	default:
+		return fmt.Errorf("GrpcLogEntry.Payload has unexpected type %T", x)
+	}
+	return nil
+}
+
+func _GrpcLogEntry_OneofUnmarshaler(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error) {
+	m := msg.(*GrpcLogEntry)
+	switch tag {
+	case 6: // payload.client_header
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(ClientHeader)
+		err := b.DecodeMessage(msg)
+		m.Payload = &GrpcLogEntry_ClientHeader{msg}
+		return true, err
+	case 7: // payload.server_header
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(ServerHeader)
+		err := b.DecodeMessage(msg)
+		m.Payload = &GrpcLogEntry_ServerHeader{msg}
+		return true, err
+	case 8: // payload.message
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(Message)
+		err := b.DecodeMessage(msg)
+		m.Payload = &GrpcLogEntry_Message{msg}
+		return true, err
+	case 9: // payload.trailer
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(Trailer)
+		err := b.DecodeMessage(msg)
+		m.Payload = &GrpcLogEntry_Trailer{msg}
+		return true, err
+	default:
+		return false, nil
+	}
+}
+
+func _GrpcLogEntry_OneofSizer(msg proto.Message) (n int) {
+	m := msg.(*GrpcLogEntry)
+	// payload
+	switch x := m.Payload.(type) {
+	case *GrpcLogEntry_ClientHeader:
+		s := proto.Size(x.ClientHeader)
+		n += 1 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
+	case *GrpcLogEntry_ServerHeader:
+		s := proto.Size(x.ServerHeader)
+		n += 1 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
+	case *GrpcLogEntry_Message:
+		s := proto.Size(x.Message)
+		n += 1 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
+	case *GrpcLogEntry_Trailer:
+		s := proto.Size(x.Trailer)
+		n += 1 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
+	case nil:
+	default:
+		panic(fmt.Sprintf("proto: unexpected type %T in oneof", x))
+	}
+	return n
+}
+
+type ClientHeader struct {
+	// This contains only the metadata from the application.
+	Metadata *Metadata `protobuf:"bytes,1,opt,name=metadata,proto3" json:"metadata,omitempty"`
+	// The name of the RPC method, which looks something like:
+	// /<service>/<method>
+	// Note the leading "/" character.
+	MethodName string `protobuf:"bytes,2,opt,name=method_name,json=methodName,proto3" json:"method_name,omitempty"`
+	// A single process may be used to run multiple virtual
+	// servers with different identities.
+	// The authority is the name of such a server identitiy.
+	// It is typically a portion of the URI in the form of
+	// <host> or <host>:<port> .
+	Authority string `protobuf:"bytes,3,opt,name=authority,proto3" json:"authority,omitempty"`
+	// the RPC timeout
+	Timeout              *duration.Duration `protobuf:"bytes,4,opt,name=timeout,proto3" json:"timeout,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
+	XXX_unrecognized     []byte             `json:"-"`
+	XXX_sizecache        int32              `json:"-"`
+}
+
+func (m *ClientHeader) Reset()         { *m = ClientHeader{} }
+func (m *ClientHeader) String() string { return proto.CompactTextString(m) }
+func (*ClientHeader) ProtoMessage()    {}
+func (*ClientHeader) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{1}
+}
+func (m *ClientHeader) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_ClientHeader.Unmarshal(m, b)
+}
+func (m *ClientHeader) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_ClientHeader.Marshal(b, m, deterministic)
+}
+func (dst *ClientHeader) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_ClientHeader.Merge(dst, src)
+}
+func (m *ClientHeader) XXX_Size() int {
+	return xxx_messageInfo_ClientHeader.Size(m)
+}
+func (m *ClientHeader) XXX_DiscardUnknown() {
+	xxx_messageInfo_ClientHeader.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_ClientHeader proto.InternalMessageInfo
+
+func (m *ClientHeader) GetMetadata() *Metadata {
+	if m != nil {
+		return m.Metadata
+	}
+	return nil
+}
+
+func (m *ClientHeader) GetMethodName() string {
+	if m != nil {
+		return m.MethodName
+	}
+	return ""
+}
+
+func (m *ClientHeader) GetAuthority() string {
+	if m != nil {
+		return m.Authority
+	}
+	return ""
+}
+
+func (m *ClientHeader) GetTimeout() *duration.Duration {
+	if m != nil {
+		return m.Timeout
+	}
+	return nil
+}
+
+type ServerHeader struct {
+	// This contains only the metadata from the application.
+	Metadata             *Metadata `protobuf:"bytes,1,opt,name=metadata,proto3" json:"metadata,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}  `json:"-"`
+	XXX_unrecognized     []byte    `json:"-"`
+	XXX_sizecache        int32     `json:"-"`
+}
+
+func (m *ServerHeader) Reset()         { *m = ServerHeader{} }
+func (m *ServerHeader) String() string { return proto.CompactTextString(m) }
+func (*ServerHeader) ProtoMessage()    {}
+func (*ServerHeader) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{2}
+}
+func (m *ServerHeader) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_ServerHeader.Unmarshal(m, b)
+}
+func (m *ServerHeader) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_ServerHeader.Marshal(b, m, deterministic)
+}
+func (dst *ServerHeader) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_ServerHeader.Merge(dst, src)
+}
+func (m *ServerHeader) XXX_Size() int {
+	return xxx_messageInfo_ServerHeader.Size(m)
+}
+func (m *ServerHeader) XXX_DiscardUnknown() {
+	xxx_messageInfo_ServerHeader.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_ServerHeader proto.InternalMessageInfo
+
+func (m *ServerHeader) GetMetadata() *Metadata {
+	if m != nil {
+		return m.Metadata
+	}
+	return nil
+}
+
+type Trailer struct {
+	// This contains only the metadata from the application.
+	Metadata *Metadata `protobuf:"bytes,1,opt,name=metadata,proto3" json:"metadata,omitempty"`
+	// The gRPC status code.
+	StatusCode uint32 `protobuf:"varint,2,opt,name=status_code,json=statusCode,proto3" json:"status_code,omitempty"`
+	// An original status message before any transport specific
+	// encoding.
+	StatusMessage string `protobuf:"bytes,3,opt,name=status_message,json=statusMessage,proto3" json:"status_message,omitempty"`
+	// The value of the 'grpc-status-details-bin' metadata key. If
+	// present, this is always an encoded 'google.rpc.Status' message.
+	StatusDetails        []byte   `protobuf:"bytes,4,opt,name=status_details,json=statusDetails,proto3" json:"status_details,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *Trailer) Reset()         { *m = Trailer{} }
+func (m *Trailer) String() string { return proto.CompactTextString(m) }
+func (*Trailer) ProtoMessage()    {}
+func (*Trailer) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{3}
+}
+func (m *Trailer) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_Trailer.Unmarshal(m, b)
+}
+func (m *Trailer) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_Trailer.Marshal(b, m, deterministic)
+}
+func (dst *Trailer) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_Trailer.Merge(dst, src)
+}
+func (m *Trailer) XXX_Size() int {
+	return xxx_messageInfo_Trailer.Size(m)
+}
+func (m *Trailer) XXX_DiscardUnknown() {
+	xxx_messageInfo_Trailer.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_Trailer proto.InternalMessageInfo
+
+func (m *Trailer) GetMetadata() *Metadata {
+	if m != nil {
+		return m.Metadata
+	}
+	return nil
+}
+
+func (m *Trailer) GetStatusCode() uint32 {
+	if m != nil {
+		return m.StatusCode
+	}
+	return 0
+}
+
+func (m *Trailer) GetStatusMessage() string {
+	if m != nil {
+		return m.StatusMessage
+	}
+	return ""
+}
+
+func (m *Trailer) GetStatusDetails() []byte {
+	if m != nil {
+		return m.StatusDetails
+	}
+	return nil
+}
+
+// Message payload, used by CLIENT_MESSAGE and SERVER_MESSAGE
+type Message struct {
+	// Length of the message. It may not be the same as the length of the
+	// data field, as the logging payload can be truncated or omitted.
+	Length uint32 `protobuf:"varint,1,opt,name=length,proto3" json:"length,omitempty"`
+	// May be truncated or omitted.
+	Data                 []byte   `protobuf:"bytes,2,opt,name=data,proto3" json:"data,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *Message) Reset()         { *m = Message{} }
+func (m *Message) String() string { return proto.CompactTextString(m) }
+func (*Message) ProtoMessage()    {}
+func (*Message) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{4}
+}
+func (m *Message) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_Message.Unmarshal(m, b)
+}
+func (m *Message) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_Message.Marshal(b, m, deterministic)
+}
+func (dst *Message) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_Message.Merge(dst, src)
+}
+func (m *Message) XXX_Size() int {
+	return xxx_messageInfo_Message.Size(m)
+}
+func (m *Message) XXX_DiscardUnknown() {
+	xxx_messageInfo_Message.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_Message proto.InternalMessageInfo
+
+func (m *Message) GetLength() uint32 {
+	if m != nil {
+		return m.Length
+	}
+	return 0
+}
+
+func (m *Message) GetData() []byte {
+	if m != nil {
+		return m.Data
+	}
+	return nil
+}
+
+// A list of metadata pairs, used in the payload of client header,
+// server header, and server trailer.
+// Implementations may omit some entries to honor the header limits
+// of GRPC_BINARY_LOG_CONFIG.
+//
+// Header keys added by gRPC are omitted. To be more specific,
+// implementations will not log the following entries, and this is
+// not to be treated as a truncation:
+// - entries handled by grpc that are not user visible, such as those
+//   that begin with 'grpc-' (with exception of grpc-trace-bin)
+//   or keys like 'lb-token'
+// - transport specific entries, including but not limited to:
+//   ':path', ':authority', 'content-encoding', 'user-agent', 'te', etc
+// - entries added for call credentials
+//
+// Implementations must always log grpc-trace-bin if it is present.
+// Practically speaking it will only be visible on server side because
+// grpc-trace-bin is managed by low level client side mechanisms
+// inaccessible from the application level. On server side, the
+// header is just a normal metadata key.
+// The pair will not count towards the size limit.
+type Metadata struct {
+	Entry                []*MetadataEntry `protobuf:"bytes,1,rep,name=entry,proto3" json:"entry,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}         `json:"-"`
+	XXX_unrecognized     []byte           `json:"-"`
+	XXX_sizecache        int32            `json:"-"`
+}
+
+func (m *Metadata) Reset()         { *m = Metadata{} }
+func (m *Metadata) String() string { return proto.CompactTextString(m) }
+func (*Metadata) ProtoMessage()    {}
+func (*Metadata) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{5}
+}
+func (m *Metadata) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_Metadata.Unmarshal(m, b)
+}
+func (m *Metadata) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_Metadata.Marshal(b, m, deterministic)
+}
+func (dst *Metadata) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_Metadata.Merge(dst, src)
+}
+func (m *Metadata) XXX_Size() int {
+	return xxx_messageInfo_Metadata.Size(m)
+}
+func (m *Metadata) XXX_DiscardUnknown() {
+	xxx_messageInfo_Metadata.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_Metadata proto.InternalMessageInfo
+
+func (m *Metadata) GetEntry() []*MetadataEntry {
+	if m != nil {
+		return m.Entry
+	}
+	return nil
+}
+
+// A metadata key value pair
+type MetadataEntry struct {
+	Key                  string   `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
+	Value                []byte   `protobuf:"bytes,2,opt,name=value,proto3" json:"value,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *MetadataEntry) Reset()         { *m = MetadataEntry{} }
+func (m *MetadataEntry) String() string { return proto.CompactTextString(m) }
+func (*MetadataEntry) ProtoMessage()    {}
+func (*MetadataEntry) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{6}
+}
+func (m *MetadataEntry) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_MetadataEntry.Unmarshal(m, b)
+}
+func (m *MetadataEntry) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_MetadataEntry.Marshal(b, m, deterministic)
+}
+func (dst *MetadataEntry) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_MetadataEntry.Merge(dst, src)
+}
+func (m *MetadataEntry) XXX_Size() int {
+	return xxx_messageInfo_MetadataEntry.Size(m)
+}
+func (m *MetadataEntry) XXX_DiscardUnknown() {
+	xxx_messageInfo_MetadataEntry.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_MetadataEntry proto.InternalMessageInfo
+
+func (m *MetadataEntry) GetKey() string {
+	if m != nil {
+		return m.Key
+	}
+	return ""
+}
+
+func (m *MetadataEntry) GetValue() []byte {
+	if m != nil {
+		return m.Value
+	}
+	return nil
+}
+
+// Address information
+type Address struct {
+	Type    Address_Type `protobuf:"varint,1,opt,name=type,proto3,enum=grpc.binarylog.v1.Address_Type" json:"type,omitempty"`
+	Address string       `protobuf:"bytes,2,opt,name=address,proto3" json:"address,omitempty"`
+	// only for TYPE_IPV4 and TYPE_IPV6
+	IpPort               uint32   `protobuf:"varint,3,opt,name=ip_port,json=ipPort,proto3" json:"ip_port,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *Address) Reset()         { *m = Address{} }
+func (m *Address) String() string { return proto.CompactTextString(m) }
+func (*Address) ProtoMessage()    {}
+func (*Address) Descriptor() ([]byte, []int) {
+	return fileDescriptor_binarylog_264c8c9c551ce911, []int{7}
+}
+func (m *Address) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_Address.Unmarshal(m, b)
+}
+func (m *Address) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_Address.Marshal(b, m, deterministic)
+}
+func (dst *Address) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_Address.Merge(dst, src)
+}
+func (m *Address) XXX_Size() int {
+	return xxx_messageInfo_Address.Size(m)
+}
+func (m *Address) XXX_DiscardUnknown() {
+	xxx_messageInfo_Address.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_Address proto.InternalMessageInfo
+
+func (m *Address) GetType() Address_Type {
+	if m != nil {
+		return m.Type
+	}
+	return Address_TYPE_UNKNOWN
+}
+
+func (m *Address) GetAddress() string {
+	if m != nil {
+		return m.Address
+	}
+	return ""
+}
+
+func (m *Address) GetIpPort() uint32 {
+	if m != nil {
+		return m.IpPort
+	}
+	return 0
+}
+
+func init() {
+	proto.RegisterType((*GrpcLogEntry)(nil), "grpc.binarylog.v1.GrpcLogEntry")
+	proto.RegisterType((*ClientHeader)(nil), "grpc.binarylog.v1.ClientHeader")
+	proto.RegisterType((*ServerHeader)(nil), "grpc.binarylog.v1.ServerHeader")
+	proto.RegisterType((*Trailer)(nil), "grpc.binarylog.v1.Trailer")
+	proto.RegisterType((*Message)(nil), "grpc.binarylog.v1.Message")
+	proto.RegisterType((*Metadata)(nil), "grpc.binarylog.v1.Metadata")
+	proto.RegisterType((*MetadataEntry)(nil), "grpc.binarylog.v1.MetadataEntry")
+	proto.RegisterType((*Address)(nil), "grpc.binarylog.v1.Address")
+	proto.RegisterEnum("grpc.binarylog.v1.GrpcLogEntry_EventType", GrpcLogEntry_EventType_name, GrpcLogEntry_EventType_value)
+	proto.RegisterEnum("grpc.binarylog.v1.GrpcLogEntry_Logger", GrpcLogEntry_Logger_name, GrpcLogEntry_Logger_value)
+	proto.RegisterEnum("grpc.binarylog.v1.Address_Type", Address_Type_name, Address_Type_value)
+}
+
+func init() {
+	proto.RegisterFile("grpc/binarylog/grpc_binarylog_v1/binarylog.proto", fileDescriptor_binarylog_264c8c9c551ce911)
+}
+
+var fileDescriptor_binarylog_264c8c9c551ce911 = []byte{
+	// 900 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xa4, 0x55, 0x51, 0x6f, 0xe3, 0x44,
+	0x10, 0x3e, 0x37, 0x69, 0xdc, 0x4c, 0x92, 0xca, 0x5d, 0x95, 0x3b, 0x5f, 0x29, 0x34, 0xb2, 0x04,
+	0x0a, 0x42, 0x72, 0xb9, 0x94, 0xeb, 0xf1, 0x02, 0x52, 0x92, 0xfa, 0xd2, 0x88, 0x5c, 0x1a, 0x6d,
+	0x72, 0x3d, 0x40, 0x48, 0xd6, 0x36, 0x5e, 0x1c, 0x0b, 0xc7, 0x6b, 0xd6, 0x9b, 0xa0, 0xfc, 0x2c,
+	0xde, 0x90, 0xee, 0x77, 0xf1, 0x8e, 0xbc, 0x6b, 0x27, 0xa6, 0x69, 0x0f, 0x09, 0xde, 0x3c, 0xdf,
+	0x7c, 0xf3, 0xcd, 0xee, 0x78, 0x66, 0x16, 0xbe, 0xf2, 0x79, 0x3c, 0x3b, 0xbf, 0x0b, 0x22, 0xc2,
+	0xd7, 0x21, 0xf3, 0xcf, 0x53, 0xd3, 0xdd, 0x98, 0xee, 0xea, 0xc5, 0xd6, 0x67, 0xc7, 0x9c, 0x09,
+	0x86, 0x8e, 0x52, 0x8a, 0xbd, 0x45, 0x57, 0x2f, 0x4e, 0x3e, 0xf5, 0x19, 0xf3, 0x43, 0x7a, 0x2e,
+	0x09, 0x77, 0xcb, 0x5f, 0xce, 0xbd, 0x25, 0x27, 0x22, 0x60, 0x91, 0x0a, 0x39, 0x39, 0xbb, 0xef,
+	0x17, 0xc1, 0x82, 0x26, 0x82, 0x2c, 0x62, 0x45, 0xb0, 0xde, 0xeb, 0x50, 0xef, 0xf3, 0x78, 0x36,
+	0x64, 0xbe, 0x13, 0x09, 0xbe, 0x46, 0xdf, 0x40, 0x75, 0xc3, 0x31, 0xb5, 0xa6, 0xd6, 0xaa, 0xb5,
+	0x4f, 0x6c, 0xa5, 0x62, 0xe7, 0x2a, 0xf6, 0x34, 0x67, 0xe0, 0x2d, 0x19, 0x3d, 0x03, 0x7d, 0x46,
+	0xc2, 0xd0, 0x0d, 0x3c, 0x73, 0xaf, 0xa9, 0xb5, 0xca, 0xb8, 0x92, 0x9a, 0x03, 0x0f, 0xbd, 0x84,
+	0x67, 0x09, 0xfd, 0x6d, 0x49, 0xa3, 0x19, 0x75, 0x03, 0xcf, 0xfd, 0x3d, 0x10, 0xf3, 0x20, 0x72,
+	0x53, 0xa7, 0x59, 0x92, 0xc4, 0xe3, 0xdc, 0x3d, 0xf0, 0xde, 0x49, 0x67, 0x8f, 0x84, 0x21, 0xfa,
+	0x16, 0xca, 0x62, 0x1d, 0x53, 0xb3, 0xdc, 0xd4, 0x5a, 0x87, 0xed, 0x2f, 0xec, 0x9d, 0xdb, 0xdb,
+	0xc5, 0x83, 0xdb, 0xce, 0x8a, 0x46, 0x62, 0xba, 0x8e, 0x29, 0x96, 0x61, 0xe8, 0x3b, 0xa8, 0x84,
+	0xcc, 0xf7, 0x29, 0x37, 0xf7, 0xa5, 0xc0, 0xe7, 0xff, 0x26, 0x30, 0x94, 0x6c, 0x9c, 0x45, 0xa1,
+	0xd7, 0xd0, 0x98, 0x85, 0x01, 0x8d, 0x84, 0x3b, 0xa7, 0xc4, 0xa3, 0xdc, 0xac, 0xc8, 0x62, 0x9c,
+	0x3d, 0x20, 0xd3, 0x93, 0xbc, 0x6b, 0x49, 0xbb, 0x7e, 0x82, 0xeb, 0xb3, 0x82, 0x9d, 0xea, 0x24,
+	0x94, 0xaf, 0x28, 0xcf, 0x75, 0xf4, 0x47, 0x75, 0x26, 0x92, 0xb7, 0xd5, 0x49, 0x0a, 0x36, 0xba,
+	0x04, 0x7d, 0x41, 0x93, 0x84, 0xf8, 0xd4, 0x3c, 0xc8, 0x7f, 0xcb, 0x8e, 0xc2, 0x1b, 0xc5, 0xb8,
+	0x7e, 0x82, 0x73, 0x72, 0x1a, 0x27, 0x38, 0x09, 0x42, 0xca, 0xcd, 0xea, 0xa3, 0x71, 0x53, 0xc5,
+	0x48, 0xe3, 0x32, 0x32, 0xfa, 0x12, 0x8e, 0x62, 0xb2, 0x0e, 0x19, 0xf1, 0x5c, 0xc1, 0x97, 0xd1,
+	0x8c, 0x08, 0xea, 0x99, 0xd0, 0xd4, 0x5a, 0x07, 0xd8, 0xc8, 0x1c, 0xd3, 0x1c, 0x47, 0x36, 0x94,
+	0x63, 0x4a, 0xb9, 0x59, 0x7b, 0x34, 0x43, 0xc7, 0xf3, 0x38, 0x4d, 0x12, 0x2c, 0x79, 0xd6, 0x5f,
+	0x1a, 0x54, 0x37, 0x3f, 0x0c, 0x3d, 0x05, 0xe4, 0xdc, 0x3a, 0xa3, 0xa9, 0x3b, 0xfd, 0x71, 0xec,
+	0xb8, 0x6f, 0x47, 0xdf, 0x8f, 0x6e, 0xde, 0x8d, 0x8c, 0x27, 0xe8, 0x14, 0xcc, 0x02, 0xde, 0x1b,
+	0x0e, 0xd2, 0xef, 0x6b, 0xa7, 0x73, 0xe5, 0x60, 0x43, 0xbb, 0xe7, 0x9d, 0x38, 0xf8, 0xd6, 0xc1,
+	0xb9, 0x77, 0x0f, 0x7d, 0x02, 0xcf, 0x77, 0x63, 0xdf, 0x38, 0x93, 0x49, 0xa7, 0xef, 0x18, 0xa5,
+	0x7b, 0xee, 0x2c, 0x38, 0x77, 0x97, 0x51, 0x13, 0x4e, 0x1f, 0xc8, 0xdc, 0x19, 0xbe, 0x76, 0x7b,
+	0xc3, 0x9b, 0x89, 0x63, 0xec, 0x3f, 0x2c, 0x30, 0xc5, 0x9d, 0xc1, 0xd0, 0xc1, 0x46, 0x05, 0x7d,
+	0x04, 0x47, 0x45, 0x81, 0xce, 0xa8, 0xe7, 0x0c, 0x0d, 0xdd, 0xea, 0x42, 0x45, 0xb5, 0x19, 0x42,
+	0x70, 0x38, 0xbc, 0xe9, 0xf7, 0x1d, 0x5c, 0xb8, 0xef, 0x11, 0x34, 0x32, 0x4c, 0x65, 0x34, 0xb4,
+	0x02, 0xa4, 0x52, 0x18, 0x7b, 0xdd, 0x2a, 0xe8, 0x59, 0xfd, 0xad, 0xf7, 0x1a, 0xd4, 0x8b, 0xcd,
+	0x87, 0x5e, 0xc1, 0xc1, 0x82, 0x0a, 0xe2, 0x11, 0x41, 0xb2, 0xe1, 0xfd, 0xf8, 0xc1, 0x2e, 0x51,
+	0x14, 0xbc, 0x21, 0xa3, 0x33, 0xa8, 0x2d, 0xa8, 0x98, 0x33, 0xcf, 0x8d, 0xc8, 0x82, 0xca, 0x01,
+	0xae, 0x62, 0x50, 0xd0, 0x88, 0x2c, 0x28, 0x3a, 0x85, 0x2a, 0x59, 0x8a, 0x39, 0xe3, 0x81, 0x58,
+	0xcb, 0xb1, 0xad, 0xe2, 0x2d, 0x80, 0x2e, 0x40, 0x4f, 0x17, 0x01, 0x5b, 0x0a, 0x39, 0xae, 0xb5,
+	0xf6, 0xf3, 0x9d, 0x9d, 0x71, 0x95, 0x6d, 0x26, 0x9c, 0x33, 0xad, 0x3e, 0xd4, 0x8b, 0x1d, 0xff,
+	0x9f, 0x0f, 0x6f, 0xfd, 0xa1, 0x81, 0x9e, 0x75, 0xf0, 0xff, 0xaa, 0x40, 0x22, 0x88, 0x58, 0x26,
+	0xee, 0x8c, 0x79, 0xaa, 0x02, 0x0d, 0x0c, 0x0a, 0xea, 0x31, 0x8f, 0xa2, 0xcf, 0xe0, 0x30, 0x23,
+	0xe4, 0x73, 0xa8, 0xca, 0xd0, 0x50, 0x68, 0x36, 0x7a, 0x05, 0x9a, 0x47, 0x05, 0x09, 0xc2, 0x44,
+	0x56, 0xa4, 0x9e, 0xd3, 0xae, 0x14, 0x68, 0xbd, 0x04, 0x3d, 0x8f, 0x78, 0x0a, 0x95, 0x90, 0x46,
+	0xbe, 0x98, 0xcb, 0x03, 0x37, 0x70, 0x66, 0x21, 0x04, 0x65, 0x79, 0x8d, 0x3d, 0x19, 0x2f, 0xbf,
+	0xad, 0x2e, 0x1c, 0xe4, 0x67, 0x47, 0x97, 0xb0, 0x4f, 0xd3, 0xcd, 0x65, 0x6a, 0xcd, 0x52, 0xab,
+	0xd6, 0x6e, 0x7e, 0xe0, 0x9e, 0x72, 0xc3, 0x61, 0x45, 0xb7, 0x5e, 0x41, 0xe3, 0x1f, 0x38, 0x32,
+	0xa0, 0xf4, 0x2b, 0x5d, 0xcb, 0xec, 0x55, 0x9c, 0x7e, 0xa2, 0x63, 0xd8, 0x5f, 0x91, 0x70, 0x49,
+	0xb3, 0xdc, 0xca, 0xb0, 0xfe, 0xd4, 0x40, 0xcf, 0xe6, 0x18, 0x5d, 0x64, 0xdb, 0x59, 0x93, 0xcb,
+	0xf5, 0xec, 0xf1, 0x89, 0xb7, 0x0b, 0x3b, 0xd9, 0x04, 0x9d, 0x28, 0x34, 0xeb, 0xb0, 0xdc, 0x4c,
+	0x1f, 0x8f, 0x20, 0x76, 0x63, 0xc6, 0x85, 0xac, 0x6a, 0x03, 0x57, 0x82, 0x78, 0xcc, 0xb8, 0xb0,
+	0x1c, 0x28, 0xcb, 0x1d, 0x61, 0x40, 0xfd, 0xde, 0x76, 0x68, 0x40, 0x55, 0x22, 0x83, 0xf1, 0xed,
+	0xd7, 0x86, 0x56, 0x34, 0x2f, 0x8d, 0xbd, 0x8d, 0xf9, 0x76, 0x34, 0xf8, 0xc1, 0x28, 0x75, 0x7f,
+	0x86, 0xe3, 0x80, 0xed, 0x1e, 0xb2, 0x7b, 0xd8, 0x95, 0xd6, 0x90, 0xf9, 0xe3, 0xb4, 0x51, 0xc7,
+	0xda, 0x4f, 0xed, 0xac, 0x71, 0x7d, 0x16, 0x92, 0xc8, 0xb7, 0x19, 0x57, 0x4f, 0xf3, 0x87, 0x5e,
+	0xea, 0xbb, 0x8a, 0xec, 0xf2, 0x8b, 0xbf, 0x03, 0x00, 0x00, 0xff, 0xff, 0xe7, 0xf6, 0x4b, 0x50,
+	0xd4, 0x07, 0x00, 0x00,
+}
diff --git a/vendor/google.golang.org/grpc/call.go b/vendor/google.golang.org/grpc/call.go
index 180d79d06..9e20e4d38 100644
--- a/vendor/google.golang.org/grpc/call.go
+++ b/vendor/google.golang.org/grpc/call.go
@@ -19,7 +19,7 @@
 package grpc

 import (
-	"golang.org/x/net/context"
+	"context"
 )

 // Invoke sends the RPC request on the wire and returns after response is
@@ -40,7 +40,7 @@ func (cc *ClientConn) Invoke(ctx context.Context, method string, args, reply int
 func combine(o1 []CallOption, o2 []CallOption) []CallOption {
 	// we don't use append because o1 could have extra capacity whose
 	// elements would be overwritten, which could cause inadvertent
-	// sharing (and race connditions) between concurrent calls
+	// sharing (and race conditions) between concurrent calls
 	if len(o1) == 0 {
 		return o2
 	} else if len(o2) == 0 {
diff --git a/vendor/google.golang.org/grpc/clientconn.go b/vendor/google.golang.org/grpc/clientconn.go
index 318ac4073..edbed8762 100644
--- a/vendor/google.golang.org/grpc/clientconn.go
+++ b/vendor/google.golang.org/grpc/clientconn.go
@@ -19,6 +19,7 @@
 package grpc

 import (
+	"context"
 	"errors"
 	"fmt"
 	"math"
@@ -29,8 +30,6 @@ import (
 	"sync/atomic"
 	"time"

-	"golang.org/x/net/context"
-	"golang.org/x/net/trace"
 	"google.golang.org/grpc/balancer"
 	_ "google.golang.org/grpc/balancer/roundrobin" // To register roundrobin.
 	"google.golang.org/grpc/codes"
@@ -39,8 +38,11 @@ import (
 	"google.golang.org/grpc/grpclog"
 	"google.golang.org/grpc/internal/backoff"
 	"google.golang.org/grpc/internal/channelz"
+	"google.golang.org/grpc/internal/envconfig"
+	"google.golang.org/grpc/internal/grpcsync"
 	"google.golang.org/grpc/internal/transport"
 	"google.golang.org/grpc/keepalive"
+	"google.golang.org/grpc/metadata"
 	"google.golang.org/grpc/resolver"
 	_ "google.golang.org/grpc/resolver/dns"         // To register dns resolver.
 	_ "google.golang.org/grpc/resolver/passthrough" // To register passthrough resolver.
@@ -80,8 +82,11 @@ var (
 	// being set for ClientConn. Users should either set one or explicitly
 	// call WithInsecure DialOption to disable security.
 	errNoTransportSecurity = errors.New("grpc: no transport security set (use grpc.WithInsecure() explicitly or set credentials)")
+	// errTransportCredsAndBundle indicates that creds bundle is used together
+	// with other individual Transport Credentials.
+	errTransportCredsAndBundle = errors.New("grpc: credentials.Bundle may not be used with individual TransportCredentials")
 	// errTransportCredentialsMissing indicates that users want to transmit security
-	// information (e.g., oauth2 token) which requires secure connection on an insecure
+	// information (e.g., OAuth2 token) which requires secure connection on an insecure
 	// connection.
 	errTransportCredentialsMissing = errors.New("grpc: the credentials require transport level security (use grpc.WithTransportCredentials() to set)")
 	// errCredentialsConflict indicates that grpc.WithTransportCredentials()
@@ -120,12 +125,13 @@ func Dial(target string, opts ...DialOption) (*ClientConn, error) {
 // e.g. to use dns resolver, a "dns:///" prefix should be applied to the target.
 func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {
 	cc := &ClientConn{
-		target:         target,
-		csMgr:          &connectivityStateManager{},
-		conns:          make(map[*addrConn]struct{}),
-		dopts:          defaultDialOptions(),
-		blockingpicker: newPickerWrapper(),
-		czData:         new(channelzData),
+		target:            target,
+		csMgr:             &connectivityStateManager{},
+		conns:             make(map[*addrConn]struct{}),
+		dopts:             defaultDialOptions(),
+		blockingpicker:    newPickerWrapper(),
+		czData:            new(channelzData),
+		firstResolveEvent: grpcsync.NewEvent(),
 	}
 	cc.retryThrottler.Store((*retryThrottler)(nil))
 	cc.ctx, cc.cancel = context.WithCancel(context.Background())
@@ -137,17 +143,33 @@ func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *
 	if channelz.IsOn() {
 		if cc.dopts.channelzParentID != 0 {
 			cc.channelzID = channelz.RegisterChannel(&channelzChannel{cc}, cc.dopts.channelzParentID, target)
+			channelz.AddTraceEvent(cc.channelzID, &channelz.TraceEventDesc{
+				Desc:     "Channel Created",
+				Severity: channelz.CtINFO,
+				Parent: &channelz.TraceEventDesc{
+					Desc:     fmt.Sprintf("Nested Channel(id:%d) created", cc.channelzID),
+					Severity: channelz.CtINFO,
+				},
+			})
 		} else {
 			cc.channelzID = channelz.RegisterChannel(&channelzChannel{cc}, 0, target)
+			channelz.AddTraceEvent(cc.channelzID, &channelz.TraceEventDesc{
+				Desc:     "Channel Created",
+				Severity: channelz.CtINFO,
+			})
 		}
+		cc.csMgr.channelzID = cc.channelzID
 	}

 	if !cc.dopts.insecure {
-		if cc.dopts.copts.TransportCredentials == nil {
+		if cc.dopts.copts.TransportCredentials == nil && cc.dopts.copts.CredsBundle == nil {
 			return nil, errNoTransportSecurity
 		}
+		if cc.dopts.copts.TransportCredentials != nil && cc.dopts.copts.CredsBundle != nil {
+			return nil, errTransportCredsAndBundle
+		}
 	} else {
-		if cc.dopts.copts.TransportCredentials != nil {
+		if cc.dopts.copts.TransportCredentials != nil || cc.dopts.copts.CredsBundle != nil {
 			return nil, errCredentialsConflict
 		}
 		for _, cd := range cc.dopts.copts.PerRPCCredentials {
@@ -163,7 +185,7 @@ func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *
 		cc.dopts.copts.Dialer = newProxyDialer(
 			func(ctx context.Context, addr string) (net.Conn, error) {
 				network, addr := parseDialTarget(addr)
-				return dialContext(ctx, network, addr)
+				return (&net.Dialer{}).DialContext(ctx, network, addr)
 			},
 		)
 	}
@@ -215,9 +237,9 @@ func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *
 		grpclog.Infof("parsed scheme: %q", cc.parsedTarget.Scheme)
 		cc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)
 		if cc.dopts.resolverBuilder == nil {
-			// If resolver builder is still nil, the parse target's scheme is
+			// If resolver builder is still nil, the parsed target's scheme is
 			// not registered. Fallback to default resolver and set Endpoint to
-			// the original unparsed target.
+			// the original target.
 			grpclog.Infof("scheme %q not registered, fallback to default scheme", cc.parsedTarget.Scheme)
 			cc.parsedTarget = resolver.Target{
 				Scheme:   resolver.GetDefaultScheme(),
@@ -260,24 +282,20 @@ func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *
 	}
 	cc.balancerBuildOpts = balancer.BuildOptions{
 		DialCreds:        credsClone,
+		CredsBundle:      cc.dopts.copts.CredsBundle,
 		Dialer:           cc.dopts.copts.Dialer,
 		ChannelzParentID: cc.channelzID,
 	}

 	// Build the resolver.
-	cc.resolverWrapper, err = newCCResolverWrapper(cc)
+	rWrapper, err := newCCResolverWrapper(cc)
 	if err != nil {
 		return nil, fmt.Errorf("failed to build resolver: %v", err)
 	}
-	// Start the resolver wrapper goroutine after resolverWrapper is created.
-	//
-	// If the goroutine is started before resolverWrapper is ready, the
-	// following may happen: The goroutine sends updates to cc. cc forwards
-	// those to balancer. Balancer creates new addrConn. addrConn fails to
-	// connect, and calls resolveNow(). resolveNow() tries to use the non-ready
-	// resolverWrapper.
-	cc.resolverWrapper.start()

+	cc.mu.Lock()
+	cc.resolverWrapper = rWrapper
+	cc.mu.Unlock()
 	// A blocking dial blocks until the clientConn is ready.
 	if cc.dopts.block {
 		for {
@@ -286,7 +304,9 @@ func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *
 				break
 			} else if cc.dopts.copts.FailOnNonTempDialError && s == connectivity.TransientFailure {
 				if err = cc.blockingpicker.connectionError(); err != nil {
-					terr, ok := err.(interface{ Temporary() bool })
+					terr, ok := err.(interface {
+						Temporary() bool
+					})
 					if ok && !terr.Temporary() {
 						return nil, err
 					}
@@ -308,6 +328,7 @@ type connectivityStateManager struct {
 	mu         sync.Mutex
 	state      connectivity.State
 	notifyChan chan struct{}
+	channelzID int64
 }

 // updateState updates the connectivity.State of ClientConn.
@@ -323,6 +344,12 @@ func (csm *connectivityStateManager) updateState(state connectivity.State) {
 		return
 	}
 	csm.state = state
+	if channelz.IsOn() {
+		channelz.AddTraceEvent(csm.channelzID, &channelz.TraceEventDesc{
+			Desc:     fmt.Sprintf("Channel Connectivity change to %v", state),
+			Severity: channelz.CtINFO,
+		})
+	}
 	if csm.notifyChan != nil {
 		// There are other goroutines waiting on this channel.
 		close(csm.notifyChan)
@@ -357,13 +384,13 @@ type ClientConn struct {
 	csMgr        *connectivityStateManager

 	balancerBuildOpts balancer.BuildOptions
-	resolverWrapper   *ccResolverWrapper
 	blockingpicker    *pickerWrapper

-	mu    sync.RWMutex
-	sc    ServiceConfig
-	scRaw string
-	conns map[*addrConn]struct{}
+	mu              sync.RWMutex
+	resolverWrapper *ccResolverWrapper
+	sc              ServiceConfig
+	scRaw           string
+	conns           map[*addrConn]struct{}
 	// Keepalive parameter can be updated if a GoAway is received.
 	mkp             keepalive.ClientParameters
 	curBalancerName string
@@ -372,6 +399,8 @@ type ClientConn struct {
 	balancerWrapper *ccBalancerWrapper
 	retryThrottler  atomic.Value

+	firstResolveEvent *grpcsync.Event
+
 	channelzID int64 // channelz unique identification number
 	czData     *channelzData
 }
@@ -407,7 +436,7 @@ func (cc *ClientConn) scWatcher() {
 			}
 			cc.mu.Lock()
 			// TODO: load balance policy runtime change is ignored.
-			// We may revist this decision in the future.
+			// We may revisit this decision in the future.
 			cc.sc = sc
 			cc.scRaw = ""
 			cc.mu.Unlock()
@@ -417,6 +446,25 @@ func (cc *ClientConn) scWatcher() {
 	}
 }

+// waitForResolvedAddrs blocks until the resolver has provided addresses or the
+// context expires.  Returns nil unless the context expires first; otherwise
+// returns a status error based on the context.
+func (cc *ClientConn) waitForResolvedAddrs(ctx context.Context) error {
+	// This is on the RPC path, so we use a fast path to avoid the
+	// more-expensive "select" below after the resolver has returned once.
+	if cc.firstResolveEvent.HasFired() {
+		return nil
+	}
+	select {
+	case <-cc.firstResolveEvent.Done():
+		return nil
+	case <-ctx.Done():
+		return status.FromContextError(ctx.Err()).Err()
+	case <-cc.ctx.Done():
+		return ErrClientConnClosing
+	}
+}
+
 func (cc *ClientConn) handleResolvedAddrs(addrs []resolver.Address, err error) {
 	cc.mu.Lock()
 	defer cc.mu.Unlock()
@@ -430,6 +478,7 @@ func (cc *ClientConn) handleResolvedAddrs(addrs []resolver.Address, err error) {
 	}

 	cc.curAddresses = addrs
+	cc.firstResolveEvent.Fire()

 	if cc.dopts.balancerBuilder == nil {
 		// Only look at balancer types and switch balancer if balancer dial
@@ -500,10 +549,26 @@ func (cc *ClientConn) switchBalancer(name string) {
 	}

 	builder := balancer.Get(name)
+	// TODO(yuxuanli): If user send a service config that does not contain a valid balancer name, should
+	// we reuse previous one?
+	if channelz.IsOn() {
+		if builder == nil {
+			channelz.AddTraceEvent(cc.channelzID, &channelz.TraceEventDesc{
+				Desc:     fmt.Sprintf("Channel switches to new LB policy %q due to fallback from invalid balancer name", PickFirstBalancerName),
+				Severity: channelz.CtWarning,
+			})
+		} else {
+			channelz.AddTraceEvent(cc.channelzID, &channelz.TraceEventDesc{
+				Desc:     fmt.Sprintf("Channel switches to new LB policy %q", name),
+				Severity: channelz.CtINFO,
+			})
+		}
+	}
 	if builder == nil {
 		grpclog.Infof("failed to get balancer builder for: %v, using pick_first instead", name)
 		builder = newPickfirstBuilder()
 	}
+
 	cc.preBalancerName = cc.curBalancerName
 	cc.curBalancerName = builder.Name()
 	cc.balancerWrapper = newCCBalancerWrapper(cc, builder, cc.balancerBuildOpts)
@@ -524,10 +589,11 @@ func (cc *ClientConn) handleSubConnStateChange(sc balancer.SubConn, s connectivi
 // newAddrConn creates an addrConn for addrs and adds it to cc.conns.
 //
 // Caller needs to make sure len(addrs) > 0.
-func (cc *ClientConn) newAddrConn(addrs []resolver.Address) (*addrConn, error) {
+func (cc *ClientConn) newAddrConn(addrs []resolver.Address, opts balancer.NewSubConnOptions) (*addrConn, error) {
 	ac := &addrConn{
 		cc:           cc,
 		addrs:        addrs,
+		scopts:       opts,
 		dopts:        cc.dopts,
 		czData:       new(channelzData),
 		resetBackoff: make(chan struct{}),
@@ -541,6 +607,14 @@ func (cc *ClientConn) newAddrConn(addrs []resolver.Address) (*addrConn, error) {
 	}
 	if channelz.IsOn() {
 		ac.channelzID = channelz.RegisterSubChannel(ac, cc.channelzID, "")
+		channelz.AddTraceEvent(ac.channelzID, &channelz.TraceEventDesc{
+			Desc:     "Subchannel Created",
+			Severity: channelz.CtINFO,
+			Parent: &channelz.TraceEventDesc{
+				Desc:     fmt.Sprintf("Subchannel(id:%d) created", ac.channelzID),
+				Severity: channelz.CtINFO,
+			},
+		})
 	}
 	cc.conns[ac] = struct{}{}
 	cc.mu.Unlock()
@@ -590,11 +664,9 @@ func (cc *ClientConn) incrCallsFailed() {
 	atomic.AddInt64(&cc.czData.callsFailed, 1)
 }

-// connect starts to creating transport and also starts the transport monitor
-// goroutine for this ac.
+// connect starts creating a transport.
 // It does nothing if the ac is not IDLE.
 // TODO(bar) Move this to the addrConn section.
-// This was part of resetAddrConn, keep it here to make the diff look clean.
 func (ac *addrConn) connect() error {
 	ac.mu.Lock()
 	if ac.state == connectivity.Shutdown {
@@ -605,22 +677,11 @@ func (ac *addrConn) connect() error {
 		ac.mu.Unlock()
 		return nil
 	}
-	ac.state = connectivity.Connecting
-	ac.cc.handleSubConnStateChange(ac.acbw, ac.state)
+	ac.updateConnectivityState(connectivity.Connecting)
 	ac.mu.Unlock()

 	// Start a goroutine connecting to the server asynchronously.
-	go func() {
-		if err := ac.resetTransport(); err != nil {
-			grpclog.Warningf("Failed to dial %s: %v; please retry.", ac.addrs[0].Addr, err)
-			if err != errConnClosing {
-				// Keep this ac in cc.conns, to get the reason it's torn down.
-				ac.tearDown(err)
-			}
-			return
-		}
-		ac.transportMonitor()
-	}()
+	go ac.resetTransport()
 	return nil
 }

@@ -639,6 +700,12 @@ func (ac *addrConn) tryUpdateAddrs(addrs []resolver.Address) bool {
 		return true
 	}

+	// Unless we're busy reconnecting already, let's reconnect from the top of
+	// the list.
+	if ac.state != connectivity.Ready {
+		return false
+	}
+
 	var curAddrFound bool
 	for _, a := range addrs {
 		if reflect.DeepEqual(ac.curAddr, a) {
@@ -649,7 +716,6 @@ func (ac *addrConn) tryUpdateAddrs(addrs []resolver.Address) bool {
 	grpclog.Infof("addrConn: tryUpdateAddrs curAddrFound: %v", curAddrFound)
 	if curAddrFound {
 		ac.addrs = addrs
-		ac.reconnectIdx = 0 // Start reconnecting from beginning in the new list.
 	}

 	return curAddrFound
@@ -674,9 +740,17 @@ func (cc *ClientConn) GetMethodConfig(method string) MethodConfig {
 	return m
 }

+func (cc *ClientConn) healthCheckConfig() *healthCheckConfig {
+	cc.mu.RLock()
+	defer cc.mu.RUnlock()
+	return cc.sc.healthCheckConfig
+}
+
 func (cc *ClientConn) getTransport(ctx context.Context, failfast bool, method string) (transport.ClientTransport, func(balancer.DoneInfo), error) {
+	hdr, _ := metadata.FromOutgoingContext(ctx)
 	t, done, err := cc.blockingpicker.pick(ctx, failfast, balancer.PickOptions{
 		FullMethodName: method,
+		Header:         hdr,
 	})
 	if err != nil {
 		return nil, nil, toRPCErr(err)
@@ -690,11 +764,29 @@ func (cc *ClientConn) handleServiceConfig(js string) error {
 	if cc.dopts.disableServiceConfig {
 		return nil
 	}
+	if cc.scRaw == js {
+		return nil
+	}
+	if channelz.IsOn() {
+		channelz.AddTraceEvent(cc.channelzID, &channelz.TraceEventDesc{
+			// The special formatting of \"%s\" instead of %q is to provide nice printing of service config
+			// for human consumption.
+			Desc:     fmt.Sprintf("Channel has a new service config \"%s\"", js),
+			Severity: channelz.CtINFO,
+		})
+	}
 	sc, err := parseServiceConfig(js)
 	if err != nil {
 		return err
 	}
 	cc.mu.Lock()
+	// Check if the ClientConn is already closed. Some fields (e.g.
+	// balancerWrapper) are set to nil when closing the ClientConn, and could
+	// cause nil pointer panic if we don't have this check.
+	if cc.conns == nil {
+		cc.mu.Unlock()
+		return nil
+	}
 	cc.scRaw = js
 	cc.sc = sc

@@ -788,6 +880,19 @@ func (cc *ClientConn) Close() error {
 		ac.tearDown(ErrClientConnClosing)
 	}
 	if channelz.IsOn() {
+		ted := &channelz.TraceEventDesc{
+			Desc:     "Channel Deleted",
+			Severity: channelz.CtINFO,
+		}
+		if cc.dopts.channelzParentID != 0 {
+			ted.Parent = &channelz.TraceEventDesc{
+				Desc:     fmt.Sprintf("Nested channel(id:%d) deleted", cc.channelzID),
+				Severity: channelz.CtINFO,
+			}
+		}
+		channelz.AddTraceEvent(cc.channelzID, ted)
+		// TraceEvent needs to be called before RemoveEntry, as TraceEvent may add trace reference to
+		// the entity beng deleted, and thus prevent it from being deleted right away.
 		channelz.RemoveEntry(cc.channelzID)
 	}
 	return nil
@@ -799,37 +904,49 @@ type addrConn struct {
 	cancel context.CancelFunc

 	cc     *ClientConn
-	addrs  []resolver.Address
 	dopts  dialOptions
-	events trace.EventLog
 	acbw   balancer.SubConn
+	scopts balancer.NewSubConnOptions
+
+	// transport is set when there's a viable transport (note: ac state may not be READY as LB channel
+	// health checking may require server to report healthy to set ac to READY), and is reset
+	// to nil when the current transport should no longer be used to create a stream (e.g. after GoAway
+	// is received, transport is closed, ac has been torn down).
+	transport transport.ClientTransport // The current transport.

-	mu           sync.Mutex
-	curAddr      resolver.Address
-	reconnectIdx int // The index in addrs list to start reconnecting from.
-	state        connectivity.State
-	// ready is closed and becomes nil when a new transport is up or failed
-	// due to timeout.
-	ready     chan struct{}
-	transport transport.ClientTransport
-
-	// The reason this addrConn is torn down.
-	tearDownErr error
-
-	connectRetryNum int
-	// backoffDeadline is the time until which resetTransport needs to
-	// wait before increasing connectRetryNum count.
-	backoffDeadline time.Time
-	// connectDeadline is the time by which all connection
-	// negotiations must complete.
-	connectDeadline time.Time
+	mu      sync.Mutex
+	curAddr resolver.Address   // The current address.
+	addrs   []resolver.Address // All addresses that the resolver resolved to.

+	// Use updateConnectivityState for updating addrConn's connectivity state.
+	state connectivity.State
+
+	tearDownErr error // The reason this addrConn is torn down.
+
+	backoffIdx   int // Needs to be stateful for resetConnectBackoff.
 	resetBackoff chan struct{}

-	channelzID int64 // channelz unique identification number
+	channelzID int64 // channelz unique identification number.
 	czData     *channelzData
 }

+// Note: this requires a lock on ac.mu.
+func (ac *addrConn) updateConnectivityState(s connectivity.State) {
+	if ac.state == s {
+		return
+	}
+
+	updateMsg := fmt.Sprintf("Subchannel Connectivity change to %v", s)
+	ac.state = s
+	if channelz.IsOn() {
+		channelz.AddTraceEvent(ac.channelzID, &channelz.TraceEventDesc{
+			Desc:     updateMsg,
+			Severity: channelz.CtINFO,
+		})
+	}
+	ac.cc.handleSubConnStateChange(ac.acbw, s)
+}
+
 // adjustParams updates parameters used to create transports upon
 // receiving a GoAway.
 func (ac *addrConn) adjustParams(r transport.GoAwayReason) {
@@ -844,301 +961,343 @@ func (ac *addrConn) adjustParams(r transport.GoAwayReason) {
 	}
 }

-// printf records an event in ac's event log, unless ac has been closed.
-// REQUIRES ac.mu is held.
-func (ac *addrConn) printf(format string, a ...interface{}) {
-	if ac.events != nil {
-		ac.events.Printf(format, a...)
-	}
-}
+func (ac *addrConn) resetTransport() {
+	for i := 0; ; i++ {
+		tryNextAddrFromStart := grpcsync.NewEvent()

-// resetTransport recreates a transport to the address for ac.  The old
-// transport will close itself on error or when the clientconn is closed.
-// The created transport must receive initial settings frame from the server.
-// In case that doesn't happen, transportMonitor will kill the newly created
-// transport after connectDeadline has expired.
-// In case there was an error on the transport before the settings frame was
-// received, resetTransport resumes connecting to backends after the one that
-// was previously connected to. In case end of the list is reached, resetTransport
-// backs off until the original deadline.
-// If the DialOption WithWaitForHandshake was set, resetTrasport returns
-// successfully only after server settings are received.
-//
-// TODO(bar) make sure all state transitions are valid.
-func (ac *addrConn) resetTransport() error {
-	ac.mu.Lock()
-	if ac.state == connectivity.Shutdown {
-		ac.mu.Unlock()
-		return errConnClosing
-	}
-	if ac.ready != nil {
-		close(ac.ready)
-		ac.ready = nil
-	}
-	ac.transport = nil
-	ridx := ac.reconnectIdx
-	ac.mu.Unlock()
-	ac.cc.mu.RLock()
-	ac.dopts.copts.KeepaliveParams = ac.cc.mkp
-	ac.cc.mu.RUnlock()
-	var backoffDeadline, connectDeadline time.Time
-	var resetBackoff chan struct{}
-	for connectRetryNum := 0; ; connectRetryNum++ {
 		ac.mu.Lock()
-		if ac.backoffDeadline.IsZero() {
-			// This means either a successful HTTP2 connection was established
-			// or this is the first time this addrConn is trying to establish a
-			// connection.
-			backoffFor := ac.dopts.bs.Backoff(connectRetryNum) // time.Duration.
-			resetBackoff = ac.resetBackoff
-			// This will be the duration that dial gets to finish.
-			dialDuration := getMinConnectTimeout()
-			if backoffFor > dialDuration {
-				// Give dial more time as we keep failing to connect.
-				dialDuration = backoffFor
-			}
-			start := time.Now()
-			backoffDeadline = start.Add(backoffFor)
-			connectDeadline = start.Add(dialDuration)
-			ridx = 0 // Start connecting from the beginning.
-		} else {
-			// Continue trying to connect with the same deadlines.
-			connectRetryNum = ac.connectRetryNum
-			backoffDeadline = ac.backoffDeadline
-			connectDeadline = ac.connectDeadline
-			ac.backoffDeadline = time.Time{}
-			ac.connectDeadline = time.Time{}
-			ac.connectRetryNum = 0
-		}
-		if ac.state == connectivity.Shutdown {
-			ac.mu.Unlock()
-			return errConnClosing
+		if i > 0 {
+			ac.cc.resolveNow(resolver.ResolveNowOption{})
 		}
-		ac.printf("connecting")
-		if ac.state != connectivity.Connecting {
-			ac.state = connectivity.Connecting
-			ac.cc.handleSubConnStateChange(ac.acbw, ac.state)
+		addrs := ac.addrs
+		backoffFor := ac.dopts.bs.Backoff(ac.backoffIdx)
+
+		// This will be the duration that dial gets to finish.
+		dialDuration := getMinConnectTimeout()
+		if dialDuration < backoffFor {
+			// Give dial more time as we keep failing to connect.
+			dialDuration = backoffFor
 		}
-		// copy ac.addrs in case of race
-		addrsIter := make([]resolver.Address, len(ac.addrs))
-		copy(addrsIter, ac.addrs)
-		copts := ac.dopts.copts
+		connectDeadline := time.Now().Add(dialDuration)
 		ac.mu.Unlock()
-		connected, err := ac.createTransport(connectRetryNum, ridx, backoffDeadline, connectDeadline, addrsIter, copts, resetBackoff)
-		if err != nil {
-			return err
-		}
-		if connected {
-			return nil
-		}
-	}
-}

-// createTransport creates a connection to one of the backends in addrs.
-// It returns true if a connection was established.
-func (ac *addrConn) createTransport(connectRetryNum, ridx int, backoffDeadline, connectDeadline time.Time, addrs []resolver.Address, copts transport.ConnectOptions, resetBackoff chan struct{}) (bool, error) {
-	for i := ridx; i < len(addrs); i++ {
-		addr := addrs[i]
-		target := transport.TargetInfo{
-			Addr:      addr.Addr,
-			Metadata:  addr.Metadata,
-			Authority: ac.cc.authority,
-		}
-		done := make(chan struct{})
-		onPrefaceReceipt := func() {
+	addrLoop:
+		for _, addr := range addrs {
 			ac.mu.Lock()
-			close(done)
-			if !ac.backoffDeadline.IsZero() {
-				// If we haven't already started reconnecting to
-				// other backends.
-				// Note, this can happen when writer notices an error
-				// and triggers resetTransport while at the same time
-				// reader receives the preface and invokes this closure.
-				ac.backoffDeadline = time.Time{}
-				ac.connectDeadline = time.Time{}
-				ac.connectRetryNum = 0
+
+			if ac.state == connectivity.Shutdown {
+				ac.mu.Unlock()
+				return
+			}
+			ac.updateConnectivityState(connectivity.Connecting)
+			ac.transport = nil
+
+			ac.cc.mu.RLock()
+			ac.dopts.copts.KeepaliveParams = ac.cc.mkp
+			ac.cc.mu.RUnlock()
+
+			if ac.state == connectivity.Shutdown {
+				ac.mu.Unlock()
+				return
 			}
+
+			copts := ac.dopts.copts
+			if ac.scopts.CredsBundle != nil {
+				copts.CredsBundle = ac.scopts.CredsBundle
+			}
+			hctx, hcancel := context.WithCancel(ac.ctx)
+			defer hcancel()
 			ac.mu.Unlock()
-		}
-		// Do not cancel in the success path because of
-		// this issue in Go1.6: https://github.com/golang/go/issues/15078.
-		connectCtx, cancel := context.WithDeadline(ac.ctx, connectDeadline)
-		if channelz.IsOn() {
-			copts.ChannelzParentID = ac.channelzID
-		}
-		newTr, err := transport.NewClientTransport(connectCtx, ac.cc.ctx, target, copts, onPrefaceReceipt)
-		if err != nil {
-			cancel()
-			ac.cc.blockingpicker.updateConnectionError(err)
+
+			if channelz.IsOn() {
+				channelz.AddTraceEvent(ac.channelzID, &channelz.TraceEventDesc{
+					Desc:     fmt.Sprintf("Subchannel picks a new address %q to connect", addr.Addr),
+					Severity: channelz.CtINFO,
+				})
+			}
+
+			reconnect := grpcsync.NewEvent()
+			prefaceReceived := make(chan struct{})
+			newTr, err := ac.createTransport(addr, copts, connectDeadline, reconnect, prefaceReceived)
+			if err == nil {
+				ac.mu.Lock()
+				ac.curAddr = addr
+				ac.transport = newTr
+				ac.mu.Unlock()
+
+				healthCheckConfig := ac.cc.healthCheckConfig()
+				// LB channel health checking is only enabled when all the four requirements below are met:
+				// 1. it is not disabled by the user with the WithDisableHealthCheck DialOption,
+				// 2. the internal.HealthCheckFunc is set by importing the grpc/healthcheck package,
+				// 3. a service config with non-empty healthCheckConfig field is provided,
+				// 4. the current load balancer allows it.
+				healthcheckManagingState := false
+				if !ac.cc.dopts.disableHealthCheck && healthCheckConfig != nil && ac.scopts.HealthCheckEnabled {
+					if ac.cc.dopts.healthCheckFunc == nil {
+						// TODO: add a link to the health check doc in the error message.
+						grpclog.Error("the client side LB channel health check function has not been set.")
+					} else {
+						// TODO(deklerk) refactor to just return transport
+						go ac.startHealthCheck(hctx, newTr, addr, healthCheckConfig.ServiceName)
+						healthcheckManagingState = true
+					}
+				}
+				if !healthcheckManagingState {
+					ac.mu.Lock()
+					ac.updateConnectivityState(connectivity.Ready)
+					ac.mu.Unlock()
+				}
+			} else {
+				hcancel()
+				if err == errConnClosing {
+					return
+				}
+
+				if tryNextAddrFromStart.HasFired() {
+					break addrLoop
+				}
+				continue
+			}
+
 			ac.mu.Lock()
-			if ac.state == connectivity.Shutdown {
-				// ac.tearDown(...) has been invoked.
+			reqHandshake := ac.dopts.reqHandshake
+			ac.mu.Unlock()
+
+			<-reconnect.Done()
+			hcancel()
+
+			if reqHandshake == envconfig.RequireHandshakeHybrid {
+				// In RequireHandshakeHybrid mode, we must check to see whether
+				// server preface has arrived yet to decide whether to start
+				// reconnecting at the top of the list (server preface received)
+				// or continue with the next addr in the list as if the
+				// connection were not successful (server preface not received).
+				select {
+				case <-prefaceReceived:
+					// We received a server preface - huzzah! We consider this
+					// a success and restart from the top of the addr list.
+					ac.mu.Lock()
+					ac.backoffIdx = 0
+					ac.mu.Unlock()
+					break addrLoop
+				default:
+					// Despite having set state to READY, in hybrid mode we
+					// consider this a failure and continue connecting at the
+					// next addr in the list.
+					ac.mu.Lock()
+					if ac.state == connectivity.Shutdown {
+						ac.mu.Unlock()
+						return
+					}
+
+					ac.updateConnectivityState(connectivity.TransientFailure)
+					ac.mu.Unlock()
+
+					if tryNextAddrFromStart.HasFired() {
+						break addrLoop
+					}
+				}
+			} else {
+				// In RequireHandshakeOn mode, we would have already waited for
+				// the server preface, so we consider this a success and restart
+				// from the top of the addr list. In RequireHandshakeOff mode,
+				// we don't care to wait for the server preface before
+				// considering this a success, so we also restart from the top
+				// of the addr list.
+				ac.mu.Lock()
+				ac.backoffIdx = 0
 				ac.mu.Unlock()
-				return false, errConnClosing
+				break addrLoop
 			}
+		}
+
+		// After exhausting all addresses, or after need to reconnect after a
+		// READY, the addrConn enters TRANSIENT_FAILURE.
+		ac.mu.Lock()
+		if ac.state == connectivity.Shutdown {
+			ac.mu.Unlock()
+			return
+		}
+		ac.updateConnectivityState(connectivity.TransientFailure)
+
+		// Backoff.
+		b := ac.resetBackoff
+		timer := time.NewTimer(backoffFor)
+		acctx := ac.ctx
+		ac.mu.Unlock()
+
+		select {
+		case <-timer.C:
+			ac.mu.Lock()
+			ac.backoffIdx++
 			ac.mu.Unlock()
-			grpclog.Warningf("grpc: addrConn.createTransport failed to connect to %v. Err :%v. Reconnecting...", addr, err)
-			continue
+		case <-b:
+			timer.Stop()
+		case <-acctx.Done():
+			timer.Stop()
+			return
 		}
-		if ac.dopts.waitForHandshake {
+	}
+}
+
+// createTransport creates a connection to one of the backends in addrs. It
+// sets ac.transport in the success case, or it returns an error if it was
+// unable to successfully create a transport.
+//
+// If waitForHandshake is enabled, it blocks until server preface arrives.
+func (ac *addrConn) createTransport(addr resolver.Address, copts transport.ConnectOptions, connectDeadline time.Time, reconnect *grpcsync.Event, prefaceReceived chan struct{}) (transport.ClientTransport, error) {
+	onCloseCalled := make(chan struct{})
+
+	target := transport.TargetInfo{
+		Addr:      addr.Addr,
+		Metadata:  addr.Metadata,
+		Authority: ac.cc.authority,
+	}
+
+	prefaceTimer := time.NewTimer(time.Until(connectDeadline))
+
+	onGoAway := func(r transport.GoAwayReason) {
+		ac.mu.Lock()
+		ac.adjustParams(r)
+		ac.mu.Unlock()
+		reconnect.Fire()
+	}
+
+	onClose := func() {
+		close(onCloseCalled)
+		prefaceTimer.Stop()
+		reconnect.Fire()
+	}
+
+	onPrefaceReceipt := func() {
+		close(prefaceReceived)
+		prefaceTimer.Stop()
+	}
+
+	connectCtx, cancel := context.WithDeadline(ac.ctx, connectDeadline)
+	defer cancel()
+	if channelz.IsOn() {
+		copts.ChannelzParentID = ac.channelzID
+	}
+
+	newTr, err := transport.NewClientTransport(connectCtx, ac.cc.ctx, target, copts, onPrefaceReceipt, onGoAway, onClose)
+
+	if err == nil {
+		if ac.dopts.reqHandshake == envconfig.RequireHandshakeOn {
 			select {
-			case <-done:
-			case <-connectCtx.Done():
-				// Didn't receive server preface, must kill this new transport now.
-				grpclog.Warningf("grpc: addrConn.createTransport failed to receive server preface before deadline.")
+			case <-prefaceTimer.C:
+				// We didn't get the preface in time.
 				newTr.Close()
-				continue
-			case <-ac.ctx.Done():
+				err = errors.New("timed out waiting for server handshake")
+			case <-prefaceReceived:
+				// We got the preface - huzzah! things are good.
+			case <-onCloseCalled:
+				// The transport has already closed - noop.
+				return nil, errors.New("connection closed")
 			}
+		} else if ac.dopts.reqHandshake == envconfig.RequireHandshakeHybrid {
+			go func() {
+				select {
+				case <-prefaceTimer.C:
+					// We didn't get the preface in time.
+					newTr.Close()
+				case <-prefaceReceived:
+					// We got the preface just in the nick of time - huzzah!
+				case <-onCloseCalled:
+					// The transport has already closed - noop.
+				}
+			}()
 		}
+	}
+
+	if err != nil {
+		// newTr is either nil, or closed.
+		ac.cc.blockingpicker.updateConnectionError(err)
 		ac.mu.Lock()
 		if ac.state == connectivity.Shutdown {
+			// ac.tearDown(...) has been invoked.
 			ac.mu.Unlock()
-			// ac.tearDonn(...) has been invoked.
-			newTr.Close()
-			return false, errConnClosing
-		}
-		ac.printf("ready")
-		ac.state = connectivity.Ready
-		ac.cc.handleSubConnStateChange(ac.acbw, ac.state)
-		ac.transport = newTr
-		ac.curAddr = addr
-		if ac.ready != nil {
-			close(ac.ready)
-			ac.ready = nil
-		}
-		select {
-		case <-done:
-			// If the server has responded back with preface already,
-			// don't set the reconnect parameters.
-		default:
-			ac.connectRetryNum = connectRetryNum
-			ac.backoffDeadline = backoffDeadline
-			ac.connectDeadline = connectDeadline
-			ac.reconnectIdx = i + 1 // Start reconnecting from the next backend in the list.
+
+			return nil, errConnClosing
 		}
 		ac.mu.Unlock()
-		return true, nil
+		grpclog.Warningf("grpc: addrConn.createTransport failed to connect to %v. Err :%v. Reconnecting...", addr, err)
+		return nil, err
 	}
+
+	// Now there is a viable transport to be use, so set ac.transport to reflect the new viable transport.
 	ac.mu.Lock()
 	if ac.state == connectivity.Shutdown {
 		ac.mu.Unlock()
-		return false, errConnClosing
-	}
-	ac.state = connectivity.TransientFailure
-	ac.cc.handleSubConnStateChange(ac.acbw, ac.state)
-	ac.cc.resolveNow(resolver.ResolveNowOption{})
-	if ac.ready != nil {
-		close(ac.ready)
-		ac.ready = nil
+		newTr.Close()
+		return nil, errConnClosing
 	}
 	ac.mu.Unlock()
-	timer := time.NewTimer(backoffDeadline.Sub(time.Now()))
-	select {
-	case <-timer.C:
-	case <-resetBackoff:
-		timer.Stop()
-	case <-ac.ctx.Done():
-		timer.Stop()
-		return false, ac.ctx.Err()
-	}
-	return false, nil
-}

-func (ac *addrConn) resetConnectBackoff() {
+	// Now there is a viable transport to be use, so set ac.transport to reflect the new viable transport.
 	ac.mu.Lock()
-	close(ac.resetBackoff)
-	ac.resetBackoff = make(chan struct{})
-	ac.connectRetryNum = 0
+	if ac.state == connectivity.Shutdown {
+		ac.mu.Unlock()
+		newTr.Close()
+		return nil, errConnClosing
+	}
 	ac.mu.Unlock()
+
+	return newTr, nil
 }

-// Run in a goroutine to track the error in transport and create the
-// new transport if an error happens. It returns when the channel is closing.
-func (ac *addrConn) transportMonitor() {
-	for {
-		var timer *time.Timer
-		var cdeadline <-chan time.Time
+func (ac *addrConn) startHealthCheck(ctx context.Context, newTr transport.ClientTransport, addr resolver.Address, serviceName string) {
+	// Set up the health check helper functions
+	newStream := func() (interface{}, error) {
+		return ac.newClientStream(ctx, &StreamDesc{ServerStreams: true}, "/grpc.health.v1.Health/Watch", newTr)
+	}
+	firstReady := true
+	reportHealth := func(ok bool) {
 		ac.mu.Lock()
-		t := ac.transport
-		if !ac.connectDeadline.IsZero() {
-			timer = time.NewTimer(ac.connectDeadline.Sub(time.Now()))
-			cdeadline = timer.C
+		defer ac.mu.Unlock()
+		if ac.transport != newTr {
+			return
 		}
-		ac.mu.Unlock()
-		// Block until we receive a goaway or an error occurs.
-		select {
-		case <-t.GoAway():
-			done := t.Error()
-			cleanup := t.Close
-			// Since this transport will be orphaned (won't have a transportMonitor)
-			// we need to launch a goroutine to keep track of clientConn.Close()
-			// happening since it might not be noticed by any other goroutine for a while.
-			go func() {
-				<-done
-				cleanup()
-			}()
-		case <-t.Error():
-			// In case this is triggered because clientConn.Close()
-			// was called, we want to immeditately close the transport
-			// since no other goroutine might notice it for a while.
-			t.Close()
-		case <-cdeadline:
-			ac.mu.Lock()
-			// This implies that client received server preface.
-			if ac.backoffDeadline.IsZero() {
-				ac.mu.Unlock()
-				continue
+		if ok {
+			if firstReady {
+				firstReady = false
+				ac.curAddr = addr
 			}
-			ac.mu.Unlock()
-			timer = nil
-			// No server preface received until deadline.
-			// Kill the connection.
-			grpclog.Warningf("grpc: addrConn.transportMonitor didn't get server preface after waiting. Closing the new transport now.")
-			t.Close()
-		}
-		if timer != nil {
-			timer.Stop()
-		}
-		// If a GoAway happened, regardless of error, adjust our keepalive
-		// parameters as appropriate.
-		select {
-		case <-t.GoAway():
-			ac.adjustParams(t.GetGoAwayReason())
-		default:
-		}
-		ac.mu.Lock()
-		if ac.state == connectivity.Shutdown {
-			ac.mu.Unlock()
-			return
+			ac.updateConnectivityState(connectivity.Ready)
+		} else {
+			ac.updateConnectivityState(connectivity.TransientFailure)
 		}
-		// Set connectivity state to TransientFailure before calling
-		// resetTransport. Transition READY->CONNECTING is not valid.
-		ac.state = connectivity.TransientFailure
-		ac.cc.handleSubConnStateChange(ac.acbw, ac.state)
-		ac.cc.resolveNow(resolver.ResolveNowOption{})
-		ac.curAddr = resolver.Address{}
-		ac.mu.Unlock()
-		if err := ac.resetTransport(); err != nil {
-			ac.mu.Lock()
-			ac.printf("transport exiting: %v", err)
-			ac.mu.Unlock()
-			grpclog.Warningf("grpc: addrConn.transportMonitor exits due to: %v", err)
-			if err != errConnClosing {
-				// Keep this ac in cc.conns, to get the reason it's torn down.
-				ac.tearDown(err)
+	}
+	err := ac.cc.dopts.healthCheckFunc(ctx, newStream, reportHealth, serviceName)
+	if err != nil {
+		if status.Code(err) == codes.Unimplemented {
+			if channelz.IsOn() {
+				channelz.AddTraceEvent(ac.channelzID, &channelz.TraceEventDesc{
+					Desc:     "Subchannel health check is unimplemented at server side, thus health check is disabled",
+					Severity: channelz.CtError,
+				})
 			}
-			return
+			grpclog.Error("Subchannel health check is unimplemented at server side, thus health check is disabled")
+		} else {
+			grpclog.Errorf("HealthCheckFunc exits with unexpected error %v", err)
 		}
 	}
 }

+func (ac *addrConn) resetConnectBackoff() {
+	ac.mu.Lock()
+	close(ac.resetBackoff)
+	ac.backoffIdx = 0
+	ac.resetBackoff = make(chan struct{})
+	ac.mu.Unlock()
+}
+
 // getReadyTransport returns the transport if ac's state is READY.
 // Otherwise it returns nil, false.
 // If ac's state is IDLE, it will trigger ac to connect.
 func (ac *addrConn) getReadyTransport() (transport.ClientTransport, bool) {
 	ac.mu.Lock()
-	if ac.state == connectivity.Ready {
+	if ac.state == connectivity.Ready && ac.transport != nil {
 		t := ac.transport
 		ac.mu.Unlock()
 		return t, true
@@ -1161,34 +1320,43 @@ func (ac *addrConn) getReadyTransport() (transport.ClientTransport, bool) {
 // tight loop.
 // tearDown doesn't remove ac from ac.cc.conns.
 func (ac *addrConn) tearDown(err error) {
-	ac.cancel()
 	ac.mu.Lock()
-	defer ac.mu.Unlock()
 	if ac.state == connectivity.Shutdown {
+		ac.mu.Unlock()
 		return
 	}
+	curTr := ac.transport
+	ac.transport = nil
+	// We have to set the state to Shutdown before anything else to prevent races
+	// between setting the state and logic that waits on context cancelation / etc.
+	ac.updateConnectivityState(connectivity.Shutdown)
+	ac.cancel()
+	ac.tearDownErr = err
 	ac.curAddr = resolver.Address{}
-	if err == errConnDrain && ac.transport != nil {
+	if err == errConnDrain && curTr != nil {
 		// GracefulClose(...) may be executed multiple times when
 		// i) receiving multiple GoAway frames from the server; or
 		// ii) there are concurrent name resolver/Balancer triggered
 		// address removal and GoAway.
-		ac.transport.GracefulClose()
-	}
-	ac.state = connectivity.Shutdown
-	ac.tearDownErr = err
-	ac.cc.handleSubConnStateChange(ac.acbw, ac.state)
-	if ac.events != nil {
-		ac.events.Finish()
-		ac.events = nil
-	}
-	if ac.ready != nil {
-		close(ac.ready)
-		ac.ready = nil
+		// We have to unlock and re-lock here because GracefulClose => Close => onClose, which requires locking ac.mu.
+		ac.mu.Unlock()
+		curTr.GracefulClose()
+		ac.mu.Lock()
 	}
 	if channelz.IsOn() {
+		channelz.AddTraceEvent(ac.channelzID, &channelz.TraceEventDesc{
+			Desc:     "Subchannel Deleted",
+			Severity: channelz.CtINFO,
+			Parent: &channelz.TraceEventDesc{
+				Desc:     fmt.Sprintf("Subchanel(id:%d) deleted", ac.channelzID),
+				Severity: channelz.CtINFO,
+			},
+		})
+		// TraceEvent needs to be called before RemoveEntry, as TraceEvent may add trace reference to
+		// the entity beng deleted, and thus prevent it from being deleted right away.
 		channelz.RemoveEntry(ac.channelzID)
 	}
+	ac.mu.Unlock()
 }

 func (ac *addrConn) getState() connectivity.State {
diff --git a/vendor/google.golang.org/grpc/connectivity/connectivity.go b/vendor/google.golang.org/grpc/connectivity/connectivity.go
index 568ef5dc6..34ec36fbf 100644
--- a/vendor/google.golang.org/grpc/connectivity/connectivity.go
+++ b/vendor/google.golang.org/grpc/connectivity/connectivity.go
@@ -22,7 +22,8 @@
 package connectivity

 import (
-	"golang.org/x/net/context"
+	"context"
+
 	"google.golang.org/grpc/grpclog"
 )

@@ -51,7 +52,7 @@ func (s State) String() string {
 const (
 	// Idle indicates the ClientConn is idle.
 	Idle State = iota
-	// Connecting indicates the ClienConn is connecting.
+	// Connecting indicates the ClientConn is connecting.
 	Connecting
 	// Ready indicates the ClientConn is ready for work.
 	Ready
diff --git a/vendor/google.golang.org/grpc/credentials/credentials.go b/vendor/google.golang.org/grpc/credentials/credentials.go
index 1dae57ab1..a85156045 100644
--- a/vendor/google.golang.org/grpc/credentials/credentials.go
+++ b/vendor/google.golang.org/grpc/credentials/credentials.go
@@ -23,6 +23,7 @@
 package credentials // import "google.golang.org/grpc/credentials"

 import (
+	"context"
 	"crypto/tls"
 	"crypto/x509"
 	"errors"
@@ -32,7 +33,7 @@ import (
 	"strings"

 	"github.com/golang/protobuf/proto"
-	"golang.org/x/net/context"
+	"google.golang.org/grpc/credentials/internal"
 )

 // alpnProtoStr are the specified application level protocols for gRPC.
@@ -108,6 +109,25 @@ type TransportCredentials interface {
 	OverrideServerName(string) error
 }

+// Bundle is a combination of TransportCredentials and PerRPCCredentials.
+//
+// It also contains a mode switching method, so it can be used as a combination
+// of different credential policies.
+//
+// Bundle cannot be used together with individual TransportCredentials.
+// PerRPCCredentials from Bundle will be appended to other PerRPCCredentials.
+//
+// This API is experimental.
+type Bundle interface {
+	TransportCredentials() TransportCredentials
+	PerRPCCredentials() PerRPCCredentials
+	// NewWithMode should make a copy of Bundle, and switch mode. Modifying the
+	// existing Bundle may cause races.
+	//
+	// NewWithMode returns nil if the requested mode is not supported.
+	NewWithMode(mode string) (Bundle, error)
+}
+
 // TLSInfo contains the auth information for a TLS authenticated connection.
 // It implements the AuthInfo interface.
 type TLSInfo struct {
@@ -119,8 +139,8 @@ func (t TLSInfo) AuthType() string {
 	return "tls"
 }

-// GetChannelzSecurityValue returns security info requested by channelz.
-func (t TLSInfo) GetChannelzSecurityValue() ChannelzSecurityValue {
+// GetSecurityValue returns security info requested by channelz.
+func (t TLSInfo) GetSecurityValue() ChannelzSecurityValue {
 	v := &TLSChannelzSecurityValue{
 		StandardName: cipherSuiteLookup[t.State.CipherSuite],
 	}
@@ -168,7 +188,7 @@ func (c *tlsCreds) ClientHandshake(ctx context.Context, authority string, rawCon
 	case <-ctx.Done():
 		return nil, nil, ctx.Err()
 	}
-	return tlsConn{Conn: conn, rawConn: rawConn}, TLSInfo{conn.ConnectionState()}, nil
+	return internal.WrapSyscallConn(rawConn, conn), TLSInfo{conn.ConnectionState()}, nil
 }

 func (c *tlsCreds) ServerHandshake(rawConn net.Conn) (net.Conn, AuthInfo, error) {
@@ -176,7 +196,7 @@ func (c *tlsCreds) ServerHandshake(rawConn net.Conn) (net.Conn, AuthInfo, error)
 	if err := conn.Handshake(); err != nil {
 		return nil, nil, err
 	}
-	return tlsConn{Conn: conn, rawConn: rawConn}, TLSInfo{conn.ConnectionState()}, nil
+	return internal.WrapSyscallConn(rawConn, conn), TLSInfo{conn.ConnectionState()}, nil
 }

 func (c *tlsCreds) Clone() TransportCredentials {
@@ -266,11 +286,6 @@ type OtherChannelzSecurityValue struct {

 func (*OtherChannelzSecurityValue) isChannelzSecurityValue() {}

-type tlsConn struct {
-	*tls.Conn
-	rawConn net.Conn
-}
-
 var cipherSuiteLookup = map[uint16]string{
 	tls.TLS_RSA_WITH_RC4_128_SHA:                "TLS_RSA_WITH_RC4_128_SHA",
 	tls.TLS_RSA_WITH_3DES_EDE_CBC_SHA:           "TLS_RSA_WITH_3DES_EDE_CBC_SHA",
@@ -290,4 +305,24 @@ var cipherSuiteLookup = map[uint16]string{
 	tls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384:   "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
 	tls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384: "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384",
 	tls.TLS_FALLBACK_SCSV:                       "TLS_FALLBACK_SCSV",
+	tls.TLS_RSA_WITH_AES_128_CBC_SHA256:         "TLS_RSA_WITH_AES_128_CBC_SHA256",
+	tls.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256: "TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256",
+	tls.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256:   "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256",
+	tls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305:    "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
+	tls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305:  "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
+}
+
+// cloneTLSConfig returns a shallow clone of the exported
+// fields of cfg, ignoring the unexported sync.Once, which
+// contains a mutex and must not be copied.
+//
+// If cfg is nil, a new zero tls.Config is returned.
+//
+// TODO: inline this function if possible.
+func cloneTLSConfig(cfg *tls.Config) *tls.Config {
+	if cfg == nil {
+		return &tls.Config{}
+	}
+
+	return cfg.Clone()
 }
diff --git a/vendor/google.golang.org/grpc/credentials/go16.go b/vendor/google.golang.org/grpc/credentials/go16.go
deleted file mode 100644
index d6bbcc9fd..000000000
--- a/vendor/google.golang.org/grpc/credentials/go16.go
+++ /dev/null
@@ -1,57 +0,0 @@
-// +build !go1.7
-
-/*
- *
- * Copyright 2016 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package credentials
-
-import (
-	"crypto/tls"
-)
-
-// cloneTLSConfig returns a shallow clone of the exported
-// fields of cfg, ignoring the unexported sync.Once, which
-// contains a mutex and must not be copied.
-//
-// If cfg is nil, a new zero tls.Config is returned.
-func cloneTLSConfig(cfg *tls.Config) *tls.Config {
-	if cfg == nil {
-		return &tls.Config{}
-	}
-	return &tls.Config{
-		Rand:                     cfg.Rand,
-		Time:                     cfg.Time,
-		Certificates:             cfg.Certificates,
-		NameToCertificate:        cfg.NameToCertificate,
-		GetCertificate:           cfg.GetCertificate,
-		RootCAs:                  cfg.RootCAs,
-		NextProtos:               cfg.NextProtos,
-		ServerName:               cfg.ServerName,
-		ClientAuth:               cfg.ClientAuth,
-		ClientCAs:                cfg.ClientCAs,
-		InsecureSkipVerify:       cfg.InsecureSkipVerify,
-		CipherSuites:             cfg.CipherSuites,
-		PreferServerCipherSuites: cfg.PreferServerCipherSuites,
-		SessionTicketsDisabled:   cfg.SessionTicketsDisabled,
-		SessionTicketKey:         cfg.SessionTicketKey,
-		ClientSessionCache:       cfg.ClientSessionCache,
-		MinVersion:               cfg.MinVersion,
-		MaxVersion:               cfg.MaxVersion,
-		CurvePreferences:         cfg.CurvePreferences,
-	}
-}
diff --git a/vendor/google.golang.org/grpc/credentials/go17.go b/vendor/google.golang.org/grpc/credentials/go17.go
deleted file mode 100644
index fbd500002..000000000
--- a/vendor/google.golang.org/grpc/credentials/go17.go
+++ /dev/null
@@ -1,59 +0,0 @@
-// +build go1.7,!go1.8
-
-/*
- *
- * Copyright 2016 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package credentials
-
-import (
-	"crypto/tls"
-)
-
-// cloneTLSConfig returns a shallow clone of the exported
-// fields of cfg, ignoring the unexported sync.Once, which
-// contains a mutex and must not be copied.
-//
-// If cfg is nil, a new zero tls.Config is returned.
-func cloneTLSConfig(cfg *tls.Config) *tls.Config {
-	if cfg == nil {
-		return &tls.Config{}
-	}
-	return &tls.Config{
-		Rand:                        cfg.Rand,
-		Time:                        cfg.Time,
-		Certificates:                cfg.Certificates,
-		NameToCertificate:           cfg.NameToCertificate,
-		GetCertificate:              cfg.GetCertificate,
-		RootCAs:                     cfg.RootCAs,
-		NextProtos:                  cfg.NextProtos,
-		ServerName:                  cfg.ServerName,
-		ClientAuth:                  cfg.ClientAuth,
-		ClientCAs:                   cfg.ClientCAs,
-		InsecureSkipVerify:          cfg.InsecureSkipVerify,
-		CipherSuites:                cfg.CipherSuites,
-		PreferServerCipherSuites:    cfg.PreferServerCipherSuites,
-		SessionTicketsDisabled:      cfg.SessionTicketsDisabled,
-		SessionTicketKey:            cfg.SessionTicketKey,
-		ClientSessionCache:          cfg.ClientSessionCache,
-		MinVersion:                  cfg.MinVersion,
-		MaxVersion:                  cfg.MaxVersion,
-		CurvePreferences:            cfg.CurvePreferences,
-		DynamicRecordSizingDisabled: cfg.DynamicRecordSizingDisabled,
-		Renegotiation:               cfg.Renegotiation,
-	}
-}
diff --git a/vendor/google.golang.org/grpc/credentials/go18.go b/vendor/google.golang.org/grpc/credentials/go18.go
deleted file mode 100644
index db30d46cc..000000000
--- a/vendor/google.golang.org/grpc/credentials/go18.go
+++ /dev/null
@@ -1,46 +0,0 @@
-// +build go1.8
-
-/*
- *
- * Copyright 2017 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package credentials
-
-import (
-	"crypto/tls"
-)
-
-func init() {
-	cipherSuiteLookup[tls.TLS_RSA_WITH_AES_128_CBC_SHA256] = "TLS_RSA_WITH_AES_128_CBC_SHA256"
-	cipherSuiteLookup[tls.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256] = "TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256"
-	cipherSuiteLookup[tls.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256] = "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256"
-	cipherSuiteLookup[tls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305] = "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
-	cipherSuiteLookup[tls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305] = "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305"
-}
-
-// cloneTLSConfig returns a shallow clone of the exported
-// fields of cfg, ignoring the unexported sync.Once, which
-// contains a mutex and must not be copied.
-//
-// If cfg is nil, a new zero tls.Config is returned.
-func cloneTLSConfig(cfg *tls.Config) *tls.Config {
-	if cfg == nil {
-		return &tls.Config{}
-	}
-
-	return cfg.Clone()
-}
diff --git a/vendor/google.golang.org/grpc/credentials/internal/syscallconn.go b/vendor/google.golang.org/grpc/credentials/internal/syscallconn.go
new file mode 100644
index 000000000..2f4472bec
--- /dev/null
+++ b/vendor/google.golang.org/grpc/credentials/internal/syscallconn.go
@@ -0,0 +1,61 @@
+// +build !appengine
+
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+// Package internal contains credentials-internal code.
+package internal
+
+import (
+	"net"
+	"syscall"
+)
+
+type sysConn = syscall.Conn
+
+// syscallConn keeps reference of rawConn to support syscall.Conn for channelz.
+// SyscallConn() (the method in interface syscall.Conn) is explicitly
+// implemented on this type,
+//
+// Interface syscall.Conn is implemented by most net.Conn implementations (e.g.
+// TCPConn, UnixConn), but is not part of net.Conn interface. So wrapper conns
+// that embed net.Conn don't implement syscall.Conn. (Side note: tls.Conn
+// doesn't embed net.Conn, so even if syscall.Conn is part of net.Conn, it won't
+// help here).
+type syscallConn struct {
+	net.Conn
+	// sysConn is a type alias of syscall.Conn. It's necessary because the name
+	// `Conn` collides with `net.Conn`.
+	sysConn
+}
+
+// WrapSyscallConn tries to wrap rawConn and newConn into a net.Conn that
+// implements syscall.Conn. rawConn will be used to support syscall, and newConn
+// will be used for read/write.
+//
+// This function returns newConn if rawConn doesn't implement syscall.Conn.
+func WrapSyscallConn(rawConn, newConn net.Conn) net.Conn {
+	sysConn, ok := rawConn.(syscall.Conn)
+	if !ok {
+		return newConn
+	}
+	return &syscallConn{
+		Conn:    newConn,
+		sysConn: sysConn,
+	}
+}
diff --git a/vendor/google.golang.org/grpc/naming/go18.go b/vendor/google.golang.org/grpc/credentials/internal/syscallconn_appengine.go
similarity index 73%
rename from vendor/google.golang.org/grpc/naming/go18.go
rename to vendor/google.golang.org/grpc/credentials/internal/syscallconn_appengine.go
index b5a0f8427..d4346e9ea 100644
--- a/vendor/google.golang.org/grpc/naming/go18.go
+++ b/vendor/google.golang.org/grpc/credentials/internal/syscallconn_appengine.go
@@ -1,8 +1,8 @@
-// +build go1.8
+// +build appengine

 /*
  *
- * Copyright 2017 gRPC authors.
+ * Copyright 2018 gRPC authors.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -18,11 +18,13 @@
  *
  */

-package naming
+package internal

-import "net"
-
-var (
-	lookupHost = net.DefaultResolver.LookupHost
-	lookupSRV  = net.DefaultResolver.LookupSRV
+import (
+	"net"
 )
+
+// WrapSyscallConn returns newConn on appengine.
+func WrapSyscallConn(rawConn, newConn net.Conn) net.Conn {
+	return newConn
+}
diff --git a/vendor/google.golang.org/grpc/credentials/go19.go b/vendor/google.golang.org/grpc/credentials/tls13.go
similarity index 60%
rename from vendor/google.golang.org/grpc/credentials/go19.go
rename to vendor/google.golang.org/grpc/credentials/tls13.go
index 2a4ca1a57..ccbf35b33 100644
--- a/vendor/google.golang.org/grpc/credentials/go19.go
+++ b/vendor/google.golang.org/grpc/credentials/tls13.go
@@ -1,8 +1,8 @@
-// +build go1.9,!appengine
+// +build go1.12

 /*
  *
- * Copyright 2018 gRPC authors.
+ * Copyright 2019 gRPC authors.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -20,16 +20,11 @@

 package credentials

-import (
-	"errors"
-	"syscall"
-)
+import "crypto/tls"

-// implements the syscall.Conn interface
-func (c tlsConn) SyscallConn() (syscall.RawConn, error) {
-	conn, ok := c.rawConn.(syscall.Conn)
-	if !ok {
-		return nil, errors.New("RawConn does not implement syscall.Conn")
-	}
-	return conn.SyscallConn()
+// This init function adds cipher suite constants only defined in Go 1.12.
+func init() {
+	cipherSuiteLookup[tls.TLS_AES_128_GCM_SHA256] = "TLS_AES_128_GCM_SHA256"
+	cipherSuiteLookup[tls.TLS_AES_256_GCM_SHA384] = "TLS_AES_256_GCM_SHA384"
+	cipherSuiteLookup[tls.TLS_CHACHA20_POLY1305_SHA256] = "TLS_CHACHA20_POLY1305_SHA256"
 }
diff --git a/vendor/google.golang.org/grpc/dialoptions.go b/vendor/google.golang.org/grpc/dialoptions.go
index 3d3c9e231..537b25860 100644
--- a/vendor/google.golang.org/grpc/dialoptions.go
+++ b/vendor/google.golang.org/grpc/dialoptions.go
@@ -19,13 +19,14 @@
 package grpc

 import (
+	"context"
 	"fmt"
 	"net"
 	"time"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/balancer"
 	"google.golang.org/grpc/credentials"
+	"google.golang.org/grpc/grpclog"
 	"google.golang.org/grpc/internal"
 	"google.golang.org/grpc/internal/backoff"
 	"google.golang.org/grpc/internal/envconfig"
@@ -55,10 +56,12 @@ type dialOptions struct {
 	balancerBuilder balancer.Builder
 	// This is to support grpclb.
 	resolverBuilder      resolver.Builder
-	waitForHandshake     bool
+	reqHandshake         envconfig.RequireHandshakeSetting
 	channelzParentID     int64
 	disableServiceConfig bool
 	disableRetry         bool
+	disableHealthCheck   bool
+	healthCheckFunc      internal.HealthChecker
 }

 // DialOption configures how we set up the connection.
@@ -91,10 +94,13 @@ func newFuncDialOption(f func(*dialOptions)) *funcDialOption {
 }

 // WithWaitForHandshake blocks until the initial settings frame is received from
-// the server before assigning RPCs to the connection. Experimental API.
+// the server before assigning RPCs to the connection.
+//
+// Deprecated: this is the default behavior, and this option will be removed
+// after the 1.18 release.
 func WithWaitForHandshake() DialOption {
 	return newFuncDialOption(func(o *dialOptions) {
-		o.waitForHandshake = true
+		o.reqHandshake = envconfig.RequireHandshakeOn
 	})
 }

@@ -159,7 +165,7 @@ func WithDefaultCallOptions(cos ...CallOption) DialOption {
 // WithCodec returns a DialOption which sets a codec for message marshaling and
 // unmarshaling.
 //
-// Deprecated: use WithDefaultCallOptions(CallCustomCodec(c)) instead.
+// Deprecated: use WithDefaultCallOptions(ForceCodec(_)) instead.
 func WithCodec(c Codec) DialOption {
 	return WithDefaultCallOptions(CallCustomCodec(c))
 }
@@ -286,7 +292,8 @@ func WithInsecure() DialOption {
 }

 // WithTransportCredentials returns a DialOption which configures a connection
-// level security credentials (e.g., TLS/SSL).
+// level security credentials (e.g., TLS/SSL). This should not be used together
+// with WithCredentialsBundle.
 func WithTransportCredentials(creds credentials.TransportCredentials) DialOption {
 	return newFuncDialOption(func(o *dialOptions) {
 		o.copts.TransportCredentials = creds
@@ -301,6 +308,17 @@ func WithPerRPCCredentials(creds credentials.PerRPCCredentials) DialOption {
 	})
 }

+// WithCredentialsBundle returns a DialOption to set a credentials bundle for
+// the ClientConn.WithCreds. This should not be used together with
+// WithTransportCredentials.
+//
+// This API is experimental.
+func WithCredentialsBundle(b credentials.Bundle) DialOption {
+	return newFuncDialOption(func(o *dialOptions) {
+		o.copts.CredsBundle = b
+	})
+}
+
 // WithTimeout returns a DialOption that configures a timeout for dialing a
 // ClientConn initially. This is valid if and only if WithBlock() is present.
 //
@@ -311,26 +329,32 @@ func WithTimeout(d time.Duration) DialOption {
 	})
 }

-func withContextDialer(f func(context.Context, string) (net.Conn, error)) DialOption {
+// WithContextDialer returns a DialOption that sets a dialer to create
+// connections. If FailOnNonTempDialError() is set to true, and an error is
+// returned by f, gRPC checks the error's Temporary() method to decide if it
+// should try to reconnect to the network address.
+func WithContextDialer(f func(context.Context, string) (net.Conn, error)) DialOption {
 	return newFuncDialOption(func(o *dialOptions) {
 		o.copts.Dialer = f
 	})
 }

 func init() {
-	internal.WithContextDialer = withContextDialer
 	internal.WithResolverBuilder = withResolverBuilder
+	internal.WithHealthCheckFunc = withHealthCheckFunc
 }

 // WithDialer returns a DialOption that specifies a function to use for dialing
 // network addresses. If FailOnNonTempDialError() is set to true, and an error
 // is returned by f, gRPC checks the error's Temporary() method to decide if it
 // should try to reconnect to the network address.
+//
+// Deprecated: use WithContextDialer instead
 func WithDialer(f func(string, time.Duration) (net.Conn, error)) DialOption {
-	return withContextDialer(
+	return WithContextDialer(
 		func(ctx context.Context, addr string) (net.Conn, error) {
 			if deadline, ok := ctx.Deadline(); ok {
-				return f(addr, deadline.Sub(time.Now()))
+				return f(addr, time.Until(deadline))
 			}
 			return f(addr, 0)
 		})
@@ -370,6 +394,10 @@ func WithUserAgent(s string) DialOption {
 // WithKeepaliveParams returns a DialOption that specifies keepalive parameters
 // for the client transport.
 func WithKeepaliveParams(kp keepalive.ClientParameters) DialOption {
+	if kp.Time < internal.KeepaliveMinPingTime {
+		grpclog.Warningf("Adjusting keepalive ping interval to minimum period of %v", internal.KeepaliveMinPingTime)
+		kp.Time = internal.KeepaliveMinPingTime
+	}
 	return newFuncDialOption(func(o *dialOptions) {
 		o.copts.KeepaliveParams = kp
 	})
@@ -442,9 +470,30 @@ func WithMaxHeaderListSize(s uint32) DialOption {
 	})
 }

+// WithDisableHealthCheck disables the LB channel health checking for all SubConns of this ClientConn.
+//
+// This API is EXPERIMENTAL.
+func WithDisableHealthCheck() DialOption {
+	return newFuncDialOption(func(o *dialOptions) {
+		o.disableHealthCheck = true
+	})
+}
+
+// withHealthCheckFunc replaces the default health check function with the provided one. It makes
+// tests easier to change the health check function.
+//
+// For testing purpose only.
+func withHealthCheckFunc(f internal.HealthChecker) DialOption {
+	return newFuncDialOption(func(o *dialOptions) {
+		o.healthCheckFunc = f
+	})
+}
+
 func defaultDialOptions() dialOptions {
 	return dialOptions{
-		disableRetry: !envconfig.Retry,
+		disableRetry:    !envconfig.Retry,
+		reqHandshake:    envconfig.RequireHandshake,
+		healthCheckFunc: internal.HealthCheckFunc,
 		copts: transport.ConnectOptions{
 			WriteBufferSize: defaultWriteBufSize,
 			ReadBufferSize:  defaultReadBufSize,
diff --git a/vendor/google.golang.org/grpc/go16.go b/vendor/google.golang.org/grpc/go16.go
deleted file mode 100644
index b1db21af6..000000000
--- a/vendor/google.golang.org/grpc/go16.go
+++ /dev/null
@@ -1,71 +0,0 @@
-// +build go1.6,!go1.7
-
-/*
- *
- * Copyright 2016 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package grpc
-
-import (
-	"fmt"
-	"io"
-	"net"
-	"net/http"
-
-	"golang.org/x/net/context"
-	"google.golang.org/grpc/codes"
-	"google.golang.org/grpc/internal/transport"
-	"google.golang.org/grpc/status"
-)
-
-// dialContext connects to the address on the named network.
-func dialContext(ctx context.Context, network, address string) (net.Conn, error) {
-	return (&net.Dialer{Cancel: ctx.Done()}).Dial(network, address)
-}
-
-func sendHTTPRequest(ctx context.Context, req *http.Request, conn net.Conn) error {
-	req.Cancel = ctx.Done()
-	if err := req.Write(conn); err != nil {
-		return fmt.Errorf("failed to write the HTTP request: %v", err)
-	}
-	return nil
-}
-
-// toRPCErr converts an error into an error from the status package.
-func toRPCErr(err error) error {
-	if err == nil || err == io.EOF {
-		return err
-	}
-	if err == io.ErrUnexpectedEOF {
-		return status.Error(codes.Internal, err.Error())
-	}
-	if _, ok := status.FromError(err); ok {
-		return err
-	}
-	switch e := err.(type) {
-	case transport.ConnectionError:
-		return status.Error(codes.Unavailable, e.Desc)
-	default:
-		switch err {
-		case context.DeadlineExceeded:
-			return status.Error(codes.DeadlineExceeded, err.Error())
-		case context.Canceled:
-			return status.Error(codes.Canceled, err.Error())
-		}
-	}
-	return status.Error(codes.Unknown, err.Error())
-}
diff --git a/vendor/google.golang.org/grpc/go17.go b/vendor/google.golang.org/grpc/go17.go
deleted file mode 100644
index 71a72e8fe..000000000
--- a/vendor/google.golang.org/grpc/go17.go
+++ /dev/null
@@ -1,72 +0,0 @@
-// +build go1.7
-
-/*
- *
- * Copyright 2016 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package grpc
-
-import (
-	"context"
-	"fmt"
-	"io"
-	"net"
-	"net/http"
-
-	netctx "golang.org/x/net/context"
-	"google.golang.org/grpc/codes"
-	"google.golang.org/grpc/internal/transport"
-	"google.golang.org/grpc/status"
-)
-
-// dialContext connects to the address on the named network.
-func dialContext(ctx context.Context, network, address string) (net.Conn, error) {
-	return (&net.Dialer{}).DialContext(ctx, network, address)
-}
-
-func sendHTTPRequest(ctx context.Context, req *http.Request, conn net.Conn) error {
-	req = req.WithContext(ctx)
-	if err := req.Write(conn); err != nil {
-		return fmt.Errorf("failed to write the HTTP request: %v", err)
-	}
-	return nil
-}
-
-// toRPCErr converts an error into an error from the status package.
-func toRPCErr(err error) error {
-	if err == nil || err == io.EOF {
-		return err
-	}
-	if err == io.ErrUnexpectedEOF {
-		return status.Error(codes.Internal, err.Error())
-	}
-	if _, ok := status.FromError(err); ok {
-		return err
-	}
-	switch e := err.(type) {
-	case transport.ConnectionError:
-		return status.Error(codes.Unavailable, e.Desc)
-	default:
-		switch err {
-		case context.DeadlineExceeded, netctx.DeadlineExceeded:
-			return status.Error(codes.DeadlineExceeded, err.Error())
-		case context.Canceled, netctx.Canceled:
-			return status.Error(codes.Canceled, err.Error())
-		}
-	}
-	return status.Error(codes.Unknown, err.Error())
-}
diff --git a/vendor/google.golang.org/grpc/interceptor.go b/vendor/google.golang.org/grpc/interceptor.go
index 1f6ef6780..8b7350022 100644
--- a/vendor/google.golang.org/grpc/interceptor.go
+++ b/vendor/google.golang.org/grpc/interceptor.go
@@ -19,7 +19,7 @@
 package grpc

 import (
-	"golang.org/x/net/context"
+	"context"
 )

 // UnaryInvoker is called by UnaryClientInterceptor to complete RPCs.
diff --git a/vendor/google.golang.org/grpc/internal/binarylog/binarylog.go b/vendor/google.golang.org/grpc/internal/binarylog/binarylog.go
new file mode 100644
index 000000000..fee6aecd0
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/binarylog/binarylog.go
@@ -0,0 +1,167 @@
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+// Package binarylog implementation binary logging as defined in
+// https://github.com/grpc/proposal/blob/master/A16-binary-logging.md.
+package binarylog
+
+import (
+	"fmt"
+	"os"
+
+	"google.golang.org/grpc/grpclog"
+)
+
+// Logger is the global binary logger. It can be used to get binary logger for
+// each method.
+type Logger interface {
+	getMethodLogger(methodName string) *MethodLogger
+}
+
+// binLogger is the global binary logger for the binary. One of this should be
+// built at init time from the configuration (environment varialbe or flags).
+//
+// It is used to get a methodLogger for each individual method.
+var binLogger Logger
+
+// SetLogger sets the binarg logger.
+//
+// Only call this at init time.
+func SetLogger(l Logger) {
+	binLogger = l
+}
+
+// GetMethodLogger returns the methodLogger for the given methodName.
+//
+// methodName should be in the format of "/service/method".
+//
+// Each methodLogger returned by this method is a new instance. This is to
+// generate sequence id within the call.
+func GetMethodLogger(methodName string) *MethodLogger {
+	if binLogger == nil {
+		return nil
+	}
+	return binLogger.getMethodLogger(methodName)
+}
+
+func init() {
+	const envStr = "GRPC_BINARY_LOG_FILTER"
+	configStr := os.Getenv(envStr)
+	binLogger = NewLoggerFromConfigString(configStr)
+}
+
+type methodLoggerConfig struct {
+	// Max length of header and message.
+	hdr, msg uint64
+}
+
+type logger struct {
+	all      *methodLoggerConfig
+	services map[string]*methodLoggerConfig
+	methods  map[string]*methodLoggerConfig
+
+	blacklist map[string]struct{}
+}
+
+// newEmptyLogger creates an empty logger. The map fields need to be filled in
+// using the set* functions.
+func newEmptyLogger() *logger {
+	return &logger{}
+}
+
+// Set method logger for "*".
+func (l *logger) setDefaultMethodLogger(ml *methodLoggerConfig) error {
+	if l.all != nil {
+		return fmt.Errorf("conflicting global rules found")
+	}
+	l.all = ml
+	return nil
+}
+
+// Set method logger for "service/*".
+//
+// New methodLogger with same service overrides the old one.
+func (l *logger) setServiceMethodLogger(service string, ml *methodLoggerConfig) error {
+	if _, ok := l.services[service]; ok {
+		return fmt.Errorf("conflicting rules for service %v found", service)
+	}
+	if l.services == nil {
+		l.services = make(map[string]*methodLoggerConfig)
+	}
+	l.services[service] = ml
+	return nil
+}
+
+// Set method logger for "service/method".
+//
+// New methodLogger with same method overrides the old one.
+func (l *logger) setMethodMethodLogger(method string, ml *methodLoggerConfig) error {
+	if _, ok := l.blacklist[method]; ok {
+		return fmt.Errorf("conflicting rules for method %v found", method)
+	}
+	if _, ok := l.methods[method]; ok {
+		return fmt.Errorf("conflicting rules for method %v found", method)
+	}
+	if l.methods == nil {
+		l.methods = make(map[string]*methodLoggerConfig)
+	}
+	l.methods[method] = ml
+	return nil
+}
+
+// Set blacklist method for "-service/method".
+func (l *logger) setBlacklist(method string) error {
+	if _, ok := l.blacklist[method]; ok {
+		return fmt.Errorf("conflicting rules for method %v found", method)
+	}
+	if _, ok := l.methods[method]; ok {
+		return fmt.Errorf("conflicting rules for method %v found", method)
+	}
+	if l.blacklist == nil {
+		l.blacklist = make(map[string]struct{})
+	}
+	l.blacklist[method] = struct{}{}
+	return nil
+}
+
+// getMethodLogger returns the methodLogger for the given methodName.
+//
+// methodName should be in the format of "/service/method".
+//
+// Each methodLogger returned by this method is a new instance. This is to
+// generate sequence id within the call.
+func (l *logger) getMethodLogger(methodName string) *MethodLogger {
+	s, m, err := parseMethodName(methodName)
+	if err != nil {
+		grpclog.Infof("binarylogging: failed to parse %q: %v", methodName, err)
+		return nil
+	}
+	if ml, ok := l.methods[s+"/"+m]; ok {
+		return newMethodLogger(ml.hdr, ml.msg)
+	}
+	if _, ok := l.blacklist[s+"/"+m]; ok {
+		return nil
+	}
+	if ml, ok := l.services[s]; ok {
+		return newMethodLogger(ml.hdr, ml.msg)
+	}
+	if l.all == nil {
+		return nil
+	}
+	return newMethodLogger(l.all.hdr, l.all.msg)
+}
diff --git a/vendor/google.golang.org/grpc/internal/binarylog/binarylog_testutil.go b/vendor/google.golang.org/grpc/internal/binarylog/binarylog_testutil.go
new file mode 100644
index 000000000..1ee00a39a
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/binarylog/binarylog_testutil.go
@@ -0,0 +1,42 @@
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+// This file contains exported variables/functions that are exported for testing
+// only.
+//
+// An ideal way for this would be to put those in a *_test.go but in binarylog
+// package. But this doesn't work with staticcheck with go module. Error was:
+// "MdToMetadataProto not declared by package binarylog". This could be caused
+// by the way staticcheck looks for files for a certain package, which doesn't
+// support *_test.go files.
+//
+// Move those to binary_test.go when staticcheck is fixed.
+
+package binarylog
+
+var (
+	// AllLogger is a logger that logs all headers/messages for all RPCs. It's
+	// for testing only.
+	AllLogger = NewLoggerFromConfigString("*")
+	// MdToMetadataProto converts metadata to a binary logging proto message.
+	// It's for testing only.
+	MdToMetadataProto = mdToMetadataProto
+	// AddrToProto converts an address to a binary logging proto message. It's
+	// for testing only.
+	AddrToProto = addrToProto
+)
diff --git a/vendor/google.golang.org/grpc/internal/binarylog/env_config.go b/vendor/google.golang.org/grpc/internal/binarylog/env_config.go
new file mode 100644
index 000000000..4cc2525df
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/binarylog/env_config.go
@@ -0,0 +1,210 @@
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+package binarylog
+
+import (
+	"errors"
+	"fmt"
+	"regexp"
+	"strconv"
+	"strings"
+
+	"google.golang.org/grpc/grpclog"
+)
+
+// NewLoggerFromConfigString reads the string and build a logger. It can be used
+// to build a new logger and assign it to binarylog.Logger.
+//
+// Example filter config strings:
+//  - "" Nothing will be logged
+//  - "*" All headers and messages will be fully logged.
+//  - "*{h}" Only headers will be logged.
+//  - "*{m:256}" Only the first 256 bytes of each message will be logged.
+//  - "Foo/*" Logs every method in service Foo
+//  - "Foo/*,-Foo/Bar" Logs every method in service Foo except method /Foo/Bar
+//  - "Foo/*,Foo/Bar{m:256}" Logs the first 256 bytes of each message in method
+//    /Foo/Bar, logs all headers and messages in every other method in service
+//    Foo.
+//
+// If two configs exist for one certain method or service, the one specified
+// later overrides the privous config.
+func NewLoggerFromConfigString(s string) Logger {
+	if s == "" {
+		return nil
+	}
+	l := newEmptyLogger()
+	methods := strings.Split(s, ",")
+	for _, method := range methods {
+		if err := l.fillMethodLoggerWithConfigString(method); err != nil {
+			grpclog.Warningf("failed to parse binary log config: %v", err)
+			return nil
+		}
+	}
+	return l
+}
+
+// fillMethodLoggerWithConfigString parses config, creates methodLogger and adds
+// it to the right map in the logger.
+func (l *logger) fillMethodLoggerWithConfigString(config string) error {
+	// "" is invalid.
+	if config == "" {
+		return errors.New("empty string is not a valid method binary logging config")
+	}
+
+	// "-service/method", blacklist, no * or {} allowed.
+	if config[0] == '-' {
+		s, m, suffix, err := parseMethodConfigAndSuffix(config[1:])
+		if err != nil {
+			return fmt.Errorf("invalid config: %q, %v", config, err)
+		}
+		if m == "*" {
+			return fmt.Errorf("invalid config: %q, %v", config, "* not allowd in blacklist config")
+		}
+		if suffix != "" {
+			return fmt.Errorf("invalid config: %q, %v", config, "header/message limit not allowed in blacklist config")
+		}
+		if err := l.setBlacklist(s + "/" + m); err != nil {
+			return fmt.Errorf("invalid config: %v", err)
+		}
+		return nil
+	}
+
+	// "*{h:256;m:256}"
+	if config[0] == '*' {
+		hdr, msg, err := parseHeaderMessageLengthConfig(config[1:])
+		if err != nil {
+			return fmt.Errorf("invalid config: %q, %v", config, err)
+		}
+		if err := l.setDefaultMethodLogger(&methodLoggerConfig{hdr: hdr, msg: msg}); err != nil {
+			return fmt.Errorf("invalid config: %v", err)
+		}
+		return nil
+	}
+
+	s, m, suffix, err := parseMethodConfigAndSuffix(config)
+	if err != nil {
+		return fmt.Errorf("invalid config: %q, %v", config, err)
+	}
+	hdr, msg, err := parseHeaderMessageLengthConfig(suffix)
+	if err != nil {
+		return fmt.Errorf("invalid header/message length config: %q, %v", suffix, err)
+	}
+	if m == "*" {
+		if err := l.setServiceMethodLogger(s, &methodLoggerConfig{hdr: hdr, msg: msg}); err != nil {
+			return fmt.Errorf("invalid config: %v", err)
+		}
+	} else {
+		if err := l.setMethodMethodLogger(s+"/"+m, &methodLoggerConfig{hdr: hdr, msg: msg}); err != nil {
+			return fmt.Errorf("invalid config: %v", err)
+		}
+	}
+	return nil
+}
+
+const (
+	// TODO: this const is only used by env_config now. But could be useful for
+	// other config. Move to binarylog.go if necessary.
+	maxUInt = ^uint64(0)
+
+	// For "p.s/m" plus any suffix. Suffix will be parsed again. See test for
+	// expected output.
+	longMethodConfigRegexpStr = `^([\w./]+)/((?:\w+)|[*])(.+)?$`
+
+	// For suffix from above, "{h:123,m:123}". See test for expected output.
+	optionalLengthRegexpStr      = `(?::(\d+))?` // Optional ":123".
+	headerConfigRegexpStr        = `^{h` + optionalLengthRegexpStr + `}$`
+	messageConfigRegexpStr       = `^{m` + optionalLengthRegexpStr + `}$`
+	headerMessageConfigRegexpStr = `^{h` + optionalLengthRegexpStr + `;m` + optionalLengthRegexpStr + `}$`
+)
+
+var (
+	longMethodConfigRegexp    = regexp.MustCompile(longMethodConfigRegexpStr)
+	headerConfigRegexp        = regexp.MustCompile(headerConfigRegexpStr)
+	messageConfigRegexp       = regexp.MustCompile(messageConfigRegexpStr)
+	headerMessageConfigRegexp = regexp.MustCompile(headerMessageConfigRegexpStr)
+)
+
+// Turn "service/method{h;m}" into "service", "method", "{h;m}".
+func parseMethodConfigAndSuffix(c string) (service, method, suffix string, _ error) {
+	// Regexp result:
+	//
+	// in:  "p.s/m{h:123,m:123}",
+	// out: []string{"p.s/m{h:123,m:123}", "p.s", "m", "{h:123,m:123}"},
+	match := longMethodConfigRegexp.FindStringSubmatch(c)
+	if match == nil {
+		return "", "", "", fmt.Errorf("%q contains invalid substring", c)
+	}
+	service = match[1]
+	method = match[2]
+	suffix = match[3]
+	return
+}
+
+// Turn "{h:123;m:345}" into 123, 345.
+//
+// Return maxUInt if length is unspecified.
+func parseHeaderMessageLengthConfig(c string) (hdrLenStr, msgLenStr uint64, err error) {
+	if c == "" {
+		return maxUInt, maxUInt, nil
+	}
+	// Header config only.
+	if match := headerConfigRegexp.FindStringSubmatch(c); match != nil {
+		if s := match[1]; s != "" {
+			hdrLenStr, err = strconv.ParseUint(s, 10, 64)
+			if err != nil {
+				return 0, 0, fmt.Errorf("failed to convert %q to uint", s)
+			}
+			return hdrLenStr, 0, nil
+		}
+		return maxUInt, 0, nil
+	}
+
+	// Message config only.
+	if match := messageConfigRegexp.FindStringSubmatch(c); match != nil {
+		if s := match[1]; s != "" {
+			msgLenStr, err = strconv.ParseUint(s, 10, 64)
+			if err != nil {
+				return 0, 0, fmt.Errorf("failed to convert %q to uint", s)
+			}
+			return 0, msgLenStr, nil
+		}
+		return 0, maxUInt, nil
+	}
+
+	// Header and message config both.
+	if match := headerMessageConfigRegexp.FindStringSubmatch(c); match != nil {
+		// Both hdr and msg are specified, but one or two of them might be empty.
+		hdrLenStr = maxUInt
+		msgLenStr = maxUInt
+		if s := match[1]; s != "" {
+			hdrLenStr, err = strconv.ParseUint(s, 10, 64)
+			if err != nil {
+				return 0, 0, fmt.Errorf("failed to convert %q to uint", s)
+			}
+		}
+		if s := match[2]; s != "" {
+			msgLenStr, err = strconv.ParseUint(s, 10, 64)
+			if err != nil {
+				return 0, 0, fmt.Errorf("failed to convert %q to uint", s)
+			}
+		}
+		return hdrLenStr, msgLenStr, nil
+	}
+	return 0, 0, fmt.Errorf("%q contains invalid substring", c)
+}
diff --git a/vendor/google.golang.org/grpc/internal/binarylog/method_logger.go b/vendor/google.golang.org/grpc/internal/binarylog/method_logger.go
new file mode 100644
index 000000000..160f6e861
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/binarylog/method_logger.go
@@ -0,0 +1,423 @@
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+package binarylog
+
+import (
+	"net"
+	"strings"
+	"sync/atomic"
+	"time"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/golang/protobuf/ptypes"
+	pb "google.golang.org/grpc/binarylog/grpc_binarylog_v1"
+	"google.golang.org/grpc/grpclog"
+	"google.golang.org/grpc/metadata"
+	"google.golang.org/grpc/status"
+)
+
+type callIDGenerator struct {
+	id uint64
+}
+
+func (g *callIDGenerator) next() uint64 {
+	id := atomic.AddUint64(&g.id, 1)
+	return id
+}
+
+// reset is for testing only, and doesn't need to be thread safe.
+func (g *callIDGenerator) reset() {
+	g.id = 0
+}
+
+var idGen callIDGenerator
+
+// MethodLogger is the sub-logger for each method.
+type MethodLogger struct {
+	headerMaxLen, messageMaxLen uint64
+
+	callID          uint64
+	idWithinCallGen *callIDGenerator
+
+	sink Sink // TODO(blog): make this plugable.
+}
+
+func newMethodLogger(h, m uint64) *MethodLogger {
+	return &MethodLogger{
+		headerMaxLen:  h,
+		messageMaxLen: m,
+
+		callID:          idGen.next(),
+		idWithinCallGen: &callIDGenerator{},
+
+		sink: defaultSink, // TODO(blog): make it plugable.
+	}
+}
+
+// Log creates a proto binary log entry, and logs it to the sink.
+func (ml *MethodLogger) Log(c LogEntryConfig) {
+	m := c.toProto()
+	timestamp, _ := ptypes.TimestampProto(time.Now())
+	m.Timestamp = timestamp
+	m.CallId = ml.callID
+	m.SequenceIdWithinCall = ml.idWithinCallGen.next()
+
+	switch pay := m.Payload.(type) {
+	case *pb.GrpcLogEntry_ClientHeader:
+		m.PayloadTruncated = ml.truncateMetadata(pay.ClientHeader.GetMetadata())
+	case *pb.GrpcLogEntry_ServerHeader:
+		m.PayloadTruncated = ml.truncateMetadata(pay.ServerHeader.GetMetadata())
+	case *pb.GrpcLogEntry_Message:
+		m.PayloadTruncated = ml.truncateMessage(pay.Message)
+	}
+
+	ml.sink.Write(m)
+}
+
+func (ml *MethodLogger) truncateMetadata(mdPb *pb.Metadata) (truncated bool) {
+	if ml.headerMaxLen == maxUInt {
+		return false
+	}
+	var (
+		bytesLimit = ml.headerMaxLen
+		index      int
+	)
+	// At the end of the loop, index will be the first entry where the total
+	// size is greater than the limit:
+	//
+	// len(entry[:index]) <= ml.hdr && len(entry[:index+1]) > ml.hdr.
+	for ; index < len(mdPb.Entry); index++ {
+		entry := mdPb.Entry[index]
+		if entry.Key == "grpc-trace-bin" {
+			// "grpc-trace-bin" is a special key. It's kept in the log entry,
+			// but not counted towards the size limit.
+			continue
+		}
+		currentEntryLen := uint64(len(entry.Value))
+		if currentEntryLen > bytesLimit {
+			break
+		}
+		bytesLimit -= currentEntryLen
+	}
+	truncated = index < len(mdPb.Entry)
+	mdPb.Entry = mdPb.Entry[:index]
+	return truncated
+}
+
+func (ml *MethodLogger) truncateMessage(msgPb *pb.Message) (truncated bool) {
+	if ml.messageMaxLen == maxUInt {
+		return false
+	}
+	if ml.messageMaxLen >= uint64(len(msgPb.Data)) {
+		return false
+	}
+	msgPb.Data = msgPb.Data[:ml.messageMaxLen]
+	return true
+}
+
+// LogEntryConfig represents the configuration for binary log entry.
+type LogEntryConfig interface {
+	toProto() *pb.GrpcLogEntry
+}
+
+// ClientHeader configs the binary log entry to be a ClientHeader entry.
+type ClientHeader struct {
+	OnClientSide bool
+	Header       metadata.MD
+	MethodName   string
+	Authority    string
+	Timeout      time.Duration
+	// PeerAddr is required only when it's on server side.
+	PeerAddr net.Addr
+}
+
+func (c *ClientHeader) toProto() *pb.GrpcLogEntry {
+	// This function doesn't need to set all the fields (e.g. seq ID). The Log
+	// function will set the fields when necessary.
+	clientHeader := &pb.ClientHeader{
+		Metadata:   mdToMetadataProto(c.Header),
+		MethodName: c.MethodName,
+		Authority:  c.Authority,
+	}
+	if c.Timeout > 0 {
+		clientHeader.Timeout = ptypes.DurationProto(c.Timeout)
+	}
+	ret := &pb.GrpcLogEntry{
+		Type: pb.GrpcLogEntry_EVENT_TYPE_CLIENT_HEADER,
+		Payload: &pb.GrpcLogEntry_ClientHeader{
+			ClientHeader: clientHeader,
+		},
+	}
+	if c.OnClientSide {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_CLIENT
+	} else {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_SERVER
+	}
+	if c.PeerAddr != nil {
+		ret.Peer = addrToProto(c.PeerAddr)
+	}
+	return ret
+}
+
+// ServerHeader configs the binary log entry to be a ServerHeader entry.
+type ServerHeader struct {
+	OnClientSide bool
+	Header       metadata.MD
+	// PeerAddr is required only when it's on client side.
+	PeerAddr net.Addr
+}
+
+func (c *ServerHeader) toProto() *pb.GrpcLogEntry {
+	ret := &pb.GrpcLogEntry{
+		Type: pb.GrpcLogEntry_EVENT_TYPE_SERVER_HEADER,
+		Payload: &pb.GrpcLogEntry_ServerHeader{
+			ServerHeader: &pb.ServerHeader{
+				Metadata: mdToMetadataProto(c.Header),
+			},
+		},
+	}
+	if c.OnClientSide {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_CLIENT
+	} else {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_SERVER
+	}
+	if c.PeerAddr != nil {
+		ret.Peer = addrToProto(c.PeerAddr)
+	}
+	return ret
+}
+
+// ClientMessage configs the binary log entry to be a ClientMessage entry.
+type ClientMessage struct {
+	OnClientSide bool
+	// Message can be a proto.Message or []byte. Other messages formats are not
+	// supported.
+	Message interface{}
+}
+
+func (c *ClientMessage) toProto() *pb.GrpcLogEntry {
+	var (
+		data []byte
+		err  error
+	)
+	if m, ok := c.Message.(proto.Message); ok {
+		data, err = proto.Marshal(m)
+		if err != nil {
+			grpclog.Infof("binarylogging: failed to marshal proto message: %v", err)
+		}
+	} else if b, ok := c.Message.([]byte); ok {
+		data = b
+	} else {
+		grpclog.Infof("binarylogging: message to log is neither proto.message nor []byte")
+	}
+	ret := &pb.GrpcLogEntry{
+		Type: pb.GrpcLogEntry_EVENT_TYPE_CLIENT_MESSAGE,
+		Payload: &pb.GrpcLogEntry_Message{
+			Message: &pb.Message{
+				Length: uint32(len(data)),
+				Data:   data,
+			},
+		},
+	}
+	if c.OnClientSide {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_CLIENT
+	} else {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_SERVER
+	}
+	return ret
+}
+
+// ServerMessage configs the binary log entry to be a ServerMessage entry.
+type ServerMessage struct {
+	OnClientSide bool
+	// Message can be a proto.Message or []byte. Other messages formats are not
+	// supported.
+	Message interface{}
+}
+
+func (c *ServerMessage) toProto() *pb.GrpcLogEntry {
+	var (
+		data []byte
+		err  error
+	)
+	if m, ok := c.Message.(proto.Message); ok {
+		data, err = proto.Marshal(m)
+		if err != nil {
+			grpclog.Infof("binarylogging: failed to marshal proto message: %v", err)
+		}
+	} else if b, ok := c.Message.([]byte); ok {
+		data = b
+	} else {
+		grpclog.Infof("binarylogging: message to log is neither proto.message nor []byte")
+	}
+	ret := &pb.GrpcLogEntry{
+		Type: pb.GrpcLogEntry_EVENT_TYPE_SERVER_MESSAGE,
+		Payload: &pb.GrpcLogEntry_Message{
+			Message: &pb.Message{
+				Length: uint32(len(data)),
+				Data:   data,
+			},
+		},
+	}
+	if c.OnClientSide {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_CLIENT
+	} else {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_SERVER
+	}
+	return ret
+}
+
+// ClientHalfClose configs the binary log entry to be a ClientHalfClose entry.
+type ClientHalfClose struct {
+	OnClientSide bool
+}
+
+func (c *ClientHalfClose) toProto() *pb.GrpcLogEntry {
+	ret := &pb.GrpcLogEntry{
+		Type:    pb.GrpcLogEntry_EVENT_TYPE_CLIENT_HALF_CLOSE,
+		Payload: nil, // No payload here.
+	}
+	if c.OnClientSide {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_CLIENT
+	} else {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_SERVER
+	}
+	return ret
+}
+
+// ServerTrailer configs the binary log entry to be a ServerTrailer entry.
+type ServerTrailer struct {
+	OnClientSide bool
+	Trailer      metadata.MD
+	// Err is the status error.
+	Err error
+	// PeerAddr is required only when it's on client side and the RPC is trailer
+	// only.
+	PeerAddr net.Addr
+}
+
+func (c *ServerTrailer) toProto() *pb.GrpcLogEntry {
+	st, ok := status.FromError(c.Err)
+	if !ok {
+		grpclog.Info("binarylogging: error in trailer is not a status error")
+	}
+	var (
+		detailsBytes []byte
+		err          error
+	)
+	stProto := st.Proto()
+	if stProto != nil && len(stProto.Details) != 0 {
+		detailsBytes, err = proto.Marshal(stProto)
+		if err != nil {
+			grpclog.Infof("binarylogging: failed to marshal status proto: %v", err)
+		}
+	}
+	ret := &pb.GrpcLogEntry{
+		Type: pb.GrpcLogEntry_EVENT_TYPE_SERVER_TRAILER,
+		Payload: &pb.GrpcLogEntry_Trailer{
+			Trailer: &pb.Trailer{
+				Metadata:      mdToMetadataProto(c.Trailer),
+				StatusCode:    uint32(st.Code()),
+				StatusMessage: st.Message(),
+				StatusDetails: detailsBytes,
+			},
+		},
+	}
+	if c.OnClientSide {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_CLIENT
+	} else {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_SERVER
+	}
+	if c.PeerAddr != nil {
+		ret.Peer = addrToProto(c.PeerAddr)
+	}
+	return ret
+}
+
+// Cancel configs the binary log entry to be a Cancel entry.
+type Cancel struct {
+	OnClientSide bool
+}
+
+func (c *Cancel) toProto() *pb.GrpcLogEntry {
+	ret := &pb.GrpcLogEntry{
+		Type:    pb.GrpcLogEntry_EVENT_TYPE_CANCEL,
+		Payload: nil,
+	}
+	if c.OnClientSide {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_CLIENT
+	} else {
+		ret.Logger = pb.GrpcLogEntry_LOGGER_SERVER
+	}
+	return ret
+}
+
+// metadataKeyOmit returns whether the metadata entry with this key should be
+// omitted.
+func metadataKeyOmit(key string) bool {
+	switch key {
+	case "lb-token", ":path", ":authority", "content-encoding", "content-type", "user-agent", "te":
+		return true
+	case "grpc-trace-bin": // grpc-trace-bin is special because it's visiable to users.
+		return false
+	}
+	return strings.HasPrefix(key, "grpc-")
+}
+
+func mdToMetadataProto(md metadata.MD) *pb.Metadata {
+	ret := &pb.Metadata{}
+	for k, vv := range md {
+		if metadataKeyOmit(k) {
+			continue
+		}
+		for _, v := range vv {
+			ret.Entry = append(ret.Entry,
+				&pb.MetadataEntry{
+					Key:   k,
+					Value: []byte(v),
+				},
+			)
+		}
+	}
+	return ret
+}
+
+func addrToProto(addr net.Addr) *pb.Address {
+	ret := &pb.Address{}
+	switch a := addr.(type) {
+	case *net.TCPAddr:
+		if a.IP.To4() != nil {
+			ret.Type = pb.Address_TYPE_IPV4
+		} else if a.IP.To16() != nil {
+			ret.Type = pb.Address_TYPE_IPV6
+		} else {
+			ret.Type = pb.Address_TYPE_UNKNOWN
+			// Do not set address and port fields.
+			break
+		}
+		ret.Address = a.IP.String()
+		ret.IpPort = uint32(a.Port)
+	case *net.UnixAddr:
+		ret.Type = pb.Address_TYPE_UNIX
+		ret.Address = a.String()
+	default:
+		ret.Type = pb.Address_TYPE_UNKNOWN
+	}
+	return ret
+}
diff --git a/vendor/google.golang.org/grpc/internal/binarylog/sink.go b/vendor/google.golang.org/grpc/internal/binarylog/sink.go
new file mode 100644
index 000000000..20d044f0f
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/binarylog/sink.go
@@ -0,0 +1,162 @@
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+package binarylog
+
+import (
+	"bufio"
+	"encoding/binary"
+	"fmt"
+	"io"
+	"io/ioutil"
+	"sync"
+	"time"
+
+	"github.com/golang/protobuf/proto"
+	pb "google.golang.org/grpc/binarylog/grpc_binarylog_v1"
+	"google.golang.org/grpc/grpclog"
+)
+
+var (
+	defaultSink Sink = &noopSink{} // TODO(blog): change this default (file in /tmp).
+)
+
+// SetDefaultSink sets the sink where binary logs will be written to.
+//
+// Not thread safe. Only set during initialization.
+func SetDefaultSink(s Sink) {
+	if defaultSink != nil {
+		defaultSink.Close()
+	}
+	defaultSink = s
+}
+
+// Sink writes log entry into the binary log sink.
+type Sink interface {
+	// Write will be called to write the log entry into the sink.
+	//
+	// It should be thread-safe so it can be called in parallel.
+	Write(*pb.GrpcLogEntry) error
+	// Close will be called when the Sink is replaced by a new Sink.
+	Close() error
+}
+
+type noopSink struct{}
+
+func (ns *noopSink) Write(*pb.GrpcLogEntry) error { return nil }
+func (ns *noopSink) Close() error                 { return nil }
+
+// newWriterSink creates a binary log sink with the given writer.
+//
+// Write() marshalls the proto message and writes it to the given writer. Each
+// message is prefixed with a 4 byte big endian unsigned integer as the length.
+//
+// No buffer is done, Close() doesn't try to close the writer.
+func newWriterSink(w io.Writer) *writerSink {
+	return &writerSink{out: w}
+}
+
+type writerSink struct {
+	out io.Writer
+}
+
+func (ws *writerSink) Write(e *pb.GrpcLogEntry) error {
+	b, err := proto.Marshal(e)
+	if err != nil {
+		grpclog.Infof("binary logging: failed to marshal proto message: %v", err)
+	}
+	hdr := make([]byte, 4)
+	binary.BigEndian.PutUint32(hdr, uint32(len(b)))
+	if _, err := ws.out.Write(hdr); err != nil {
+		return err
+	}
+	if _, err := ws.out.Write(b); err != nil {
+		return err
+	}
+	return nil
+}
+
+func (ws *writerSink) Close() error { return nil }
+
+type bufWriteCloserSink struct {
+	mu     sync.Mutex
+	closer io.Closer
+	out    *writerSink   // out is built on buf.
+	buf    *bufio.Writer // buf is kept for flush.
+
+	writeStartOnce sync.Once
+	writeTicker    *time.Ticker
+}
+
+func (fs *bufWriteCloserSink) Write(e *pb.GrpcLogEntry) error {
+	// Start the write loop when Write is called.
+	fs.writeStartOnce.Do(fs.startFlushGoroutine)
+	fs.mu.Lock()
+	if err := fs.out.Write(e); err != nil {
+		fs.mu.Unlock()
+		return err
+	}
+	fs.mu.Unlock()
+	return nil
+}
+
+const (
+	bufFlushDuration = 60 * time.Second
+)
+
+func (fs *bufWriteCloserSink) startFlushGoroutine() {
+	fs.writeTicker = time.NewTicker(bufFlushDuration)
+	go func() {
+		for range fs.writeTicker.C {
+			fs.mu.Lock()
+			fs.buf.Flush()
+			fs.mu.Unlock()
+		}
+	}()
+}
+
+func (fs *bufWriteCloserSink) Close() error {
+	if fs.writeTicker != nil {
+		fs.writeTicker.Stop()
+	}
+	fs.mu.Lock()
+	fs.buf.Flush()
+	fs.closer.Close()
+	fs.out.Close()
+	fs.mu.Unlock()
+	return nil
+}
+
+func newBufWriteCloserSink(o io.WriteCloser) Sink {
+	bufW := bufio.NewWriter(o)
+	return &bufWriteCloserSink{
+		closer: o,
+		out:    newWriterSink(bufW),
+		buf:    bufW,
+	}
+}
+
+// NewTempFileSink creates a temp file and returns a Sink that writes to this
+// file.
+func NewTempFileSink() (Sink, error) {
+	tempFile, err := ioutil.TempFile("/tmp", "grpcgo_binarylog_*.txt")
+	if err != nil {
+		return nil, fmt.Errorf("failed to create temp file: %v", err)
+	}
+	return newBufWriteCloserSink(tempFile), nil
+}
diff --git a/vendor/google.golang.org/grpc/status/go16.go b/vendor/google.golang.org/grpc/internal/binarylog/util.go
similarity index 51%
rename from vendor/google.golang.org/grpc/status/go16.go
rename to vendor/google.golang.org/grpc/internal/binarylog/util.go
index e59b53e82..15dc7803d 100644
--- a/vendor/google.golang.org/grpc/status/go16.go
+++ b/vendor/google.golang.org/grpc/internal/binarylog/util.go
@@ -1,5 +1,3 @@
-// +build go1.6,!go1.7
-
 /*
  *
  * Copyright 2018 gRPC authors.
@@ -18,25 +16,26 @@
  *
  */

-package status
+package binarylog

 import (
-	"golang.org/x/net/context"
-	"google.golang.org/grpc/codes"
+	"errors"
+	"strings"
 )

-// FromContextError converts a context error into a Status.  It returns a
-// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is
-// non-nil and not a context error.
-func FromContextError(err error) *Status {
-	switch err {
-	case nil:
-		return New(codes.OK, "")
-	case context.DeadlineExceeded:
-		return New(codes.DeadlineExceeded, err.Error())
-	case context.Canceled:
-		return New(codes.Canceled, err.Error())
-	default:
-		return New(codes.Unknown, err.Error())
+// parseMethodName splits service and method from the input. It expects format
+// "/service/method".
+//
+// TODO: move to internal/grpcutil.
+func parseMethodName(methodName string) (service, method string, _ error) {
+	if !strings.HasPrefix(methodName, "/") {
+		return "", "", errors.New("invalid method name: should start with /")
+	}
+	methodName = methodName[1:]
+
+	pos := strings.LastIndex(methodName, "/")
+	if pos < 0 {
+		return "", "", errors.New("invalid method name: suffix /method is missing")
 	}
+	return methodName[:pos], methodName[pos+1:], nil
 }
diff --git a/vendor/google.golang.org/grpc/internal/channelz/funcs.go b/vendor/google.golang.org/grpc/internal/channelz/funcs.go
index 586a0336b..041520d35 100644
--- a/vendor/google.golang.org/grpc/internal/channelz/funcs.go
+++ b/vendor/google.golang.org/grpc/internal/channelz/funcs.go
@@ -27,16 +27,22 @@ import (
 	"sort"
 	"sync"
 	"sync/atomic"
+	"time"

 	"google.golang.org/grpc/grpclog"
 )

+const (
+	defaultMaxTraceEntry int32 = 30
+)
+
 var (
 	db    dbWrapper
 	idGen idGenerator
 	// EntryPerPage defines the number of channelz entries to be shown on a web page.
-	EntryPerPage = 50
-	curState     int32
+	EntryPerPage  = int64(50)
+	curState      int32
+	maxTraceEntry = defaultMaxTraceEntry
 )

 // TurnOn turns on channelz data collection.
@@ -52,6 +58,22 @@ func IsOn() bool {
 	return atomic.CompareAndSwapInt32(&curState, 1, 1)
 }

+// SetMaxTraceEntry sets maximum number of trace entry per entity (i.e. channel/subchannel).
+// Setting it to 0 will disable channel tracing.
+func SetMaxTraceEntry(i int32) {
+	atomic.StoreInt32(&maxTraceEntry, i)
+}
+
+// ResetMaxTraceEntryToDefault resets the maximum number of trace entry per entity to default.
+func ResetMaxTraceEntryToDefault() {
+	atomic.StoreInt32(&maxTraceEntry, defaultMaxTraceEntry)
+}
+
+func getMaxTraceEntry() int {
+	i := atomic.LoadInt32(&maxTraceEntry)
+	return int(i)
+}
+
 // dbWarpper wraps around a reference to internal channelz data storage, and
 // provide synchronized functionality to set and get the reference.
 type dbWrapper struct {
@@ -91,20 +113,20 @@ func NewChannelzStorage() {
 // boolean indicating whether there's more top channels to be queried for.
 //
 // The arg id specifies that only top channel with id at or above it will be included
-// in the result. The returned slice is up to a length of EntryPerPage, and is
-// sorted in ascending id order.
-func GetTopChannels(id int64) ([]*ChannelMetric, bool) {
-	return db.get().GetTopChannels(id)
+// in the result. The returned slice is up to a length of the arg maxResults or
+// EntryPerPage if maxResults is zero, and is sorted in ascending id order.
+func GetTopChannels(id int64, maxResults int64) ([]*ChannelMetric, bool) {
+	return db.get().GetTopChannels(id, maxResults)
 }

 // GetServers returns a slice of server's ServerMetric, along with a
 // boolean indicating whether there's more servers to be queried for.
 //
 // The arg id specifies that only server with id at or above it will be included
-// in the result. The returned slice is up to a length of EntryPerPage, and is
-// sorted in ascending id order.
-func GetServers(id int64) ([]*ServerMetric, bool) {
-	return db.get().GetServers(id)
+// in the result. The returned slice is up to a length of the arg maxResults or
+// EntryPerPage if maxResults is zero, and is sorted in ascending id order.
+func GetServers(id int64, maxResults int64) ([]*ServerMetric, bool) {
+	return db.get().GetServers(id, maxResults)
 }

 // GetServerSockets returns a slice of server's (identified by id) normal socket's
@@ -112,10 +134,10 @@ func GetServers(id int64) ([]*ServerMetric, bool) {
 // be queried for.
 //
 // The arg startID specifies that only sockets with id at or above it will be
-// included in the result. The returned slice is up to a length of EntryPerPage,
-// and is sorted in ascending id order.
-func GetServerSockets(id int64, startID int64) ([]*SocketMetric, bool) {
-	return db.get().GetServerSockets(id, startID)
+// included in the result. The returned slice is up to a length of the arg maxResults
+// or EntryPerPage if maxResults is zero, and is sorted in ascending id order.
+func GetServerSockets(id int64, startID int64, maxResults int64) ([]*SocketMetric, bool) {
+	return db.get().GetServerSockets(id, startID, maxResults)
 }

 // GetChannel returns the ChannelMetric for the channel (identified by id).
@@ -133,6 +155,11 @@ func GetSocket(id int64) *SocketMetric {
 	return db.get().GetSocket(id)
 }

+// GetServer returns the ServerMetric for the server (identified by id).
+func GetServer(id int64) *ServerMetric {
+	return db.get().GetServer(id)
+}
+
 // RegisterChannel registers the given channel c in channelz database with ref
 // as its reference name, and add it to the child list of its parent (identified
 // by pid). pid = 0 means no parent. It returns the unique channelz tracking id
@@ -146,6 +173,7 @@ func RegisterChannel(c Channel, pid int64, ref string) int64 {
 		nestedChans: make(map[int64]string),
 		id:          id,
 		pid:         pid,
+		trace:       &channelTrace{createdTime: time.Now(), events: make([]*TraceEvent, 0, getMaxTraceEntry())},
 	}
 	if pid == 0 {
 		db.get().addChannel(id, cn, true, pid, ref)
@@ -170,6 +198,7 @@ func RegisterSubChannel(c Channel, pid int64, ref string) int64 {
 		sockets: make(map[int64]string),
 		id:      id,
 		pid:     pid,
+		trace:   &channelTrace{createdTime: time.Now(), events: make([]*TraceEvent, 0, getMaxTraceEntry())},
 	}
 	db.get().addSubChannel(id, sc, pid, ref)
 	return id
@@ -226,6 +255,24 @@ func RemoveEntry(id int64) {
 	db.get().removeEntry(id)
 }

+// TraceEventDesc is what the caller of AddTraceEvent should provide to describe the event to be added
+// to the channel trace.
+// The Parent field is optional. It is used for event that will be recorded in the entity's parent
+// trace also.
+type TraceEventDesc struct {
+	Desc     string
+	Severity Severity
+	Parent   *TraceEventDesc
+}
+
+// AddTraceEvent adds trace related to the entity with specified id, using the provided TraceEventDesc.
+func AddTraceEvent(id int64, desc *TraceEventDesc) {
+	if getMaxTraceEntry() == 0 {
+		return
+	}
+	db.get().traceEvent(id, desc)
+}
+
 // channelMap is the storage data structure for channelz.
 // Methods of channelMap can be divided in two two categories with respect to locking.
 // 1. Methods acquire the global lock.
@@ -251,6 +298,7 @@ func (c *channelMap) addServer(id int64, s *server) {
 func (c *channelMap) addChannel(id int64, cn *channel, isTopChannel bool, pid int64, ref string) {
 	c.mu.Lock()
 	cn.cm = c
+	cn.trace.cm = c
 	c.channels[id] = cn
 	if isTopChannel {
 		c.topLevelChannels[id] = struct{}{}
@@ -263,6 +311,7 @@ func (c *channelMap) addChannel(id int64, cn *channel, isTopChannel bool, pid in
 func (c *channelMap) addSubChannel(id int64, sc *subChannel, pid int64, ref string) {
 	c.mu.Lock()
 	sc.cm = c
+	sc.trace.cm = c
 	c.subChannels[id] = sc
 	c.findEntry(pid).addChild(id, sc)
 	c.mu.Unlock()
@@ -284,16 +333,25 @@ func (c *channelMap) addNormalSocket(id int64, ns *normalSocket, pid int64, ref
 	c.mu.Unlock()
 }

-// removeEntry triggers the removal of an entry, which may not indeed delete the
-// entry, if it has to wait on the deletion of its children, or may lead to a chain
-// of entry deletion. For example, deleting the last socket of a gracefully shutting
-// down server will lead to the server being also deleted.
+// removeEntry triggers the removal of an entry, which may not indeed delete the entry, if it has to
+// wait on the deletion of its children and until no other entity's channel trace references it.
+// It may lead to a chain of entry deletion. For example, deleting the last socket of a gracefully
+// shutting down server will lead to the server being also deleted.
 func (c *channelMap) removeEntry(id int64) {
 	c.mu.Lock()
 	c.findEntry(id).triggerDelete()
 	c.mu.Unlock()
 }

+// c.mu must be held by the caller
+func (c *channelMap) decrTraceRefCount(id int64) {
+	e := c.findEntry(id)
+	if v, ok := e.(tracedChannel); ok {
+		v.decrTraceRefCount()
+		e.deleteSelfIfReady()
+	}
+}
+
 // c.mu must be held by the caller.
 func (c *channelMap) findEntry(id int64) entry {
 	var v entry
@@ -347,6 +405,39 @@ func (c *channelMap) deleteEntry(id int64) {
 	}
 }

+func (c *channelMap) traceEvent(id int64, desc *TraceEventDesc) {
+	c.mu.Lock()
+	child := c.findEntry(id)
+	childTC, ok := child.(tracedChannel)
+	if !ok {
+		c.mu.Unlock()
+		return
+	}
+	childTC.getChannelTrace().append(&TraceEvent{Desc: desc.Desc, Severity: desc.Severity, Timestamp: time.Now()})
+	if desc.Parent != nil {
+		parent := c.findEntry(child.getParentID())
+		var chanType RefChannelType
+		switch child.(type) {
+		case *channel:
+			chanType = RefChannel
+		case *subChannel:
+			chanType = RefSubChannel
+		}
+		if parentTC, ok := parent.(tracedChannel); ok {
+			parentTC.getChannelTrace().append(&TraceEvent{
+				Desc:      desc.Parent.Desc,
+				Severity:  desc.Parent.Severity,
+				Timestamp: time.Now(),
+				RefID:     id,
+				RefName:   childTC.getRefName(),
+				RefType:   chanType,
+			})
+			childTC.incrTraceRefCount()
+		}
+	}
+	c.mu.Unlock()
+}
+
 type int64Slice []int64

 func (s int64Slice) Len() int           { return len(s) }
@@ -361,29 +452,32 @@ func copyMap(m map[int64]string) map[int64]string {
 	return n
 }

-func min(a, b int) int {
+func min(a, b int64) int64 {
 	if a < b {
 		return a
 	}
 	return b
 }

-func (c *channelMap) GetTopChannels(id int64) ([]*ChannelMetric, bool) {
+func (c *channelMap) GetTopChannels(id int64, maxResults int64) ([]*ChannelMetric, bool) {
+	if maxResults <= 0 {
+		maxResults = EntryPerPage
+	}
 	c.mu.RLock()
-	l := len(c.topLevelChannels)
+	l := int64(len(c.topLevelChannels))
 	ids := make([]int64, 0, l)
-	cns := make([]*channel, 0, min(l, EntryPerPage))
+	cns := make([]*channel, 0, min(l, maxResults))

 	for k := range c.topLevelChannels {
 		ids = append(ids, k)
 	}
 	sort.Sort(int64Slice(ids))
 	idx := sort.Search(len(ids), func(i int) bool { return ids[i] >= id })
-	count := 0
+	count := int64(0)
 	var end bool
 	var t []*ChannelMetric
 	for i, v := range ids[idx:] {
-		if count == EntryPerPage {
+		if count == maxResults {
 			break
 		}
 		if cn, ok := c.channels[v]; ok {
@@ -408,25 +502,29 @@ func (c *channelMap) GetTopChannels(id int64) ([]*ChannelMetric, bool) {
 		t[i].ChannelData = cn.c.ChannelzMetric()
 		t[i].ID = cn.id
 		t[i].RefName = cn.refName
+		t[i].Trace = cn.trace.dumpData()
 	}
 	return t, end
 }

-func (c *channelMap) GetServers(id int64) ([]*ServerMetric, bool) {
+func (c *channelMap) GetServers(id, maxResults int64) ([]*ServerMetric, bool) {
+	if maxResults <= 0 {
+		maxResults = EntryPerPage
+	}
 	c.mu.RLock()
-	l := len(c.servers)
+	l := int64(len(c.servers))
 	ids := make([]int64, 0, l)
-	ss := make([]*server, 0, min(l, EntryPerPage))
+	ss := make([]*server, 0, min(l, maxResults))
 	for k := range c.servers {
 		ids = append(ids, k)
 	}
 	sort.Sort(int64Slice(ids))
 	idx := sort.Search(len(ids), func(i int) bool { return ids[i] >= id })
-	count := 0
+	count := int64(0)
 	var end bool
 	var s []*ServerMetric
 	for i, v := range ids[idx:] {
-		if count == EntryPerPage {
+		if count == maxResults {
 			break
 		}
 		if svr, ok := c.servers[v]; ok {
@@ -454,7 +552,10 @@ func (c *channelMap) GetServers(id int64) ([]*ServerMetric, bool) {
 	return s, end
 }

-func (c *channelMap) GetServerSockets(id int64, startID int64) ([]*SocketMetric, bool) {
+func (c *channelMap) GetServerSockets(id int64, startID int64, maxResults int64) ([]*SocketMetric, bool) {
+	if maxResults <= 0 {
+		maxResults = EntryPerPage
+	}
 	var svr *server
 	var ok bool
 	c.mu.RLock()
@@ -464,18 +565,18 @@ func (c *channelMap) GetServerSockets(id int64, startID int64) ([]*SocketMetric,
 		return nil, true
 	}
 	svrskts := svr.sockets
-	l := len(svrskts)
+	l := int64(len(svrskts))
 	ids := make([]int64, 0, l)
-	sks := make([]*normalSocket, 0, min(l, EntryPerPage))
+	sks := make([]*normalSocket, 0, min(l, maxResults))
 	for k := range svrskts {
 		ids = append(ids, k)
 	}
-	sort.Sort((int64Slice(ids)))
-	idx := sort.Search(len(ids), func(i int) bool { return ids[i] >= id })
-	count := 0
+	sort.Sort(int64Slice(ids))
+	idx := sort.Search(len(ids), func(i int) bool { return ids[i] >= startID })
+	count := int64(0)
 	var end bool
 	for i, v := range ids[idx:] {
-		if count == EntryPerPage {
+		if count == maxResults {
 			break
 		}
 		if ns, ok := c.normalSockets[v]; ok {
@@ -514,10 +615,14 @@ func (c *channelMap) GetChannel(id int64) *ChannelMetric {
 	}
 	cm.NestedChans = copyMap(cn.nestedChans)
 	cm.SubChans = copyMap(cn.subChans)
+	// cn.c can be set to &dummyChannel{} when deleteSelfFromMap is called. Save a copy of cn.c when
+	// holding the lock to prevent potential data race.
+	chanCopy := cn.c
 	c.mu.RUnlock()
-	cm.ChannelData = cn.c.ChannelzMetric()
+	cm.ChannelData = chanCopy.ChannelzMetric()
 	cm.ID = cn.id
 	cm.RefName = cn.refName
+	cm.Trace = cn.trace.dumpData()
 	return cm
 }

@@ -532,10 +637,14 @@ func (c *channelMap) GetSubChannel(id int64) *SubChannelMetric {
 		return nil
 	}
 	cm.Sockets = copyMap(sc.sockets)
+	// sc.c can be set to &dummyChannel{} when deleteSelfFromMap is called. Save a copy of sc.c when
+	// holding the lock to prevent potential data race.
+	chanCopy := sc.c
 	c.mu.RUnlock()
-	cm.ChannelData = sc.c.ChannelzMetric()
+	cm.ChannelData = chanCopy.ChannelzMetric()
 	cm.ID = sc.id
 	cm.RefName = sc.refName
+	cm.Trace = sc.trace.dumpData()
 	return cm
 }

@@ -560,6 +669,23 @@ func (c *channelMap) GetSocket(id int64) *SocketMetric {
 	return nil
 }

+func (c *channelMap) GetServer(id int64) *ServerMetric {
+	sm := &ServerMetric{}
+	var svr *server
+	var ok bool
+	c.mu.RLock()
+	if svr, ok = c.servers[id]; !ok {
+		c.mu.RUnlock()
+		return nil
+	}
+	sm.ListenSockets = copyMap(svr.listenSockets)
+	c.mu.RUnlock()
+	sm.ID = svr.id
+	sm.RefName = svr.refName
+	sm.ServerData = svr.s.ChannelzMetric()
+	return sm
+}
+
 type idGenerator struct {
 	id int64
 }
diff --git a/vendor/google.golang.org/grpc/internal/channelz/types.go b/vendor/google.golang.org/grpc/internal/channelz/types.go
index 6fd6bb388..17c2274cb 100644
--- a/vendor/google.golang.org/grpc/internal/channelz/types.go
+++ b/vendor/google.golang.org/grpc/internal/channelz/types.go
@@ -20,6 +20,8 @@ package channelz

 import (
 	"net"
+	"sync"
+	"sync/atomic"
 	"time"

 	"google.golang.org/grpc/connectivity"
@@ -40,6 +42,8 @@ type entry interface {
 	// deleteSelfIfReady check whether triggerDelete() has been called before, and whether child
 	// list is now empty. If both conditions are met, then delete self from database.
 	deleteSelfIfReady()
+	// getParentID returns parent ID of the entry. 0 value parent ID means no parent.
+	getParentID() int64
 }

 // dummyEntry is a fake entry to handle entry not found case.
@@ -73,6 +77,10 @@ func (*dummyEntry) deleteSelfIfReady() {
 	// code should not reach here. deleteSelfIfReady is always called on an existing entry.
 }

+func (*dummyEntry) getParentID() int64 {
+	return 0
+}
+
 // ChannelMetric defines the info channelz provides for a specific Channel, which
 // includes ChannelInternalMetric and channelz-specific data, such as channelz id,
 // child list, etc.
@@ -95,6 +103,8 @@ type ChannelMetric struct {
 	// Note current grpc implementation doesn't allow channel having sockets directly,
 	// therefore, this is field is unused.
 	Sockets map[int64]string
+	// Trace contains the most recent traced events.
+	Trace *ChannelTrace
 }

 // SubChannelMetric defines the info channelz provides for a specific SubChannel,
@@ -121,6 +131,8 @@ type SubChannelMetric struct {
 	// Sockets tracks the socket type children of this subchannel in the format of a map
 	// from socket channelz id to corresponding reference string.
 	Sockets map[int64]string
+	// Trace contains the most recent traced events.
+	Trace *ChannelTrace
 }

 // ChannelInternalMetric defines the struct that the implementor of Channel interface
@@ -138,7 +150,35 @@ type ChannelInternalMetric struct {
 	CallsFailed int64
 	// The last time a call was started on the channel.
 	LastCallStartedTimestamp time.Time
-	//TODO: trace
+}
+
+// ChannelTrace stores traced events on a channel/subchannel and related info.
+type ChannelTrace struct {
+	// EventNum is the number of events that ever got traced (i.e. including those that have been deleted)
+	EventNum int64
+	// CreationTime is the creation time of the trace.
+	CreationTime time.Time
+	// Events stores the most recent trace events (up to $maxTraceEntry, newer event will overwrite the
+	// oldest one)
+	Events []*TraceEvent
+}
+
+// TraceEvent represent a single trace event
+type TraceEvent struct {
+	// Desc is a simple description of the trace event.
+	Desc string
+	// Severity states the severity of this trace event.
+	Severity Severity
+	// Timestamp is the event time.
+	Timestamp time.Time
+	// RefID is the id of the entity that gets referenced in the event. RefID is 0 if no other entity is
+	// involved in this event.
+	// e.g. SubChannel (id: 4[]) Created. --> RefID = 4, RefName = "" (inside [])
+	RefID int64
+	// RefName is the reference name for the entity that gets referenced in the event.
+	RefName string
+	// RefType indicates the referenced entity type, i.e Channel or SubChannel.
+	RefType RefChannelType
 }

 // Channel is the interface that should be satisfied in order to be tracked by
@@ -147,6 +187,12 @@ type Channel interface {
 	ChannelzMetric() *ChannelInternalMetric
 }

+type dummyChannel struct{}
+
+func (d *dummyChannel) ChannelzMetric() *ChannelInternalMetric {
+	return &ChannelInternalMetric{}
+}
+
 type channel struct {
 	refName     string
 	c           Channel
@@ -156,6 +202,10 @@ type channel struct {
 	id          int64
 	pid         int64
 	cm          *channelMap
+	trace       *channelTrace
+	// traceRefCount is the number of trace events that reference this channel.
+	// Non-zero traceRefCount means the trace of this channel cannot be deleted.
+	traceRefCount int32
 }

 func (c *channel) addChild(id int64, e entry) {
@@ -180,25 +230,96 @@ func (c *channel) triggerDelete() {
 	c.deleteSelfIfReady()
 }

-func (c *channel) deleteSelfIfReady() {
+func (c *channel) getParentID() int64 {
+	return c.pid
+}
+
+// deleteSelfFromTree tries to delete the channel from the channelz entry relation tree, which means
+// deleting the channel reference from its parent's child list.
+//
+// In order for a channel to be deleted from the tree, it must meet the criteria that, removal of the
+// corresponding grpc object has been invoked, and the channel does not have any children left.
+//
+// The returned boolean value indicates whether the channel has been successfully deleted from tree.
+func (c *channel) deleteSelfFromTree() (deleted bool) {
 	if !c.closeCalled || len(c.subChans)+len(c.nestedChans) != 0 {
-		return
+		return false
 	}
-	c.cm.deleteEntry(c.id)
 	// not top channel
 	if c.pid != 0 {
 		c.cm.findEntry(c.pid).deleteChild(c.id)
 	}
+	return true
+}
+
+// deleteSelfFromMap checks whether it is valid to delete the channel from the map, which means
+// deleting the channel from channelz's tracking entirely. Users can no longer use id to query the
+// channel, and its memory will be garbage collected.
+//
+// The trace reference count of the channel must be 0 in order to be deleted from the map. This is
+// specified in the channel tracing gRFC that as long as some other trace has reference to an entity,
+// the trace of the referenced entity must not be deleted. In order to release the resource allocated
+// by grpc, the reference to the grpc object is reset to a dummy object.
+//
+// deleteSelfFromMap must be called after deleteSelfFromTree returns true.
+//
+// It returns a bool to indicate whether the channel can be safely deleted from map.
+func (c *channel) deleteSelfFromMap() (delete bool) {
+	if c.getTraceRefCount() != 0 {
+		c.c = &dummyChannel{}
+		return false
+	}
+	return true
+}
+
+// deleteSelfIfReady tries to delete the channel itself from the channelz database.
+// The delete process includes two steps:
+// 1. delete the channel from the entry relation tree, i.e. delete the channel reference from its
+//    parent's child list.
+// 2. delete the channel from the map, i.e. delete the channel entirely from channelz. Lookup by id
+//    will return entry not found error.
+func (c *channel) deleteSelfIfReady() {
+	if !c.deleteSelfFromTree() {
+		return
+	}
+	if !c.deleteSelfFromMap() {
+		return
+	}
+	c.cm.deleteEntry(c.id)
+	c.trace.clear()
+}
+
+func (c *channel) getChannelTrace() *channelTrace {
+	return c.trace
+}
+
+func (c *channel) incrTraceRefCount() {
+	atomic.AddInt32(&c.traceRefCount, 1)
+}
+
+func (c *channel) decrTraceRefCount() {
+	atomic.AddInt32(&c.traceRefCount, -1)
+}
+
+func (c *channel) getTraceRefCount() int {
+	i := atomic.LoadInt32(&c.traceRefCount)
+	return int(i)
+}
+
+func (c *channel) getRefName() string {
+	return c.refName
 }

 type subChannel struct {
-	refName     string
-	c           Channel
-	closeCalled bool
-	sockets     map[int64]string
-	id          int64
-	pid         int64
-	cm          *channelMap
+	refName       string
+	c             Channel
+	closeCalled   bool
+	sockets       map[int64]string
+	id            int64
+	pid           int64
+	cm            *channelMap
+	trace         *channelTrace
+	traceRefCount int32
 }

 func (sc *subChannel) addChild(id int64, e entry) {
@@ -219,12 +340,82 @@ func (sc *subChannel) triggerDelete() {
 	sc.deleteSelfIfReady()
 }

-func (sc *subChannel) deleteSelfIfReady() {
+func (sc *subChannel) getParentID() int64 {
+	return sc.pid
+}
+
+// deleteSelfFromTree tries to delete the subchannel from the channelz entry relation tree, which
+// means deleting the subchannel reference from its parent's child list.
+//
+// In order for a subchannel to be deleted from the tree, it must meet the criteria that, removal of
+// the corresponding grpc object has been invoked, and the subchannel does not have any children left.
+//
+// The returned boolean value indicates whether the channel has been successfully deleted from tree.
+func (sc *subChannel) deleteSelfFromTree() (deleted bool) {
 	if !sc.closeCalled || len(sc.sockets) != 0 {
+		return false
+	}
+	sc.cm.findEntry(sc.pid).deleteChild(sc.id)
+	return true
+}
+
+// deleteSelfFromMap checks whether it is valid to delete the subchannel from the map, which means
+// deleting the subchannel from channelz's tracking entirely. Users can no longer use id to query
+// the subchannel, and its memory will be garbage collected.
+//
+// The trace reference count of the subchannel must be 0 in order to be deleted from the map. This is
+// specified in the channel tracing gRFC that as long as some other trace has reference to an entity,
+// the trace of the referenced entity must not be deleted. In order to release the resource allocated
+// by grpc, the reference to the grpc object is reset to a dummy object.
+//
+// deleteSelfFromMap must be called after deleteSelfFromTree returns true.
+//
+// It returns a bool to indicate whether the channel can be safely deleted from map.
+func (sc *subChannel) deleteSelfFromMap() (delete bool) {
+	if sc.getTraceRefCount() != 0 {
+		// free the grpc struct (i.e. addrConn)
+		sc.c = &dummyChannel{}
+		return false
+	}
+	return true
+}
+
+// deleteSelfIfReady tries to delete the subchannel itself from the channelz database.
+// The delete process includes two steps:
+// 1. delete the subchannel from the entry relation tree, i.e. delete the subchannel reference from
+//    its parent's child list.
+// 2. delete the subchannel from the map, i.e. delete the subchannel entirely from channelz. Lookup
+//    by id will return entry not found error.
+func (sc *subChannel) deleteSelfIfReady() {
+	if !sc.deleteSelfFromTree() {
+		return
+	}
+	if !sc.deleteSelfFromMap() {
 		return
 	}
 	sc.cm.deleteEntry(sc.id)
-	sc.cm.findEntry(sc.pid).deleteChild(sc.id)
+	sc.trace.clear()
+}
+
+func (sc *subChannel) getChannelTrace() *channelTrace {
+	return sc.trace
+}
+
+func (sc *subChannel) incrTraceRefCount() {
+	atomic.AddInt32(&sc.traceRefCount, 1)
+}
+
+func (sc *subChannel) decrTraceRefCount() {
+	atomic.AddInt32(&sc.traceRefCount, -1)
+}
+
+func (sc *subChannel) getTraceRefCount() int {
+	i := atomic.LoadInt32(&sc.traceRefCount)
+	return int(i)
+}
+
+func (sc *subChannel) getRefName() string {
+	return sc.refName
 }

 // SocketMetric defines the info channelz provides for a specific Socket, which
@@ -318,6 +509,10 @@ func (ls *listenSocket) deleteSelfIfReady() {
 	grpclog.Errorf("cannot call deleteSelfIfReady on a listen socket")
 }

+func (ls *listenSocket) getParentID() int64 {
+	return ls.pid
+}
+
 type normalSocket struct {
 	refName string
 	s       Socket
@@ -343,6 +538,10 @@ func (ns *normalSocket) deleteSelfIfReady() {
 	grpclog.Errorf("cannot call deleteSelfIfReady on a normal socket")
 }

+func (ns *normalSocket) getParentID() int64 {
+	return ns.pid
+}
+
 // ServerMetric defines the info channelz provides for a specific Server, which
 // includes ServerInternalMetric and channelz-specific data, such as channelz id,
 // child list, etc.
@@ -370,7 +569,6 @@ type ServerInternalMetric struct {
 	CallsFailed int64
 	// The last time a call was started on the server.
 	LastCallStartedTimestamp time.Time
-	//TODO: trace
 }

 // Server is the interface to be satisfied in order to be tracked by channelz as
@@ -417,3 +615,88 @@ func (s *server) deleteSelfIfReady() {
 	}
 	s.cm.deleteEntry(s.id)
 }
+
+func (s *server) getParentID() int64 {
+	return 0
+}
+
+type tracedChannel interface {
+	getChannelTrace() *channelTrace
+	incrTraceRefCount()
+	decrTraceRefCount()
+	getRefName() string
+}
+
+type channelTrace struct {
+	cm          *channelMap
+	createdTime time.Time
+	eventCount  int64
+	mu          sync.Mutex
+	events      []*TraceEvent
+}
+
+func (c *channelTrace) append(e *TraceEvent) {
+	c.mu.Lock()
+	if len(c.events) == getMaxTraceEntry() {
+		del := c.events[0]
+		c.events = c.events[1:]
+		if del.RefID != 0 {
+			// start recursive cleanup in a goroutine to not block the call originated from grpc.
+			go func() {
+				// need to acquire c.cm.mu lock to call the unlocked attemptCleanup func.
+				c.cm.mu.Lock()
+				c.cm.decrTraceRefCount(del.RefID)
+				c.cm.mu.Unlock()
+			}()
+		}
+	}
+	e.Timestamp = time.Now()
+	c.events = append(c.events, e)
+	c.eventCount++
+	c.mu.Unlock()
+}
+
+func (c *channelTrace) clear() {
+	c.mu.Lock()
+	for _, e := range c.events {
+		if e.RefID != 0 {
+			// caller should have already held the c.cm.mu lock.
+			c.cm.decrTraceRefCount(e.RefID)
+		}
+	}
+	c.mu.Unlock()
+}
+
+// Severity is the severity level of a trace event.
+// The canonical enumeration of all valid values is here:
+// https://github.com/grpc/grpc-proto/blob/9b13d199cc0d4703c7ea26c9c330ba695866eb23/grpc/channelz/v1/channelz.proto#L126.
+type Severity int
+
+const (
+	// CtUNKNOWN indicates unknown severity of a trace event.
+	CtUNKNOWN Severity = iota
+	// CtINFO indicates info level severity of a trace event.
+	CtINFO
+	// CtWarning indicates warning level severity of a trace event.
+	CtWarning
+	// CtError indicates error level severity of a trace event.
+	CtError
+)
+
+// RefChannelType is the type of the entity being referenced in a trace event.
+type RefChannelType int
+
+const (
+	// RefChannel indicates the referenced entity is a Channel.
+	RefChannel RefChannelType = iota
+	// RefSubChannel indicates the referenced entity is a SubChannel.
+	RefSubChannel
+)
+
+func (c *channelTrace) dumpData() *ChannelTrace {
+	c.mu.Lock()
+	ct := &ChannelTrace{EventNum: c.eventCount, CreationTime: c.createdTime}
+	ct.Events = c.events[:len(c.events)]
+	c.mu.Unlock()
+	return ct
+}
diff --git a/vendor/google.golang.org/grpc/internal/channelz/types_linux.go b/vendor/google.golang.org/grpc/internal/channelz/types_linux.go
index 07215396d..692dd6181 100644
--- a/vendor/google.golang.org/grpc/internal/channelz/types_linux.go
+++ b/vendor/google.golang.org/grpc/internal/channelz/types_linux.go
@@ -1,4 +1,4 @@
-// +build !appengine,go1.7
+// +build !appengine

 /*
  *
diff --git a/vendor/google.golang.org/grpc/internal/channelz/types_nonlinux.go b/vendor/google.golang.org/grpc/internal/channelz/types_nonlinux.go
index 884910c4e..79edbefc4 100644
--- a/vendor/google.golang.org/grpc/internal/channelz/types_nonlinux.go
+++ b/vendor/google.golang.org/grpc/internal/channelz/types_nonlinux.go
@@ -1,4 +1,4 @@
-// +build !linux appengine !go1.7
+// +build !linux appengine

 /*
  *
@@ -20,11 +20,13 @@

 package channelz

-import "google.golang.org/grpc/grpclog"
+import (
+	"sync"

-func init() {
-	grpclog.Infof("Channelz: socket options are not supported on non-linux os and appengine.")
-}
+	"google.golang.org/grpc/grpclog"
+)
+
+var once sync.Once

 // SocketOptionData defines the struct to hold socket option data, and related
 // getter function to obtain info from fd.
@@ -35,4 +37,8 @@ type SocketOptionData struct {
 // Getsockopt defines the function to get socket options requested by channelz.
 // It is to be passed to syscall.RawConn.Control().
 // Windows OS doesn't support Socket Option
-func (s *SocketOptionData) Getsockopt(fd uintptr) {}
+func (s *SocketOptionData) Getsockopt(fd uintptr) {
+	once.Do(func() {
+		grpclog.Warningln("Channelz: socket options are not supported on non-linux os and appengine.")
+	})
+}
diff --git a/vendor/google.golang.org/grpc/internal/channelz/util_linux_go19.go b/vendor/google.golang.org/grpc/internal/channelz/util_linux.go
similarity index 96%
rename from vendor/google.golang.org/grpc/internal/channelz/util_linux_go19.go
rename to vendor/google.golang.org/grpc/internal/channelz/util_linux.go
index e1e9e32d7..fdf409d55 100644
--- a/vendor/google.golang.org/grpc/internal/channelz/util_linux_go19.go
+++ b/vendor/google.golang.org/grpc/internal/channelz/util_linux.go
@@ -1,4 +1,4 @@
-// +build linux,go1.9,!appengine
+// +build linux,!appengine

 /*
  *
diff --git a/vendor/google.golang.org/grpc/internal/channelz/util_nonlinux_pre_go19.go b/vendor/google.golang.org/grpc/internal/channelz/util_nonlinux.go
similarity index 95%
rename from vendor/google.golang.org/grpc/internal/channelz/util_nonlinux_pre_go19.go
rename to vendor/google.golang.org/grpc/internal/channelz/util_nonlinux.go
index 1d4da952d..8864a0811 100644
--- a/vendor/google.golang.org/grpc/internal/channelz/util_nonlinux_pre_go19.go
+++ b/vendor/google.golang.org/grpc/internal/channelz/util_nonlinux.go
@@ -1,4 +1,4 @@
-// +build !linux !go1.9 appengine
+// +build !linux appengine

 /*
  *
diff --git a/vendor/google.golang.org/grpc/internal/envconfig/envconfig.go b/vendor/google.golang.org/grpc/internal/envconfig/envconfig.go
index 3ee8740f1..62ed0f2f1 100644
--- a/vendor/google.golang.org/grpc/internal/envconfig/envconfig.go
+++ b/vendor/google.golang.org/grpc/internal/envconfig/envconfig.go
@@ -25,11 +25,47 @@ import (
 )

 const (
-	prefix   = "GRPC_GO_"
-	retryStr = prefix + "RETRY"
+	prefix              = "GRPC_GO_"
+	retryStr            = prefix + "RETRY"
+	requireHandshakeStr = prefix + "REQUIRE_HANDSHAKE"
+)
+
+// RequireHandshakeSetting describes the settings for handshaking.
+type RequireHandshakeSetting int
+
+const (
+	// RequireHandshakeHybrid (default, deprecated) indicates to not wait for
+	// handshake before considering a connection ready, but wait before
+	// considering successful.
+	RequireHandshakeHybrid RequireHandshakeSetting = iota
+	// RequireHandshakeOn (default after the 1.17 release) indicates to wait
+	// for handshake before considering a connection ready/successful.
+	RequireHandshakeOn
+	// RequireHandshakeOff indicates to not wait for handshake before
+	// considering a connection ready/successful.
+	RequireHandshakeOff
 )

 var (
 	// Retry is set if retry is explicitly enabled via "GRPC_GO_RETRY=on".
 	Retry = strings.EqualFold(os.Getenv(retryStr), "on")
+	// RequireHandshake is set based upon the GRPC_GO_REQUIRE_HANDSHAKE
+	// environment variable.
+	//
+	// Will be removed after the 1.18 release.
+	RequireHandshake RequireHandshakeSetting
 )
+
+func init() {
+	switch strings.ToLower(os.Getenv(requireHandshakeStr)) {
+	case "on":
+		fallthrough
+	default:
+		RequireHandshake = RequireHandshakeOn
+	case "off":
+		RequireHandshake = RequireHandshakeOff
+	case "hybrid":
+		// Will be removed after the 1.17 release.
+		RequireHandshake = RequireHandshakeHybrid
+	}
+}
diff --git a/vendor/google.golang.org/grpc/internal/grpcsync/event.go b/vendor/google.golang.org/grpc/internal/grpcsync/event.go
new file mode 100644
index 000000000..fbe697c37
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/grpcsync/event.go
@@ -0,0 +1,61 @@
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+// Package grpcsync implements additional synchronization primitives built upon
+// the sync package.
+package grpcsync
+
+import (
+	"sync"
+	"sync/atomic"
+)
+
+// Event represents a one-time event that may occur in the future.
+type Event struct {
+	fired int32
+	c     chan struct{}
+	o     sync.Once
+}
+
+// Fire causes e to complete.  It is safe to call multiple times, and
+// concurrently.  It returns true iff this call to Fire caused the signaling
+// channel returned by Done to close.
+func (e *Event) Fire() bool {
+	ret := false
+	e.o.Do(func() {
+		atomic.StoreInt32(&e.fired, 1)
+		close(e.c)
+		ret = true
+	})
+	return ret
+}
+
+// Done returns a channel that will be closed when Fire is called.
+func (e *Event) Done() <-chan struct{} {
+	return e.c
+}
+
+// HasFired returns true if Fire has been called.
+func (e *Event) HasFired() bool {
+	return atomic.LoadInt32(&e.fired) == 1
+}
+
+// NewEvent returns a new, ready-to-use Event.
+func NewEvent() *Event {
+	return &Event{c: make(chan struct{})}
+}
diff --git a/vendor/google.golang.org/grpc/internal/internal.go b/vendor/google.golang.org/grpc/internal/internal.go
index c35afb05e..c1d2c690c 100644
--- a/vendor/google.golang.org/grpc/internal/internal.go
+++ b/vendor/google.golang.org/grpc/internal/internal.go
@@ -20,9 +20,35 @@
 // symbols to avoid circular dependencies.
 package internal

+import (
+	"context"
+	"time"
+)
+
 var (
-	// WithContextDialer is exported by clientconn.go
-	WithContextDialer interface{} // func(context.Context, string) (net.Conn, error) grpc.DialOption
-	// WithResolverBuilder is exported by clientconn.go
+	// WithResolverBuilder is exported by dialoptions.go
 	WithResolverBuilder interface{} // func (resolver.Builder) grpc.DialOption
+	// WithHealthCheckFunc is not exported by dialoptions.go
+	WithHealthCheckFunc interface{} // func (HealthChecker) DialOption
+	// HealthCheckFunc is used to provide client-side LB channel health checking
+	HealthCheckFunc HealthChecker
+	// BalancerUnregister is exported by package balancer to unregister a balancer.
+	BalancerUnregister func(name string)
+	// KeepaliveMinPingTime is the minimum ping interval.  This must be 10s by
+	// default, but tests may wish to set it lower for convenience.
+	KeepaliveMinPingTime = 10 * time.Second
+)
+
+// HealthChecker defines the signature of the client-side LB channel health checking function.
+type HealthChecker func(ctx context.Context, newStream func() (interface{}, error), reportHealth func(bool), serviceName string) error
+
+const (
+	// CredsBundleModeFallback switches GoogleDefaultCreds to fallback mode.
+	CredsBundleModeFallback = "fallback"
+	// CredsBundleModeBalancer switches GoogleDefaultCreds to grpclb balancer
+	// mode.
+	CredsBundleModeBalancer = "balancer"
+	// CredsBundleModeBackendFromBalancer switches GoogleDefaultCreds to mode
+	// that supports backend returned by grpclb balancer.
+	CredsBundleModeBackendFromBalancer = "backend-from-balancer"
 )
diff --git a/vendor/google.golang.org/grpc/internal/syscall/syscall_linux.go b/vendor/google.golang.org/grpc/internal/syscall/syscall_linux.go
new file mode 100644
index 000000000..43281a3e0
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/syscall/syscall_linux.go
@@ -0,0 +1,114 @@
+// +build !appengine
+
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+// Package syscall provides functionalities that grpc uses to get low-level operating system
+// stats/info.
+package syscall
+
+import (
+	"fmt"
+	"net"
+	"syscall"
+	"time"
+
+	"golang.org/x/sys/unix"
+	"google.golang.org/grpc/grpclog"
+)
+
+// GetCPUTime returns the how much CPU time has passed since the start of this process.
+func GetCPUTime() int64 {
+	var ts unix.Timespec
+	if err := unix.ClockGettime(unix.CLOCK_PROCESS_CPUTIME_ID, &ts); err != nil {
+		grpclog.Fatal(err)
+	}
+	return ts.Nano()
+}
+
+// Rusage is an alias for syscall.Rusage under linux non-appengine environment.
+type Rusage syscall.Rusage
+
+// GetRusage returns the resource usage of current process.
+func GetRusage() (rusage *Rusage) {
+	rusage = new(Rusage)
+	syscall.Getrusage(syscall.RUSAGE_SELF, (*syscall.Rusage)(rusage))
+	return
+}
+
+// CPUTimeDiff returns the differences of user CPU time and system CPU time used
+// between two Rusage structs.
+func CPUTimeDiff(first *Rusage, latest *Rusage) (float64, float64) {
+	f := (*syscall.Rusage)(first)
+	l := (*syscall.Rusage)(latest)
+	var (
+		utimeDiffs  = l.Utime.Sec - f.Utime.Sec
+		utimeDiffus = l.Utime.Usec - f.Utime.Usec
+		stimeDiffs  = l.Stime.Sec - f.Stime.Sec
+		stimeDiffus = l.Stime.Usec - f.Stime.Usec
+	)
+
+	uTimeElapsed := float64(utimeDiffs) + float64(utimeDiffus)*1.0e-6
+	sTimeElapsed := float64(stimeDiffs) + float64(stimeDiffus)*1.0e-6
+
+	return uTimeElapsed, sTimeElapsed
+}
+
+// SetTCPUserTimeout sets the TCP user timeout on a connection's socket
+func SetTCPUserTimeout(conn net.Conn, timeout time.Duration) error {
+	tcpconn, ok := conn.(*net.TCPConn)
+	if !ok {
+		// not a TCP connection. exit early
+		return nil
+	}
+	rawConn, err := tcpconn.SyscallConn()
+	if err != nil {
+		return fmt.Errorf("error getting raw connection: %v", err)
+	}
+	err = rawConn.Control(func(fd uintptr) {
+		err = syscall.SetsockoptInt(int(fd), syscall.IPPROTO_TCP, unix.TCP_USER_TIMEOUT, int(timeout/time.Millisecond))
+	})
+	if err != nil {
+		return fmt.Errorf("error setting option on socket: %v", err)
+	}
+
+	return nil
+}
+
+// GetTCPUserTimeout gets the TCP user timeout on a connection's socket
+func GetTCPUserTimeout(conn net.Conn) (opt int, err error) {
+	tcpconn, ok := conn.(*net.TCPConn)
+	if !ok {
+		err = fmt.Errorf("conn is not *net.TCPConn. got %T", conn)
+		return
+	}
+	rawConn, err := tcpconn.SyscallConn()
+	if err != nil {
+		err = fmt.Errorf("error getting raw connection: %v", err)
+		return
+	}
+	err = rawConn.Control(func(fd uintptr) {
+		opt, err = syscall.GetsockoptInt(int(fd), syscall.IPPROTO_TCP, unix.TCP_USER_TIMEOUT)
+	})
+	if err != nil {
+		err = fmt.Errorf("error getting option on socket: %v", err)
+		return
+	}
+
+	return
+}
diff --git a/vendor/google.golang.org/grpc/internal/syscall/syscall_nonlinux.go b/vendor/google.golang.org/grpc/internal/syscall/syscall_nonlinux.go
new file mode 100644
index 000000000..61678feb0
--- /dev/null
+++ b/vendor/google.golang.org/grpc/internal/syscall/syscall_nonlinux.go
@@ -0,0 +1,63 @@
+// +build !linux appengine
+
+/*
+ *
+ * Copyright 2018 gRPC authors.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+package syscall
+
+import (
+	"net"
+	"time"
+
+	"google.golang.org/grpc/grpclog"
+)
+
+func init() {
+	grpclog.Info("CPU time info is unavailable on non-linux or appengine environment.")
+}
+
+// GetCPUTime returns the how much CPU time has passed since the start of this process.
+// It always returns 0 under non-linux or appengine environment.
+func GetCPUTime() int64 {
+	return 0
+}
+
+// Rusage is an empty struct under non-linux or appengine environment.
+type Rusage struct{}
+
+// GetRusage is a no-op function under non-linux or appengine environment.
+func GetRusage() (rusage *Rusage) {
+	return nil
+}
+
+// CPUTimeDiff returns the differences of user CPU time and system CPU time used
+// between two Rusage structs. It a no-op function for non-linux or appengine environment.
+func CPUTimeDiff(first *Rusage, latest *Rusage) (float64, float64) {
+	return 0, 0
+}
+
+// SetTCPUserTimeout is a no-op function under non-linux or appengine environments
+func SetTCPUserTimeout(conn net.Conn, timeout time.Duration) error {
+	return nil
+}
+
+// GetTCPUserTimeout is a no-op function under non-linux or appengine environments
+// a negative return value indicates the operation is not supported
+func GetTCPUserTimeout(conn net.Conn) (int, error) {
+	return -1, nil
+}
diff --git a/vendor/google.golang.org/grpc/internal/transport/bdp_estimator.go b/vendor/google.golang.org/grpc/internal/transport/bdp_estimator.go
index 63cd2627c..070680edb 100644
--- a/vendor/google.golang.org/grpc/internal/transport/bdp_estimator.go
+++ b/vendor/google.golang.org/grpc/internal/transport/bdp_estimator.go
@@ -24,9 +24,10 @@ import (
 )

 const (
-	// bdpLimit is the maximum value the flow control windows
-	// will be increased to.
-	bdpLimit = (1 << 20) * 4
+	// bdpLimit is the maximum value the flow control windows will be increased
+	// to.  TCP typically limits this to 4MB, but some systems go up to 16MB.
+	// Since this is only a limit, it is safe to make it optimistic.
+	bdpLimit = (1 << 20) * 16
 	// alpha is a constant factor used to keep a moving average
 	// of RTTs.
 	alpha = 0.9
diff --git a/vendor/google.golang.org/grpc/internal/transport/go16.go b/vendor/google.golang.org/grpc/internal/transport/go16.go
deleted file mode 100644
index e0d00115d..000000000
--- a/vendor/google.golang.org/grpc/internal/transport/go16.go
+++ /dev/null
@@ -1,52 +0,0 @@
-// +build go1.6,!go1.7
-
-/*
- *
- * Copyright 2016 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package transport
-
-import (
-	"net"
-	"net/http"
-
-	"google.golang.org/grpc/codes"
-	"google.golang.org/grpc/status"
-
-	"golang.org/x/net/context"
-)
-
-// dialContext connects to the address on the named network.
-func dialContext(ctx context.Context, network, address string) (net.Conn, error) {
-	return (&net.Dialer{Cancel: ctx.Done()}).Dial(network, address)
-}
-
-// ContextErr converts the error from context package into a status error.
-func ContextErr(err error) error {
-	switch err {
-	case context.DeadlineExceeded:
-		return status.Error(codes.DeadlineExceeded, err.Error())
-	case context.Canceled:
-		return status.Error(codes.Canceled, err.Error())
-	}
-	return status.Errorf(codes.Internal, "Unexpected error from context packet: %v", err)
-}
-
-// contextFromRequest returns a background context.
-func contextFromRequest(r *http.Request) context.Context {
-	return context.Background()
-}
diff --git a/vendor/google.golang.org/grpc/internal/transport/go17.go b/vendor/google.golang.org/grpc/internal/transport/go17.go
deleted file mode 100644
index 4d515b00d..000000000
--- a/vendor/google.golang.org/grpc/internal/transport/go17.go
+++ /dev/null
@@ -1,53 +0,0 @@
-// +build go1.7
-
-/*
- *
- * Copyright 2016 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package transport
-
-import (
-	"context"
-	"net"
-	"net/http"
-
-	"google.golang.org/grpc/codes"
-	"google.golang.org/grpc/status"
-
-	netctx "golang.org/x/net/context"
-)
-
-// dialContext connects to the address on the named network.
-func dialContext(ctx context.Context, network, address string) (net.Conn, error) {
-	return (&net.Dialer{}).DialContext(ctx, network, address)
-}
-
-// ContextErr converts the error from context package into a status error.
-func ContextErr(err error) error {
-	switch err {
-	case context.DeadlineExceeded, netctx.DeadlineExceeded:
-		return status.Error(codes.DeadlineExceeded, err.Error())
-	case context.Canceled, netctx.Canceled:
-		return status.Error(codes.Canceled, err.Error())
-	}
-	return status.Errorf(codes.Internal, "Unexpected error from context packet: %v", err)
-}
-
-// contextFromRequest returns a context from the HTTP Request.
-func contextFromRequest(r *http.Request) context.Context {
-	return r.Context()
-}
diff --git a/vendor/google.golang.org/grpc/internal/transport/handler_server.go b/vendor/google.golang.org/grpc/internal/transport/handler_server.go
index c6fb4b9c1..73b41ea7e 100644
--- a/vendor/google.golang.org/grpc/internal/transport/handler_server.go
+++ b/vendor/google.golang.org/grpc/internal/transport/handler_server.go
@@ -24,6 +24,7 @@
 package transport

 import (
+	"context"
 	"errors"
 	"fmt"
 	"io"
@@ -34,7 +35,6 @@ import (
 	"time"

 	"github.com/golang/protobuf/proto"
-	"golang.org/x/net/context"
 	"golang.org/x/net/http2"
 	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/credentials"
@@ -307,7 +307,7 @@ func (ht *serverHandlerTransport) WriteHeader(s *Stream, md metadata.MD) error {
 func (ht *serverHandlerTransport) HandleStreams(startStream func(*Stream), traceCtx func(context.Context, string) context.Context) {
 	// With this transport type there will be exactly 1 stream: this HTTP request.

-	ctx := contextFromRequest(ht.req)
+	ctx := ht.req.Context()
 	var cancel context.CancelFunc
 	if ht.timeoutSet {
 		ctx, cancel = context.WithTimeout(ctx, ht.timeout)
diff --git a/vendor/google.golang.org/grpc/internal/transport/http2_client.go b/vendor/google.golang.org/grpc/internal/transport/http2_client.go
index 904e790c4..ff8f4db08 100644
--- a/vendor/google.golang.org/grpc/internal/transport/http2_client.go
+++ b/vendor/google.golang.org/grpc/internal/transport/http2_client.go
@@ -19,6 +19,8 @@
 package transport

 import (
+	"context"
+	"fmt"
 	"io"
 	"math"
 	"net"
@@ -28,13 +30,13 @@ import (
 	"sync/atomic"
 	"time"

-	"golang.org/x/net/context"
 	"golang.org/x/net/http2"
 	"golang.org/x/net/http2/hpack"

 	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/credentials"
 	"google.golang.org/grpc/internal/channelz"
+	"google.golang.org/grpc/internal/syscall"
 	"google.golang.org/grpc/keepalive"
 	"google.golang.org/grpc/metadata"
 	"google.golang.org/grpc/peer"
@@ -73,7 +75,7 @@ type http2Client struct {

 	isSecure bool

-	creds []credentials.PerRPCCredentials
+	perRPCCreds []credentials.PerRPCCredentials

 	// Boolean to keep track of reading activity on transport.
 	// 1 is true and 0 is false.
@@ -89,10 +91,10 @@ type http2Client struct {
 	maxSendHeaderListSize *uint32

 	bdpEst *bdpEstimator
-	// onSuccess is a callback that client transport calls upon
+	// onPrefaceReceipt is a callback that client transport calls upon
 	// receiving server preface to signal that a succefull HTTP2
 	// connection was established.
-	onSuccess func()
+	onPrefaceReceipt func()

 	maxConcurrentStreams  uint32
 	streamQuota           int64
@@ -112,13 +114,16 @@ type http2Client struct {
 	// Fields below are for channelz metric collection.
 	channelzID int64 // channelz unique identification number
 	czData     *channelzData
+
+	onGoAway func(GoAwayReason)
+	onClose  func()
 }

 func dial(ctx context.Context, fn func(context.Context, string) (net.Conn, error), addr string) (net.Conn, error) {
 	if fn != nil {
 		return fn(ctx, addr)
 	}
-	return dialContext(ctx, "tcp", addr)
+	return (&net.Dialer{}).DialContext(ctx, "tcp", addr)
 }

 func isTemporary(err error) bool {
@@ -140,7 +145,7 @@ func isTemporary(err error) bool {
 // newHTTP2Client constructs a connected ClientTransport to addr based on HTTP2
 // and starts to receive messages on it. Non-nil error returns if construction
 // fails.
-func newHTTP2Client(connectCtx, ctx context.Context, addr TargetInfo, opts ConnectOptions, onSuccess func()) (_ *http2Client, err error) {
+func newHTTP2Client(connectCtx, ctx context.Context, addr TargetInfo, opts ConnectOptions, onPrefaceReceipt func(), onGoAway func(GoAwayReason), onClose func()) (_ *http2Client, err error) {
 	scheme := "http"
 	ctx, cancel := context.WithCancel(ctx)
 	defer func() {
@@ -162,26 +167,44 @@ func newHTTP2Client(connectCtx, ctx context.Context, addr TargetInfo, opts Conne
 			conn.Close()
 		}
 	}(conn)
+	kp := opts.KeepaliveParams
+	// Validate keepalive parameters.
+	if kp.Time == 0 {
+		kp.Time = defaultClientKeepaliveTime
+	}
+	if kp.Timeout == 0 {
+		kp.Timeout = defaultClientKeepaliveTimeout
+	}
+	keepaliveEnabled := false
+	if kp.Time != infinity {
+		if err = syscall.SetTCPUserTimeout(conn, kp.Timeout); err != nil {
+			return nil, connectionErrorf(false, err, "transport: failed to set TCP_USER_TIMEOUT: %v", err)
+		}
+		keepaliveEnabled = true
+	}
 	var (
 		isSecure bool
 		authInfo credentials.AuthInfo
 	)
-	if creds := opts.TransportCredentials; creds != nil {
+	transportCreds := opts.TransportCredentials
+	perRPCCreds := opts.PerRPCCredentials
+
+	if b := opts.CredsBundle; b != nil {
+		if t := b.TransportCredentials(); t != nil {
+			transportCreds = t
+		}
+		if t := b.PerRPCCredentials(); t != nil {
+			perRPCCreds = append(perRPCCreds, t)
+		}
+	}
+	if transportCreds != nil {
 		scheme = "https"
-		conn, authInfo, err = creds.ClientHandshake(connectCtx, addr.Authority, conn)
+		conn, authInfo, err = transportCreds.ClientHandshake(connectCtx, addr.Authority, conn)
 		if err != nil {
 			return nil, connectionErrorf(isTemporary(err), err, "transport: authentication handshake failed: %v", err)
 		}
 		isSecure = true
 	}
-	kp := opts.KeepaliveParams
-	// Validate keepalive parameters.
-	if kp.Time == 0 {
-		kp.Time = defaultClientKeepaliveTime
-	}
-	if kp.Timeout == 0 {
-		kp.Timeout = defaultClientKeepaliveTimeout
-	}
 	dynamicWindow := true
 	icwz := int32(initialWindowSize)
 	if opts.InitialConnWindowSize >= defaultWindowSize {
@@ -213,16 +236,19 @@ func newHTTP2Client(connectCtx, ctx context.Context, addr TargetInfo, opts Conne
 		scheme:                scheme,
 		activeStreams:         make(map[uint32]*Stream),
 		isSecure:              isSecure,
-		creds:                 opts.PerRPCCredentials,
+		perRPCCreds:           perRPCCreds,
 		kp:                    kp,
 		statsHandler:          opts.StatsHandler,
 		initialWindowSize:     initialWindowSize,
-		onSuccess:             onSuccess,
+		onPrefaceReceipt:      onPrefaceReceipt,
 		nextID:                1,
 		maxConcurrentStreams:  defaultMaxStreamsClient,
 		streamQuota:           defaultMaxStreamsClient,
 		streamsQuotaAvailable: make(chan struct{}, 1),
 		czData:                new(channelzData),
+		onGoAway:              onGoAway,
+		onClose:               onClose,
+		keepaliveEnabled:      keepaliveEnabled,
 	}
 	t.controlBuf = newControlBuffer(t.ctxDone)
 	if opts.InitialWindowSize >= defaultWindowSize {
@@ -249,16 +275,16 @@ func newHTTP2Client(connectCtx, ctx context.Context, addr TargetInfo, opts Conne
 		t.statsHandler.HandleConn(t.ctx, connBegin)
 	}
 	if channelz.IsOn() {
-		t.channelzID = channelz.RegisterNormalSocket(t, opts.ChannelzParentID, "")
+		t.channelzID = channelz.RegisterNormalSocket(t, opts.ChannelzParentID, fmt.Sprintf("%s -> %s", t.localAddr, t.remoteAddr))
 	}
-	if t.kp.Time != infinity {
-		t.keepaliveEnabled = true
+	if t.keepaliveEnabled {
 		go t.keepalive()
 	}
 	// Start the reader goroutine for incoming message. Each transport has
 	// a dedicated goroutine which reads HTTP2 frame from network. Then it
 	// dispatches the frame to the corresponding stream entity.
 	go t.reader()
+
 	// Send connection preface to server.
 	n, err := t.conn.Write(clientPreface)
 	if err != nil {
@@ -295,7 +321,10 @@ func newHTTP2Client(connectCtx, ctx context.Context, addr TargetInfo, opts Conne
 			return nil, connectionErrorf(true, err, "transport: failed to write window update: %v", err)
 		}
 	}
-	t.framer.writer.Flush()
+
+	if err := t.framer.writer.Flush(); err != nil {
+		return nil, err
+	}
 	go func() {
 		t.loopy = newLoopyWriter(clientSide, t.framer, t.controlBuf, t.bdpEst)
 		err := t.loopy.run()
@@ -335,6 +364,9 @@ func (t *http2Client) newStream(ctx context.Context, callHdr *CallHdr) *Stream {
 			ctx:     s.ctx,
 			ctxDone: s.ctx.Done(),
 			recv:    s.buf,
+			closeStream: func(err error) {
+				t.CloseStream(s, err)
+			},
 		},
 		windowHandler: func(n int) {
 			t.updateWindow(s, uint32(n))
@@ -387,7 +419,7 @@ func (t *http2Client) createHeaderFields(ctx context.Context, callHdr *CallHdr)
 	if dl, ok := ctx.Deadline(); ok {
 		// Send out timeout regardless its value. The server can detect timeout context by itself.
 		// TODO(mmukhi): Perhaps this field should be updated when actually writing out to the wire.
-		timeout := dl.Sub(time.Now())
+		timeout := time.Until(dl)
 		headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-timeout", Value: encodeTimeout(timeout)})
 	}
 	for k, v := range authData {
@@ -443,7 +475,7 @@ func (t *http2Client) createHeaderFields(ctx context.Context, callHdr *CallHdr)

 func (t *http2Client) createAudience(callHdr *CallHdr) string {
 	// Create an audience string only if needed.
-	if len(t.creds) == 0 && callHdr.Creds == nil {
+	if len(t.perRPCCreds) == 0 && callHdr.Creds == nil {
 		return ""
 	}
 	// Construct URI required to get auth request metadata.
@@ -458,7 +490,7 @@ func (t *http2Client) createAudience(callHdr *CallHdr) string {

 func (t *http2Client) getTrAuthData(ctx context.Context, audience string) (map[string]string, error) {
 	authData := map[string]string{}
-	for _, c := range t.creds {
+	for _, c := range t.perRPCCreds {
 		data, err := c.GetRequestMetadata(ctx, audience)
 		if err != nil {
 			if _, ok := status.FromError(err); ok {
@@ -664,7 +696,9 @@ func (t *http2Client) CloseStream(s *Stream, err error) {
 func (t *http2Client) closeStream(s *Stream, err error, rst bool, rstCode http2.ErrCode, st *status.Status, mdata map[string][]string, eosReceived bool) {
 	// Set stream status to done.
 	if s.swapState(streamDone) == streamDone {
-		// If it was already done, return.
+		// If it was already done, return.  If multiple closeStream calls
+		// happen simultaneously, wait for the first to finish.
+		<-s.done
 		return
 	}
 	// status and trailers can be updated here without any synchronization because the stream goroutine will
@@ -678,8 +712,6 @@ func (t *http2Client) closeStream(s *Stream, err error, rst bool, rstCode http2.
 		// This will unblock reads eventually.
 		s.write(recvMsg{err: err})
 	}
-	// This will unblock write.
-	close(s.done)
 	// If headerChan isn't closed, then close it.
 	if atomic.SwapUint32(&s.headerDone, 1) == 0 {
 		s.noHeaders = true
@@ -715,11 +747,17 @@ func (t *http2Client) closeStream(s *Stream, err error, rst bool, rstCode http2.
 		return true
 	}
 	t.controlBuf.executeAndPut(addBackStreamQuota, cleanup)
+	// This will unblock write.
+	close(s.done)
 }

 // Close kicks off the shutdown process of the transport. This should be called
 // only once on a transport. Once it is called, the transport should not be
 // accessed any more.
+//
+// This method blocks until the addrConn that initiated this transport is
+// re-connected. This happens because t.onClose() begins reconnect logic at the
+// addrConn level and blocks until the addrConn is successfully connected.
 func (t *http2Client) Close() error {
 	t.mu.Lock()
 	// Make sure we only Close once.
@@ -747,6 +785,7 @@ func (t *http2Client) Close() error {
 		}
 		t.statsHandler.HandleConn(t.ctx, connEnd)
 	}
+	t.onClose()
 	return err
 }

@@ -1043,6 +1082,9 @@ func (t *http2Client) handleGoAway(f *http2.GoAwayFrame) {
 		close(t.goAway)
 		t.state = draining
 		t.controlBuf.put(&incomingGoAway{})
+
+		// This has to be a new goroutine because we're still using the current goroutine to read in the transport.
+		t.onGoAway(t.goAwayReason)
 	}
 	// All streams with IDs greater than the GoAwayId
 	// and smaller than the previous GoAway ID should be killed.
@@ -1145,7 +1187,9 @@ func (t *http2Client) operateHeaders(frame *http2.MetaHeadersFrame) {
 	if !endStream {
 		return
 	}
-	t.closeStream(s, io.EOF, false, http2.ErrCodeNo, state.status(), state.mdata, true)
+	// if client received END_STREAM from server while stream was still active, send RST_STREAM
+	rst := s.getState() == streamActive
+	t.closeStream(s, io.EOF, rst, http2.ErrCodeNo, state.status(), state.mdata, true)
 }

 // reader runs as a separate goroutine in charge of reading data from network
@@ -1159,18 +1203,19 @@ func (t *http2Client) reader() {
 	// Check the validity of server preface.
 	frame, err := t.framer.fr.ReadFrame()
 	if err != nil {
-		t.Close()
+		t.Close() // this kicks off resetTransport, so must be last before return
 		return
 	}
+	t.conn.SetReadDeadline(time.Time{}) // reset deadline once we get the settings frame (we didn't time out, yay!)
 	if t.keepaliveEnabled {
 		atomic.CompareAndSwapUint32(&t.activity, 0, 1)
 	}
 	sf, ok := frame.(*http2.SettingsFrame)
 	if !ok {
-		t.Close()
+		t.Close() // this kicks off resetTransport, so must be last before return
 		return
 	}
-	t.onSuccess()
+	t.onPrefaceReceipt()
 	t.handleSettings(sf, true)

 	// loop to keep reading incoming messages on this transport.
diff --git a/vendor/google.golang.org/grpc/internal/transport/http2_server.go b/vendor/google.golang.org/grpc/internal/transport/http2_server.go
index efb7f53ff..d038b2dfe 100644
--- a/vendor/google.golang.org/grpc/internal/transport/http2_server.go
+++ b/vendor/google.golang.org/grpc/internal/transport/http2_server.go
@@ -20,6 +20,7 @@ package transport

 import (
 	"bytes"
+	"context"
 	"errors"
 	"fmt"
 	"io"
@@ -31,7 +32,6 @@ import (
 	"time"

 	"github.com/golang/protobuf/proto"
-	"golang.org/x/net/context"
 	"golang.org/x/net/http2"
 	"golang.org/x/net/http2/hpack"

@@ -237,7 +237,7 @@ func newHTTP2Server(conn net.Conn, config *ServerConfig) (_ ServerTransport, err
 		t.stats.HandleConn(t.ctx, connBegin)
 	}
 	if channelz.IsOn() {
-		t.channelzID = channelz.RegisterNormalSocket(t, config.ChannelzParentID, "")
+		t.channelzID = channelz.RegisterNormalSocket(t, config.ChannelzParentID, fmt.Sprintf("%s -> %s", t.remoteAddr, t.localAddr))
 	}
 	t.framer.writer.Flush()

@@ -1004,45 +1004,74 @@ func (t *http2Server) Close() error {
 	return err
 }

+// deleteStream deletes the stream s from transport's active streams.
+func (t *http2Server) deleteStream(s *Stream, eosReceived bool) {
+	t.mu.Lock()
+	if _, ok := t.activeStreams[s.id]; !ok {
+		t.mu.Unlock()
+		return
+	}
+
+	delete(t.activeStreams, s.id)
+	if len(t.activeStreams) == 0 {
+		t.idle = time.Now()
+	}
+	t.mu.Unlock()
+
+	if channelz.IsOn() {
+		if eosReceived {
+			atomic.AddInt64(&t.czData.streamsSucceeded, 1)
+		} else {
+			atomic.AddInt64(&t.czData.streamsFailed, 1)
+		}
+	}
+}
+
 // closeStream clears the footprint of a stream when the stream is not needed
 // any more.
 func (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {
-	if s.swapState(streamDone) == streamDone {
-		// If the stream was already done, return.
-		return
-	}
+	// Mark the stream as done
+	oldState := s.swapState(streamDone)
+
 	// In case stream sending and receiving are invoked in separate
 	// goroutines (e.g., bi-directional streaming), cancel needs to be
 	// called to interrupt the potential blocking on other goroutines.
 	s.cancel()
+
+	// Deletes the stream from active streams
+	t.deleteStream(s, eosReceived)
+
 	cleanup := &cleanupStream{
 		streamID: s.id,
 		rst:      rst,
 		rstCode:  rstCode,
-		onWrite: func() {
-			t.mu.Lock()
-			if t.activeStreams != nil {
-				delete(t.activeStreams, s.id)
-				if len(t.activeStreams) == 0 {
-					t.idle = time.Now()
-				}
-			}
-			t.mu.Unlock()
-			if channelz.IsOn() {
-				if eosReceived {
-					atomic.AddInt64(&t.czData.streamsSucceeded, 1)
-				} else {
-					atomic.AddInt64(&t.czData.streamsFailed, 1)
-				}
-			}
-		},
+		onWrite:  func() {},
 	}
-	if hdr != nil {
-		hdr.cleanup = cleanup
-		t.controlBuf.put(hdr)
-	} else {
+
+	// No trailer. Puts cleanupFrame into transport's control buffer.
+	if hdr == nil {
 		t.controlBuf.put(cleanup)
+		return
 	}
+
+	// We do the check here, because of the following scenario:
+	// 1. closeStream is called first with a trailer. A trailer item with a piggybacked cleanup item
+	// is put to control buffer.
+	// 2. Loopy writer is waiting on a stream quota. It will never get it because client errored at
+	// some point. So loopy can't act on trailer
+	// 3. Client sends a RST_STREAM due to the error. Then closeStream is called without a trailer as
+	// the result of the received RST_STREAM.
+	// If we do this check at the beginning of the closeStream, then we won't put a cleanup item in
+	// response to received RST_STREAM into the control buffer and outStream in loopy writer will
+	// never get cleaned up.
+
+	// If the stream is already done, don't send the trailer.
+	if oldState == streamDone {
+		return
+	}
+
+	hdr.cleanup = cleanup
+	t.controlBuf.put(hdr)
 }

 func (t *http2Server) RemoteAddr() net.Addr {
@@ -1155,7 +1184,7 @@ func (t *http2Server) IncrMsgRecv() {
 }

 func (t *http2Server) getOutFlowWindow() int64 {
-	resp := make(chan uint32)
+	resp := make(chan uint32, 1)
 	timer := time.NewTimer(time.Second)
 	defer timer.Stop()
 	t.controlBuf.put(&outFlowControlSizeRequest{resp})
diff --git a/vendor/google.golang.org/grpc/internal/transport/http_util.go b/vendor/google.golang.org/grpc/internal/transport/http_util.go
index 21da6e80b..77a2cfaae 100644
--- a/vendor/google.golang.org/grpc/internal/transport/http_util.go
+++ b/vendor/google.golang.org/grpc/internal/transport/http_util.go
@@ -24,6 +24,7 @@ import (
 	"encoding/base64"
 	"fmt"
 	"io"
+	"math"
 	"net"
 	"net/http"
 	"strconv"
@@ -435,6 +436,10 @@ func decodeTimeout(s string) (time.Duration, error) {
 	if size < 2 {
 		return 0, fmt.Errorf("transport: timeout string is too short: %q", s)
 	}
+	if size > 9 {
+		// Spec allows for 8 digits plus the unit.
+		return 0, fmt.Errorf("transport: timeout string is too long: %q", s)
+	}
 	unit := timeoutUnit(s[size-1])
 	d, ok := timeoutUnitToDuration(unit)
 	if !ok {
@@ -444,6 +449,11 @@ func decodeTimeout(s string) (time.Duration, error) {
 	if err != nil {
 		return 0, err
 	}
+	const maxHours = math.MaxInt64 / int64(time.Hour)
+	if d == time.Hour && t > maxHours {
+		// This timeout would overflow math.MaxInt64; clamp it.
+		return time.Duration(math.MaxInt64), nil
+	}
 	return d * time.Duration(t), nil
 }

diff --git a/vendor/google.golang.org/grpc/internal/transport/transport.go b/vendor/google.golang.org/grpc/internal/transport/transport.go
index fdf8ad684..2580aa7d3 100644
--- a/vendor/google.golang.org/grpc/internal/transport/transport.go
+++ b/vendor/google.golang.org/grpc/internal/transport/transport.go
@@ -22,6 +22,7 @@
 package transport

 import (
+	"context"
 	"errors"
 	"fmt"
 	"io"
@@ -29,7 +30,6 @@ import (
 	"sync"
 	"sync/atomic"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/credentials"
 	"google.golang.org/grpc/keepalive"
@@ -110,15 +110,15 @@ func (b *recvBuffer) get() <-chan recvMsg {
 	return b.c
 }

-//
 // recvBufferReader implements io.Reader interface to read the data from
 // recvBuffer.
 type recvBufferReader struct {
-	ctx     context.Context
-	ctxDone <-chan struct{} // cache of ctx.Done() (for performance).
-	recv    *recvBuffer
-	last    []byte // Stores the remaining data in the previous calls.
-	err     error
+	closeStream func(error) // Closes the client transport stream with the given error and nil trailer metadata.
+	ctx         context.Context
+	ctxDone     <-chan struct{} // cache of ctx.Done() (for performance).
+	recv        *recvBuffer
+	last        []byte // Stores the remaining data in the previous calls.
+	err         error
 }

 // Read reads the next len(p) bytes from last. If last is drained, it tries to
@@ -128,31 +128,53 @@ func (r *recvBufferReader) Read(p []byte) (n int, err error) {
 	if r.err != nil {
 		return 0, r.err
 	}
-	n, r.err = r.read(p)
-	return n, r.err
-}
-
-func (r *recvBufferReader) read(p []byte) (n int, err error) {
 	if r.last != nil && len(r.last) > 0 {
 		// Read remaining data left in last call.
 		copied := copy(p, r.last)
 		r.last = r.last[copied:]
 		return copied, nil
 	}
+	if r.closeStream != nil {
+		n, r.err = r.readClient(p)
+	} else {
+		n, r.err = r.read(p)
+	}
+	return n, r.err
+}
+
+func (r *recvBufferReader) read(p []byte) (n int, err error) {
 	select {
 	case <-r.ctxDone:
 		return 0, ContextErr(r.ctx.Err())
 	case m := <-r.recv.get():
-		r.recv.load()
-		if m.err != nil {
-			return 0, m.err
-		}
-		copied := copy(p, m.data)
-		r.last = m.data[copied:]
-		return copied, nil
+		return r.readAdditional(m, p)
 	}
 }

+func (r *recvBufferReader) readClient(p []byte) (n int, err error) {
+	// If the context is canceled, then closes the stream with nil metadata.
+	// closeStream writes its error parameter to r.recv as a recvMsg.
+	// r.readAdditional acts on that message and returns the necessary error.
+	select {
+	case <-r.ctxDone:
+		r.closeStream(ContextErr(r.ctx.Err()))
+		m := <-r.recv.get()
+		return r.readAdditional(m, p)
+	case m := <-r.recv.get():
+		return r.readAdditional(m, p)
+	}
+}
+
+func (r *recvBufferReader) readAdditional(m recvMsg, p []byte) (n int, err error) {
+	r.recv.load()
+	if m.err != nil {
+		return 0, m.err
+	}
+	copied := copy(p, m.data)
+	r.last = m.data[copied:]
+	return copied, nil
+}
+
 type streamState uint32

 const (
@@ -186,8 +208,12 @@ type Stream struct {
 	headerDone uint32        // set when headerChan is closed. Used to avoid closing headerChan multiple times.

 	// hdrMu protects header and trailer metadata on the server-side.
-	hdrMu   sync.Mutex
-	header  metadata.MD // the received header metadata.
+	hdrMu sync.Mutex
+	// On client side, header keeps the received header metadata.
+	//
+	// On server side, header keeps the header set by SetHeader(). The complete
+	// header will merged into this after t.WriteHeader() is called.
+	header  metadata.MD
 	trailer metadata.MD // the key-value map of trailer metadata.

 	noHeaders bool // set if the client never received headers (set only after the stream is done).
@@ -266,10 +292,19 @@ func (s *Stream) Done() <-chan struct{} {
 	return s.done
 }

-// Header acquires the key-value pairs of header metadata once it
-// is available. It blocks until i) the metadata is ready or ii) there is no
-// header metadata or iii) the stream is canceled/expired.
+// Header returns the header metadata of the stream.
+//
+// On client side, it acquires the key-value pairs of header metadata once it is
+// available. It blocks until i) the metadata is ready or ii) there is no header
+// metadata or iii) the stream is canceled/expired.
+//
+// On server side, it returns the out header after t.WriteHeader is called.
 func (s *Stream) Header() (metadata.MD, error) {
+	if s.headerChan == nil && s.header != nil {
+		// On server side, return the header in stream. It will be the out
+		// header after t.WriteHeader is called.
+		return s.header.Copy(), nil
+	}
 	err := s.waitOnHeader()
 	// Even if the stream is closed, header is returned if available.
 	select {
@@ -465,8 +500,12 @@ type ConnectOptions struct {
 	FailOnNonTempDialError bool
 	// PerRPCCredentials stores the PerRPCCredentials required to issue RPCs.
 	PerRPCCredentials []credentials.PerRPCCredentials
-	// TransportCredentials stores the Authenticator required to setup a client connection.
+	// TransportCredentials stores the Authenticator required to setup a client
+	// connection. Only one of TransportCredentials and CredsBundle is non-nil.
 	TransportCredentials credentials.TransportCredentials
+	// CredsBundle is the credentials bundle to be used. Only one of
+	// TransportCredentials and CredsBundle is non-nil.
+	CredsBundle credentials.Bundle
 	// KeepaliveParams stores the keepalive parameters.
 	KeepaliveParams keepalive.ClientParameters
 	// StatsHandler stores the handler for stats.
@@ -494,8 +533,8 @@ type TargetInfo struct {

 // NewClientTransport establishes the transport with the required ConnectOptions
 // and returns it to the caller.
-func NewClientTransport(connectCtx, ctx context.Context, target TargetInfo, opts ConnectOptions, onSuccess func()) (ClientTransport, error) {
-	return newHTTP2Client(connectCtx, ctx, target, opts, onSuccess)
+func NewClientTransport(connectCtx, ctx context.Context, target TargetInfo, opts ConnectOptions, onPrefaceReceipt func(), onGoAway func(GoAwayReason), onClose func()) (ClientTransport, error) {
+	return newHTTP2Client(connectCtx, ctx, target, opts, onPrefaceReceipt, onGoAway, onClose)
 }

 // Options provides additional hints and information for message
@@ -706,3 +745,14 @@ type channelzData struct {
 	lastMsgSentTime       int64
 	lastMsgRecvTime       int64
 }
+
+// ContextErr converts the error from context package into a status error.
+func ContextErr(err error) error {
+	switch err {
+	case context.DeadlineExceeded:
+		return status.Error(codes.DeadlineExceeded, err.Error())
+	case context.Canceled:
+		return status.Error(codes.Canceled, err.Error())
+	}
+	return status.Errorf(codes.Internal, "Unexpected error from context packet: %v", err)
+}
diff --git a/vendor/google.golang.org/grpc/keepalive/keepalive.go b/vendor/google.golang.org/grpc/keepalive/keepalive.go
index f8adc7e6d..34d31b5e7 100644
--- a/vendor/google.golang.org/grpc/keepalive/keepalive.go
+++ b/vendor/google.golang.org/grpc/keepalive/keepalive.go
@@ -16,7 +16,8 @@
  *
  */

-// Package keepalive defines configurable parameters for point-to-point healthcheck.
+// Package keepalive defines configurable parameters for point-to-point
+// healthcheck.
 package keepalive

 import (
@@ -24,42 +25,61 @@ import (
 )

 // ClientParameters is used to set keepalive parameters on the client-side.
-// These configure how the client will actively probe to notice when a connection is broken
-// and send pings so intermediaries will be aware of the liveness of the connection.
-// Make sure these parameters are set in coordination with the keepalive policy on the server,
-// as incompatible settings can result in closing of connection.
+// These configure how the client will actively probe to notice when a
+// connection is broken and send pings so intermediaries will be aware of the
+// liveness of the connection. Make sure these parameters are set in
+// coordination with the keepalive policy on the server, as incompatible
+// settings can result in closing of connection.
 type ClientParameters struct {
-	// After a duration of this time if the client doesn't see any activity it pings the server to see if the transport is still alive.
+	// After a duration of this time if the client doesn't see any activity it
+	// pings the server to see if the transport is still alive.
+	// If set below 10s, a minimum value of 10s will be used instead.
 	Time time.Duration // The current default value is infinity.
-	// After having pinged for keepalive check, the client waits for a duration of Timeout and if no activity is seen even after that
-	// the connection is closed.
+	// After having pinged for keepalive check, the client waits for a duration
+	// of Timeout and if no activity is seen even after that the connection is
+	// closed.
 	Timeout time.Duration // The current default value is 20 seconds.
-	// If true, client runs keepalive checks even with no active RPCs.
+	// If true, client sends keepalive pings even with no active RPCs. If false,
+	// when there are no active RPCs, Time and Timeout will be ignored and no
+	// keepalive pings will be sent.
 	PermitWithoutStream bool // false by default.
 }

-// ServerParameters is used to set keepalive and max-age parameters on the server-side.
+// ServerParameters is used to set keepalive and max-age parameters on the
+// server-side.
 type ServerParameters struct {
-	// MaxConnectionIdle is a duration for the amount of time after which an idle connection would be closed by sending a GoAway.
-	// Idleness duration is defined since the most recent time the number of outstanding RPCs became zero or the connection establishment.
+	// MaxConnectionIdle is a duration for the amount of time after which an
+	// idle connection would be closed by sending a GoAway. Idleness duration is
+	// defined since the most recent time the number of outstanding RPCs became
+	// zero or the connection establishment.
 	MaxConnectionIdle time.Duration // The current default value is infinity.
-	// MaxConnectionAge is a duration for the maximum amount of time a connection may exist before it will be closed by sending a GoAway.
-	// A random jitter of +/-10% will be added to MaxConnectionAge to spread out connection storms.
+	// MaxConnectionAge is a duration for the maximum amount of time a
+	// connection may exist before it will be closed by sending a GoAway. A
+	// random jitter of +/-10% will be added to MaxConnectionAge to spread out
+	// connection storms.
 	MaxConnectionAge time.Duration // The current default value is infinity.
-	// MaxConnectinoAgeGrace is an additive period after MaxConnectionAge after which the connection will be forcibly closed.
+	// MaxConnectionAgeGrace is an additive period after MaxConnectionAge after
+	// which the connection will be forcibly closed.
 	MaxConnectionAgeGrace time.Duration // The current default value is infinity.
-	// After a duration of this time if the server doesn't see any activity it pings the client to see if the transport is still alive.
+	// After a duration of this time if the server doesn't see any activity it
+	// pings the client to see if the transport is still alive.
+	// If set below 1s, a minimum value of 1s will be used instead.
 	Time time.Duration // The current default value is 2 hours.
-	// After having pinged for keepalive check, the server waits for a duration of Timeout and if no activity is seen even after that
-	// the connection is closed.
+	// After having pinged for keepalive check, the server waits for a duration
+	// of Timeout and if no activity is seen even after that the connection is
+	// closed.
 	Timeout time.Duration // The current default value is 20 seconds.
 }

-// EnforcementPolicy is used to set keepalive enforcement policy on the server-side.
-// Server will close connection with a client that violates this policy.
+// EnforcementPolicy is used to set keepalive enforcement policy on the
+// server-side. Server will close connection with a client that violates this
+// policy.
 type EnforcementPolicy struct {
-	// MinTime is the minimum amount of time a client should wait before sending a keepalive ping.
+	// MinTime is the minimum amount of time a client should wait before sending
+	// a keepalive ping.
 	MinTime time.Duration // The current default value is 5 minutes.
-	// If true, server expects keepalive pings even when there are no active streams(RPCs).
+	// If true, server allows keepalive pings even when there are no active
+	// streams(RPCs). If false, and client sends ping when there are no active
+	// streams, server will send GOAWAY and close the connection.
 	PermitWithoutStream bool // false by default.
 }
diff --git a/vendor/google.golang.org/grpc/metadata/metadata.go b/vendor/google.golang.org/grpc/metadata/metadata.go
index bd2eaf408..cf6d1b947 100644
--- a/vendor/google.golang.org/grpc/metadata/metadata.go
+++ b/vendor/google.golang.org/grpc/metadata/metadata.go
@@ -22,10 +22,9 @@
 package metadata // import "google.golang.org/grpc/metadata"

 import (
+	"context"
 	"fmt"
 	"strings"
-
-	"golang.org/x/net/context"
 )

 // DecodeKeyValue returns k, v, nil.
diff --git a/vendor/google.golang.org/grpc/naming/dns_resolver.go b/vendor/google.golang.org/grpc/naming/dns_resolver.go
index 0f8a908ea..c9f79dc53 100644
--- a/vendor/google.golang.org/grpc/naming/dns_resolver.go
+++ b/vendor/google.golang.org/grpc/naming/dns_resolver.go
@@ -19,13 +19,13 @@
 package naming

 import (
+	"context"
 	"errors"
 	"fmt"
 	"net"
 	"strconv"
 	"time"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/grpclog"
 )

@@ -37,6 +37,9 @@ const (
 var (
 	errMissingAddr  = errors.New("missing address")
 	errWatcherClose = errors.New("watcher has been closed")
+
+	lookupHost = net.DefaultResolver.LookupHost
+	lookupSRV  = net.DefaultResolver.LookupSRV
 )

 // NewDNSResolverWithFreq creates a DNS Resolver that can resolve DNS names, and
@@ -73,8 +76,8 @@ func formatIP(addr string) (addrIP string, ok bool) {

 // parseTarget takes the user input target string, returns formatted host and port info.
 // If target doesn't specify a port, set the port to be the defaultPort.
-// If target is in IPv6 format and host-name is enclosed in sqarue brackets, brackets
-// are strippd when setting the host.
+// If target is in IPv6 format and host-name is enclosed in square brackets, brackets
+// are stripped when setting the host.
 // examples:
 // target: "www.google.com" returns host: "www.google.com", port: "443"
 // target: "ipv4-host:80" returns host: "ipv4-host", port: "80"
@@ -218,7 +221,7 @@ func (w *dnsWatcher) lookupSRV() map[string]*Update {
 	for _, s := range srvs {
 		lbAddrs, err := lookupHost(w.ctx, s.Target)
 		if err != nil {
-			grpclog.Warningf("grpc: failed load banlacer address dns lookup due to %v.\n", err)
+			grpclog.Warningf("grpc: failed load balancer address dns lookup due to %v.\n", err)
 			continue
 		}
 		for _, a := range lbAddrs {
diff --git a/vendor/google.golang.org/grpc/naming/go17.go b/vendor/google.golang.org/grpc/naming/go17.go
deleted file mode 100644
index 57b65d7b8..000000000
--- a/vendor/google.golang.org/grpc/naming/go17.go
+++ /dev/null
@@ -1,34 +0,0 @@
-// +build go1.6,!go1.8
-
-/*
- *
- * Copyright 2017 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package naming
-
-import (
-	"net"
-
-	"golang.org/x/net/context"
-)
-
-var (
-	lookupHost = func(ctx context.Context, host string) ([]string, error) { return net.LookupHost(host) }
-	lookupSRV  = func(ctx context.Context, service, proto, name string) (string, []*net.SRV, error) {
-		return net.LookupSRV(service, proto, name)
-	}
-)
diff --git a/vendor/google.golang.org/grpc/naming/naming.go b/vendor/google.golang.org/grpc/naming/naming.go
index 8cc39e937..c99fdbef4 100644
--- a/vendor/google.golang.org/grpc/naming/naming.go
+++ b/vendor/google.golang.org/grpc/naming/naming.go
@@ -17,7 +17,7 @@
  */

 // Package naming defines the naming API and related data structures for gRPC.
-// The interface is EXPERIMENTAL and may be suject to change.
+// The interface is EXPERIMENTAL and may be subject to change.
 //
 // Deprecated: please use package resolver.
 package naming
diff --git a/vendor/google.golang.org/grpc/peer/peer.go b/vendor/google.golang.org/grpc/peer/peer.go
index 317b8b9d0..e01d219ff 100644
--- a/vendor/google.golang.org/grpc/peer/peer.go
+++ b/vendor/google.golang.org/grpc/peer/peer.go
@@ -21,9 +21,9 @@
 package peer

 import (
+	"context"
 	"net"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/credentials"
 )

diff --git a/vendor/google.golang.org/grpc/picker_wrapper.go b/vendor/google.golang.org/grpc/picker_wrapper.go
index 76cc456aa..a2575c963 100644
--- a/vendor/google.golang.org/grpc/picker_wrapper.go
+++ b/vendor/google.golang.org/grpc/picker_wrapper.go
@@ -19,10 +19,10 @@
 package grpc

 import (
+	"context"
 	"io"
 	"sync"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/balancer"
 	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/grpclog"
@@ -101,10 +101,7 @@ func doneChannelzWrapper(acw *acBalancerWrapper, done func(balancer.DoneInfo)) f
 // - the subConn returned by the current picker is not READY
 // When one of these situations happens, pick blocks until the picker gets updated.
 func (bp *pickerWrapper) pick(ctx context.Context, failfast bool, opts balancer.PickOptions) (transport.ClientTransport, func(balancer.DoneInfo), error) {
-	var (
-		p  balancer.Picker
-		ch chan struct{}
-	)
+	var ch chan struct{}

 	for {
 		bp.mu.Lock()
@@ -130,7 +127,7 @@ func (bp *pickerWrapper) pick(ctx context.Context, failfast bool, opts balancer.
 		}

 		ch = bp.blockingCh
-		p = bp.picker
+		p := bp.picker
 		bp.mu.Unlock()

 		subConn, done, err := p.Pick(ctx, opts)
@@ -144,15 +141,22 @@ func (bp *pickerWrapper) pick(ctx context.Context, failfast bool, opts balancer.
 					continue
 				}
 				return nil, nil, status.Errorf(codes.Unavailable, "%v, latest connection error: %v", err, bp.connectionError())
+			case context.DeadlineExceeded:
+				return nil, nil, status.Error(codes.DeadlineExceeded, err.Error())
+			case context.Canceled:
+				return nil, nil, status.Error(codes.Canceled, err.Error())
 			default:
+				if _, ok := status.FromError(err); ok {
+					return nil, nil, err
+				}
 				// err is some other error.
-				return nil, nil, toRPCErr(err)
+				return nil, nil, status.Error(codes.Unknown, err.Error())
 			}
 		}

 		acw, ok := subConn.(*acBalancerWrapper)
 		if !ok {
-			grpclog.Infof("subconn returned from pick is not *acBalancerWrapper")
+			grpclog.Error("subconn returned from pick is not *acBalancerWrapper")
 			continue
 		}
 		if t, ok := acw.getAddrConn().getReadyTransport(); ok {
diff --git a/vendor/google.golang.org/grpc/pickfirst.go b/vendor/google.golang.org/grpc/pickfirst.go
index bf659d49d..d1e38aad7 100644
--- a/vendor/google.golang.org/grpc/pickfirst.go
+++ b/vendor/google.golang.org/grpc/pickfirst.go
@@ -19,7 +19,8 @@
 package grpc

 import (
-	"golang.org/x/net/context"
+	"context"
+
 	"google.golang.org/grpc/balancer"
 	"google.golang.org/grpc/connectivity"
 	"google.golang.org/grpc/grpclog"
@@ -56,6 +57,7 @@ func (b *pickfirstBalancer) HandleResolvedAddrs(addrs []resolver.Address, err er
 	if b.sc == nil {
 		b.sc, err = b.cc.NewSubConn(addrs, balancer.NewSubConnOptions{})
 		if err != nil {
+			//TODO(yuxuanli): why not change the cc state to Idle?
 			grpclog.Errorf("pickfirstBalancer: failed to NewSubConn: %v", err)
 			return
 		}
diff --git a/vendor/google.golang.org/grpc/proxy.go b/vendor/google.golang.org/grpc/proxy.go
index 2d40236e2..f8f69bfb7 100644
--- a/vendor/google.golang.org/grpc/proxy.go
+++ b/vendor/google.golang.org/grpc/proxy.go
@@ -20,6 +20,8 @@ package grpc

 import (
 	"bufio"
+	"context"
+	"encoding/base64"
 	"errors"
 	"fmt"
 	"io"
@@ -27,10 +29,10 @@ import (
 	"net/http"
 	"net/http/httputil"
 	"net/url"
-
-	"golang.org/x/net/context"
 )

+const proxyAuthHeaderKey = "Proxy-Authorization"
+
 var (
 	// errDisabled indicates that proxy is disabled for the address.
 	errDisabled = errors.New("proxy is disabled for the address")
@@ -38,7 +40,7 @@ var (
 	httpProxyFromEnvironment = http.ProxyFromEnvironment
 )

-func mapAddress(ctx context.Context, address string) (string, error) {
+func mapAddress(ctx context.Context, address string) (*url.URL, error) {
 	req := &http.Request{
 		URL: &url.URL{
 			Scheme: "https",
@@ -47,12 +49,12 @@ func mapAddress(ctx context.Context, address string) (string, error) {
 	}
 	url, err := httpProxyFromEnvironment(req)
 	if err != nil {
-		return "", err
+		return nil, err
 	}
 	if url == nil {
-		return "", errDisabled
+		return nil, errDisabled
 	}
-	return url.Host, nil
+	return url, nil
 }

 // To read a response from a net.Conn, http.ReadResponse() takes a bufio.Reader.
@@ -69,18 +71,28 @@ func (c *bufConn) Read(b []byte) (int, error) {
 	return c.r.Read(b)
 }

-func doHTTPConnectHandshake(ctx context.Context, conn net.Conn, addr string) (_ net.Conn, err error) {
+func basicAuth(username, password string) string {
+	auth := username + ":" + password
+	return base64.StdEncoding.EncodeToString([]byte(auth))
+}
+
+func doHTTPConnectHandshake(ctx context.Context, conn net.Conn, backendAddr string, proxyURL *url.URL) (_ net.Conn, err error) {
 	defer func() {
 		if err != nil {
 			conn.Close()
 		}
 	}()

-	req := (&http.Request{
+	req := &http.Request{
 		Method: http.MethodConnect,
-		URL:    &url.URL{Host: addr},
+		URL:    &url.URL{Host: backendAddr},
 		Header: map[string][]string{"User-Agent": {grpcUA}},
-	})
+	}
+	if t := proxyURL.User; t != nil {
+		u := t.Username()
+		p, _ := t.Password()
+		req.Header.Add(proxyAuthHeaderKey, "Basic "+basicAuth(u, p))
+	}

 	if err := sendHTTPRequest(ctx, req, conn); err != nil {
 		return nil, fmt.Errorf("failed to write the HTTP request: %v", err)
@@ -108,23 +120,33 @@ func doHTTPConnectHandshake(ctx context.Context, conn net.Conn, addr string) (_
 // provided dialer, does HTTP CONNECT handshake and returns the connection.
 func newProxyDialer(dialer func(context.Context, string) (net.Conn, error)) func(context.Context, string) (net.Conn, error) {
 	return func(ctx context.Context, addr string) (conn net.Conn, err error) {
-		var skipHandshake bool
-		newAddr, err := mapAddress(ctx, addr)
+		var newAddr string
+		proxyURL, err := mapAddress(ctx, addr)
 		if err != nil {
 			if err != errDisabled {
 				return nil, err
 			}
-			skipHandshake = true
 			newAddr = addr
+		} else {
+			newAddr = proxyURL.Host
 		}

 		conn, err = dialer(ctx, newAddr)
 		if err != nil {
 			return
 		}
-		if !skipHandshake {
-			conn, err = doHTTPConnectHandshake(ctx, conn, addr)
+		if proxyURL != nil {
+			// proxy is disabled if proxyURL is nil.
+			conn, err = doHTTPConnectHandshake(ctx, conn, addr, proxyURL)
 		}
 		return
 	}
 }
+
+func sendHTTPRequest(ctx context.Context, req *http.Request, conn net.Conn) error {
+	req = req.WithContext(ctx)
+	if err := req.Write(conn); err != nil {
+		return fmt.Errorf("failed to write the HTTP request: %v", err)
+	}
+	return nil
+}
diff --git a/vendor/google.golang.org/grpc/resolver/dns/dns_resolver.go b/vendor/google.golang.org/grpc/resolver/dns/dns_resolver.go
index 4ce81671d..2d8da331d 100644
--- a/vendor/google.golang.org/grpc/resolver/dns/dns_resolver.go
+++ b/vendor/google.golang.org/grpc/resolver/dns/dns_resolver.go
@@ -1,6 +1,6 @@
 /*
  *
- * Copyright 2017 gRPC authors.
+ * Copyright 2018 gRPC authors.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -21,6 +21,7 @@
 package dns

 import (
+	"context"
 	"encoding/json"
 	"errors"
 	"fmt"
@@ -31,7 +32,6 @@ import (
 	"sync"
 	"time"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/grpclog"
 	"google.golang.org/grpc/internal/backoff"
 	"google.golang.org/grpc/internal/grpcrand"
@@ -43,9 +43,10 @@ func init() {
 }

 const (
-	defaultPort = "443"
-	defaultFreq = time.Minute * 30
-	golang      = "GO"
+	defaultPort       = "443"
+	defaultFreq       = time.Minute * 30
+	defaultDNSSvrPort = "53"
+	golang            = "GO"
 	// In DNS, service config is encoded in a TXT record via the mechanism
 	// described in RFC-1464 using the attribute name grpc_config.
 	txtAttribute = "grpc_config="
@@ -61,6 +62,31 @@ var (
 	errEndsWithColon = errors.New("dns resolver: missing port after port-separator colon")
 )

+var (
+	defaultResolver netResolver = net.DefaultResolver
+)
+
+var customAuthorityDialler = func(authority string) func(ctx context.Context, network, address string) (net.Conn, error) {
+	return func(ctx context.Context, network, address string) (net.Conn, error) {
+		var dialer net.Dialer
+		return dialer.DialContext(ctx, network, authority)
+	}
+}
+
+var customAuthorityResolver = func(authority string) (netResolver, error) {
+	host, port, err := parseTarget(authority, defaultDNSSvrPort)
+	if err != nil {
+		return nil, err
+	}
+
+	authorityWithPort := net.JoinHostPort(host, port)
+
+	return &net.Resolver{
+		PreferGo: true,
+		Dial:     customAuthorityDialler(authorityWithPort),
+	}, nil
+}
+
 // NewBuilder creates a dnsBuilder which is used to factory DNS resolvers.
 func NewBuilder() resolver.Builder {
 	return &dnsBuilder{minFreq: defaultFreq}
@@ -73,10 +99,7 @@ type dnsBuilder struct {

 // Build creates and starts a DNS resolver that watches the name resolution of the target.
 func (b *dnsBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOption) (resolver.Resolver, error) {
-	if target.Authority != "" {
-		return nil, fmt.Errorf("default DNS resolver does not support custom DNS server")
-	}
-	host, port, err := parseTarget(target.Endpoint)
+	host, port, err := parseTarget(target.Endpoint, defaultPort)
 	if err != nil {
 		return nil, err
 	}
@@ -111,6 +134,15 @@ func (b *dnsBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts
 		disableServiceConfig: opts.DisableServiceConfig,
 	}

+	if target.Authority == "" {
+		d.resolver = defaultResolver
+	} else {
+		d.resolver, err = customAuthorityResolver(target.Authority)
+		if err != nil {
+			return nil, err
+		}
+	}
+
 	d.wg.Add(1)
 	go d.watcher()
 	return d, nil
@@ -121,6 +153,12 @@ func (b *dnsBuilder) Scheme() string {
 	return "dns"
 }

+type netResolver interface {
+	LookupHost(ctx context.Context, host string) (addrs []string, err error)
+	LookupSRV(ctx context.Context, service, proto, name string) (cname string, addrs []*net.SRV, err error)
+	LookupTXT(ctx context.Context, name string) (txts []string, err error)
+}
+
 // ipResolver watches for the name resolution update for an IP address.
 type ipResolver struct {
 	cc resolver.ClientConn
@@ -161,6 +199,7 @@ type dnsResolver struct {
 	retryCount int
 	host       string
 	port       string
+	resolver   netResolver
 	ctx        context.Context
 	cancel     context.CancelFunc
 	cc         resolver.ClientConn
@@ -218,13 +257,13 @@ func (d *dnsResolver) watcher() {

 func (d *dnsResolver) lookupSRV() []resolver.Address {
 	var newAddrs []resolver.Address
-	_, srvs, err := lookupSRV(d.ctx, "grpclb", "tcp", d.host)
+	_, srvs, err := d.resolver.LookupSRV(d.ctx, "grpclb", "tcp", d.host)
 	if err != nil {
 		grpclog.Infof("grpc: failed dns SRV record lookup due to %v.\n", err)
 		return nil
 	}
 	for _, s := range srvs {
-		lbAddrs, err := lookupHost(d.ctx, s.Target)
+		lbAddrs, err := d.resolver.LookupHost(d.ctx, s.Target)
 		if err != nil {
 			grpclog.Infof("grpc: failed load balancer address dns lookup due to %v.\n", err)
 			continue
@@ -243,7 +282,7 @@ func (d *dnsResolver) lookupSRV() []resolver.Address {
 }

 func (d *dnsResolver) lookupTXT() string {
-	ss, err := lookupTXT(d.ctx, d.host)
+	ss, err := d.resolver.LookupTXT(d.ctx, d.host)
 	if err != nil {
 		grpclog.Infof("grpc: failed dns TXT record lookup due to %v.\n", err)
 		return ""
@@ -263,7 +302,7 @@ func (d *dnsResolver) lookupTXT() string {

 func (d *dnsResolver) lookupHost() []resolver.Address {
 	var newAddrs []resolver.Address
-	addrs, err := lookupHost(d.ctx, d.host)
+	addrs, err := d.resolver.LookupHost(d.ctx, d.host)
 	if err != nil {
 		grpclog.Warningf("grpc: failed dns A record lookup due to %v.\n", err)
 		return nil
@@ -305,16 +344,16 @@ func formatIP(addr string) (addrIP string, ok bool) {
 	return "[" + addr + "]", true
 }

-// parseTarget takes the user input target string, returns formatted host and port info.
+// parseTarget takes the user input target string and default port, returns formatted host and port info.
 // If target doesn't specify a port, set the port to be the defaultPort.
-// If target is in IPv6 format and host-name is enclosed in sqarue brackets, brackets
-// are strippd when setting the host.
+// If target is in IPv6 format and host-name is enclosed in square brackets, brackets
+// are stripped when setting the host.
 // examples:
-// target: "www.google.com" returns host: "www.google.com", port: "443"
-// target: "ipv4-host:80" returns host: "ipv4-host", port: "80"
-// target: "[ipv6-host]" returns host: "ipv6-host", port: "443"
-// target: ":80" returns host: "localhost", port: "80"
-func parseTarget(target string) (host, port string, err error) {
+// target: "www.google.com" defaultPort: "443" returns host: "www.google.com", port: "443"
+// target: "ipv4-host:80" defaultPort: "443" returns host: "ipv4-host", port: "80"
+// target: "[ipv6-host]" defaultPort: "443" returns host: "ipv6-host", port: "443"
+// target: ":80" defaultPort: "443" returns host: "localhost", port: "80"
+func parseTarget(target, defaultPort string) (host, port string, err error) {
 	if target == "" {
 		return "", "", errMissingAddr
 	}
diff --git a/vendor/google.golang.org/grpc/resolver/dns/go17.go b/vendor/google.golang.org/grpc/resolver/dns/go17.go
deleted file mode 100644
index b466bc8f6..000000000
--- a/vendor/google.golang.org/grpc/resolver/dns/go17.go
+++ /dev/null
@@ -1,35 +0,0 @@
-// +build go1.6, !go1.8
-
-/*
- *
- * Copyright 2017 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package dns
-
-import (
-	"net"
-
-	"golang.org/x/net/context"
-)
-
-var (
-	lookupHost = func(ctx context.Context, host string) ([]string, error) { return net.LookupHost(host) }
-	lookupSRV  = func(ctx context.Context, service, proto, name string) (string, []*net.SRV, error) {
-		return net.LookupSRV(service, proto, name)
-	}
-	lookupTXT = func(ctx context.Context, name string) ([]string, error) { return net.LookupTXT(name) }
-)
diff --git a/vendor/google.golang.org/grpc/resolver/dns/go18.go b/vendor/google.golang.org/grpc/resolver/dns/go18.go
deleted file mode 100644
index fa34f14ca..000000000
--- a/vendor/google.golang.org/grpc/resolver/dns/go18.go
+++ /dev/null
@@ -1,29 +0,0 @@
-// +build go1.8
-
-/*
- *
- * Copyright 2017 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package dns
-
-import "net"
-
-var (
-	lookupHost = net.DefaultResolver.LookupHost
-	lookupSRV  = net.DefaultResolver.LookupSRV
-	lookupTXT  = net.DefaultResolver.LookupTXT
-)
diff --git a/vendor/google.golang.org/grpc/resolver_conn_wrapper.go b/vendor/google.golang.org/grpc/resolver_conn_wrapper.go
index 494d6931e..50991eafb 100644
--- a/vendor/google.golang.org/grpc/resolver_conn_wrapper.go
+++ b/vendor/google.golang.org/grpc/resolver_conn_wrapper.go
@@ -23,21 +23,23 @@ import (
 	"strings"

 	"google.golang.org/grpc/grpclog"
+	"google.golang.org/grpc/internal/channelz"
 	"google.golang.org/grpc/resolver"
 )

 // ccResolverWrapper is a wrapper on top of cc for resolvers.
 // It implements resolver.ClientConnection interface.
 type ccResolverWrapper struct {
-	cc       *ClientConn
-	resolver resolver.Resolver
-	addrCh   chan []resolver.Address
-	scCh     chan string
-	done     chan struct{}
+	cc                 *ClientConn
+	resolver           resolver.Resolver
+	addrCh             chan []resolver.Address
+	scCh               chan string
+	done               chan struct{}
+	lastAddressesCount int
 }

 // split2 returns the values from strings.SplitN(s, sep, 2).
-// If sep is not found, it returns ("", s, false) instead.
+// If sep is not found, it returns ("", "", false) instead.
 func split2(s, sep string) (string, string, bool) {
 	spl := strings.SplitN(s, sep, 2)
 	if len(spl) < 2 {
@@ -91,44 +93,6 @@ func newCCResolverWrapper(cc *ClientConn) (*ccResolverWrapper, error) {
 	return ccr, nil
 }

-func (ccr *ccResolverWrapper) start() {
-	go ccr.watcher()
-}
-
-// watcher processes address updates and service config updates sequentially.
-// Otherwise, we need to resolve possible races between address and service
-// config (e.g. they specify different balancer types).
-func (ccr *ccResolverWrapper) watcher() {
-	for {
-		select {
-		case <-ccr.done:
-			return
-		default:
-		}
-
-		select {
-		case addrs := <-ccr.addrCh:
-			select {
-			case <-ccr.done:
-				return
-			default:
-			}
-			grpclog.Infof("ccResolverWrapper: sending new addresses to cc: %v", addrs)
-			ccr.cc.handleResolvedAddrs(addrs, nil)
-		case sc := <-ccr.scCh:
-			select {
-			case <-ccr.done:
-				return
-			default:
-			}
-			grpclog.Infof("ccResolverWrapper: got new service config: %v", sc)
-			ccr.cc.handleServiceConfig(sc)
-		case <-ccr.done:
-			return
-		}
-	}
-}
-
 func (ccr *ccResolverWrapper) resolveNow(o resolver.ResolveNowOption) {
 	ccr.resolver.ResolveNow(o)
 }
@@ -141,18 +105,51 @@ func (ccr *ccResolverWrapper) close() {
 // NewAddress is called by the resolver implemenetion to send addresses to gRPC.
 func (ccr *ccResolverWrapper) NewAddress(addrs []resolver.Address) {
 	select {
-	case <-ccr.addrCh:
+	case <-ccr.done:
+		return
 	default:
 	}
-	ccr.addrCh <- addrs
+	grpclog.Infof("ccResolverWrapper: sending new addresses to cc: %v", addrs)
+	if channelz.IsOn() {
+		ccr.addChannelzTraceEvent(addrs)
+	}
+	ccr.cc.handleResolvedAddrs(addrs, nil)
 }

 // NewServiceConfig is called by the resolver implemenetion to send service
 // configs to gRPC.
 func (ccr *ccResolverWrapper) NewServiceConfig(sc string) {
 	select {
-	case <-ccr.scCh:
+	case <-ccr.done:
+		return
 	default:
 	}
-	ccr.scCh <- sc
+	grpclog.Infof("ccResolverWrapper: got new service config: %v", sc)
+	ccr.cc.handleServiceConfig(sc)
+}
+
+func (ccr *ccResolverWrapper) addChannelzTraceEvent(addrs []resolver.Address) {
+	if len(addrs) == 0 && ccr.lastAddressesCount != 0 {
+		channelz.AddTraceEvent(ccr.cc.channelzID, &channelz.TraceEventDesc{
+			Desc:     "Resolver returns an empty address list",
+			Severity: channelz.CtWarning,
+		})
+	} else if len(addrs) != 0 && ccr.lastAddressesCount == 0 {
+		var s string
+		for i, a := range addrs {
+			if a.ServerName != "" {
+				s += a.Addr + "(" + a.ServerName + ")"
+			} else {
+				s += a.Addr
+			}
+			if i != len(addrs)-1 {
+				s += " "
+			}
+		}
+		channelz.AddTraceEvent(ccr.cc.channelzID, &channelz.TraceEventDesc{
+			Desc:     fmt.Sprintf("Resolver returns a non-empty address list (previous one was empty) %q", s),
+			Severity: channelz.CtINFO,
+		})
+	}
+	ccr.lastAddressesCount = len(addrs)
 }
diff --git a/vendor/google.golang.org/grpc/rpc_util.go b/vendor/google.golang.org/grpc/rpc_util.go
index fa0568302..2a595622d 100644
--- a/vendor/google.golang.org/grpc/rpc_util.go
+++ b/vendor/google.golang.org/grpc/rpc_util.go
@@ -21,6 +21,7 @@ package grpc
 import (
 	"bytes"
 	"compress/gzip"
+	"context"
 	"encoding/binary"
 	"fmt"
 	"io"
@@ -31,7 +32,6 @@ import (
 	"sync"
 	"time"

-	"golang.org/x/net/context"
 	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/credentials"
 	"google.golang.org/grpc/encoding"
@@ -155,7 +155,7 @@ func (d *gzipDecompressor) Type() string {
 type callInfo struct {
 	compressorType        string
 	failFast              bool
-	stream                *clientStream
+	stream                ClientStream
 	maxReceiveMessageSize *int
 	maxSendMessageSize    *int
 	creds                 credentials.PerRPCCredentials
@@ -253,8 +253,8 @@ func (o PeerCallOption) after(c *callInfo) {
 	}
 }

-// FailFast configures the action to take when an RPC is attempted on broken
-// connections or unreachable servers.  If failFast is true, the RPC will fail
+// WaitForReady configures the action to take when an RPC is attempted on broken
+// connections or unreachable servers. If waitForReady is false, the RPC will fail
 // immediately. Otherwise, the RPC client will block the call until a
 // connection is available (or the call is canceled or times out) and will
 // retry the call if it fails due to a transient error.  gRPC will not retry if
@@ -262,7 +262,14 @@ func (o PeerCallOption) after(c *callInfo) {
 // the data.  Please refer to
 // https://github.com/grpc/grpc/blob/master/doc/wait-for-ready.md.
 //
-// By default, RPCs are "Fail Fast".
+// By default, RPCs don't "wait for ready".
+func WaitForReady(waitForReady bool) CallOption {
+	return FailFastCallOption{FailFast: !waitForReady}
+}
+
+// FailFast is the opposite of WaitForReady.
+//
+// Deprecated: use WaitForReady.
 func FailFast(failFast bool) CallOption {
 	return FailFastCallOption{FailFast: failFast}
 }
@@ -363,13 +370,13 @@ func (o CompressorCallOption) after(c *callInfo) {}
 // https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for
 // more details.
 //
-// If CallCustomCodec is not also used, the content-subtype will be used to
-// look up the Codec to use in the registry controlled by RegisterCodec. See
-// the documentation on RegisterCodec for details on registration. The lookup
-// of content-subtype is case-insensitive. If no such Codec is found, the call
+// If ForceCodec is not also used, the content-subtype will be used to look up
+// the Codec to use in the registry controlled by RegisterCodec. See the
+// documentation on RegisterCodec for details on registration. The lookup of
+// content-subtype is case-insensitive. If no such Codec is found, the call
 // will result in an error with code codes.Internal.
 //
-// If CallCustomCodec is also used, that Codec will be used for all request and
+// If ForceCodec is also used, that Codec will be used for all request and
 // response messages, with the content-subtype set to the given contentSubtype
 // here for requests.
 func CallContentSubtype(contentSubtype string) CallOption {
@@ -389,7 +396,7 @@ func (o ContentSubtypeCallOption) before(c *callInfo) error {
 }
 func (o ContentSubtypeCallOption) after(c *callInfo) {}

-// CallCustomCodec returns a CallOption that will set the given Codec to be
+// ForceCodec returns a CallOption that will set the given Codec to be
 // used for all request and response messages for a call. The result of calling
 // String() will be used as the content-subtype in a case-insensitive manner.
 //
@@ -401,12 +408,37 @@ func (o ContentSubtypeCallOption) after(c *callInfo) {}
 //
 // This function is provided for advanced users; prefer to use only
 // CallContentSubtype to select a registered codec instead.
+//
+// This is an EXPERIMENTAL API.
+func ForceCodec(codec encoding.Codec) CallOption {
+	return ForceCodecCallOption{Codec: codec}
+}
+
+// ForceCodecCallOption is a CallOption that indicates the codec used for
+// marshaling messages.
+//
+// This is an EXPERIMENTAL API.
+type ForceCodecCallOption struct {
+	Codec encoding.Codec
+}
+
+func (o ForceCodecCallOption) before(c *callInfo) error {
+	c.codec = o.Codec
+	return nil
+}
+func (o ForceCodecCallOption) after(c *callInfo) {}
+
+// CallCustomCodec behaves like ForceCodec, but accepts a grpc.Codec instead of
+// an encoding.Codec.
+//
+// Deprecated: use ForceCodec instead.
 func CallCustomCodec(codec Codec) CallOption {
 	return CustomCodecCallOption{Codec: codec}
 }

 // CustomCodecCallOption is a CallOption that indicates the codec used for
 // marshaling messages.
+//
 // This is an EXPERIMENTAL API.
 type CustomCodecCallOption struct {
 	Codec Codec
@@ -531,7 +563,10 @@ func compress(in []byte, cp Compressor, compressor encoding.Compressor) ([]byte,
 	}
 	cbuf := &bytes.Buffer{}
 	if compressor != nil {
-		z, _ := compressor.Compress(cbuf)
+		z, err := compressor.Compress(cbuf)
+		if err != nil {
+			return nil, wrapErr(err)
+		}
 		if _, err := z.Write(in); err != nil {
 			return nil, wrapErr(err)
 		}
@@ -595,20 +630,22 @@ func checkRecvPayload(pf payloadFormat, recvCompress string, haveCompressor bool
 	return nil
 }

-// For the two compressor parameters, both should not be set, but if they are,
-// dc takes precedence over compressor.
-// TODO(dfawley): wrap the old compressor/decompressor using the new API?
-func recv(p *parser, c baseCodec, s *transport.Stream, dc Decompressor, m interface{}, maxReceiveMessageSize int, inPayload *stats.InPayload, compressor encoding.Compressor) error {
+type payloadInfo struct {
+	wireLength        int // The compressed length got from wire.
+	uncompressedBytes []byte
+}
+
+func recvAndDecompress(p *parser, s *transport.Stream, dc Decompressor, maxReceiveMessageSize int, payInfo *payloadInfo, compressor encoding.Compressor) ([]byte, error) {
 	pf, d, err := p.recvMsg(maxReceiveMessageSize)
 	if err != nil {
-		return err
+		return nil, err
 	}
-	if inPayload != nil {
-		inPayload.WireLength = len(d)
+	if payInfo != nil {
+		payInfo.wireLength = len(d)
 	}

 	if st := checkRecvPayload(pf, s.RecvCompress(), compressor != nil || dc != nil); st != nil {
-		return st.Err()
+		return nil, st.Err()
 	}

 	if pf == compressionMade {
@@ -617,33 +654,42 @@ func recv(p *parser, c baseCodec, s *transport.Stream, dc Decompressor, m interf
 		if dc != nil {
 			d, err = dc.Do(bytes.NewReader(d))
 			if err != nil {
-				return status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
+				return nil, status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
 			}
 		} else {
 			dcReader, err := compressor.Decompress(bytes.NewReader(d))
 			if err != nil {
-				return status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
+				return nil, status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
 			}
-			d, err = ioutil.ReadAll(dcReader)
+			// Read from LimitReader with limit max+1. So if the underlying
+			// reader is over limit, the result will be bigger than max.
+			d, err = ioutil.ReadAll(io.LimitReader(dcReader, int64(maxReceiveMessageSize)+1))
 			if err != nil {
-				return status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
+				return nil, status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
 			}
 		}
 	}
 	if len(d) > maxReceiveMessageSize {
 		// TODO: Revisit the error code. Currently keep it consistent with java
 		// implementation.
-		return status.Errorf(codes.ResourceExhausted, "grpc: received message larger than max (%d vs. %d)", len(d), maxReceiveMessageSize)
+		return nil, status.Errorf(codes.ResourceExhausted, "grpc: received message larger than max (%d vs. %d)", len(d), maxReceiveMessageSize)
+	}
+	return d, nil
+}
+
+// For the two compressor parameters, both should not be set, but if they are,
+// dc takes precedence over compressor.
+// TODO(dfawley): wrap the old compressor/decompressor using the new API?
+func recv(p *parser, c baseCodec, s *transport.Stream, dc Decompressor, m interface{}, maxReceiveMessageSize int, payInfo *payloadInfo, compressor encoding.Compressor) error {
+	d, err := recvAndDecompress(p, s, dc, maxReceiveMessageSize, payInfo, compressor)
+	if err != nil {
+		return err
 	}
 	if err := c.Unmarshal(d, m); err != nil {
 		return status.Errorf(codes.Internal, "grpc: failed to unmarshal the received message %v", err)
 	}
-	if inPayload != nil {
-		inPayload.RecvTime = time.Now()
-		inPayload.Payload = m
-		// TODO truncate large payload.
-		inPayload.Data = d
-		inPayload.Length = len(d)
+	if payInfo != nil {
+		payInfo.uncompressedBytes = d
 	}
 	return nil
 }
@@ -666,23 +712,17 @@ func rpcInfoFromContext(ctx context.Context) (s *rpcInfo, ok bool) {
 // Code returns the error code for err if it was produced by the rpc system.
 // Otherwise, it returns codes.Unknown.
 //
-// Deprecated: use status.FromError and Code method instead.
+// Deprecated: use status.Code instead.
 func Code(err error) codes.Code {
-	if s, ok := status.FromError(err); ok {
-		return s.Code()
-	}
-	return codes.Unknown
+	return status.Code(err)
 }

 // ErrorDesc returns the error description of err if it was produced by the rpc system.
 // Otherwise, it returns err.Error() or empty string when err is nil.
 //
-// Deprecated: use status.FromError and Message method instead.
+// Deprecated: use status.Convert and Message method instead.
 func ErrorDesc(err error) string {
-	if s, ok := status.FromError(err); ok {
-		return s.Message()
-	}
-	return err.Error()
+	return status.Convert(err).Message()
 }

 // Errorf returns an error containing an error code and a description;
@@ -693,6 +733,31 @@ func Errorf(c codes.Code, format string, a ...interface{}) error {
 	return status.Errorf(c, format, a...)
 }

+// toRPCErr converts an error into an error from the status package.
+func toRPCErr(err error) error {
+	if err == nil || err == io.EOF {
+		return err
+	}
+	if err == io.ErrUnexpectedEOF {
+		return status.Error(codes.Internal, err.Error())
+	}
+	if _, ok := status.FromError(err); ok {
+		return err
+	}
+	switch e := err.(type) {
+	case transport.ConnectionError:
+		return status.Error(codes.Unavailable, e.Desc)
+	default:
+		switch err {
+		case context.DeadlineExceeded:
+			return status.Error(codes.DeadlineExceeded, err.Error())
+		case context.Canceled:
+			return status.Error(codes.Canceled, err.Error())
+		}
+	}
+	return status.Error(codes.Unknown, err.Error())
+}
+
 // setCallInfoCodec should only be called after CallOptions have been applied.
 func setCallInfoCodec(c *callInfo) error {
 	if c.codec != nil {
diff --git a/vendor/google.golang.org/grpc/server.go b/vendor/google.golang.org/grpc/server.go
index 5c7d5b635..33272a47a 100644
--- a/vendor/google.golang.org/grpc/server.go
+++ b/vendor/google.golang.org/grpc/server.go
@@ -19,7 +19,7 @@
 package grpc

 import (
-	"bytes"
+	"context"
 	"errors"
 	"fmt"
 	"io"
@@ -33,9 +33,6 @@ import (
 	"sync/atomic"
 	"time"

-	"io/ioutil"
-
-	"golang.org/x/net/context"
 	"golang.org/x/net/trace"

 	"google.golang.org/grpc/codes"
@@ -43,10 +40,12 @@ import (
 	"google.golang.org/grpc/encoding"
 	"google.golang.org/grpc/encoding/proto"
 	"google.golang.org/grpc/grpclog"
+	"google.golang.org/grpc/internal/binarylog"
 	"google.golang.org/grpc/internal/channelz"
 	"google.golang.org/grpc/internal/transport"
 	"google.golang.org/grpc/keepalive"
 	"google.golang.org/grpc/metadata"
+	"google.golang.org/grpc/peer"
 	"google.golang.org/grpc/stats"
 	"google.golang.org/grpc/status"
 	"google.golang.org/grpc/tap"
@@ -183,6 +182,11 @@ func InitialConnWindowSize(s int32) ServerOption {

 // KeepaliveParams returns a ServerOption that sets keepalive and max-age parameters for the server.
 func KeepaliveParams(kp keepalive.ServerParameters) ServerOption {
+	if kp.Time > 0 && kp.Time < time.Second {
+		grpclog.Warning("Adjusting keepalive ping interval to minimum period of 1s")
+		kp.Time = time.Second
+	}
+
 	return func(o *options) {
 		o.keepaliveParams = kp
 	}
@@ -245,7 +249,7 @@ func MaxRecvMsgSize(m int) ServerOption {
 }

 // MaxSendMsgSize returns a ServerOption to set the max message size in bytes the server can send.
-// If this is not set, gRPC uses the default 4MB.
+// If this is not set, gRPC uses the default `math.MaxInt32`.
 func MaxSendMsgSize(m int) ServerOption {
 	return func(o *options) {
 		o.maxSendMessageSize = m
@@ -538,7 +542,7 @@ func (s *Server) Serve(lis net.Listener) error {
 	s.lis[ls] = true

 	if channelz.IsOn() {
-		ls.channelzID = channelz.RegisterListenSocket(ls, s.channelzID, "")
+		ls.channelzID = channelz.RegisterListenSocket(ls, s.channelzID, lis.Addr().String())
 	}
 	s.mu.Unlock()

@@ -749,7 +753,7 @@ func (s *Server) traceInfo(st transport.ServerTransport, stream *transport.Strea
 	trInfo.firstLine.remoteAddr = st.RemoteAddr()

 	if dl, ok := stream.Context().Deadline(); ok {
-		trInfo.firstLine.deadline = dl.Sub(time.Now())
+		trInfo.firstLine.deadline = time.Until(dl)
 	}
 	return trInfo
 }
@@ -865,6 +869,30 @@ func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.
 		}()
 	}

+	binlog := binarylog.GetMethodLogger(stream.Method())
+	if binlog != nil {
+		ctx := stream.Context()
+		md, _ := metadata.FromIncomingContext(ctx)
+		logEntry := &binarylog.ClientHeader{
+			Header:     md,
+			MethodName: stream.Method(),
+			PeerAddr:   nil,
+		}
+		if deadline, ok := ctx.Deadline(); ok {
+			logEntry.Timeout = time.Until(deadline)
+			if logEntry.Timeout < 0 {
+				logEntry.Timeout = 0
+			}
+		}
+		if a := md[":authority"]; len(a) > 0 {
+			logEntry.Authority = a[0]
+		}
+		if peer, ok := peer.FromContext(ctx); ok {
+			logEntry.PeerAddr = peer.Addr
+		}
+		binlog.Log(logEntry)
+	}
+
 	// comp and cp are used for compression.  decomp and dc are used for
 	// decompression.  If comp and decomp are both set, they are the same;
 	// however they are kept separate to ensure that at most one of the
@@ -901,77 +929,38 @@ func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.
 		}
 	}

-	p := &parser{r: stream}
-	pf, req, err := p.recvMsg(s.opts.maxReceiveMessageSize)
-	if err == io.EOF {
-		// The entire stream is done (for unary RPC only).
-		return err
-	}
-	if err == io.ErrUnexpectedEOF {
-		err = status.Errorf(codes.Internal, io.ErrUnexpectedEOF.Error())
+	var payInfo *payloadInfo
+	if sh != nil || binlog != nil {
+		payInfo = &payloadInfo{}
 	}
+	d, err := recvAndDecompress(&parser{r: stream}, stream, dc, s.opts.maxReceiveMessageSize, payInfo, decomp)
 	if err != nil {
 		if st, ok := status.FromError(err); ok {
 			if e := t.WriteStatus(stream, st); e != nil {
 				grpclog.Warningf("grpc: Server.processUnaryRPC failed to write status %v", e)
 			}
-		} else {
-			switch st := err.(type) {
-			case transport.ConnectionError:
-				// Nothing to do here.
-			default:
-				panic(fmt.Sprintf("grpc: Unexpected error (%T) from recvMsg: %v", st, st))
-			}
 		}
 		return err
 	}
 	if channelz.IsOn() {
 		t.IncrMsgRecv()
 	}
-	if st := checkRecvPayload(pf, stream.RecvCompress(), dc != nil || decomp != nil); st != nil {
-		if e := t.WriteStatus(stream, st); e != nil {
-			grpclog.Warningf("grpc: Server.processUnaryRPC failed to write status %v", e)
-		}
-		return st.Err()
-	}
-	var inPayload *stats.InPayload
-	if sh != nil {
-		inPayload = &stats.InPayload{
-			RecvTime: time.Now(),
-		}
-	}
 	df := func(v interface{}) error {
-		if inPayload != nil {
-			inPayload.WireLength = len(req)
-		}
-		if pf == compressionMade {
-			var err error
-			if dc != nil {
-				req, err = dc.Do(bytes.NewReader(req))
-				if err != nil {
-					return status.Errorf(codes.Internal, err.Error())
-				}
-			} else {
-				tmp, _ := decomp.Decompress(bytes.NewReader(req))
-				req, err = ioutil.ReadAll(tmp)
-				if err != nil {
-					return status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
-				}
-			}
-		}
-		if len(req) > s.opts.maxReceiveMessageSize {
-			// TODO: Revisit the error code. Currently keep it consistent with
-			// java implementation.
-			return status.Errorf(codes.ResourceExhausted, "grpc: received message larger than max (%d vs. %d)", len(req), s.opts.maxReceiveMessageSize)
-		}
-		if err := s.getCodec(stream.ContentSubtype()).Unmarshal(req, v); err != nil {
+		if err := s.getCodec(stream.ContentSubtype()).Unmarshal(d, v); err != nil {
 			return status.Errorf(codes.Internal, "grpc: error unmarshalling request: %v", err)
 		}
-		if inPayload != nil {
-			inPayload.Payload = v
-			inPayload.Data = req
-			inPayload.Length = len(req)
-			sh.HandleRPC(stream.Context(), inPayload)
+		if sh != nil {
+			sh.HandleRPC(stream.Context(), &stats.InPayload{
+				RecvTime: time.Now(),
+				Payload:  v,
+				Data:     d,
+				Length:   len(d),
+			})
+		}
+		if binlog != nil {
+			binlog.Log(&binarylog.ClientMessage{
+				Message: d,
+			})
 		}
 		if trInfo != nil {
 			trInfo.tr.LazyLog(&payload{sent: false, msg: v}, true)
@@ -994,6 +983,19 @@ func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.
 		if e := t.WriteStatus(stream, appStatus); e != nil {
 			grpclog.Warningf("grpc: Server.processUnaryRPC failed to write status: %v", e)
 		}
+		if binlog != nil {
+			if h, _ := stream.Header(); h.Len() > 0 {
+				// Only log serverHeader if there was header. Otherwise it can
+				// be trailer only.
+				binlog.Log(&binarylog.ServerHeader{
+					Header: h,
+				})
+			}
+			binlog.Log(&binarylog.ServerTrailer{
+				Trailer: stream.Trailer(),
+				Err:     appErr,
+			})
+		}
 		return appErr
 	}
 	if trInfo != nil {
@@ -1018,8 +1020,27 @@ func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.
 				panic(fmt.Sprintf("grpc: Unexpected error (%T) from sendResponse: %v", st, st))
 			}
 		}
+		if binlog != nil {
+			h, _ := stream.Header()
+			binlog.Log(&binarylog.ServerHeader{
+				Header: h,
+			})
+			binlog.Log(&binarylog.ServerTrailer{
+				Trailer: stream.Trailer(),
+				Err:     appErr,
+			})
+		}
 		return err
 	}
+	if binlog != nil {
+		h, _ := stream.Header()
+		binlog.Log(&binarylog.ServerHeader{
+			Header: h,
+		})
+		binlog.Log(&binarylog.ServerMessage{
+			Message: reply,
+		})
+	}
 	if channelz.IsOn() {
 		t.IncrMsgSent()
 	}
@@ -1029,7 +1050,14 @@ func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.
 	// TODO: Should we be logging if writing status failed here, like above?
 	// Should the logging be in WriteStatus?  Should we ignore the WriteStatus
 	// error or allow the stats handler to see it?
-	return t.WriteStatus(stream, status.New(codes.OK, ""))
+	err = t.WriteStatus(stream, status.New(codes.OK, ""))
+	if binlog != nil {
+		binlog.Log(&binarylog.ServerTrailer{
+			Trailer: stream.Trailer(),
+			Err:     appErr,
+		})
+	}
+	return err
 }

 func (s *Server) processStreamingRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, sd *StreamDesc, trInfo *traceInfo) (err error) {
@@ -1074,6 +1102,29 @@ func (s *Server) processStreamingRPC(t transport.ServerTransport, stream *transp
 		statsHandler:          sh,
 	}

+	ss.binlog = binarylog.GetMethodLogger(stream.Method())
+	if ss.binlog != nil {
+		md, _ := metadata.FromIncomingContext(ctx)
+		logEntry := &binarylog.ClientHeader{
+			Header:     md,
+			MethodName: stream.Method(),
+			PeerAddr:   nil,
+		}
+		if deadline, ok := ctx.Deadline(); ok {
+			logEntry.Timeout = time.Until(deadline)
+			if logEntry.Timeout < 0 {
+				logEntry.Timeout = 0
+			}
+		}
+		if a := md[":authority"]; len(a) > 0 {
+			logEntry.Authority = a[0]
+		}
+		if peer, ok := peer.FromContext(ss.Context()); ok {
+			logEntry.PeerAddr = peer.Addr
+		}
+		ss.binlog.Log(logEntry)
+	}
+
 	// If dc is set and matches the stream's compression, use it.  Otherwise, try
 	// to find a matching registered compressor for decomp.
 	if rc := stream.RecvCompress(); s.opts.dc != nil && s.opts.dc.Type() == rc {
@@ -1143,6 +1194,12 @@ func (s *Server) processStreamingRPC(t transport.ServerTransport, stream *transp
 			ss.mu.Unlock()
 		}
 		t.WriteStatus(ss.s, appStatus)
+		if ss.binlog != nil {
+			ss.binlog.Log(&binarylog.ServerTrailer{
+				Trailer: ss.s.Trailer(),
+				Err:     appErr,
+			})
+		}
 		// TODO: Should we log an error from WriteStatus here and below?
 		return appErr
 	}
@@ -1151,7 +1208,14 @@ func (s *Server) processStreamingRPC(t transport.ServerTransport, stream *transp
 		ss.trInfo.tr.LazyLog(stringer("OK"), false)
 		ss.mu.Unlock()
 	}
-	return t.WriteStatus(ss.s, status.New(codes.OK, ""))
+	err = t.WriteStatus(ss.s, status.New(codes.OK, ""))
+	if ss.binlog != nil {
+		ss.binlog.Log(&binarylog.ServerTrailer{
+			Trailer: ss.s.Trailer(),
+			Err:     appErr,
+		})
+	}
+	return err
 }

 func (s *Server) handleStream(t transport.ServerTransport, stream *transport.Stream, trInfo *traceInfo) {
@@ -1180,47 +1244,27 @@ func (s *Server) handleStream(t transport.ServerTransport, stream *transport.Str
 	}
 	service := sm[:pos]
 	method := sm[pos+1:]
-	srv, ok := s.m[service]
-	if !ok {
-		if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {
-			s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)
+
+	if srv, ok := s.m[service]; ok {
+		if md, ok := srv.md[method]; ok {
+			s.processUnaryRPC(t, stream, srv, md, trInfo)
 			return
 		}
-		if trInfo != nil {
-			trInfo.tr.LazyLog(&fmtStringer{"Unknown service %v", []interface{}{service}}, true)
-			trInfo.tr.SetError()
-		}
-		errDesc := fmt.Sprintf("unknown service %v", service)
-		if err := t.WriteStatus(stream, status.New(codes.Unimplemented, errDesc)); err != nil {
-			if trInfo != nil {
-				trInfo.tr.LazyLog(&fmtStringer{"%v", []interface{}{err}}, true)
-				trInfo.tr.SetError()
-			}
-			grpclog.Warningf("grpc: Server.handleStream failed to write status: %v", err)
-		}
-		if trInfo != nil {
-			trInfo.tr.Finish()
+		if sd, ok := srv.sd[method]; ok {
+			s.processStreamingRPC(t, stream, srv, sd, trInfo)
+			return
 		}
-		return
 	}
-	// Unary RPC or Streaming RPC?
-	if md, ok := srv.md[method]; ok {
-		s.processUnaryRPC(t, stream, srv, md, trInfo)
-		return
-	}
-	if sd, ok := srv.sd[method]; ok {
-		s.processStreamingRPC(t, stream, srv, sd, trInfo)
+	// Unknown service, or known server unknown method.
+	if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {
+		s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)
 		return
 	}
 	if trInfo != nil {
-		trInfo.tr.LazyLog(&fmtStringer{"Unknown method %v", []interface{}{method}}, true)
+		trInfo.tr.LazyLog(&fmtStringer{"Unknown service %v", []interface{}{service}}, true)
 		trInfo.tr.SetError()
 	}
-	if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {
-		s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)
-		return
-	}
-	errDesc := fmt.Sprintf("unknown method %v", method)
+	errDesc := fmt.Sprintf("unknown service %v", service)
 	if err := t.WriteStatus(stream, status.New(codes.Unimplemented, errDesc)); err != nil {
 		if trInfo != nil {
 			trInfo.tr.LazyLog(&fmtStringer{"%v", []interface{}{err}}, true)
diff --git a/vendor/google.golang.org/grpc/service_config.go b/vendor/google.golang.org/grpc/service_config.go
index e0d735265..162857e20 100644
--- a/vendor/google.golang.org/grpc/service_config.go
+++ b/vendor/google.golang.org/grpc/service_config.go
@@ -96,6 +96,15 @@ type ServiceConfig struct {
 	// If token_count is less than or equal to maxTokens / 2, then RPCs will not
 	// be retried and hedged RPCs will not be sent.
 	retryThrottling *retryThrottlingPolicy
+	// healthCheckConfig must be set as one of the requirement to enable LB channel
+	// health check.
+	healthCheckConfig *healthCheckConfig
+}
+
+// healthCheckConfig defines the go-native version of the LB channel health check config.
+type healthCheckConfig struct {
+	// serviceName is the service name to use in the health-checking request.
+	ServiceName string
 }

 // retryPolicy defines the go-native version of the retry policy defined by the
@@ -226,9 +235,13 @@ type jsonSC struct {
 	LoadBalancingPolicy *string
 	MethodConfig        *[]jsonMC
 	RetryThrottling     *retryThrottlingPolicy
+	HealthCheckConfig   *healthCheckConfig
 }

 func parseServiceConfig(js string) (ServiceConfig, error) {
+	if len(js) == 0 {
+		return ServiceConfig{}, fmt.Errorf("no JSON service config provided")
+	}
 	var rsc jsonSC
 	err := json.Unmarshal([]byte(js), &rsc)
 	if err != nil {
@@ -236,9 +249,10 @@ func parseServiceConfig(js string) (ServiceConfig, error) {
 		return ServiceConfig{}, err
 	}
 	sc := ServiceConfig{
-		LB:              rsc.LoadBalancingPolicy,
-		Methods:         make(map[string]MethodConfig),
-		retryThrottling: rsc.RetryThrottling,
+		LB:                rsc.LoadBalancingPolicy,
+		Methods:           make(map[string]MethodConfig),
+		retryThrottling:   rsc.RetryThrottling,
+		healthCheckConfig: rsc.HealthCheckConfig,
 	}
 	if rsc.MethodConfig == nil {
 		return sc, nil
diff --git a/vendor/google.golang.org/grpc/stats/handlers.go b/vendor/google.golang.org/grpc/stats/handlers.go
index 05b384c69..dc03731e4 100644
--- a/vendor/google.golang.org/grpc/stats/handlers.go
+++ b/vendor/google.golang.org/grpc/stats/handlers.go
@@ -19,9 +19,8 @@
 package stats

 import (
+	"context"
 	"net"
-
-	"golang.org/x/net/context"
 )

 // ConnTagInfo defines the relevant information needed by connection context tagger.
diff --git a/vendor/google.golang.org/grpc/stats/stats.go b/vendor/google.golang.org/grpc/stats/stats.go
index 3f13190a0..84f77dafa 100644
--- a/vendor/google.golang.org/grpc/stats/stats.go
+++ b/vendor/google.golang.org/grpc/stats/stats.go
@@ -24,10 +24,9 @@
 package stats // import "google.golang.org/grpc/stats"

 import (
+	"context"
 	"net"
 	"time"
-
-	"golang.org/x/net/context"
 )

 // RPCStats contains stats information about RPCs.
diff --git a/vendor/google.golang.org/grpc/status/go17.go b/vendor/google.golang.org/grpc/status/go17.go
deleted file mode 100644
index 090215149..000000000
--- a/vendor/google.golang.org/grpc/status/go17.go
+++ /dev/null
@@ -1,44 +0,0 @@
-// +build go1.7
-
-/*
- *
- * Copyright 2018 gRPC authors.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *
- */
-
-package status
-
-import (
-	"context"
-
-	netctx "golang.org/x/net/context"
-	"google.golang.org/grpc/codes"
-)
-
-// FromContextError converts a context error into a Status.  It returns a
-// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is
-// non-nil and not a context error.
-func FromContextError(err error) *Status {
-	switch err {
-	case nil:
-		return New(codes.OK, "")
-	case context.DeadlineExceeded, netctx.DeadlineExceeded:
-		return New(codes.DeadlineExceeded, err.Error())
-	case context.Canceled, netctx.Canceled:
-		return New(codes.Canceled, err.Error())
-	default:
-		return New(codes.Unknown, err.Error())
-	}
-}
diff --git a/vendor/google.golang.org/grpc/status/status.go b/vendor/google.golang.org/grpc/status/status.go
index 9c61b0945..ed36681bb 100644
--- a/vendor/google.golang.org/grpc/status/status.go
+++ b/vendor/google.golang.org/grpc/status/status.go
@@ -28,6 +28,7 @@
 package status

 import (
+	"context"
 	"errors"
 	"fmt"

@@ -126,7 +127,9 @@ func FromError(err error) (s *Status, ok bool) {
 	if err == nil {
 		return &Status{s: &spb.Status{Code: int32(codes.OK)}}, true
 	}
-	if se, ok := err.(interface{ GRPCStatus() *Status }); ok {
+	if se, ok := err.(interface {
+		GRPCStatus() *Status
+	}); ok {
 		return se.GRPCStatus(), true
 	}
 	return New(codes.Unknown, err.Error()), false
@@ -182,8 +185,26 @@ func Code(err error) codes.Code {
 	if err == nil {
 		return codes.OK
 	}
-	if se, ok := err.(interface{ GRPCStatus() *Status }); ok {
+	if se, ok := err.(interface {
+		GRPCStatus() *Status
+	}); ok {
 		return se.GRPCStatus().Code()
 	}
 	return codes.Unknown
 }
+
+// FromContextError converts a context error into a Status.  It returns a
+// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is
+// non-nil and not a context error.
+func FromContextError(err error) *Status {
+	switch err {
+	case nil:
+		return New(codes.OK, "")
+	case context.DeadlineExceeded:
+		return New(codes.DeadlineExceeded, err.Error())
+	case context.Canceled:
+		return New(codes.Canceled, err.Error())
+	default:
+		return New(codes.Unknown, err.Error())
+	}
+}
diff --git a/vendor/google.golang.org/grpc/stream.go b/vendor/google.golang.org/grpc/stream.go
index 65d45a1d9..ccf996b4b 100644
--- a/vendor/google.golang.org/grpc/stream.go
+++ b/vendor/google.golang.org/grpc/stream.go
@@ -19,6 +19,7 @@
 package grpc

 import (
+	"context"
 	"errors"
 	"io"
 	"math"
@@ -26,16 +27,18 @@ import (
 	"sync"
 	"time"

-	"golang.org/x/net/context"
 	"golang.org/x/net/trace"
 	"google.golang.org/grpc/balancer"
 	"google.golang.org/grpc/codes"
+	"google.golang.org/grpc/connectivity"
 	"google.golang.org/grpc/encoding"
 	"google.golang.org/grpc/grpclog"
+	"google.golang.org/grpc/internal/binarylog"
 	"google.golang.org/grpc/internal/channelz"
 	"google.golang.org/grpc/internal/grpcrand"
 	"google.golang.org/grpc/internal/transport"
 	"google.golang.org/grpc/metadata"
+	"google.golang.org/grpc/peer"
 	"google.golang.org/grpc/stats"
 	"google.golang.org/grpc/status"
 )
@@ -82,7 +85,8 @@ type ClientStream interface {
 	// stream.Recv has returned a non-nil error (including io.EOF).
 	Trailer() metadata.MD
 	// CloseSend closes the send direction of the stream. It closes the stream
-	// when non-nil error is met.
+	// when non-nil error is met. It is also not safe to call CloseSend
+	// concurrently with SendMsg.
 	CloseSend() error
 	// Context returns the context for this stream.
 	//
@@ -105,7 +109,8 @@ type ClientStream interface {
 	//
 	// It is safe to have a goroutine calling SendMsg and another goroutine
 	// calling RecvMsg on the same stream at the same time, but it is not safe
-	// to call SendMsg on the same stream in different goroutines.
+	// to call SendMsg on the same stream in different goroutines. It is also
+	// not safe to call CloseSend concurrently with SendMsg.
 	SendMsg(m interface{}) error
 	// RecvMsg blocks until it receives a message into m or the stream is
 	// done. It returns io.EOF when the stream completes successfully. On
@@ -160,6 +165,11 @@ func newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, meth
 		}()
 	}
 	c := defaultCallInfo()
+	// Provide an opportunity for the first RPC to see the first service config
+	// provided by the resolver.
+	if err := cc.waitForResolvedAddrs(ctx); err != nil {
+		return nil, err
+	}
 	mc := cc.GetMethodConfig(method)
 	if mc.WaitForReady != nil {
 		c.failFast = !*mc.WaitForReady
@@ -225,7 +235,7 @@ func newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, meth
 		trInfo.tr = trace.New("grpc.Sent."+methodFamily(method), method)
 		trInfo.firstLine.client = true
 		if deadline, ok := ctx.Deadline(); ok {
-			trInfo.firstLine.deadline = deadline.Sub(time.Now())
+			trInfo.firstLine.deadline = time.Until(deadline)
 		}
 		trInfo.tr.LazyLog(&trInfo.firstLine, false)
 		ctx = trace.NewContext(ctx, trInfo.tr)
@@ -262,6 +272,7 @@ func newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, meth
 	if !cc.dopts.disableRetry {
 		cs.retryThrottler = cc.retryThrottler.Load().(*retryThrottler)
 	}
+	cs.binlog = binarylog.GetMethodLogger(method)

 	cs.callInfo.stream = cs
 	// Only this initial attempt has stats/tracing.
@@ -277,6 +288,23 @@ func newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, meth
 		return nil, err
 	}

+	if cs.binlog != nil {
+		md, _ := metadata.FromOutgoingContext(ctx)
+		logEntry := &binarylog.ClientHeader{
+			OnClientSide: true,
+			Header:       md,
+			MethodName:   method,
+			Authority:    cs.cc.authority,
+		}
+		if deadline, ok := ctx.Deadline(); ok {
+			logEntry.Timeout = time.Until(deadline)
+			if logEntry.Timeout < 0 {
+				logEntry.Timeout = 0
+			}
+		}
+		cs.binlog.Log(logEntry)
+	}
+
 	if desc != unaryStreamDesc {
 		// Listen on cc and stream contexts to cleanup when the user closes the
 		// ClientConn or cancels the stream context.  In all other cases, an error
@@ -350,6 +378,15 @@ type clientStream struct {

 	retryThrottler *retryThrottler // The throttler active when the RPC began.

+	binlog *binarylog.MethodLogger // Binary logger, can be nil.
+	// serverHeaderBinlogged is a boolean for whether server header has been
+	// logged. Server header will be logged when the first time one of those
+	// happens: stream.Header(), stream.Recv().
+	//
+	// It's only read and used by Recv() and Header(), so it doesn't need to be
+	// synchronized.
+	serverHeaderBinlogged bool
+
 	mu                      sync.Mutex
 	firstAttempt            bool       // if true, transparent retry is valid
 	numRetries              int        // exclusive of transparent retry attempt(s)
@@ -425,10 +462,7 @@ func (cs *clientStream) shouldRetry(err error) error {
 	pushback := 0
 	hasPushback := false
 	if cs.attempt.s != nil {
-		if to, toErr := cs.attempt.s.TrailersOnly(); toErr != nil {
-			// Context error; stop now.
-			return toErr
-		} else if !to {
+		if to, toErr := cs.attempt.s.TrailersOnly(); toErr != nil || !to {
 			return err
 		}

@@ -561,6 +595,20 @@ func (cs *clientStream) Header() (metadata.MD, error) {
 	}, cs.commitAttemptLocked)
 	if err != nil {
 		cs.finish(err)
+		return nil, err
+	}
+	if cs.binlog != nil && !cs.serverHeaderBinlogged {
+		// Only log if binary log is on and header has not been logged.
+		logEntry := &binarylog.ServerHeader{
+			OnClientSide: true,
+			Header:       m,
+			PeerAddr:     nil,
+		}
+		if peer, ok := peer.FromContext(cs.Context()); ok {
+			logEntry.PeerAddr = peer.Addr
+		}
+		cs.binlog.Log(logEntry)
+		cs.serverHeaderBinlogged = true
 	}
 	return m, err
 }
@@ -633,6 +681,7 @@ func (cs *clientStream) SendMsg(m interface{}) (err error) {
 	if len(payload) > *cs.callInfo.maxSendMessageSize {
 		return status.Errorf(codes.ResourceExhausted, "trying to send message larger than max (%d vs. %d)", len(payload), *cs.callInfo.maxSendMessageSize)
 	}
+	msgBytes := data // Store the pointer before setting to nil. For binary logging.
 	op := func(a *csAttempt) error {
 		err := a.sendMsg(m, hdr, payload, data)
 		// nil out the message and uncomp when replaying; they are only needed for
@@ -640,16 +689,53 @@ func (cs *clientStream) SendMsg(m interface{}) (err error) {
 		m, data = nil, nil
 		return err
 	}
-	return cs.withRetry(op, func() { cs.bufferForRetryLocked(len(hdr)+len(payload), op) })
+	err = cs.withRetry(op, func() { cs.bufferForRetryLocked(len(hdr)+len(payload), op) })
+	if cs.binlog != nil && err == nil {
+		cs.binlog.Log(&binarylog.ClientMessage{
+			OnClientSide: true,
+			Message:      msgBytes,
+		})
+	}
+	return
 }

 func (cs *clientStream) RecvMsg(m interface{}) error {
+	if cs.binlog != nil && !cs.serverHeaderBinlogged {
+		// Call Header() to binary log header if it's not already logged.
+		cs.Header()
+	}
+	var recvInfo *payloadInfo
+	if cs.binlog != nil {
+		recvInfo = &payloadInfo{}
+	}
 	err := cs.withRetry(func(a *csAttempt) error {
-		return a.recvMsg(m)
+		return a.recvMsg(m, recvInfo)
 	}, cs.commitAttemptLocked)
+	if cs.binlog != nil && err == nil {
+		cs.binlog.Log(&binarylog.ServerMessage{
+			OnClientSide: true,
+			Message:      recvInfo.uncompressedBytes,
+		})
+	}
 	if err != nil || !cs.desc.ServerStreams {
 		// err != nil or non-server-streaming indicates end of stream.
 		cs.finish(err)
+
+		if cs.binlog != nil {
+			// finish will not log Trailer. Log Trailer here.
+			logEntry := &binarylog.ServerTrailer{
+				OnClientSide: true,
+				Trailer:      cs.Trailer(),
+				Err:          err,
+			}
+			if logEntry.Err == io.EOF {
+				logEntry.Err = nil
+			}
+			if peer, ok := peer.FromContext(cs.Context()); ok {
+				logEntry.PeerAddr = peer.Addr
+			}
+			cs.binlog.Log(logEntry)
+		}
 	}
 	return err
 }
@@ -660,8 +746,20 @@ func (cs *clientStream) CloseSend() error {
 		return nil
 	}
 	cs.sentLast = true
-	op := func(a *csAttempt) error { return a.t.Write(a.s, nil, nil, &transport.Options{Last: true}) }
+	op := func(a *csAttempt) error {
+		a.t.Write(a.s, nil, nil, &transport.Options{Last: true})
+		// Always return nil; io.EOF is the only error that might make sense
+		// instead, but there is no need to signal the client to call RecvMsg
+		// as the only use left for the stream after CloseSend is to call
+		// RecvMsg.  This also matches historical behavior.
+		return nil
+	}
 	cs.withRetry(op, func() { cs.bufferForRetryLocked(0, op) })
+	if cs.binlog != nil {
+		cs.binlog.Log(&binarylog.ClientHalfClose{
+			OnClientSide: true,
+		})
+	}
 	// We never returned an error here for reasons.
 	return nil
 }
@@ -679,6 +777,16 @@ func (cs *clientStream) finish(err error) {
 	cs.finished = true
 	cs.commitAttemptLocked()
 	cs.mu.Unlock()
+	// For binary logging. only log cancel in finish (could be caused by RPC ctx
+	// canceled or ClientConn closed). Trailer will be logged in RecvMsg.
+	//
+	// Only one of cancel or trailer needs to be logged. In the cases where
+	// users don't call RecvMsg, users must have already canceled the RPC.
+	if cs.binlog != nil && status.Code(err) == codes.Canceled {
+		cs.binlog.Log(&binarylog.Cancel{
+			OnClientSide: true,
+		})
+	}
 	if err == nil {
 		cs.retryThrottler.successfulRPC()
 	}
@@ -728,14 +836,12 @@ func (a *csAttempt) sendMsg(m interface{}, hdr, payld, data []byte) error {
 	return nil
 }

-func (a *csAttempt) recvMsg(m interface{}) (err error) {
+func (a *csAttempt) recvMsg(m interface{}, payInfo *payloadInfo) (err error) {
 	cs := a.cs
-	var inPayload *stats.InPayload
-	if a.statsHandler != nil {
-		inPayload = &stats.InPayload{
-			Client: true,
-		}
+	if a.statsHandler != nil && payInfo == nil {
+		payInfo = &payloadInfo{}
 	}
+
 	if !a.decompSet {
 		// Block until we receive headers containing received message encoding.
 		if ct := a.s.RecvCompress(); ct != "" && ct != encoding.Identity {
@@ -752,7 +858,7 @@ func (a *csAttempt) recvMsg(m interface{}) (err error) {
 		// Only initialize this state once per stream.
 		a.decompSet = true
 	}
-	err = recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, inPayload, a.decomp)
+	err = recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, payInfo, a.decomp)
 	if err != nil {
 		if err == io.EOF {
 			if statusErr := a.s.Status().Err(); statusErr != nil {
@@ -769,8 +875,15 @@ func (a *csAttempt) recvMsg(m interface{}) (err error) {
 		}
 		a.mu.Unlock()
 	}
-	if inPayload != nil {
-		a.statsHandler.HandleRPC(cs.ctx, inPayload)
+	if a.statsHandler != nil {
+		a.statsHandler.HandleRPC(cs.ctx, &stats.InPayload{
+			Client:   true,
+			RecvTime: time.Now(),
+			Payload:  m,
+			// TODO truncate large payload.
+			Data:   payInfo.uncompressedBytes,
+			Length: len(payInfo.uncompressedBytes),
+		})
 	}
 	if channelz.IsOn() {
 		a.t.IncrMsgRecv()
@@ -779,7 +892,6 @@ func (a *csAttempt) recvMsg(m interface{}) (err error) {
 		// Subsequent messages should be received by subsequent RecvMsg calls.
 		return nil
 	}
-
 	// Special handling for non-server-stream rpcs.
 	// This recv expects EOF or errors, so we don't collect inPayload.
 	err = recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, nil, a.decomp)
@@ -809,11 +921,14 @@ func (a *csAttempt) finish(err error) {

 	if a.done != nil {
 		br := false
+		var tr metadata.MD
 		if a.s != nil {
 			br = a.s.BytesReceived()
+			tr = a.s.Trailer()
 		}
 		a.done(balancer.DoneInfo{
 			Err:           err,
+			Trailer:       tr,
 			BytesSent:     a.s != nil,
 			BytesReceived: br,
 		})
@@ -840,6 +955,298 @@ func (a *csAttempt) finish(err error) {
 	a.mu.Unlock()
 }

+func (ac *addrConn) newClientStream(ctx context.Context, desc *StreamDesc, method string, t transport.ClientTransport, opts ...CallOption) (_ ClientStream, err error) {
+	ac.mu.Lock()
+	if ac.transport != t {
+		ac.mu.Unlock()
+		return nil, status.Error(codes.Canceled, "the provided transport is no longer valid to use")
+	}
+	// transition to CONNECTING state when an attempt starts
+	if ac.state != connectivity.Connecting {
+		ac.updateConnectivityState(connectivity.Connecting)
+		ac.cc.handleSubConnStateChange(ac.acbw, ac.state)
+	}
+	ac.mu.Unlock()
+
+	if t == nil {
+		// TODO: return RPC error here?
+		return nil, errors.New("transport provided is nil")
+	}
+	// defaultCallInfo contains unnecessary info(i.e. failfast, maxRetryRPCBufferSize), so we just initialize an empty struct.
+	c := &callInfo{}
+
+	for _, o := range opts {
+		if err := o.before(c); err != nil {
+			return nil, toRPCErr(err)
+		}
+	}
+	c.maxReceiveMessageSize = getMaxSize(nil, c.maxReceiveMessageSize, defaultClientMaxReceiveMessageSize)
+	c.maxSendMessageSize = getMaxSize(nil, c.maxSendMessageSize, defaultServerMaxSendMessageSize)
+
+	// Possible context leak:
+	// The cancel function for the child context we create will only be called
+	// when RecvMsg returns a non-nil error, if the ClientConn is closed, or if
+	// an error is generated by SendMsg.
+	// https://github.com/grpc/grpc-go/issues/1818.
+	ctx, cancel := context.WithCancel(ctx)
+	defer func() {
+		if err != nil {
+			cancel()
+		}
+	}()
+
+	if err := setCallInfoCodec(c); err != nil {
+		return nil, err
+	}
+
+	callHdr := &transport.CallHdr{
+		Host:           ac.cc.authority,
+		Method:         method,
+		ContentSubtype: c.contentSubtype,
+	}
+
+	// Set our outgoing compression according to the UseCompressor CallOption, if
+	// set.  In that case, also find the compressor from the encoding package.
+	// Otherwise, use the compressor configured by the WithCompressor DialOption,
+	// if set.
+	var cp Compressor
+	var comp encoding.Compressor
+	if ct := c.compressorType; ct != "" {
+		callHdr.SendCompress = ct
+		if ct != encoding.Identity {
+			comp = encoding.GetCompressor(ct)
+			if comp == nil {
+				return nil, status.Errorf(codes.Internal, "grpc: Compressor is not installed for requested grpc-encoding %q", ct)
+			}
+		}
+	} else if ac.cc.dopts.cp != nil {
+		callHdr.SendCompress = ac.cc.dopts.cp.Type()
+		cp = ac.cc.dopts.cp
+	}
+	if c.creds != nil {
+		callHdr.Creds = c.creds
+	}
+
+	as := &addrConnStream{
+		callHdr:  callHdr,
+		ac:       ac,
+		ctx:      ctx,
+		cancel:   cancel,
+		opts:     opts,
+		callInfo: c,
+		desc:     desc,
+		codec:    c.codec,
+		cp:       cp,
+		comp:     comp,
+		t:        t,
+	}
+
+	as.callInfo.stream = as
+	s, err := as.t.NewStream(as.ctx, as.callHdr)
+	if err != nil {
+		err = toRPCErr(err)
+		return nil, err
+	}
+	as.s = s
+	as.p = &parser{r: s}
+	ac.incrCallsStarted()
+	if desc != unaryStreamDesc {
+		// Listen on cc and stream contexts to cleanup when the user closes the
+		// ClientConn or cancels the stream context.  In all other cases, an error
+		// should already be injected into the recv buffer by the transport, which
+		// the client will eventually receive, and then we will cancel the stream's
+		// context in clientStream.finish.
+		go func() {
+			select {
+			case <-ac.ctx.Done():
+				as.finish(status.Error(codes.Canceled, "grpc: the SubConn is closing"))
+			case <-ctx.Done():
+				as.finish(toRPCErr(ctx.Err()))
+			}
+		}()
+	}
+	return as, nil
+}
+
+type addrConnStream struct {
+	s         *transport.Stream
+	ac        *addrConn
+	callHdr   *transport.CallHdr
+	cancel    context.CancelFunc
+	opts      []CallOption
+	callInfo  *callInfo
+	t         transport.ClientTransport
+	ctx       context.Context
+	sentLast  bool
+	desc      *StreamDesc
+	codec     baseCodec
+	cp        Compressor
+	comp      encoding.Compressor
+	decompSet bool
+	dc        Decompressor
+	decomp    encoding.Compressor
+	p         *parser
+	mu        sync.Mutex
+	finished  bool
+}
+
+func (as *addrConnStream) Header() (metadata.MD, error) {
+	m, err := as.s.Header()
+	if err != nil {
+		as.finish(toRPCErr(err))
+	}
+	return m, err
+}
+
+func (as *addrConnStream) Trailer() metadata.MD {
+	return as.s.Trailer()
+}
+
+func (as *addrConnStream) CloseSend() error {
+	if as.sentLast {
+		// TODO: return an error and finish the stream instead, due to API misuse?
+		return nil
+	}
+	as.sentLast = true
+
+	as.t.Write(as.s, nil, nil, &transport.Options{Last: true})
+	// Always return nil; io.EOF is the only error that might make sense
+	// instead, but there is no need to signal the client to call RecvMsg
+	// as the only use left for the stream after CloseSend is to call
+	// RecvMsg.  This also matches historical behavior.
+	return nil
+}
+
+func (as *addrConnStream) Context() context.Context {
+	return as.s.Context()
+}
+
+func (as *addrConnStream) SendMsg(m interface{}) (err error) {
+	defer func() {
+		if err != nil && err != io.EOF {
+			// Call finish on the client stream for errors generated by this SendMsg
+			// call, as these indicate problems created by this client.  (Transport
+			// errors are converted to an io.EOF error in csAttempt.sendMsg; the real
+			// error will be returned from RecvMsg eventually in that case, or be
+			// retried.)
+			as.finish(err)
+		}
+	}()
+	if as.sentLast {
+		return status.Errorf(codes.Internal, "SendMsg called after CloseSend")
+	}
+	if !as.desc.ClientStreams {
+		as.sentLast = true
+	}
+	data, err := encode(as.codec, m)
+	if err != nil {
+		return err
+	}
+	compData, err := compress(data, as.cp, as.comp)
+	if err != nil {
+		return err
+	}
+	hdr, payld := msgHeader(data, compData)
+	// TODO(dfawley): should we be checking len(data) instead?
+	if len(payld) > *as.callInfo.maxSendMessageSize {
+		return status.Errorf(codes.ResourceExhausted, "trying to send message larger than max (%d vs. %d)", len(payld), *as.callInfo.maxSendMessageSize)
+	}
+
+	if err := as.t.Write(as.s, hdr, payld, &transport.Options{Last: !as.desc.ClientStreams}); err != nil {
+		if !as.desc.ClientStreams {
+			// For non-client-streaming RPCs, we return nil instead of EOF on error
+			// because the generated code requires it.  finish is not called; RecvMsg()
+			// will call it with the stream's status independently.
+			return nil
+		}
+		return io.EOF
+	}
+
+	if channelz.IsOn() {
+		as.t.IncrMsgSent()
+	}
+	return nil
+}
+
+func (as *addrConnStream) RecvMsg(m interface{}) (err error) {
+	defer func() {
+		if err != nil || !as.desc.ServerStreams {
+			// err != nil or non-server-streaming indicates end of stream.
+			as.finish(err)
+		}
+	}()
+
+	if !as.decompSet {
+		// Block until we receive headers containing received message encoding.
+		if ct := as.s.RecvCompress(); ct != "" && ct != encoding.Identity {
+			if as.dc == nil || as.dc.Type() != ct {
+				// No configured decompressor, or it does not match the incoming
+				// message encoding; attempt to find a registered compressor that does.
+				as.dc = nil
+				as.decomp = encoding.GetCompressor(ct)
+			}
+		} else {
+			// No compression is used; disable our decompressor.
+			as.dc = nil
+		}
+		// Only initialize this state once per stream.
+		as.decompSet = true
+	}
+	err = recv(as.p, as.codec, as.s, as.dc, m, *as.callInfo.maxReceiveMessageSize, nil, as.decomp)
+	if err != nil {
+		if err == io.EOF {
+			if statusErr := as.s.Status().Err(); statusErr != nil {
+				return statusErr
+			}
+			return io.EOF // indicates successful end of stream.
+		}
+		return toRPCErr(err)
+	}
+
+	if channelz.IsOn() {
+		as.t.IncrMsgRecv()
+	}
+	if as.desc.ServerStreams {
+		// Subsequent messages should be received by subsequent RecvMsg calls.
+		return nil
+	}
+
+	// Special handling for non-server-stream rpcs.
+	// This recv expects EOF or errors, so we don't collect inPayload.
+	err = recv(as.p, as.codec, as.s, as.dc, m, *as.callInfo.maxReceiveMessageSize, nil, as.decomp)
+	if err == nil {
+		return toRPCErr(errors.New("grpc: client streaming protocol violation: get <nil>, want <EOF>"))
+	}
+	if err == io.EOF {
+		return as.s.Status().Err() // non-server streaming Recv returns nil on success
+	}
+	return toRPCErr(err)
+}
+
+func (as *addrConnStream) finish(err error) {
+	as.mu.Lock()
+	if as.finished {
+		as.mu.Unlock()
+		return
+	}
+	as.finished = true
+	if err == io.EOF {
+		// Ending a stream with EOF indicates a success.
+		err = nil
+	}
+	if as.s != nil {
+		as.t.CloseStream(as.s, err)
+	}
+
+	if err != nil {
+		as.ac.incrCallsFailed()
+	} else {
+		as.ac.incrCallsSucceeded()
+	}
+	as.cancel()
+	as.mu.Unlock()
+}
+
 // ServerStream defines the server-side behavior of a streaming RPC.
 //
 // All errors returned from ServerStream methods are compatible with the
@@ -906,6 +1313,15 @@ type serverStream struct {

 	statsHandler stats.Handler

+	binlog *binarylog.MethodLogger
+	// serverHeaderBinlogged indicates whether server header has been logged. It
+	// will happen when one of the following two happens: stream.SendHeader(),
+	// stream.Send().
+	//
+	// It's only checked in send and sendHeader, doesn't need to be
+	// synchronized.
+	serverHeaderBinlogged bool
+
 	mu sync.Mutex // protects trInfo.tr after the service handler runs.
 }

@@ -921,7 +1337,15 @@ func (ss *serverStream) SetHeader(md metadata.MD) error {
 }

 func (ss *serverStream) SendHeader(md metadata.MD) error {
-	return ss.t.WriteHeader(ss.s, md)
+	err := ss.t.WriteHeader(ss.s, md)
+	if ss.binlog != nil && !ss.serverHeaderBinlogged {
+		h, _ := ss.s.Header()
+		ss.binlog.Log(&binarylog.ServerHeader{
+			Header: h,
+		})
+		ss.serverHeaderBinlogged = true
+	}
+	return err
 }

 func (ss *serverStream) SetTrailer(md metadata.MD) {
@@ -948,6 +1372,12 @@ func (ss *serverStream) SendMsg(m interface{}) (err error) {
 		if err != nil && err != io.EOF {
 			st, _ := status.FromError(toRPCErr(err))
 			ss.t.WriteStatus(ss.s, st)
+			// Non-user specified status was sent out. This should be an error
+			// case (as a server side Cancel maybe).
+			//
+			// This is not handled specifically now. User will return a final
+			// status from the service handler, we will log that error instead.
+			// This behavior is similar to an interceptor.
 		}
 		if channelz.IsOn() && err == nil {
 			ss.t.IncrMsgSent()
@@ -969,6 +1399,18 @@ func (ss *serverStream) SendMsg(m interface{}) (err error) {
 	if err := ss.t.Write(ss.s, hdr, payload, &transport.Options{Last: false}); err != nil {
 		return toRPCErr(err)
 	}
+	if ss.binlog != nil {
+		if !ss.serverHeaderBinlogged {
+			h, _ := ss.s.Header()
+			ss.binlog.Log(&binarylog.ServerHeader{
+				Header: h,
+			})
+			ss.serverHeaderBinlogged = true
+		}
+		ss.binlog.Log(&binarylog.ServerMessage{
+			Message: data,
+		})
+	}
 	if ss.statsHandler != nil {
 		ss.statsHandler.HandleRPC(ss.s.Context(), outPayload(false, m, data, payload, time.Now()))
 	}
@@ -992,17 +1434,26 @@ func (ss *serverStream) RecvMsg(m interface{}) (err error) {
 		if err != nil && err != io.EOF {
 			st, _ := status.FromError(toRPCErr(err))
 			ss.t.WriteStatus(ss.s, st)
+			// Non-user specified status was sent out. This should be an error
+			// case (as a server side Cancel maybe).
+			//
+			// This is not handled specifically now. User will return a final
+			// status from the service handler, we will log that error instead.
+			// This behavior is similar to an interceptor.
 		}
 		if channelz.IsOn() && err == nil {
 			ss.t.IncrMsgRecv()
 		}
 	}()
-	var inPayload *stats.InPayload
-	if ss.statsHandler != nil {
-		inPayload = &stats.InPayload{}
+	var payInfo *payloadInfo
+	if ss.statsHandler != nil || ss.binlog != nil {
+		payInfo = &payloadInfo{}
 	}
-	if err := recv(ss.p, ss.codec, ss.s, ss.dc, m, ss.maxReceiveMessageSize, inPayload, ss.decomp); err != nil {
+	if err := recv(ss.p, ss.codec, ss.s, ss.dc, m, ss.maxReceiveMessageSize, payInfo, ss.decomp); err != nil {
 		if err == io.EOF {
+			if ss.binlog != nil {
+				ss.binlog.Log(&binarylog.ClientHalfClose{})
+			}
 			return err
 		}
 		if err == io.ErrUnexpectedEOF {
@@ -1010,8 +1461,19 @@ func (ss *serverStream) RecvMsg(m interface{}) (err error) {
 		}
 		return toRPCErr(err)
 	}
-	if inPayload != nil {
-		ss.statsHandler.HandleRPC(ss.s.Context(), inPayload)
+	if ss.statsHandler != nil {
+		ss.statsHandler.HandleRPC(ss.s.Context(), &stats.InPayload{
+			RecvTime: time.Now(),
+			Payload:  m,
+			// TODO truncate large payload.
+			Data:   payInfo.uncompressedBytes,
+			Length: len(payInfo.uncompressedBytes),
+		})
+	}
+	if ss.binlog != nil {
+		ss.binlog.Log(&binarylog.ClientMessage{
+			Message: payInfo.uncompressedBytes,
+		})
 	}
 	return nil
 }
diff --git a/vendor/google.golang.org/grpc/tap/tap.go b/vendor/google.golang.org/grpc/tap/tap.go
index 22b8fb50d..584360f68 100644
--- a/vendor/google.golang.org/grpc/tap/tap.go
+++ b/vendor/google.golang.org/grpc/tap/tap.go
@@ -21,7 +21,7 @@
 package tap

 import (
-	"golang.org/x/net/context"
+	"context"
 )

 // Info defines the relevant information needed by the handles.
diff --git a/vendor/google.golang.org/grpc/version.go b/vendor/google.golang.org/grpc/version.go
index 8ee619bf6..c30e84c00 100644
--- a/vendor/google.golang.org/grpc/version.go
+++ b/vendor/google.golang.org/grpc/version.go
@@ -19,4 +19,4 @@
 package grpc

 // Version is the current grpc version.
-const Version = "1.15.0"
+const Version = "1.19.0"
--
2.21.0 (Apple Git-122.2)

