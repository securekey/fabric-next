From 3e443d93e859591183100964867868b8022c60a4 Mon Sep 17 00:00:00 2001
From: Baha Shaaban <baha.shaaban@securekey.com>
Date: Wed, 15 May 2019 16:57:45 -0400
Subject: [PATCH] Backport transient data to Fabric 1.4.1

Change-Id: Idf59b3915d2d14e1411747c258b673bc5d9ebc15
Signed-off-by: Baha Shaaban <baha.shaaban@securekey.com>
---
 Gopkg.lock                                         |   17 +
 Gopkg.toml                                         |    4 +
 common/util/retry/retry.go                         |   91 ++
 common/util/retry/retry_test.go                    |   74 ++
 .../kvledger/txmgmt/rwsetutil/rwset_builder.go     |    2 +-
 .../kvledger/txmgmt/rwsetutil/rwset_proto_util.go  |    5 +-
 core/ledger/ledger_interface.go                    |    2 +
 core/ledger/ledgermgmt/ledger_mgmt.go              |    3 +
 core/ledger/pvtdatastorage/store.go                |   15 +
 core/ledger/util/couchdb/couchdb_ext.go            |  153 +++
 core/ledger/util/couchdb/couchdb_ext_test.go       |  172 ++++
 core/ledger/util/couchdb/couchdbutil_ext.go        |   57 ++
 core/ledger/util/couchdb/couchdbutil_ext_test.go   |   28 +
 core/peer/peer.go                                  |   47 +-
 core/peer/peer_impl.go                             |   10 +-
 .../collections/api/dissemination/dissemination.go |   18 +
 extensions/collections/api/offledger/provider.go   |   56 ++
 extensions/collections/api/store/key.go            |   57 ++
 extensions/collections/api/store/provider.go       |   91 ++
 extensions/collections/api/support/support.go      |   23 +
 .../collections/api/transientdata/provider.go      |   52 +
 .../collections/dissemination/disseminationplan.go |   72 ++
 .../dissemination/disseminationplan_test.go        |  101 ++
 extensions/collections/offledger/client/client.go  |  269 +++++
 .../collections/offledger/client/client_test.go    |  205 ++++
 .../collections/offledger/client/test_exports.go   |   38 +
 extensions/collections/offledger/dcas/dcas.go      |   54 +
 extensions/collections/offledger/dcas/dcas_test.go |   83 ++
 .../offledger/dcas/dcasclient/dcasclient.go        |   51 +
 .../offledger/dcas/dcasclient/dcasclient_test.go   |  126 +++
 .../offledger/dissemination/disseminationplan.go   |  100 ++
 .../offledger/dissemination/disseminator.go        |  120 +++
 .../offledger/dissemination/disseminator_test.go   |  300 ++++++
 .../collections/offledger/mocks/mockprovider.go    |   40 +
 extensions/collections/offledger/olprovider.go     |  402 ++++++++
 .../collections/offledger/olprovider_test.go       |  341 +++++++
 .../collections/offledger/storeprovider/olstore.go |  316 ++++++
 .../offledger/storeprovider/olstore_test.go        |  427 ++++++++
 .../offledger/storeprovider/olstoreprovider.go     |  108 ++
 .../storeprovider/olstoreprovider_test.go          |   90 ++
 .../offledger/storeprovider/store/api/api.go       |   61 ++
 .../offledger/storeprovider/store/cache/cache.go   |  159 +++
 .../storeprovider/store/cache/cache_test.go        |  276 ++++++
 .../storeprovider/store/couchdbstore/dbstore.go    |  212 ++++
 .../store/couchdbstore/dbstore_provider.go         |  145 +++
 .../store/couchdbstore/dbstore_test.go             |  134 +++
 .../storeprovider/store/leveldbstore/dbstore.go    |  136 +++
 .../store/leveldbstore/dbstore_provider.go         |  117 +++
 .../store/leveldbstore/dbstore_test.go             |  127 +++
 .../storeprovider/store/mocks/mockdbprovider.go    |  142 +++
 extensions/collections/policy/validator.go         |  133 +++
 extensions/collections/policy/validator_test.go    |  217 ++++
 .../collections/pvtdatahandler/pvtdatahandler.go   |  156 +++
 .../pvtdatahandler/pvtdatahandler_test.go          |  104 ++
 extensions/collections/retriever/retriever.go      |  115 +++
 extensions/collections/retriever/retriever_test.go |   63 ++
 .../collections/storeprovider/mocks/mockolstore.go |  129 +++
 .../storeprovider/mocks/mocktransientdatastore.go  |  119 +++
 extensions/collections/storeprovider/store.go      |   75 ++
 .../collections/storeprovider/storeprovider.go     |   87 ++
 .../storeprovider/storeprovider_test.go            |  175 ++++
 .../dissemination/disseminationplan.go             |   94 ++
 .../transientdata/dissemination/disseminator.go    |  130 +++
 .../dissemination/disseminator_test.go             |  317 ++++++
 .../transientdata/mocks/mockprovider.go            |   40 +
 .../transientdata/storeprovider/store/api/api.go   |   30 +
 .../storeprovider/store/cache/cache.go             |  151 +++
 .../storeprovider/store/cache/cache_test.go        |  320 ++++++
 .../storeprovider/store/dbstore/dbstore.go         |  130 +++
 .../store/dbstore/dbstore_provider.go              |   41 +
 .../storeprovider/store/dbstore/dbstore_test.go    |  113 +++
 .../transientdata/storeprovider/store/memstore.go  |  189 ++++
 .../storeprovider/store/memstore_test.go           |  218 ++++
 .../storeprovider/transientdatastore.go            |   56 ++
 .../storeprovider/transientdatastore_test.go       |   62 ++
 .../transientdata/transientdataprovider.go         |  371 +++++++
 .../transientdata/transientdataprovider_test.go    |  311 ++++++
 extensions/common/cas.go                           |   41 +
 extensions/common/cas_test.go                      |   18 +
 extensions/common/common.go                        |   88 ++
 extensions/common/common_test.go                   |  110 ++
 extensions/common/discovery/discovery.go           |  120 +++
 extensions/common/discovery/discovery_test.go      |  134 +++
 extensions/common/discovery/member.go              |   23 +
 extensions/common/discovery/member_test.go         |   98 ++
 extensions/common/discovery/peergroup.go           |  136 +++
 extensions/common/discovery/peergroup_test.go      |  103 ++
 extensions/common/discovery/peergroups.go          |   80 ++
 extensions/common/discovery/peergroups_test.go     |   61 ++
 extensions/common/discovery/peergroupsiterator.go  |   42 +
 .../common/discovery/peergroupsiterator_test.go    |   35 +
 extensions/common/discovery/permutations.go        |   69 ++
 extensions/common/discovery/permutations_test.go   |   85 ++
 extensions/common/multirequest/multirequest.go     |   97 ++
 .../common/multirequest/multirequest_test.go       |  154 +++
 extensions/common/requestmgr/requestmgr.go         |  207 ++++
 extensions/common/requestmgr/requestmgr_test.go    |  153 +++
 extensions/common/support/collconfigretriever.go   |  206 ++++
 .../common/support/collconfigretriever_test.go     |  171 ++++
 extensions/common/support/support.go               |   66 ++
 extensions/common/support/support_test.go          |   75 ++
 extensions/config/config.go                        |  397 ++++++++
 extensions/config/config_test.go                   |  472 +++++++++
 extensions/gossip/api/gossipapi.go                 |   44 +
 extensions/gossip/blockpublisher/blockpublisher.go |  511 ++++++++++
 .../gossip/blockpublisher/blockpublisher_test.go   |  310 ++++++
 extensions/gossip/coordinator/coordinator.go       |  159 +++
 extensions/gossip/coordinator/coordinator_test.go  |  176 ++++
 extensions/gossip/dispatcher/dispatcher.go         |  249 +++++
 extensions/gossip/dispatcher/dispatcher_test.go    |  420 ++++++++
 extensions/gossip/mocks/blockpublisher.go          |   51 +
 extensions/mocks/mockgossipadapter.go              |   94 ++
 .../cachedpvtdatastore/store_impl.go               |  274 +++++
 .../cachedpvtdatastore/store_impl_test.go          |  305 ++++++
 .../cachedpvtdatastore/test_exports.go             |   46 +
 .../pvtdatastorage/cdbpvtdatastore/couchdb_conv.go |  277 ++++++
 .../pvtdatastorage/cdbpvtdatastore/store_impl.go   |  828 ++++++++++++++++
 .../cdbpvtdatastore/store_impl_test.go             | 1047 ++++++++++++++++++++
 .../pvtdatastorage/cdbpvtdatastore/test_exports.go |   77 ++
 extensions/pvtdatastorage/common/collelgproc.go    |  139 +++
 extensions/pvtdatastorage/common/helper.go         |  235 +++++
 extensions/pvtdatastorage/common/kv_encoding.go    |  192 ++++
 extensions/pvtdatastorage/common/store.go          |  404 ++++++++
 extensions/pvtdatastorage/common/v11.go            |   78 ++
 extensions/pvtdatastorage/store_impl.go            |  205 ++++
 extensions/pvtdatastorage/store_impl_test.go       |  220 ++++
 extensions/pvtdatastorage/test_exports.go          |   77 ++
 extensions/testutil/ext_test_env.go                |   17 +
 .../transientstore/common/common_store_helper.go   |  168 ++++
 extensions/transientstore/store.go                 |  471 +++++++++
 extensions/transientstore/store_helper.go          |  190 ++++
 extensions/transientstore/store_test.go            |  648 ++++++++++++
 extensions/transientstore/storeprovider.go         |   33 +
 extensions/transientstore/transientstore_test.go   |  410 ++++++++
 gossip/privdata/coordinator.go                     |   19 +-
 gossip/privdata/coordinator_test.go                |   18 +
 gossip/privdata/dissemination.go                   |   33 +
 gossip/privdata/distributor.go                     |    3 +
 gossip/privdata/distributor_test.go                |    4 +
 gossip/service/gossip_service.go                   |   13 +-
 gossip/service/gossip_service_test.go              |   30 +-
 gossip/service/integration_test.go                 |    8 +-
 gossip/state/state.go                              |   23 +-
 gossip/state/state_test.go                         |    8 +-
 peer/node/start.go                                 |   15 +-
 protos/common/collection.pb.go                     |  147 ++-
 protos/common/collection.proto                     |   17 +
 protos/gossip/message.pb.go                        |  679 ++++++++++---
 protos/gossip/message.proto                        |   46 +-
 vendor/github.com/bluele/gcache/LICENSE            |   21 +
 vendor/github.com/bluele/gcache/arc.go             |  452 +++++++++
 vendor/github.com/bluele/gcache/cache.go           |  205 ++++
 vendor/github.com/bluele/gcache/clock.go           |   53 +
 vendor/github.com/bluele/gcache/lfu.go             |  335 +++++++
 vendor/github.com/bluele/gcache/lru.go             |  301 ++++++
 vendor/github.com/bluele/gcache/simple.go          |  289 ++++++
 vendor/github.com/bluele/gcache/singleflight.go    |   82 ++
 vendor/github.com/bluele/gcache/stats.go           |   53 +
 vendor/github.com/bluele/gcache/utils.go           |   15 +
 vendor/github.com/btcsuite/btcutil/LICENSE         |   16 +
 .../github.com/btcsuite/btcutil/base58/alphabet.go |   49 +
 .../github.com/btcsuite/btcutil/base58/base58.go   |   75 ++
 .../btcsuite/btcutil/base58/base58check.go         |   52 +
 vendor/github.com/btcsuite/btcutil/base58/doc.go   |   29 +
 .../btcsuite/btcutil/base58/genalphabet.go         |   79 ++
 165 files changed, 24200 insertions(+), 240 deletions(-)
 create mode 100644 common/util/retry/retry.go
 create mode 100644 common/util/retry/retry_test.go
 create mode 100644 core/ledger/util/couchdb/couchdb_ext.go
 create mode 100644 core/ledger/util/couchdb/couchdb_ext_test.go
 create mode 100644 core/ledger/util/couchdb/couchdbutil_ext.go
 create mode 100644 core/ledger/util/couchdb/couchdbutil_ext_test.go
 create mode 100644 extensions/collections/api/dissemination/dissemination.go
 create mode 100644 extensions/collections/api/offledger/provider.go
 create mode 100644 extensions/collections/api/store/key.go
 create mode 100644 extensions/collections/api/store/provider.go
 create mode 100644 extensions/collections/api/support/support.go
 create mode 100644 extensions/collections/api/transientdata/provider.go
 create mode 100644 extensions/collections/dissemination/disseminationplan.go
 create mode 100644 extensions/collections/dissemination/disseminationplan_test.go
 create mode 100644 extensions/collections/offledger/client/client.go
 create mode 100644 extensions/collections/offledger/client/client_test.go
 create mode 100644 extensions/collections/offledger/client/test_exports.go
 create mode 100644 extensions/collections/offledger/dcas/dcas.go
 create mode 100644 extensions/collections/offledger/dcas/dcas_test.go
 create mode 100644 extensions/collections/offledger/dcas/dcasclient/dcasclient.go
 create mode 100644 extensions/collections/offledger/dcas/dcasclient/dcasclient_test.go
 create mode 100644 extensions/collections/offledger/dissemination/disseminationplan.go
 create mode 100644 extensions/collections/offledger/dissemination/disseminator.go
 create mode 100644 extensions/collections/offledger/dissemination/disseminator_test.go
 create mode 100644 extensions/collections/offledger/mocks/mockprovider.go
 create mode 100644 extensions/collections/offledger/olprovider.go
 create mode 100644 extensions/collections/offledger/olprovider_test.go
 create mode 100644 extensions/collections/offledger/storeprovider/olstore.go
 create mode 100644 extensions/collections/offledger/storeprovider/olstore_test.go
 create mode 100644 extensions/collections/offledger/storeprovider/olstoreprovider.go
 create mode 100644 extensions/collections/offledger/storeprovider/olstoreprovider_test.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/api/api.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/cache/cache.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/cache/cache_test.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_test.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_provider.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_test.go
 create mode 100644 extensions/collections/offledger/storeprovider/store/mocks/mockdbprovider.go
 create mode 100644 extensions/collections/policy/validator.go
 create mode 100644 extensions/collections/policy/validator_test.go
 create mode 100644 extensions/collections/pvtdatahandler/pvtdatahandler.go
 create mode 100644 extensions/collections/pvtdatahandler/pvtdatahandler_test.go
 create mode 100644 extensions/collections/retriever/retriever.go
 create mode 100644 extensions/collections/retriever/retriever_test.go
 create mode 100644 extensions/collections/storeprovider/mocks/mockolstore.go
 create mode 100644 extensions/collections/storeprovider/mocks/mocktransientdatastore.go
 create mode 100644 extensions/collections/storeprovider/store.go
 create mode 100644 extensions/collections/storeprovider/storeprovider.go
 create mode 100644 extensions/collections/storeprovider/storeprovider_test.go
 create mode 100644 extensions/collections/transientdata/dissemination/disseminationplan.go
 create mode 100644 extensions/collections/transientdata/dissemination/disseminator.go
 create mode 100644 extensions/collections/transientdata/dissemination/disseminator_test.go
 create mode 100644 extensions/collections/transientdata/mocks/mockprovider.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/api/api.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/cache/cache.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/cache/cache_test.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/dbstore/dbstore.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_test.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/memstore.go
 create mode 100644 extensions/collections/transientdata/storeprovider/store/memstore_test.go
 create mode 100644 extensions/collections/transientdata/storeprovider/transientdatastore.go
 create mode 100644 extensions/collections/transientdata/storeprovider/transientdatastore_test.go
 create mode 100644 extensions/collections/transientdata/transientdataprovider.go
 create mode 100644 extensions/collections/transientdata/transientdataprovider_test.go
 create mode 100644 extensions/common/cas.go
 create mode 100644 extensions/common/cas_test.go
 create mode 100644 extensions/common/common.go
 create mode 100644 extensions/common/common_test.go
 create mode 100644 extensions/common/discovery/discovery.go
 create mode 100644 extensions/common/discovery/discovery_test.go
 create mode 100644 extensions/common/discovery/member.go
 create mode 100644 extensions/common/discovery/member_test.go
 create mode 100644 extensions/common/discovery/peergroup.go
 create mode 100644 extensions/common/discovery/peergroup_test.go
 create mode 100644 extensions/common/discovery/peergroups.go
 create mode 100644 extensions/common/discovery/peergroups_test.go
 create mode 100644 extensions/common/discovery/peergroupsiterator.go
 create mode 100644 extensions/common/discovery/peergroupsiterator_test.go
 create mode 100644 extensions/common/discovery/permutations.go
 create mode 100644 extensions/common/discovery/permutations_test.go
 create mode 100644 extensions/common/multirequest/multirequest.go
 create mode 100644 extensions/common/multirequest/multirequest_test.go
 create mode 100644 extensions/common/requestmgr/requestmgr.go
 create mode 100644 extensions/common/requestmgr/requestmgr_test.go
 create mode 100644 extensions/common/support/collconfigretriever.go
 create mode 100644 extensions/common/support/collconfigretriever_test.go
 create mode 100644 extensions/common/support/support.go
 create mode 100644 extensions/common/support/support_test.go
 create mode 100644 extensions/config/config.go
 create mode 100644 extensions/config/config_test.go
 create mode 100644 extensions/gossip/api/gossipapi.go
 create mode 100644 extensions/gossip/blockpublisher/blockpublisher.go
 create mode 100644 extensions/gossip/blockpublisher/blockpublisher_test.go
 create mode 100644 extensions/gossip/coordinator/coordinator.go
 create mode 100644 extensions/gossip/coordinator/coordinator_test.go
 create mode 100644 extensions/gossip/dispatcher/dispatcher.go
 create mode 100644 extensions/gossip/dispatcher/dispatcher_test.go
 create mode 100644 extensions/gossip/mocks/blockpublisher.go
 create mode 100644 extensions/mocks/mockgossipadapter.go
 create mode 100644 extensions/pvtdatastorage/cachedpvtdatastore/store_impl.go
 create mode 100644 extensions/pvtdatastorage/cachedpvtdatastore/store_impl_test.go
 create mode 100644 extensions/pvtdatastorage/cachedpvtdatastore/test_exports.go
 create mode 100644 extensions/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go
 create mode 100644 extensions/pvtdatastorage/cdbpvtdatastore/store_impl.go
 create mode 100644 extensions/pvtdatastorage/cdbpvtdatastore/store_impl_test.go
 create mode 100644 extensions/pvtdatastorage/cdbpvtdatastore/test_exports.go
 create mode 100644 extensions/pvtdatastorage/common/collelgproc.go
 create mode 100644 extensions/pvtdatastorage/common/helper.go
 create mode 100644 extensions/pvtdatastorage/common/kv_encoding.go
 create mode 100644 extensions/pvtdatastorage/common/store.go
 create mode 100644 extensions/pvtdatastorage/common/v11.go
 create mode 100644 extensions/pvtdatastorage/store_impl.go
 create mode 100644 extensions/pvtdatastorage/store_impl_test.go
 create mode 100644 extensions/pvtdatastorage/test_exports.go
 create mode 100644 extensions/testutil/ext_test_env.go
 create mode 100644 extensions/transientstore/common/common_store_helper.go
 create mode 100644 extensions/transientstore/store.go
 create mode 100644 extensions/transientstore/store_helper.go
 create mode 100644 extensions/transientstore/store_test.go
 create mode 100644 extensions/transientstore/storeprovider.go
 create mode 100644 extensions/transientstore/transientstore_test.go
 create mode 100644 gossip/privdata/dissemination.go
 create mode 100644 vendor/github.com/bluele/gcache/LICENSE
 create mode 100644 vendor/github.com/bluele/gcache/arc.go
 create mode 100644 vendor/github.com/bluele/gcache/cache.go
 create mode 100644 vendor/github.com/bluele/gcache/clock.go
 create mode 100644 vendor/github.com/bluele/gcache/lfu.go
 create mode 100644 vendor/github.com/bluele/gcache/lru.go
 create mode 100644 vendor/github.com/bluele/gcache/simple.go
 create mode 100644 vendor/github.com/bluele/gcache/singleflight.go
 create mode 100644 vendor/github.com/bluele/gcache/stats.go
 create mode 100644 vendor/github.com/bluele/gcache/utils.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/LICENSE
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/alphabet.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/base58.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/base58check.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/doc.go
 create mode 100644 vendor/github.com/btcsuite/btcutil/base58/genalphabet.go

diff --git a/Gopkg.lock b/Gopkg.lock
index b10513ea..2b7f213b 100644
--- a/Gopkg.lock
+++ b/Gopkg.lock
@@ -93,6 +93,21 @@
   pruneopts = "NUT"
   revision = "3a771d992973f24aa725d07868b467d1ddfceafb"
 
+[[projects]]
+  digest = "1:b870f4fc8ac5a04b3dbb6540e81a4c16da874e9f922dbe8762a90d9bcc294910"
+  name = "github.com/bluele/gcache"
+  packages = ["."]
+  pruneopts = "NUT"
+  revision = "79ae3b2d8680cbc7ad3dba9db66b8a648575221c"
+
+[[projects]]
+  branch = "master"
+  digest = "1:9af5b27cadd7fefde7d20dfc72470d39679337e972460a251c129062a70f8518"
+  name = "github.com/btcsuite/btcutil"
+  packages = ["base58"]
+  pruneopts = "NUT"
+  revision = "9e5f4b9a998d263e3ce9c56664a7816001ac8000"
+
 [[projects]]
   branch = "master"
   digest = "1:4a029051269e04c040c092eb4ddd92732f8f3a3921a8b43b82b30804e00f3357"
@@ -958,6 +973,8 @@
     "github.com/Knetic/govaluate",
     "github.com/Shopify/sarama",
     "github.com/Shopify/sarama/mocks",
+    "github.com/bluele/gcache",
+    "github.com/btcsuite/btcutil/base58",
     "github.com/davecgh/go-spew/spew",
     "github.com/fsouza/go-dockerclient",
     "github.com/go-kit/kit/metrics",
diff --git a/Gopkg.toml b/Gopkg.toml
index 8ee86e9c..1140d7f8 100644
--- a/Gopkg.toml
+++ b/Gopkg.toml
@@ -197,3 +197,7 @@ noverify = [
 [[constraint]]
   name = "github.com/prometheus/client_golang"
   version = "0.9.0"
+
+[[constraint]]
+  name = "github.com/bluele/gcache"
+  revision = "79ae3b2d8680cbc7ad3dba9db66b8a648575221c"
\ No newline at end of file
diff --git a/common/util/retry/retry.go b/common/util/retry/retry.go
new file mode 100644
index 00000000..c3344f05
--- /dev/null
+++ b/common/util/retry/retry.go
@@ -0,0 +1,91 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retry
+
+import (
+	"errors"
+	"time"
+)
+
+type retryOpts struct {
+	MaxAttempts    int
+	InitialBackoff time.Duration
+	BackoffFactor  float32
+	MaxBackoff     time.Duration
+	BeforeRetry    BeforeRetryHandler
+}
+
+// BeforeRetryHandler is a function that is invoked before a retry attemp.
+// Return true to perform the retry; false otherwise.
+type BeforeRetryHandler func(err error, attempt int, backoff time.Duration) bool
+
+// Opt is a retry option
+type Opt func(opts *retryOpts)
+
+// WithMaxAttempts sets the maximum number of retry attempts
+func WithMaxAttempts(value int) Opt {
+	return func(opts *retryOpts) {
+		opts.MaxAttempts = value
+	}
+}
+
+// WithBeforeRetry sets the handler to be invoked before a retry
+func WithBeforeRetry(value BeforeRetryHandler) Opt {
+	return func(opts *retryOpts) {
+		opts.BeforeRetry = value
+	}
+}
+
+// Invocation is the function to invoke on each attempt
+type Invocation func() (interface{}, error)
+
+// Invoke invokes the given invocation with the given retry options
+func Invoke(invoke Invocation, opts ...Opt) (interface{}, error) {
+	retryOpts := &retryOpts{
+		MaxAttempts:    5,
+		BackoffFactor:  1.5,
+		InitialBackoff: 250 * time.Millisecond,
+		MaxBackoff:     5 * time.Second,
+	}
+
+	// Apply the options
+	for _, opt := range opts {
+		opt(retryOpts)
+	}
+
+	if retryOpts.MaxAttempts == 0 {
+		return nil, errors.New("MaxAttempts must be greater than 0")
+	}
+
+	backoff := retryOpts.InitialBackoff
+	var lastErr error
+	var retVal interface{}
+	for i := 1; i <= retryOpts.MaxAttempts; i++ {
+		retVal, lastErr = invoke()
+		if lastErr == nil {
+			return retVal, nil
+		}
+
+		if i+1 < retryOpts.MaxAttempts {
+			backoff = time.Duration(float32(backoff) * retryOpts.BackoffFactor)
+			if backoff > retryOpts.MaxBackoff {
+				backoff = retryOpts.MaxBackoff
+			}
+
+			if retryOpts.BeforeRetry != nil {
+				if !retryOpts.BeforeRetry(lastErr, i, backoff) {
+					// No retry for this error
+					return nil, lastErr
+				}
+			}
+
+			time.Sleep(backoff)
+		}
+	}
+
+	return nil, lastErr
+}
diff --git a/common/util/retry/retry_test.go b/common/util/retry/retry_test.go
new file mode 100644
index 00000000..5e5be781
--- /dev/null
+++ b/common/util/retry/retry_test.go
@@ -0,0 +1,74 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retry
+
+import (
+	"testing"
+	"time"
+
+	"github.com/pkg/errors"
+	"github.com/stretchr/testify/require"
+)
+
+func TestWithMaxAttempt(t *testing.T) {
+	count := 0
+	_, err := Invoke(
+		func() (interface{}, error) {
+			count++
+			return nil, errors.New("error")
+
+		},
+		WithMaxAttempts(4),
+	)
+
+	require.Error(t, err)
+	require.Equal(t, 4, count)
+
+	count = 0
+	v, err := Invoke(
+		func() (interface{}, error) {
+			count++
+			if count == 4 {
+				return "success", nil
+			}
+			return nil, errors.New("error")
+
+		},
+		WithMaxAttempts(4),
+	)
+
+	require.NoError(t, err)
+	require.Equal(t, 4, count)
+	require.Equal(t, "success", v)
+}
+
+func TestWithBeforeRetry(t *testing.T) {
+	count := 0
+	_, err := Invoke(
+		func() (interface{}, error) {
+			count++
+			if count == 2 {
+				return nil, errors.New("noretry")
+			}
+			return nil, errors.New("retry")
+
+		},
+		WithBeforeRetry(func(err error, attempt int, backoff time.Duration) bool {
+			require.Error(t, err)
+			if err.Error() == "retry" {
+				return true
+			}
+			require.Equal(t, "noretry", err.Error())
+			require.Equal(t, 2, attempt)
+			return false
+		}),
+		WithMaxAttempts(4),
+	)
+
+	require.Error(t, err)
+	require.Equal(t, 2, count)
+}
diff --git a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go
index 83041964..b3e7b2cc 100644
--- a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go
+++ b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go
@@ -138,7 +138,7 @@ func (b *RWSetBuilder) GetTxSimulationResults() (*ledger.TxSimulationResults, er
 
 	// Populate the collection-level hashes into pub rwset and compute the proto bytes for pvt rwset
 	if pvtData != nil {
-		if pvtDataProto, err = pvtData.toProtoMsg(); err != nil {
+		if pvtDataProto, err = pvtData.ToProtoMsg(); err != nil {
 			return nil, err
 		}
 		for _, ns := range pvtDataProto.NsPvtRwset {
diff --git a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go
index 0851b1ab..c48fb86b 100644
--- a/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go
+++ b/core/ledger/kvledger/txmgmt/rwsetutil/rwset_proto_util.go
@@ -128,7 +128,7 @@ func (txRwSet *TxRwSet) FromProtoBytes(protoBytes []byte) error {
 func (txPvtRwSet *TxPvtRwSet) ToProtoBytes() ([]byte, error) {
 	var protoMsg *rwset.TxPvtReadWriteSet
 	var err error
-	if protoMsg, err = txPvtRwSet.toProtoMsg(); err != nil {
+	if protoMsg, err = txPvtRwSet.ToProtoMsg(); err != nil {
 		return nil, err
 	}
 	return proto.Marshal(protoMsg)
@@ -249,7 +249,8 @@ func (txRwSet *TxRwSet) NumCollections() int {
 // functions for private read-write set
 ///////////////////////////////////////////////////////////////////////////////
 
-func (txPvtRwSet *TxPvtRwSet) toProtoMsg() (*rwset.TxPvtReadWriteSet, error) {
+// ToProtoMsg returns a TxPvtReadWriteSet from the current TxPvtRwSet
+func (txPvtRwSet *TxPvtRwSet) ToProtoMsg() (*rwset.TxPvtReadWriteSet, error) {
 	protoMsg := &rwset.TxPvtReadWriteSet{DataModel: rwset.TxReadWriteSet_KV}
 	var nsProtoMsg *rwset.NsPvtReadWriteSet
 	var err error
diff --git a/core/ledger/ledger_interface.go b/core/ledger/ledger_interface.go
index 929df1d5..69a54984 100644
--- a/core/ledger/ledger_interface.go
+++ b/core/ledger/ledger_interface.go
@@ -13,6 +13,7 @@ import (
 	"github.com/hyperledger/fabric-lib-go/healthz"
 	commonledger "github.com/hyperledger/fabric/common/ledger"
 	"github.com/hyperledger/fabric/common/metrics"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/ledger/rwset"
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
@@ -26,6 +27,7 @@ type Initializer struct {
 	MembershipInfoProvider        MembershipInfoProvider
 	MetricsProvider               metrics.Provider
 	HealthCheckRegistry           HealthCheckRegistry
+	CollDataProvider              storeapi.Provider
 }
 
 // PeerLedgerProvider provides handle to ledger instances
diff --git a/core/ledger/ledgermgmt/ledger_mgmt.go b/core/ledger/ledgermgmt/ledger_mgmt.go
index eb92047f..bfb0ac6e 100644
--- a/core/ledger/ledgermgmt/ledger_mgmt.go
+++ b/core/ledger/ledgermgmt/ledger_mgmt.go
@@ -18,6 +18,7 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/cceventmgmt"
 	"github.com/hyperledger/fabric/core/ledger/customtx"
 	"github.com/hyperledger/fabric/core/ledger/kvledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	"github.com/hyperledger/fabric/protos/utils"
 	"github.com/pkg/errors"
@@ -45,6 +46,7 @@ type Initializer struct {
 	MembershipInfoProvider        ledger.MembershipInfoProvider
 	MetricsProvider               metrics.Provider
 	HealthCheckRegistry           ledger.HealthCheckRegistry
+	CollDataProvider              storeapi.Provider
 }
 
 // Initialize initializes ledgermgmt
@@ -76,6 +78,7 @@ func initialize(initializer *Initializer) {
 		MembershipInfoProvider:        initializer.MembershipInfoProvider,
 		MetricsProvider:               initializer.MetricsProvider,
 		HealthCheckRegistry:           initializer.HealthCheckRegistry,
+		CollDataProvider:              initializer.CollDataProvider,
 	})
 	if err != nil {
 		panic(errors.WithMessage(err, "Error initializing ledger provider"))
diff --git a/core/ledger/pvtdatastorage/store.go b/core/ledger/pvtdatastorage/store.go
index 62ea1070..73e76dd9 100644
--- a/core/ledger/pvtdatastorage/store.go
+++ b/core/ledger/pvtdatastorage/store.go
@@ -86,6 +86,11 @@ type Store interface {
 	Shutdown()
 }
 
+// NewErrIllegalCall creates an illegal call error
+func NewErrIllegalCall(msg string) *ErrIllegalCall {
+	return &ErrIllegalCall{msg: msg}
+}
+
 // ErrIllegalCall is to be thrown by a store impl if the store does not expect a call to Prepare/Commit/Rollback/InitLastCommittedBlock
 type ErrIllegalCall struct {
 	msg string
@@ -95,6 +100,11 @@ func (err *ErrIllegalCall) Error() string {
 	return err.msg
 }
 
+// NewErrIllegalArgs creates an illegal args error
+func NewErrIllegalArgs(msg string) *ErrIllegalArgs {
+	return &ErrIllegalArgs{msg: msg}
+}
+
 // ErrIllegalArgs is to be thrown by a store impl if the args passed are not allowed
 type ErrIllegalArgs struct {
 	msg string
@@ -104,6 +114,11 @@ func (err *ErrIllegalArgs) Error() string {
 	return err.msg
 }
 
+// NewErrOutOfRange creates an out of range error
+func NewErrOutOfRange(msg string) *ErrOutOfRange {
+	return &ErrOutOfRange{msg: msg}
+}
+
 // ErrOutOfRange is to be thrown for the request for the data that is not yet committed
 type ErrOutOfRange struct {
 	msg string
diff --git a/core/ledger/util/couchdb/couchdb_ext.go b/core/ledger/util/couchdb/couchdb_ext.go
new file mode 100644
index 00000000..cf6f489e
--- /dev/null
+++ b/core/ledger/util/couchdb/couchdb_ext.go
@@ -0,0 +1,153 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdb
+
+import (
+	"net/http"
+	"strings"
+	"time"
+
+	"github.com/hyperledger/fabric/common/util/retry"
+	"github.com/pkg/errors"
+)
+
+// CreateNewIndexWithRetry method provides a function creating an index but retries on failure
+func (dbclient *CouchDatabase) CreateNewIndexWithRetry(indexdefinition string, designDoc string) error {
+	//get the number of retries
+	maxRetries := dbclient.CouchInstance.conf.MaxRetries
+
+	_, err := retry.Invoke(
+		func() (interface{}, error) {
+			exists, err := dbclient.IndexDesignDocExists(designDoc)
+			if err != nil {
+				return nil, err
+			}
+			if exists {
+				return nil, nil
+			}
+
+			return dbclient.CreateIndex(indexdefinition)
+		},
+		retry.WithMaxAttempts(maxRetries),
+	)
+	return err
+}
+
+// Exists determines if the database exists
+func (dbclient *CouchDatabase) Exists() (bool, error) {
+	_, dbReturn, err := dbclient.GetDatabaseInfo()
+	if dbReturn != nil && dbReturn.StatusCode == http.StatusNotFound {
+		return false, nil
+	}
+	if err != nil {
+		return false, err
+	}
+	return true, nil
+}
+
+var errDBNotFound = errors.Errorf("DB not found")
+
+func isPvtDataDB(dbName string) bool {
+	return strings.Contains(dbName, "$$h") || strings.Contains(dbName, "$$p")
+}
+
+func (dbclient *CouchDatabase) shouldRetry(err error) bool {
+	return err == errDBNotFound && !isPvtDataDB(dbclient.DBName)
+}
+
+// ExistsWithRetry determines if the database exists, but retries until it does
+func (dbclient *CouchDatabase) ExistsWithRetry() (bool, error) {
+	//get the number of retries
+	maxRetries := dbclient.CouchInstance.conf.MaxRetries
+
+	_, err := retry.Invoke(
+		func() (interface{}, error) {
+			dbExists, err := dbclient.Exists()
+			if err != nil {
+				return nil, err
+			}
+			if !dbExists {
+				return nil, errDBNotFound
+			}
+
+			// DB exists
+			return nil, nil
+		},
+		retry.WithMaxAttempts(maxRetries),
+		retry.WithBeforeRetry(func(err error, attempt int, backoff time.Duration) bool {
+			if dbclient.shouldRetry(err) {
+				logger.Debugf("Got error [%s] checking if DB [%s] exists on attempt #%d. Will retry in %s.", err, dbclient.DBName, attempt, backoff)
+				return true
+			}
+			logger.Debugf("Got error [%s] checking if DB [%s] exists on attempt #%d. Will NOT retry", err, dbclient.DBName, attempt)
+			return false
+		}),
+	)
+
+	if err != nil {
+		if err == errDBNotFound {
+			return false, nil
+		}
+		return false, err
+	}
+
+	return true, nil
+}
+
+// IndexDesignDocExists determines if all the passed design documents exists in the database.
+func (dbclient *CouchDatabase) IndexDesignDocExists(designDocs ...string) (bool, error) {
+	designDocExists := make([]bool, len(designDocs))
+
+	indices, err := dbclient.ListIndex()
+	if err != nil {
+		return false, errors.WithMessage(err, "retrieval of DB index list failed")
+	}
+
+	for _, dbIndexDef := range indices {
+		for j, docName := range designDocs {
+			if dbIndexDef.DesignDocument == docName {
+				designDocExists[j] = true
+			}
+		}
+	}
+
+	for _, exists := range designDocExists {
+		if !exists {
+			return false, nil
+		}
+	}
+
+	return true, nil
+}
+
+// IndexDesignDocExists determines if all the passed design documents exists in the database.
+func (dbclient *CouchDatabase) IndexDesignDocExistsWithRetry(designDocs ...string) (bool, error) {
+	//get the number of retries
+	maxRetries := dbclient.CouchInstance.conf.MaxRetries
+
+	_, err := retry.Invoke(
+		func() (interface{}, error) {
+			indexExists, err := dbclient.IndexDesignDocExists(designDocs...)
+			if err != nil {
+				return nil, err
+			}
+			if !indexExists {
+				return nil, errors.Errorf("DB index not found: [%s]", dbclient.DBName)
+			}
+
+			// DB index exists
+			return nil, nil
+		},
+		retry.WithMaxAttempts(maxRetries),
+	)
+
+	if err != nil {
+		return false, err
+	}
+
+	return true, nil
+}
diff --git a/core/ledger/util/couchdb/couchdb_ext_test.go b/core/ledger/util/couchdb/couchdb_ext_test.go
new file mode 100644
index 00000000..f53280c2
--- /dev/null
+++ b/core/ledger/util/couchdb/couchdb_ext_test.go
@@ -0,0 +1,172 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdb
+
+import (
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/stretchr/testify/require"
+)
+
+const testIndexDef = `
+	{
+		"index": {
+			"fields": ["testnumber"]
+		},
+		"name": "by_test_number",
+		"ddoc": "indexTestNumber",
+		"type": "json"
+	}`
+
+func TestCreateIndexWithRetry(t *testing.T) {
+	database := "testcreateindexwithretry"
+	err := cleanup(database)
+	require.NoError(t, err, "Error when trying to cleanup  Error: %s", err)
+	defer cleanup(database)
+
+	//create a new instance and database object
+	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	require.NoError(t, err, "Error when trying to create couch instance")
+	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
+
+	errdb := db.CreateDatabaseIfNotExist()
+	require.NoError(t, errdb, "Error when trying to create database")
+
+	// Create successful index
+	err = db.CreateNewIndexWithRetry(testIndexDef, "indexTestNumber")
+	require.NoError(t, err)
+
+	// Create wrong index
+	err = db.CreateNewIndexWithRetry("wrongindex", "wrongindex")
+	require.Error(t, err)
+	require.Contains(t, err.Error(), "JSON format is not valid")
+
+}
+
+func TestIndexDesignDocExists(t *testing.T) {
+	database := "testindexdesigndocexists"
+	err := cleanup(database)
+	require.NoError(t, err, "Error when trying to cleanup  Error: %s", err)
+	defer cleanup(database)
+
+	//create a new instance and database object
+	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	require.NoError(t, err, "Error when trying to create couch instance")
+	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
+
+	errdb := db.CreateDatabaseIfNotExist()
+	require.NoError(t, errdb, "Error when trying to create database")
+
+	// check index if exist before create
+	exists, err := db.IndexDesignDocExists("indexTestNumber")
+	require.NoError(t, err)
+	require.Equal(t, exists, false)
+
+	// Create successful index
+	err = db.CreateNewIndexWithRetry(testIndexDef, "indexTestNumber")
+	require.NoError(t, err)
+
+	// check index if exist after create
+	exists, err = db.IndexDesignDocExists("indexTestNumber")
+	require.NoError(t, err)
+	require.Equal(t, exists, true)
+
+}
+
+func TestIndexDesignDocExistsWithRetry(t *testing.T) {
+	database := "testindexdesigndocexistswithretry"
+	err := cleanup(database)
+	require.NoError(t, err, "Error when trying to cleanup  Error: %s", err)
+	defer cleanup(database)
+
+	//create a new instance and database object
+	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+		5, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	require.NoError(t, err, "Error when trying to create couch instance")
+	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
+
+	errdb := db.CreateDatabaseIfNotExist()
+	require.NoError(t, errdb, "Error when trying to create database")
+
+	// check index if exist before create
+	exists, err := db.IndexDesignDocExistsWithRetry("indexTestNumber")
+	require.Error(t, err)
+	require.Equal(t, exists, false)
+
+	go func() {
+		time.Sleep(300 * time.Millisecond)
+		// Create successful index
+		err := db.CreateNewIndexWithRetry(testIndexDef, "indexTestNumber")
+		require.NoError(t, err)
+	}()
+
+	// check index if exist after create
+	exists, err = db.IndexDesignDocExistsWithRetry("indexTestNumber")
+	require.NoError(t, err)
+	require.Equal(t, exists, true)
+
+}
+
+func TestDBExists(t *testing.T) {
+	database := "testdbexists"
+	err := cleanup(database)
+	require.NoError(t, err, "Error when trying to cleanup  Error: %s", err)
+	defer cleanup(database)
+
+	//create a new instance and database object
+	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	require.NoError(t, err, "Error when trying to create couch instance")
+	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
+
+	// check if db exists before create
+	exists, err := db.Exists()
+	require.NoError(t, err)
+	require.Equal(t, exists, false)
+
+	errdb := db.CreateDatabaseIfNotExist()
+	require.NoError(t, errdb, "Error when trying to create database")
+
+	// check if db exists after create
+	exists, err = db.Exists()
+	require.NoError(t, err)
+	require.Equal(t, exists, true)
+
+}
+
+func TestDBExistsWithRetry(t *testing.T) {
+	database := "testdbexistswithretry"
+	err := cleanup(database)
+	require.NoError(t, err, "Error when trying to cleanup  Error: %s", err)
+	defer cleanup(database)
+
+	//create a new instance and database object
+	couchInstance, err := CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+		5, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	require.NoError(t, err, "Error when trying to create couch instance")
+	db := CouchDatabase{CouchInstance: couchInstance, DBName: database}
+
+	// check if db exists before create
+	exists, err := db.ExistsWithRetry()
+	require.NoError(t, err)
+	require.Equal(t, exists, false)
+
+	go func() {
+		time.Sleep(300 * time.Millisecond)
+		errdb := db.CreateDatabaseIfNotExist()
+		require.NoError(t, errdb, "Error when trying to create database")
+	}()
+	// check if db exists after create
+	exists, err = db.ExistsWithRetry()
+	require.NoError(t, err)
+	require.Equal(t, exists, true)
+
+}
diff --git a/core/ledger/util/couchdb/couchdbutil_ext.go b/core/ledger/util/couchdb/couchdbutil_ext.go
new file mode 100644
index 00000000..ee8ab9c1
--- /dev/null
+++ b/core/ledger/util/couchdb/couchdbutil_ext.go
@@ -0,0 +1,57 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdb
+
+import (
+	"encoding/hex"
+
+	"github.com/hyperledger/fabric/common/util"
+)
+
+// ConstructBlockchainDBName truncates the db name to couchdb allowed length to
+// construct the blockchain-related databases.
+func ConstructBlockchainDBName(chainName, dbName string) string {
+	chainDBName := joinSystemDBName(chainName, dbName)
+
+	if len(chainDBName) > maxLength {
+		untruncatedDBName := chainDBName
+
+		// As truncated namespaceDBName is of form 'chainName_escapedNamespace', both chainName
+		// and escapedNamespace need to be truncated to defined allowed length.
+		if len(chainName) > chainNameAllowedLength {
+			// Truncate chainName to chainNameAllowedLength
+			chainName = chainName[:chainNameAllowedLength]
+		}
+
+		// For metadataDB (i.e., chain/channel DB), the dbName contains <first 50 chars
+		// (i.e., chainNameAllowedLength) of chainName> + (SHA256 hash of actual chainName)
+		chainDBName = joinSystemDBName(chainName, dbName) + "(" + hex.EncodeToString(util.ComputeSHA256([]byte(untruncatedDBName))) + ")"
+		// 50 chars for dbName + 1 char for ( + 64 chars for sha256 + 1 char for ) = 116 chars
+	}
+	return chainDBName + "_"
+}
+
+func joinSystemDBName(chainName, dbName string) string {
+	systemDBName := chainName
+	if len(dbName) > 0 {
+		systemDBName += "$$" + dbName
+	}
+	return systemDBName
+}
+
+// NewCouchDatabase creates a CouchDB database object, but not the underlying database if it does not exist
+func NewCouchDatabase(couchInstance *CouchInstance, dbName string) (*CouchDatabase, error) {
+
+	databaseName, err := mapAndValidateDatabaseName(dbName)
+	if err != nil {
+		logger.Errorf("Error during CouchDB CreateDatabaseIfNotExist() for dbName: %s  error: %s", dbName, err.Error())
+		return nil, err
+	}
+
+	couchDBDatabase := CouchDatabase{CouchInstance: couchInstance, DBName: databaseName, IndexWarmCounter: 1}
+	return &couchDBDatabase, nil
+}
diff --git a/core/ledger/util/couchdb/couchdbutil_ext_test.go b/core/ledger/util/couchdb/couchdbutil_ext_test.go
new file mode 100644
index 00000000..1cbac354
--- /dev/null
+++ b/core/ledger/util/couchdb/couchdbutil_ext_test.go
@@ -0,0 +1,28 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdb
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/require"
+)
+
+func TestConstructBlockchainDBName(t *testing.T) {
+	dbName := ConstructBlockchainDBName("testchannel", "dbname")
+	require.Equal(t, "testchannel$$dbname_", dbName)
+}
+
+func TestNewCouchDatabase(t *testing.T) {
+	_, err := NewCouchDatabase(nil, "_dbtest")
+	require.NotNil(t, err)
+	require.Contains(t, err.Error(), "'_dbtest' does not match pattern")
+
+	dbName := ConstructBlockchainDBName("testchannel", "dbname")
+	_, err = NewCouchDatabase(&CouchInstance{}, dbName)
+	require.Nil(t, err)
+}
diff --git a/core/peer/peer.go b/core/peer/peer.go
index d73e27ad..5f715431 100644
--- a/core/peer/peer.go
+++ b/core/peer/peer.go
@@ -33,6 +33,10 @@ import (
 	"github.com/hyperledger/fabric/core/ledger/customtx"
 	"github.com/hyperledger/fabric/core/ledger/ledgermgmt"
 	"github.com/hyperledger/fabric/core/transientstore"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/collections/storeprovider"
+	"github.com/hyperledger/fabric/extensions/gossip/blockpublisher"
+	extensionstransientstore "github.com/hyperledger/fabric/extensions/transientstore"
 	"github.com/hyperledger/fabric/gossip/api"
 	"github.com/hyperledger/fabric/gossip/service"
 	"github.com/hyperledger/fabric/msp"
@@ -78,6 +82,20 @@ type chainSupport struct {
 
 var TransientStoreFactory = &storeProvider{stores: make(map[string]transientstore.Store)}
 
+var collectionDataStoreFactory *storeprovider.StoreProvider
+var initCollDataStoreFactoryOnce sync.Once
+
+// CollectionDataStoreFactory returns transient data stores by channel ID
+func CollectionDataStoreFactory() *storeprovider.StoreProvider {
+	initCollDataStoreFactoryOnce.Do(func() {
+		collectionDataStoreFactory = storeprovider.NewProviderFactory()
+	})
+	return collectionDataStoreFactory
+}
+
+// publisher manages the block publishers for all channels
+var BlockPublisher = blockpublisher.NewProvider()
+
 type storeProvider struct {
 	stores map[string]transientstore.Store
 	transientstore.StoreProvider
@@ -94,7 +112,7 @@ func (sp *storeProvider) OpenStore(ledgerID string) (transientstore.Store, error
 	sp.Lock()
 	defer sp.Unlock()
 	if sp.StoreProvider == nil {
-		sp.StoreProvider = transientstore.NewStoreProvider()
+		sp.StoreProvider = extensionstransientstore.NewStoreProvider()
 	}
 	store, err := sp.StoreProvider.OpenStore(ledgerID)
 	if err == nil {
@@ -206,7 +224,8 @@ var validationWorkersSemaphore *semaphore.Weighted
 // ready
 func Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider,
 	pm txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider,
-	membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider) {
+	membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider,
+	collDataProvider storeapi.Provider) {
 	nWorkers := viper.GetInt("peer.validatorPoolSize")
 	if nWorkers <= 0 {
 		nWorkers = runtime.NumCPU()
@@ -224,6 +243,7 @@ func Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccp
 		DeployedChaincodeInfoProvider: deployedCCInfoProvider,
 		MembershipInfoProvider:        membershipProvider,
 		MetricsProvider:               metricsProvider,
+		CollDataProvider:              collDataProvider,
 	})
 	ledgerIds, err := ledgermgmt.GetLedgerIDs()
 	if err != nil {
@@ -385,12 +405,20 @@ func createChain(cid string, ledger ledger.PeerLedger, cb *common.Block, ccp ccp
 		*semaphore.Weighted
 	}{cs, validationWorkersSemaphore}
 	validator := txvalidator.NewTxValidator(cid, vcs, sccp, pm)
+
+	blockPublisher := BlockPublisher.ForChannel(cid)
 	c := committer.NewLedgerCommitterReactive(ledger, func(block *common.Block) error {
-		chainID, err := utils.GetChainIDFromBlock(block)
-		if err != nil {
-			return err
+		// Updating CSCC with new configuration block
+		if utils.IsConfigBlock(block) {
+			logger.Debug("Received configuration update, calling CSCC ConfigUpdate")
+			err := SetCurrConfigBlock(block, cid)
+			if err != nil {
+				return err
+			}
 		}
-		return SetCurrConfigBlock(block, chainID)
+		// Inform applicable registered handlers of the new block
+		blockPublisher.Publish(block)
+		return nil
 	})
 
 	ordererAddresses := bundle.ChannelConfig().OrdererAddresses()
@@ -403,6 +431,10 @@ func createChain(cid string, ledger ledger.PeerLedger, cb *common.Block, ccp ccp
 	if err != nil {
 		return errors.Wrapf(err, "[channel %s] failed opening transient store", bundle.ConfigtxValidator().ChainID())
 	}
+	collDataStore, err := CollectionDataStoreFactory().OpenStore(bundle.ConfigtxValidator().ChainID())
+	if err != nil {
+		return errors.Wrapf(err, "[channel %s] failed opening transient data store", bundle.ConfigtxValidator().ChainID())
+	}
 	csStoreSupport := &CollectionSupport{
 		PeerLedger: ledger,
 	}
@@ -412,8 +444,11 @@ func createChain(cid string, ledger ledger.PeerLedger, cb *common.Block, ccp ccp
 		Validator:            validator,
 		Committer:            c,
 		Store:                store,
+		CollDataStore:        collDataStore,
 		Cs:                   simpleCollectionStore,
 		IdDeserializeFactory: csStoreSupport,
+		Ledger:               ledger,
+		BlockPublisher:       blockPublisher,
 	})
 
 	chains.Lock()
diff --git a/core/peer/peer_impl.go b/core/peer/peer_impl.go
index 7e33563d..8665c040 100644
--- a/core/peer/peer_impl.go
+++ b/core/peer/peer_impl.go
@@ -15,6 +15,7 @@ import (
 	"github.com/hyperledger/fabric/core/common/ccprovider"
 	"github.com/hyperledger/fabric/core/common/sysccprovider"
 	"github.com/hyperledger/fabric/core/ledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
 	"github.com/hyperledger/fabric/protos/common"
 	pb "github.com/hyperledger/fabric/protos/peer"
 )
@@ -31,7 +32,7 @@ type Operations interface {
 	GetMSPIDs(cid string) []string
 	GetPolicyManager(cid string) policies.Manager
 	InitChain(cid string)
-	Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, pm txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider)
+	Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, pm txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider, collDataProvider storeapi.Provider)
 }
 
 type peerImpl struct {
@@ -43,7 +44,8 @@ type peerImpl struct {
 	getMSPIDs            func(cid string) []string
 	getPolicyManager     func(cid string) policies.Manager
 	initChain            func(cid string)
-	initialize           func(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider)
+	initialize           func(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider,
+		collDataProvider storeapi.Provider)
 }
 
 // Default provides in implementation of the Peer interface that provides
@@ -74,6 +76,6 @@ func (p *peerImpl) GetLedger(cid string) ledger.PeerLedger       { return p.getL
 func (p *peerImpl) GetMSPIDs(cid string) []string                { return p.getMSPIDs(cid) }
 func (p *peerImpl) GetPolicyManager(cid string) policies.Manager { return p.getPolicyManager(cid) }
 func (p *peerImpl) InitChain(cid string)                         { p.initChain(cid) }
-func (p *peerImpl) Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider) {
-	p.initialize(init, ccp, sccp, mapper, pr, deployedCCInfoProvider, membershipProvider, metricsProvider)
+func (p *peerImpl) Initialize(init func(string), ccp ccprovider.ChaincodeProvider, sccp sysccprovider.SystemChaincodeProvider, mapper txvalidator.PluginMapper, pr *platforms.Registry, deployedCCInfoProvider ledger.DeployedChaincodeInfoProvider, membershipProvider ledger.MembershipInfoProvider, metricsProvider metrics.Provider, tdp storeapi.Provider) {
+	p.initialize(init, ccp, sccp, mapper, pr, deployedCCInfoProvider, membershipProvider, metricsProvider, tdp)
 }
diff --git a/extensions/collections/api/dissemination/dissemination.go b/extensions/collections/api/dissemination/dissemination.go
new file mode 100644
index 00000000..2f2925e5
--- /dev/null
+++ b/extensions/collections/api/dissemination/dissemination.go
@@ -0,0 +1,18 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"github.com/hyperledger/fabric/gossip/gossip"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+)
+
+// Plan contains the dissemination plan for Kevlar private data types
+type Plan struct {
+	Msg      *proto.SignedGossipMessage
+	Criteria gossip.SendCriteria
+}
diff --git a/extensions/collections/api/offledger/provider.go b/extensions/collections/api/offledger/provider.go
new file mode 100644
index 00000000..d0e34590
--- /dev/null
+++ b/extensions/collections/api/offledger/provider.go
@@ -0,0 +1,56 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package offledger
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// Store manages the storage of private data collections.
+type Store interface {
+	// Persist stores the private write set of a transaction.
+	Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error
+
+	// PutData stores the key/value.
+	PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error
+
+	// GetData gets the value for the given item
+	GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple items in a single call
+	GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+
+	// Close closes the store
+	Close()
+}
+
+// StoreProvider is an interface to open/close a store
+type StoreProvider interface {
+	// OpenStore creates a handle to the private data store for the given ledger ID
+	OpenStore(ledgerid string) (Store, error)
+
+	// Close cleans up the provider
+	Close()
+}
+
+// Retriever retrieves data
+type Retriever interface {
+	// GetData gets the value for the given data item
+	GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple data items in a single call
+	GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+}
+
+// Provider provides data retrievers
+type Provider interface {
+	RetrieverForChannel(channel string) Retriever
+}
diff --git a/extensions/collections/api/store/key.go b/extensions/collections/api/store/key.go
new file mode 100644
index 00000000..68b4b17f
--- /dev/null
+++ b/extensions/collections/api/store/key.go
@@ -0,0 +1,57 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package store
+
+import (
+	"fmt"
+)
+
+// Key is a key for retrieving collection data
+type Key struct {
+	EndorsedAtTxID string
+	Namespace      string
+	Collection     string
+	Key            string
+}
+
+// NewKey returns a new collection key
+func NewKey(endorsedAtTxID string, ns string, coll string, key string) *Key {
+	return &Key{
+		EndorsedAtTxID: endorsedAtTxID,
+		Namespace:      ns,
+		Collection:     coll,
+		Key:            key,
+	}
+}
+
+// String returns the string representation of the key
+func (k *Key) String() string {
+	return fmt.Sprintf("%s:%s:%s-%s", k.Namespace, k.Collection, k.Key, k.EndorsedAtTxID)
+}
+
+// MultiKey is a key for retrieving collection data for multiple keys
+type MultiKey struct {
+	EndorsedAtTxID string
+	Namespace      string
+	Collection     string
+	Keys           []string
+}
+
+// NewMultiKey returns a new collection data multi-key
+func NewMultiKey(endorsedAtTxID string, ns string, coll string, keys ...string) *MultiKey {
+	return &MultiKey{
+		EndorsedAtTxID: endorsedAtTxID,
+		Namespace:      ns,
+		Collection:     coll,
+		Keys:           keys,
+	}
+}
+
+// String returns the string representation of the key
+func (k *MultiKey) String() string {
+	return fmt.Sprintf("%s:%s:%s-%s", k.Namespace, k.Collection, k.Keys, k.EndorsedAtTxID)
+}
diff --git a/extensions/collections/api/store/provider.go b/extensions/collections/api/store/provider.go
new file mode 100644
index 00000000..67c695c2
--- /dev/null
+++ b/extensions/collections/api/store/provider.go
@@ -0,0 +1,91 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package store
+
+import (
+	"context"
+	"time"
+
+	"github.com/hyperledger/fabric/extensions/common"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// ExpiringValue holds the value and expiration time.
+type ExpiringValue struct {
+	Value  []byte
+	Expiry time.Time
+}
+
+// ExpiringValues is an array of ExpiringValue
+type ExpiringValues []*ExpiringValue
+
+// Store manages the storage of private data collections.
+type Store interface {
+	// Persist stores the private write set of a transaction.
+	Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error
+
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(key *Key) (*ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(key *MultiKey) (ExpiringValues, error)
+
+	// GetData gets the value for the given item
+	GetData(key *Key) (*ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple items in a single call
+	GetDataMultipleKeys(key *MultiKey) (ExpiringValues, error)
+
+	// PutData stores the key/value.
+	PutData(config *cb.StaticCollectionConfig, key *Key, value *ExpiringValue) error
+
+	// Close closes the store
+	Close()
+}
+
+// Retriever retrieves private data
+type Retriever interface {
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(ctxt context.Context, key *Key) (*ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(ctxt context.Context, key *MultiKey) (ExpiringValues, error)
+
+	// GetData gets the value for the given data item
+	GetData(ctxt context.Context, key *Key) (*ExpiringValue, error)
+
+	// GetDataMultipleKeys gets the values for the multiple data items in a single call
+	GetDataMultipleKeys(ctxt context.Context, key *MultiKey) (ExpiringValues, error)
+}
+
+// Provider provides transient data retrievers
+type Provider interface {
+	RetrieverForChannel(channel string) Retriever
+}
+
+// Values returns the ExpiringValues as Values
+func (ev ExpiringValues) Values() common.Values {
+	vals := make(common.Values, len(ev))
+	for i, v := range ev {
+		vals[i] = v
+	}
+	return vals
+}
+
+// AsExpiringValues converts Values into ExpiringValues
+func AsExpiringValues(cv common.Values) ExpiringValues {
+	vals := make(ExpiringValues, len(cv))
+	for i, v := range cv {
+		if common.IsNil(v) {
+			vals[i] = nil
+		} else {
+			vals[i] = v.(*ExpiringValue)
+		}
+	}
+	return vals
+}
diff --git a/extensions/collections/api/support/support.go b/extensions/collections/api/support/support.go
new file mode 100644
index 00000000..b010205f
--- /dev/null
+++ b/extensions/collections/api/support/support.go
@@ -0,0 +1,23 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+)
+
+// GossipAdapter defines the Gossip functions that are required for transient data processing
+type GossipAdapter interface {
+	PeersOfChannel(gcommon.ChainID) []discovery.NetworkMember
+	SelfMembershipInfo() discovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+	Send(msg *gproto.GossipMessage, peers ...*comm.RemotePeer)
+}
diff --git a/extensions/collections/api/transientdata/provider.go b/extensions/collections/api/transientdata/provider.go
new file mode 100644
index 00000000..b8a49da3
--- /dev/null
+++ b/extensions/collections/api/transientdata/provider.go
@@ -0,0 +1,52 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientdata
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// Store manages the storage of transient data.
+type Store interface {
+	// Persist stores the private write set of a transaction.
+	Persist(txID string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error
+
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+
+	// Close closes the store
+	Close()
+}
+
+// StoreProvider is an interface to open/close a provider
+type StoreProvider interface {
+	// OpenStore creates a handle to the transient data store for the given ledger ID
+	OpenStore(ledgerid string) (Store, error)
+
+	// Close cleans up the provider
+	Close()
+}
+
+// Retriever retrieves transient data
+type Retriever interface {
+	// GetTransientData gets the value for the given transient data item
+	GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error)
+
+	// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+	GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error)
+}
+
+// Provider provides transient data retrievers
+type Provider interface {
+	RetrieverForChannel(channel string) Retriever
+}
diff --git a/extensions/collections/dissemination/disseminationplan.go b/extensions/collections/dissemination/disseminationplan.go
new file mode 100644
index 00000000..8dce3a50
--- /dev/null
+++ b/extensions/collections/dissemination/disseminationplan.go
@@ -0,0 +1,72 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	oldissemination "github.com/hyperledger/fabric/extensions/collections/offledger/dissemination"
+	tdissemination "github.com/hyperledger/fabric/extensions/collections/transientdata/dissemination"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/pkg/errors"
+)
+
+type gossipAdapter interface {
+	PeersOfChannel(common.ChainID) []discovery.NetworkMember
+	SelfMembershipInfo() discovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+var computeTransientDataDisseminationPlan = func(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *proto.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	return tdissemination.ComputeDisseminationPlan(channelID, ns, rwSet, colAP, pvtDataMsg, gossipAdapter)
+}
+
+var computeOffLedgerDisseminationPlan = func(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	collConfig *cb.StaticCollectionConfig,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *proto.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	return oldissemination.ComputeDisseminationPlan(channelID, ns, rwSet, collConfig, colAP, pvtDataMsg, gossipAdapter)
+}
+
+// ComputeDisseminationPlan returns the dissemination plan for various collection types
+func ComputeDisseminationPlan(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	colCP *cb.CollectionConfig,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *proto.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+
+	collConfig := colCP.GetStaticCollectionConfig()
+	if collConfig == nil {
+		return nil, false, errors.New("static collection config not defined")
+	}
+
+	switch collConfig.Type {
+	case cb.CollectionType_COL_TRANSIENT:
+		return computeTransientDataDisseminationPlan(channelID, ns, rwSet, colAP, pvtDataMsg, gossipAdapter)
+	case cb.CollectionType_COL_DCAS:
+		fallthrough
+	case cb.CollectionType_COL_OFFLEDGER:
+		return computeOffLedgerDisseminationPlan(channelID, ns, rwSet, collConfig, colAP, pvtDataMsg, gossipAdapter)
+	default:
+		return nil, false, errors.Errorf("unsupported collection type: [%s]", collConfig.Type)
+	}
+}
diff --git a/extensions/collections/dissemination/disseminationplan_test.go b/extensions/collections/dissemination/disseminationplan_test.go
new file mode 100644
index 00000000..6c488a78
--- /dev/null
+++ b/extensions/collections/dissemination/disseminationplan_test.go
@@ -0,0 +1,101 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	"github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestDisseminationPlan(t *testing.T) {
+	const (
+		channelID = "testchannel"
+		ns        = "ns1"
+	)
+
+	computeTransientDataDisseminationPlan = func(
+		channelID, ns string,
+		rwSet *rwset.CollectionPvtReadWriteSet,
+		colAP privdata.CollectionAccessPolicy,
+		pvtDataMsg *proto.SignedGossipMessage,
+		gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+		return nil, false, nil
+	}
+
+	computeOffLedgerDisseminationPlan = func(
+		channelID, ns string,
+		rwSet *rwset.CollectionPvtReadWriteSet,
+		colCP *common.StaticCollectionConfig,
+		colAP privdata.CollectionAccessPolicy,
+		pvtDataMsg *proto.SignedGossipMessage,
+		gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+		return nil, false, nil
+	}
+
+	t.Run("Empty config", func(t *testing.T) {
+		colConfig1 := &common.CollectionConfig{}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, colConfig1, nil, nil, nil)
+		assert.EqualError(t, err, "static collection config not defined")
+	})
+
+	t.Run("Unknown config", func(t *testing.T) {
+		colConfig2 := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, colConfig2, nil, nil, nil)
+		assert.EqualError(t, err, "unsupported collection type: [COL_UNKNOWN]")
+	})
+
+	t.Run("Transient config", func(t *testing.T) {
+		transientConfig := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{
+					Type: common.CollectionType_COL_TRANSIENT,
+				},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, transientConfig, nil, nil, nil)
+		assert.NoError(t, err)
+	})
+
+	t.Run("Off-Ledger config", func(t *testing.T) {
+		dcasConfig := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{
+					Type: common.CollectionType_COL_OFFLEDGER,
+				},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, dcasConfig, nil, nil, nil)
+		assert.NoError(t, err)
+	})
+
+	t.Run("DCAS config", func(t *testing.T) {
+		dcasConfig := &common.CollectionConfig{
+			Payload: &common.CollectionConfig_StaticCollectionConfig{
+				StaticCollectionConfig: &common.StaticCollectionConfig{
+					Type: common.CollectionType_COL_DCAS,
+				},
+			},
+		}
+		_, _, err := ComputeDisseminationPlan(
+			channelID, ns, nil, dcasConfig, nil, nil, nil)
+		assert.NoError(t, err)
+	})
+}
diff --git a/extensions/collections/offledger/client/client.go b/extensions/collections/offledger/client/client.go
new file mode 100644
index 00000000..5538795a
--- /dev/null
+++ b/extensions/collections/offledger/client/client.go
@@ -0,0 +1,269 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package client
+
+import (
+	"encoding/hex"
+	"sync"
+
+	"github.com/hyperledger/fabric/bccsp"
+	"github.com/hyperledger/fabric/bccsp/factory"
+	"github.com/hyperledger/fabric/common/crypto"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/peer"
+	supp "github.com/hyperledger/fabric/extensions/common/support"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/gossip/service"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("offledger")
+
+// PeerLedger defines the ledger functions required by the client
+type PeerLedger interface {
+	// NewQueryExecutor gives handle to a query executor.
+	// A client can obtain more than one 'QueryExecutor's for parallel execution.
+	// Any synchronization should be performed at the implementation level if required
+	NewQueryExecutor() (ledger.QueryExecutor, error)
+	// NewTxSimulator gives handle to a transaction simulator.
+	// A client can obtain more than one 'TxSimulator's for parallel execution.
+	// Any snapshoting/synchronization should be performed at the implementation level if required
+	NewTxSimulator(txid string) (ledger.TxSimulator, error)
+	// GetBlockchainInfo returns basic info about blockchain
+	GetBlockchainInfo() (*cb.BlockchainInfo, error)
+}
+
+// GossipAdapter defines the Gossip functions required by the client
+type GossipAdapter interface {
+	// DistributePrivateData distributes private data to the peers in the collections
+	// according to policies induced by the PolicyStore and PolicyParser
+	DistributePrivateData(chainID string, txID string, privateData *transientstore.TxPvtReadWriteSetWithConfigInfo, blkHt uint64) error
+}
+
+// CollectionConfigRetriever defines the collection config retrieval functions required by the client
+type CollectionConfigRetriever interface {
+	Config(ns, coll string) (*cb.StaticCollectionConfig, error)
+}
+
+// KeyValue holds a key-value pair
+type KeyValue struct {
+	Key   string
+	Value []byte
+}
+
+// Client allows you to put and get Client from outside of a chaincode
+type Client struct {
+	channelID       string
+	ledger          PeerLedger
+	gossip          GossipAdapter
+	configRetriever CollectionConfigRetriever
+	creator         []byte
+	mutex           sync.RWMutex
+}
+
+// New returns a new client
+func New(channelID string) *Client {
+	ledger := getLedger(channelID)
+	blockPublisher := getBlockPublisher(channelID)
+
+	return &Client{
+		channelID:       channelID,
+		ledger:          ledger,
+		gossip:          getGossipAdapter(),
+		configRetriever: getCollConfigRetriever(channelID, ledger, blockPublisher),
+	}
+}
+
+// Put puts the value for the given key
+func (d *Client) Put(ns, coll, key string, value []byte) error {
+	return d.PutMultipleValues(ns, coll, []*KeyValue{{Key: key, Value: value}})
+}
+
+// PutMultipleValues puts the given key/values
+func (d *Client) PutMultipleValues(ns, coll string, kvs []*KeyValue) error {
+	// Generate a new TxID. The TxID doesn't really matter since this transaction is never committed.
+	// It just has to be unique.
+	txID, err := d.newTxID()
+	if err != nil {
+		logger.Warningf("[%s] Error generating transaction ID: %s", d.channelID, err)
+		return errors.WithMessagef(err, "error generating transaction ID in channel [%s]", d.channelID)
+	}
+
+	sim, err := d.ledger.NewTxSimulator(txID)
+	if err != nil {
+		logger.Warningf("[%s] Error getting TxSimulator for transaction [%s]: %s", d.channelID, txID, err)
+		return errors.WithMessagef(err, "error getting TxSimulator for transaction [%s] in channel [%s]", txID, d.channelID)
+	}
+	defer sim.Done()
+
+	mapByKey := make(map[string][]byte)
+	for _, kv := range kvs {
+		mapByKey[kv.Key] = kv.Value
+	}
+
+	err = sim.SetPrivateDataMultipleKeys(ns, coll, mapByKey)
+	if err != nil {
+		logger.Warningf("[%s] Error setting values for transaction [%s]: %s", d.channelID, txID, err)
+		return errors.WithMessagef(err, "error setting keys for transaction [%s] in channel [%s]", txID, d.channelID)
+	}
+
+	results, err := sim.GetTxSimulationResults()
+	if err != nil {
+		logger.Warningf("[%s] Error generating simulation results for transaction [%s]: %s", d.channelID, txID, err)
+		return errors.WithMessagef(err, "error generating simulation results for transaction [%s] in channel [%s]", txID, d.channelID)
+	}
+
+	bcInfo, err := d.ledger.GetBlockchainInfo()
+	if err != nil {
+		logger.Warningf("[%s] Error getting blockchain info: %s", d.channelID, err)
+		return errors.WithMessagef(err, "error getting blockchain info in channel [%s]", d.channelID)
+	}
+
+	configPkg, err := d.getCollectionConfigPackage(ns, coll)
+	if err != nil {
+		logger.Warningf("[%s] Error getting collection config for [%s:%s]: %s", d.channelID, ns, coll, err)
+		return errors.WithMessagef(err, "error getting collection config for [%s:%s] in channel [%s]", ns, coll, d.channelID)
+	}
+
+	pvtData := &transientstore.TxPvtReadWriteSetWithConfigInfo{
+		EndorsedAt: bcInfo.Height,
+		PvtRwset:   results.PvtSimulationResults,
+		CollectionConfigs: map[string]*cb.CollectionConfigPackage{
+			ns: configPkg,
+		},
+	}
+
+	err = d.gossip.DistributePrivateData(d.channelID, txID, pvtData, bcInfo.Height)
+	if err != nil {
+		logger.Warningf("[%s] Failed to distribute private data: %s", d.channelID, err)
+		return errors.WithMessagef(err, "error distributing private data in channel [%s]", d.channelID)
+	}
+
+	return nil
+}
+
+// Get retrieves the value for the given key
+func (d *Client) Get(ns, coll, key string) ([]byte, error) {
+	qe, err := d.ledger.NewQueryExecutor()
+	if err != nil {
+		logger.Warningf("[%s] Error getting QueryExecutor: %s", d.channelID, err)
+		return nil, errors.WithMessagef(err, "error getting QueryExecutor in channel [%s]", d.channelID)
+	}
+	defer qe.Done()
+
+	return qe.GetPrivateData(ns, coll, key)
+}
+
+// GetMultipleKeys retrieves the values for the given keys
+func (d *Client) GetMultipleKeys(ns, coll string, keys ...string) ([][]byte, error) {
+	qe, err := d.ledger.NewQueryExecutor()
+	if err != nil {
+		logger.Warningf("[%s] Error getting QueryExecutor: %s", d.channelID, err)
+		return nil, errors.WithMessagef(err, "error getting QueryExecutor in channel [%s]", d.channelID)
+	}
+	defer qe.Done()
+
+	return qe.GetPrivateDataMultipleKeys(ns, coll, keys)
+}
+
+func (d *Client) getCollectionConfigPackage(ns, coll string) (*cb.CollectionConfigPackage, error) {
+	collConfig, err := d.configRetriever.Config(ns, coll)
+	if err != nil {
+		return nil, err
+	}
+
+	return &cb.CollectionConfigPackage{
+		Config: []*cb.CollectionConfig{
+			{
+				Payload: &cb.CollectionConfig_StaticCollectionConfig{
+					StaticCollectionConfig: collConfig,
+				},
+			},
+		},
+	}, nil
+}
+
+func (d *Client) newTxID() (string, error) {
+	creator, err := d.getCreator()
+	if err != nil {
+		return "", errors.WithMessage(err, "error serializing local signing identity")
+	}
+
+	nonce, err := crypto.GetRandomNonce()
+	if err != nil {
+		return "", errors.WithMessage(err, "nonce creation failed")
+	}
+
+	txnID, err := computeTxID(nonce, creator)
+	if err != nil {
+		return "", errors.WithMessage(err, "txn ID computation failed")
+	}
+
+	return txnID, nil
+}
+
+func (d *Client) getCreator() ([]byte, error) {
+	d.mutex.RLock()
+	c := d.creator
+	d.mutex.RUnlock()
+
+	if c != nil {
+		return c, nil
+	}
+
+	d.mutex.Lock()
+	defer d.mutex.Unlock()
+
+	if d.creator == nil {
+		creator, err := newCreator()
+		if err != nil {
+			return nil, errors.WithMessage(err, "error serializing local signing identity")
+		}
+		d.creator = creator
+	}
+
+	return d.creator, nil
+}
+
+func computeTxID(nonce, creator []byte) (string, error) {
+	digest, err := factory.GetDefault().Hash(append(nonce, creator...), &bccsp.SHA256Opts{})
+	if err != nil {
+		return "", err
+	}
+	return hex.EncodeToString(digest), nil
+}
+
+// getLedger returns the peer ledger. This var may be overridden in unit tests
+var getLedger = func(channelID string) PeerLedger {
+	return peer.GetLedger(channelID)
+}
+
+// getLedger returns the peer ledger. This var may be overridden in unit tests
+var getBlockPublisher = func(channelID string) gossipapi.BlockPublisher {
+	return peer.BlockPublisher.ForChannel(channelID)
+}
+
+// getGossipAdapter returns the gossip adapter. This var may be overridden in unit tests
+var getGossipAdapter = func() GossipAdapter {
+	return service.GetGossipService()
+}
+
+var getCollConfigRetriever = func(channelID string, ledger PeerLedger, blockPublisher gossipapi.BlockPublisher) CollectionConfigRetriever {
+	return supp.NewCollectionConfigRetriever(channelID, ledger, blockPublisher)
+}
+
+var newCreator = func() ([]byte, error) {
+	id, err := mspmgmt.GetLocalMSP().GetDefaultSigningIdentity()
+	if err != nil {
+		return nil, errors.WithMessage(err, "error getting local signing identity")
+	}
+	return id.Serialize()
+}
diff --git a/extensions/collections/offledger/client/client_test.go b/extensions/collections/offledger/client/client_test.go
new file mode 100644
index 00000000..cbf26e38
--- /dev/null
+++ b/extensions/collections/offledger/client/client_test.go
@@ -0,0 +1,205 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package client
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/ledger"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	gmocks "github.com/hyperledger/fabric/extensions/gossip/mocks"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID   = "testchannel"
+	ns1         = "ns1"
+	coll1       = "coll1"
+	key1        = "key1"
+	key2        = "key2"
+	blockHeight = uint64(1000)
+)
+
+func TestClient_Put(t *testing.T) {
+	ledger := &mocks.Ledger{
+		TxSimulator: &mocks.TxSimulator{
+			SimulationResults: &ledger.TxSimulationResults{},
+		},
+		BlockchainInfo: &cb.BlockchainInfo{
+			Height: blockHeight,
+		},
+	}
+	gossip := &mockGossipAdapter{}
+	configRetriever := &mockCollectionConfigRetriever{}
+	var creatorError error
+
+	// Mock out all of the dependencies
+	getLedger = func(channelID string) PeerLedger { return ledger }
+	getGossipAdapter = func() GossipAdapter { return gossip }
+	getBlockPublisher = func(channelID string) gossipapi.BlockPublisher { return gmocks.NewBlockPublisher() }
+	getCollConfigRetriever = func(_ string, _ PeerLedger, _ gossipapi.BlockPublisher) CollectionConfigRetriever {
+		return configRetriever
+	}
+	newCreator = func() ([]byte, error) { return []byte("creator"), creatorError }
+
+	c := New(channelID)
+	require.NotNil(t, c)
+
+	value1 := []byte("value1")
+
+	t.Run("TxID error", func(t *testing.T) {
+		creatorError = errors.New("mock creator error")
+		defer func() { creatorError = nil }()
+
+		err := c.Put(ns1, coll1, key1, value1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error generating transaction ID")
+	})
+
+	t.Run("Success", func(t *testing.T) {
+		err := c.Put(ns1, coll1, key1, value1)
+		require.NoError(t, err)
+	})
+
+	t.Run("Simulation results error", func(t *testing.T) {
+		ledger.TxSimulator.SimError = errors.New("mock TxSimulator error")
+		defer func() { ledger.TxSimulator.SimError = nil }()
+
+		err := c.Put(ns1, coll1, key1, value1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error generating simulation results")
+	})
+
+	t.Run("GetTxSimulator error", func(t *testing.T) {
+		ledger.Error = errors.New("mock TxSimulator error")
+		defer func() { ledger.Error = nil }()
+
+		err := c.Put(ns1, coll1, key1, value1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error getting TxSimulator")
+	})
+
+	t.Run("TxSimulator - Put error", func(t *testing.T) {
+		ledger.TxSimulator.Error = errors.New("mock TxSimulator error")
+		defer func() { ledger.TxSimulator.Error = nil }()
+
+		err := c.Put(ns1, coll1, key1, value1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error setting keys")
+	})
+
+	t.Run("Gossip error", func(t *testing.T) {
+		gossip.Error = errors.New("mock gossip error")
+		defer func() { gossip.Error = nil }()
+
+		err := c.Put(ns1, coll1, key1, value1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error distributing private data")
+	})
+
+	t.Run("GetBlockchainInfo error", func(t *testing.T) {
+		ledger.BcInfoError = errors.New("mock ledger error")
+		defer func() { ledger.BcInfoError = nil }()
+
+		err := c.Put(ns1, coll1, key1, value1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error getting blockchain info")
+	})
+
+	t.Run("CollectionConfig error", func(t *testing.T) {
+		configRetriever.Error = errors.New("mock config error")
+		defer func() { configRetriever.Error = nil }()
+
+		err := c.Put(ns1, coll1, key1, value1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error getting collection config")
+	})
+}
+
+func TestClient_Get(t *testing.T) {
+	value1 := []byte("value1")
+	value2 := []byte("value2")
+
+	pvtNS := ns1 + "$" + coll1
+	state := make(map[string]map[string][]byte)
+	state[pvtNS] = make(map[string][]byte)
+	state[pvtNS][key1] = value1
+	state[pvtNS][key2] = value2
+
+	ledger := &mocks.Ledger{
+		QueryExecutor: mocks.NewQueryExecutor(state),
+	}
+
+	gossip := &mockGossipAdapter{}
+	configRetriever := &mockCollectionConfigRetriever{}
+	var creatorError error
+
+	// Mock out all of the dependencies
+	getLedger = func(channelID string) PeerLedger { return ledger }
+	getGossipAdapter = func() GossipAdapter { return gossip }
+	getBlockPublisher = func(channelID string) gossipapi.BlockPublisher { return gmocks.NewBlockPublisher() }
+	getCollConfigRetriever = func(_ string, _ PeerLedger, _ gossipapi.BlockPublisher) CollectionConfigRetriever {
+		return configRetriever
+	}
+	newCreator = func() ([]byte, error) { return []byte("creator"), creatorError }
+
+	c := New(channelID)
+	require.NotNil(t, c)
+
+	t.Run("Get - success", func(t *testing.T) {
+		value, err := c.Get(ns1, coll1, key1)
+		require.NoError(t, err)
+		assert.Equal(t, value1, value)
+	})
+
+	t.Run("Get - error", func(t *testing.T) {
+		ledger.Error = errors.New("mock QueryExecutor error")
+		defer func() { ledger.Error = nil }()
+
+		_, err := c.Get(ns1, coll1, key1)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error getting QueryExecutor")
+	})
+
+	t.Run("GetMultipleKeys - success", func(t *testing.T) {
+		values, err := c.GetMultipleKeys(ns1, coll1, key1, key2)
+		require.NoError(t, err)
+		require.Equal(t, 2, len(values))
+		assert.Equal(t, value1, values[0])
+		assert.Equal(t, value2, values[1])
+	})
+
+	t.Run("GetMultipleKeys - error", func(t *testing.T) {
+		ledger.Error = errors.New("mock QueryExecutor error")
+		defer func() { ledger.Error = nil }()
+
+		_, err := c.GetMultipleKeys(ns1, coll1, key1, key2)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "error getting QueryExecutor")
+	})
+}
+
+type mockCollectionConfigRetriever struct {
+	Error error
+}
+
+func (m *mockCollectionConfigRetriever) Config(ns, coll string) (*cb.StaticCollectionConfig, error) {
+	return &cb.StaticCollectionConfig{}, m.Error
+}
+
+type mockGossipAdapter struct {
+	Error error
+}
+
+func (m *mockGossipAdapter) DistributePrivateData(chainID string, txID string, privateData *transientstore.TxPvtReadWriteSetWithConfigInfo, blkHt uint64) error {
+	return m.Error
+}
diff --git a/extensions/collections/offledger/client/test_exports.go b/extensions/collections/offledger/client/test_exports.go
new file mode 100644
index 00000000..89c56fd5
--- /dev/null
+++ b/extensions/collections/offledger/client/test_exports.go
@@ -0,0 +1,38 @@
+// +build testing
+
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package client
+
+import (
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+)
+
+// SetLedgerProvider sets the ledger provider for unit tests
+func SetLedgerProvider(provider func(channelID string) PeerLedger) {
+	getLedger = provider
+}
+
+// SetGossipProvider sets the Gossip provider for unit tests
+func SetGossipProvider(provider func() GossipAdapter) {
+	getGossipAdapter = provider
+}
+
+// SetBlockPublisherProvider sets the block publisher provider for unit tests
+func SetBlockPublisherProvider(provider func(channelID string) gossipapi.BlockPublisher) {
+	getBlockPublisher = provider
+}
+
+// SetCollConfigRetrieverProvider sets the collection config retriever provider for unit tests
+func SetCollConfigRetrieverProvider(provider func(_ string, _ PeerLedger, _ gossipapi.BlockPublisher) CollectionConfigRetriever) {
+	getCollConfigRetriever = provider
+}
+
+// SetCreatorProvider sets the creator provider for unit tests
+func SetCreatorProvider(provider func() ([]byte, error)) {
+	newCreator = provider
+}
diff --git a/extensions/collections/offledger/dcas/dcas.go b/extensions/collections/offledger/dcas/dcas.go
new file mode 100644
index 00000000..87f7da93
--- /dev/null
+++ b/extensions/collections/offledger/dcas/dcas.go
@@ -0,0 +1,54 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dcas
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/pkg/errors"
+)
+
+// Validator is an off-ledger validator that validates the CAS key against the value
+func Validator(_, _, _, key string, value []byte) error {
+	if value == nil {
+		return errors.Errorf("nil value for key [%s]", key)
+	}
+	expectedKey := common.GetCASKey(value)
+	if key != expectedKey {
+		return errors.Errorf("Invalid CAS key [%s] - the key should be the hash of the value", key)
+	}
+	return nil
+}
+
+// Decorator is an off-ledger decorator that ensures the given key is the hash of the value. If the key is not
+// specified then it is generated. If the key is provided then it is validated against the value.
+func Decorator(key *storeapi.Key, value *storeapi.ExpiringValue) (*storeapi.Key, *storeapi.ExpiringValue, error) {
+	dcasKey, err := validateCASKey(key.Key, value.Value)
+	if err != nil {
+		return nil, nil, err
+	}
+
+	if dcasKey == key.Key {
+		return key, value, nil
+	}
+
+	newKey := *key
+	newKey.Key = dcasKey
+	return &newKey, value, nil
+}
+
+func validateCASKey(key string, value []byte) (string, error) {
+	if value == nil {
+		return "", errors.Errorf("attempt to put nil value for key [%s]", key)
+	}
+
+	casKey := common.GetCASKey(value)
+	if key != "" && key != casKey {
+		return casKey, errors.New("invalid CAS key - the key should be the hash of the value")
+	}
+	return casKey, nil
+}
diff --git a/extensions/collections/offledger/dcas/dcas_test.go b/extensions/collections/offledger/dcas/dcas_test.go
new file mode 100644
index 00000000..2759d8a5
--- /dev/null
+++ b/extensions/collections/offledger/dcas/dcas_test.go
@@ -0,0 +1,83 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dcas
+
+import (
+	"testing"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	ns1   = "chaincode1"
+	coll1 = "coll1"
+	txID1 = "txid1"
+)
+
+func TestValidator(t *testing.T) {
+	value := []byte("value1")
+
+	t.Run("Valid key/value -> success", func(t *testing.T) {
+		err := Validator("", "", "", common.GetCASKey(value), value)
+		assert.NoError(t, err)
+	})
+
+	t.Run("Invalid key -> error", func(t *testing.T) {
+		err := Validator("", "", "", "key1", value)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "the key should be the hash of the value")
+	})
+
+	t.Run("Nil value -> error", func(t *testing.T) {
+		err := Validator("", "", "", "key1", nil)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "nil value for key")
+	})
+}
+
+func TestDecorator(t *testing.T) {
+	value1_1 := []byte("value1_1")
+	value := &storeapi.ExpiringValue{
+		Value: value1_1,
+	}
+
+	t.Run("CAS key -> success", func(t *testing.T) {
+		key := storeapi.NewKey(txID1, ns1, coll1, common.GetCASKey(value1_1))
+		k, v, err := Decorator(key, value)
+		assert.NoError(t, err)
+		assert.Equal(t, key, k)
+		assert.Equal(t, value, v)
+	})
+
+	t.Run("Empty key -> success", func(t *testing.T) {
+		key := storeapi.NewKey(txID1, ns1, coll1, "")
+		k, v, err := Decorator(key, value)
+		assert.NoError(t, err)
+		assert.Equal(t, common.GetCASKey(value1_1), k.Key)
+		assert.Equal(t, value, v)
+	})
+
+	t.Run("Invalid key -> error", func(t *testing.T) {
+		key := storeapi.NewKey(txID1, ns1, coll1, "key1")
+		k, v, err := Decorator(key, value)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "the key should be the hash of the value")
+		assert.Nil(t, k)
+		assert.Nil(t, v)
+	})
+
+	t.Run("Nil value -> error", func(t *testing.T) {
+		k, v, err := Decorator(storeapi.NewKey(txID1, ns1, coll1, "key1"), &storeapi.ExpiringValue{})
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "nil value for key")
+		assert.Nil(t, k)
+		assert.Nil(t, v)
+	})
+}
diff --git a/extensions/collections/offledger/dcas/dcasclient/dcasclient.go b/extensions/collections/offledger/dcas/dcasclient/dcasclient.go
new file mode 100644
index 00000000..f872f326
--- /dev/null
+++ b/extensions/collections/offledger/dcas/dcasclient/dcasclient.go
@@ -0,0 +1,51 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dcasclient
+
+import (
+	olclient "github.com/hyperledger/fabric/extensions/collections/offledger/client"
+	"github.com/hyperledger/fabric/extensions/common"
+)
+
+// DCASClient allows you to put and get DCASClient from outside of a chaincode
+type DCASClient struct {
+	*olclient.Client
+}
+
+// New returns a new DCAS olclient
+func New(channelID string) *DCASClient {
+	return &DCASClient{
+		Client: olclient.New(channelID),
+	}
+}
+
+// Put puts the DCAS value and returns the key for the value
+func (d *DCASClient) Put(ns, coll string, value []byte) (string, error) {
+	keys, err := d.PutMultipleValues(ns, coll, [][]byte{value})
+	if err != nil {
+		return "", err
+	}
+	return keys[0], nil
+}
+
+// PutMultipleValues puts the DCAS values and returns the keys for the values
+func (d *DCASClient) PutMultipleValues(ns, coll string, values [][]byte) ([]string, error) {
+	keys := make([]string, len(values))
+	kvs := make([]*olclient.KeyValue, len(values))
+	for i, v := range values {
+		key := common.GetCASKey(v)
+		keys[i] = key
+		kvs[i] = &olclient.KeyValue{
+			Key:   key,
+			Value: v,
+		}
+	}
+	if err := d.Client.PutMultipleValues(ns, coll, kvs); err != nil {
+		return nil, err
+	}
+	return keys, nil
+}
diff --git a/extensions/collections/offledger/dcas/dcasclient/dcasclient_test.go b/extensions/collections/offledger/dcas/dcasclient/dcasclient_test.go
new file mode 100644
index 00000000..2315924b
--- /dev/null
+++ b/extensions/collections/offledger/dcas/dcasclient/dcasclient_test.go
@@ -0,0 +1,126 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dcasclient
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/ledger"
+	olclient "github.com/hyperledger/fabric/extensions/collections/offledger/client"
+	"github.com/hyperledger/fabric/extensions/common"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	gmocks "github.com/hyperledger/fabric/extensions/gossip/mocks"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID   = "testchannel"
+	ns1         = "ns1"
+	coll1       = "coll1"
+	blockHeight = uint64(1000)
+)
+
+func TestDCASClient_Put(t *testing.T) {
+	ledger := &mocks.Ledger{
+		TxSimulator: &mocks.TxSimulator{
+			SimulationResults: &ledger.TxSimulationResults{},
+		},
+		BlockchainInfo: &cb.BlockchainInfo{
+			Height: blockHeight,
+		},
+	}
+	gossip := &mockGossipAdapter{}
+	configRetriever := &mockCollectionConfigRetriever{}
+	var creatorError error
+
+	// Mock out all of the dependencies
+	olclient.SetLedgerProvider(func(channelID string) olclient.PeerLedger { return ledger })
+	olclient.SetGossipProvider(func() olclient.GossipAdapter { return gossip })
+	olclient.SetBlockPublisherProvider(func(channelID string) gossipapi.BlockPublisher { return gmocks.NewBlockPublisher() })
+	olclient.SetCollConfigRetrieverProvider(func(_ string, _ olclient.PeerLedger, _ gossipapi.BlockPublisher) olclient.CollectionConfigRetriever {
+		return configRetriever
+	})
+	olclient.SetCreatorProvider(func() ([]byte, error) { return []byte("creator"), creatorError })
+
+	c := New(channelID)
+	require.NotNil(t, c)
+
+	value1 := []byte("value1")
+
+	t.Run("Success", func(t *testing.T) {
+		key, err := c.Put(ns1, coll1, value1)
+		require.NoError(t, err)
+		assert.Equal(t, common.GetCASKey(value1), key)
+	})
+}
+
+func TestDCASClient_Get(t *testing.T) {
+	value1 := []byte("value1")
+	value2 := []byte("value2")
+	key1 := common.GetCASKey(value1)
+	key2 := common.GetCASKey(value2)
+
+	pvtNS := ns1 + "$" + coll1
+	state := make(map[string]map[string][]byte)
+	state[pvtNS] = make(map[string][]byte)
+	state[pvtNS][key1] = value1
+	state[pvtNS][key2] = value2
+
+	ledger := &mocks.Ledger{
+		QueryExecutor: mocks.NewQueryExecutor(state),
+	}
+
+	gossip := &mockGossipAdapter{}
+	configRetriever := &mockCollectionConfigRetriever{}
+	var creatorError error
+
+	// Mock out all of the dependencies
+	olclient.SetLedgerProvider(func(channelID string) olclient.PeerLedger { return ledger })
+	olclient.SetGossipProvider(func() olclient.GossipAdapter { return gossip })
+	olclient.SetBlockPublisherProvider(func(channelID string) gossipapi.BlockPublisher { return gmocks.NewBlockPublisher() })
+	olclient.SetCollConfigRetrieverProvider(func(_ string, _ olclient.PeerLedger, _ gossipapi.BlockPublisher) olclient.CollectionConfigRetriever {
+		return configRetriever
+	})
+	olclient.SetCreatorProvider(func() ([]byte, error) { return []byte("creator"), creatorError })
+
+	c := New(channelID)
+	require.NotNil(t, c)
+
+	t.Run("Get - success", func(t *testing.T) {
+		value, err := c.Get(ns1, coll1, key1)
+		require.NoError(t, err)
+		assert.Equal(t, value1, value)
+	})
+
+	t.Run("GetMultipleKeys - success", func(t *testing.T) {
+		values, err := c.GetMultipleKeys(ns1, coll1, key1, key2)
+		require.NoError(t, err)
+		require.Equal(t, 2, len(values))
+		assert.Equal(t, value1, values[0])
+		assert.Equal(t, value2, values[1])
+	})
+}
+
+type mockCollectionConfigRetriever struct {
+	Error error
+}
+
+func (m *mockCollectionConfigRetriever) Config(ns, coll string) (*cb.StaticCollectionConfig, error) {
+	return &cb.StaticCollectionConfig{}, m.Error
+}
+
+type mockGossipAdapter struct {
+	Error error
+}
+
+func (m *mockGossipAdapter) DistributePrivateData(chainID string, txID string, privateData *transientstore.TxPvtReadWriteSetWithConfigInfo, blkHt uint64) error {
+	return m.Error
+}
diff --git a/extensions/collections/offledger/dissemination/disseminationplan.go b/extensions/collections/offledger/dissemination/disseminationplan.go
new file mode 100644
index 00000000..fc5b0d55
--- /dev/null
+++ b/extensions/collections/offledger/dissemination/disseminationplan.go
@@ -0,0 +1,100 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	protobuf "github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	"github.com/hyperledger/fabric/extensions/common"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	gdiscovery "github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/gossip"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	"github.com/pkg/errors"
+	"github.com/spf13/viper"
+)
+
+type gossipAdapter interface {
+	PeersOfChannel(gcommon.ChainID) []gdiscovery.NetworkMember
+	SelfMembershipInfo() gdiscovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+// ComputeDisseminationPlan returns the dissemination plan for off ledger data
+func ComputeDisseminationPlan(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	collConfig *cb.StaticCollectionConfig,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *proto.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	logger.Debugf("Computing dissemination plan for [%s:%s]", ns, rwSet.CollectionName)
+
+	kvRwSet := &kvrwset.KVRWSet{}
+	if err := protobuf.Unmarshal(rwSet.Rwset, kvRwSet); err != nil {
+		return nil, true, errors.WithMessage(err, "error unmarshalling KV read/write set")
+	}
+
+	if err := validateAll(collConfig.Type, kvRwSet); err != nil {
+		return nil, false, errors.Wrapf(err, "one or more keys did not validate for collection [%s:%s]", ns, rwSet.CollectionName)
+	}
+
+	peers := New(channelID, ns, rwSet.CollectionName, colAP, gossipAdapter).resolvePeersForDissemination().Remote()
+
+	logger.Debugf("Peers for dissemination of collection [%s:%s]: %s", ns, rwSet.CollectionName, peers)
+
+	routingFilter := func(member gdiscovery.NetworkMember) bool {
+		if peers.ContainsPeer(member.Endpoint) {
+			logger.Debugf("Including peer [%s] for dissemination of [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+			return true
+		}
+
+		logger.Debugf("Not including peer [%s] for dissemination of [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+		return false
+	}
+
+	sc := gossip.SendCriteria{
+		Timeout:    viper.GetDuration("peer.gossip.pvtData.pushAckTimeout"),
+		Channel:    gcommon.ChainID(channelID),
+		MaxPeers:   len(peers),
+		MinAck:     colAP.RequiredPeerCount(),
+		IsEligible: routingFilter,
+	}
+
+	return []*dissemination.Plan{{
+		Criteria: sc,
+		Msg:      pvtDataMsg,
+	}}, true, nil
+}
+
+func validateAll(collType cb.CollectionType, kvRWSet *kvrwset.KVRWSet) error {
+	for _, ws := range kvRWSet.Writes {
+		if err := validate(collType, ws); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+func validate(collType cb.CollectionType, ws *kvrwset.KVWrite) error {
+	if ws.Value == nil {
+		return errors.Errorf("attempt to store nil value for key [%s]", ws.Key)
+	}
+
+	if collType == cb.CollectionType_COL_DCAS {
+		expectedKey := common.GetCASKey(ws.Value)
+		if ws.Key != expectedKey {
+			return errors.Errorf("invalid CAS key [%s] - the key should be the hash of the value", ws.Key)
+		}
+	}
+	return nil
+}
diff --git a/extensions/collections/offledger/dissemination/disseminator.go b/extensions/collections/offledger/dissemination/disseminator.go
new file mode 100644
index 00000000..2f373061
--- /dev/null
+++ b/extensions/collections/offledger/dissemination/disseminator.go
@@ -0,0 +1,120 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/common/discovery"
+	"github.com/hyperledger/fabric/extensions/config"
+)
+
+var logger = flogging.MustGetLogger("offledgerstore")
+
+// Disseminator disseminates collection data to other endorsers
+type Disseminator struct {
+	*discovery.Discovery
+	namespace  string
+	collection string
+	policy     privdata.CollectionAccessPolicy
+}
+
+// New returns a new disseminator
+func New(channelID, namespace, collection string, policy privdata.CollectionAccessPolicy, gossip gossipAdapter) *Disseminator {
+	return &Disseminator{
+		Discovery:  discovery.New(channelID, gossip),
+		namespace:  namespace,
+		collection: collection,
+		policy:     policy,
+	}
+}
+
+// resolvePeersForDissemination resolves to a set of committers to which data should be disseminated
+func (d *Disseminator) resolvePeersForDissemination() discovery.PeerGroup {
+	orgs := d.policy.MemberOrgs()
+	maxPeerCount := d.policy.MaximumPeerCount()
+
+	logger.Debugf("[%s] Member orgs: %s", d.ChannelID(), orgs)
+
+	peersForDissemination := d.getPeers(orgs)
+
+	if len(peersForDissemination) < maxPeerCount {
+		logger.Debugf("[%s] MaximumPeerCount in collection policy is %d and we only have %d committers. Adding some endorsers too...", d.ChannelID(), maxPeerCount, len(peersForDissemination))
+		for _, peer := range d.getPeers(orgs).Remote().Shuffle() {
+			if len(peersForDissemination) >= maxPeerCount {
+				// We have enough peers
+				break
+			}
+			logger.Debugf("Adding endorser [%s] ...", peer)
+			peersForDissemination = append(peersForDissemination, peer)
+		}
+	}
+
+	logger.Debugf("[%s] Peers for dissemination from orgs %s: %s", d.ChannelID(), orgs, peersForDissemination)
+
+	return peersForDissemination
+}
+
+// ResolvePeersForRetrieval resolves to a set of peers from which data should may be retrieved
+func (d *Disseminator) ResolvePeersForRetrieval() discovery.PeerGroup {
+	orgs := d.policy.MemberOrgs()
+
+	logger.Debugf("[%s] Member orgs: %s", d.ChannelID(), orgs)
+
+	// Maximum number of peers to ask for the data
+	maxPeers := getMaxPeersForRetrieval()
+
+	var peersForRetrieval discovery.PeerGroup
+	for _, peer := range d.getPeers(orgs).Remote().Shuffle() {
+		if len(peersForRetrieval) >= maxPeers {
+			// We have enough peers
+			break
+		}
+		logger.Debugf("Adding endorser [%s] ...", peer)
+		peersForRetrieval = append(peersForRetrieval, peer)
+	}
+
+	if len(peersForRetrieval) < maxPeers {
+		// Add some committers too
+		for _, peer := range d.getPeers(orgs).Remote().Shuffle() {
+			if len(peersForRetrieval) >= maxPeers {
+				// We have enough peers
+				break
+			}
+			logger.Debugf("Adding committer [%s] ...", peer)
+			peersForRetrieval = append(peersForRetrieval, peer)
+		}
+	}
+
+	logger.Debugf("[%s] Peers for retrieval from orgs %s: %s", d.ChannelID(), orgs, peersForRetrieval)
+
+	return peersForRetrieval
+}
+
+func (d *Disseminator) getPeers(mspIDs []string) discovery.PeerGroup {
+	return d.GetMembers(func(m *discovery.Member) bool {
+		if !contains(mspIDs, m.MSPID) {
+			logger.Debugf("[%s] Not adding peer [%s] since it is not in any of the orgs [%s]", d.ChannelID(), m.Endpoint, mspIDs)
+			return false
+		}
+		return true
+	})
+}
+
+func contains(mspIDs []string, mspID string) bool {
+	for _, m := range mspIDs {
+		if m == mspID {
+			return true
+		}
+	}
+	return false
+}
+
+// getMaxPeersForRetrieval may be overridden by unit tests
+var getMaxPeersForRetrieval = func() int {
+	return config.GetOLCollMaxPeersForRetrieval()
+}
diff --git a/extensions/collections/offledger/dissemination/disseminator_test.go b/extensions/collections/offledger/dissemination/disseminator_test.go
new file mode 100644
index 00000000..5073730c
--- /dev/null
+++ b/extensions/collections/offledger/dissemination/disseminator_test.go
@@ -0,0 +1,300 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"os"
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/hyperledger/fabric/extensions/roles"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+var (
+	ns1   = "chaincode1"
+	ns2   = "chaincode2"
+	coll1 = "collection1"
+	coll2 = "collection2"
+	key1  = "key1"
+	key2  = "key2"
+
+	org1MSPID      = "Org1MSP"
+	p1Org1Endpoint = "p1.org1.com"
+	p1Org1PKIID    = gcommon.PKIidType("pkiid_P1O1")
+	p2Org1Endpoint = "p2.org1.com"
+	p2Org1PKIID    = gcommon.PKIidType("pkiid_P2O1")
+	p3Org1Endpoint = "p3.org1.com"
+	p3Org1PKIID    = gcommon.PKIidType("pkiid_P3O1")
+
+	org2MSPID      = "Org2MSP"
+	p1Org2Endpoint = "p1.org2.com"
+	p1Org2PKIID    = gcommon.PKIidType("pkiid_P1O2")
+	p2Org2Endpoint = "p2.org2.com"
+	p2Org2PKIID    = gcommon.PKIidType("pkiid_P2O2")
+	p3Org2Endpoint = "p3.org2.com"
+	p3Org2PKIID    = gcommon.PKIidType("pkiid_P3O2")
+
+	org3MSPID      = "Org3MSP"
+	p1Org3Endpoint = "p1.org3.com"
+	p1Org3PKIID    = gcommon.PKIidType("pkiid_P1O3")
+	p2Org3Endpoint = "p2.org3.com"
+	p2Org3PKIID    = gcommon.PKIidType("pkiid_P2O3")
+	p3Org3Endpoint = "p3.org3.com"
+	p3Org3PKIID    = gcommon.PKIidType("pkiid_P3O3")
+
+	org4MSPID      = "Org4MSP"
+	p1Org4Endpoint = "p1.org4.com"
+	p1Org4PKIID    = gcommon.PKIidType("pkiid_P1O4")
+	p2Org4Endpoint = "p2.org4.com"
+	p2Org4PKIID    = gcommon.PKIidType("pkiid_P2O4")
+	p3Org4Endpoint = "p3.org4.com"
+	p3Org4PKIID    = gcommon.PKIidType("pkiid_P3O4")
+
+	committerRole = string(roles.CommitterRole)
+	endorserRole  = string(roles.EndorserRole)
+	validatorRole = string(roles.ValidatorRole)
+)
+
+func TestDisseminator_ResolvePeersForDissemination(t *testing.T) {
+	channelID := "testchannel"
+
+	gossip := mocks.NewMockGossipAdapter().
+		Self(org1MSPID, mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, committerRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, committerRole)).
+		Member(org3MSPID, mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole)).
+		Member(org4MSPID, mocks.NewMember(p1Org4Endpoint, p1Org4PKIID, committerRole)).
+		Member(org4MSPID, mocks.NewMember(p2Org4Endpoint, p2Org4PKIID, endorserRole))
+
+	t.Run("Enough committers", func(t *testing.T) {
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		peers := d.resolvePeersForDissemination()
+		require.Equal(t, 4, len(peers))
+
+		peersStr := peers.String()
+
+		assert.Contains(t, peersStr, p1Org1Endpoint)
+		assert.Contains(t, peersStr, p2Org1Endpoint)
+		assert.Contains(t, peersStr, p2Org2Endpoint)
+		assert.Contains(t, peersStr, p2Org3Endpoint)
+	})
+
+	t.Run("Not enough committers", func(t *testing.T) {
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				MaxPeerCount: 7,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		peers := d.resolvePeersForDissemination()
+		require.Equal(t, 7, len(peers))
+
+		var numCommitters int
+		for _, p := range peers {
+			if p.HasRole(roles.CommitterRole) {
+				numCommitters++
+			}
+		}
+		assert.Equal(t, 4, numCommitters)
+		assert.NotContains(t, peers.String(), "org4")
+	})
+
+	t.Run("Not enough committers and endorsers", func(t *testing.T) {
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				MaxPeerCount: 20,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		peers := d.resolvePeersForDissemination()
+		require.Equal(t, 9, len(peers))
+		assert.NotContains(t, peers.String(), "org4")
+	})
+}
+
+func TestDisseminator_ResolvePeersForRetrieval(t *testing.T) {
+	channelID := "testchannel"
+
+	gossip := mocks.NewMockGossipAdapter().
+		Self(org1MSPID, mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, committerRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, committerRole)).
+		Member(org3MSPID, mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole)).
+		Member(org4MSPID, mocks.NewMember(p1Org4Endpoint, p1Org4PKIID, committerRole)).
+		Member(org4MSPID, mocks.NewMember(p2Org4Endpoint, p2Org4PKIID, endorserRole))
+
+	t.Run("Enough endorsers", func(t *testing.T) {
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		peers := d.ResolvePeersForRetrieval()
+		require.Equal(t, 2, len(peers))
+
+		for _, p := range peers {
+			assert.True(t, p.HasRole(roles.EndorserRole))
+		}
+		assert.NotContains(t, peers.String(), "org4")
+	})
+
+	t.Run("Not enough endorsers", func(t *testing.T) {
+		getMaxPeersForRetrieval = func() int { return 7 }
+
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		peers := d.ResolvePeersForRetrieval()
+		require.Equal(t, 7, len(peers))
+
+		var numEndorsers int
+		for _, p := range peers {
+			if p.HasRole(roles.EndorserRole) {
+				numEndorsers++
+			}
+		}
+		assert.Equal(t, 5, numEndorsers)
+		assert.NotContains(t, peers.String(), "org4")
+	})
+
+}
+
+func TestComputeDisseminationPlan(t *testing.T) {
+	channelID := "testchannel"
+
+	p1Org1 := mocks.NewMember(p1Org1Endpoint, p1Org1PKIID, committerRole)
+	p2Org1 := mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, endorserRole)
+	p3Org1 := mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, committerRole)
+	p1Org2 := mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)
+	p2Org2 := mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, committerRole)
+	p3Org2 := mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)
+	p1Org3 := mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)
+	p2Org3 := mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, committerRole)
+	p3Org3 := mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole)
+
+	gossip := mocks.NewMockGossipAdapter().
+		Self(org1MSPID, p1Org1).
+		Member(org1MSPID, p2Org1).
+		Member(org1MSPID, p3Org1).
+		Member(org2MSPID, p1Org2).
+		Member(org2MSPID, p2Org2).
+		Member(org2MSPID, p3Org2).
+		Member(org3MSPID, p1Org3).
+		Member(org3MSPID, p2Org3).
+		Member(org3MSPID, p3Org3)
+
+	colAP := &mocks.MockAccessPolicy{
+		ReqPeerCount: 1,
+		Orgs:         []string{org2MSPID, org3MSPID},
+	}
+
+	t.Run("Success", func(t *testing.T) {
+		rwSet := mocks.NewPvtReadWriteSetCollectionBuilder(coll1).
+			Write(key1, []byte("value1")).
+			Build()
+		colConfig := &cb.StaticCollectionConfig{
+			Type: cb.CollectionType_COL_OFFLEDGER,
+		}
+
+		dPlan, handled, err := ComputeDisseminationPlan(channelID, ns1, rwSet, colConfig, colAP, nil, gossip)
+		assert.NoError(t, err)
+		assert.True(t, handled)
+		assert.NotNil(t, dPlan)
+	})
+
+	t.Run("Invalid CAS Key", func(t *testing.T) {
+		rwSet := mocks.NewPvtReadWriteSetCollectionBuilder(coll1).
+			Write(key1, []byte("value1")).
+			Build()
+		colConfig := &cb.StaticCollectionConfig{
+			Type: cb.CollectionType_COL_DCAS,
+		}
+
+		dPlan, handled, err := ComputeDisseminationPlan(channelID, ns1, rwSet, colConfig, colAP, nil, gossip)
+		require.Error(t, err)
+		assert.False(t, handled)
+		assert.Nil(t, dPlan)
+		assert.Contains(t, err.Error(), "the key should be the hash of the value")
+	})
+
+	t.Run("Valid CAS Key", func(t *testing.T) {
+		rwSet := mocks.NewPvtReadWriteSetCollectionBuilder(coll1).
+			Write(common.GetCASKey([]byte("value1")), []byte("value1")).
+			Build()
+		colConfig := &cb.StaticCollectionConfig{
+			Type: cb.CollectionType_COL_DCAS,
+		}
+
+		dPlan, handled, err := ComputeDisseminationPlan(channelID, ns1, rwSet, colConfig, colAP, nil, gossip)
+		require.NoError(t, err)
+		require.True(t, handled)
+		require.Equal(t, 1, len(dPlan))
+
+		criteria := dPlan[0].Criteria
+
+		assert.Equal(t, 2, criteria.MaxPeers)
+
+		assert.False(t, criteria.IsEligible(p1Org1))
+		assert.False(t, criteria.IsEligible(p2Org1))
+		assert.False(t, criteria.IsEligible(p3Org1))
+		assert.False(t, criteria.IsEligible(p1Org2))
+		assert.True(t, criteria.IsEligible(p2Org2))
+		assert.False(t, criteria.IsEligible(p3Org2))
+		assert.False(t, criteria.IsEligible(p1Org3))
+		assert.True(t, criteria.IsEligible(p2Org3))
+		assert.False(t, criteria.IsEligible(p3Org3))
+	})
+
+	t.Run("Nil value", func(t *testing.T) {
+		rwSet := mocks.NewPvtReadWriteSetCollectionBuilder(coll1).
+			Write(key1, nil).
+			Build()
+		colConfig := &cb.StaticCollectionConfig{
+			Type: cb.CollectionType_COL_OFFLEDGER,
+		}
+
+		dPlan, handled, err := ComputeDisseminationPlan(channelID, ns1, rwSet, colConfig, colAP, nil, gossip)
+		require.Error(t, err)
+		assert.False(t, handled)
+		assert.Nil(t, dPlan)
+		assert.Contains(t, err.Error(), "attempt to store nil value for key")
+	})
+}
+
+func TestMain(m *testing.M) {
+	// The local peer's roles are retrieved from ledgerconfig
+	viper.SetDefault("ledger.roles", "committer,endorser")
+
+	os.Exit(m.Run())
+}
diff --git a/extensions/collections/offledger/mocks/mockprovider.go b/extensions/collections/offledger/mocks/mockprovider.go
new file mode 100644
index 00000000..a6d84124
--- /dev/null
+++ b/extensions/collections/offledger/mocks/mockprovider.go
@@ -0,0 +1,40 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"context"
+
+	"github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+)
+
+// Provider is a mock off-ledger data data provider
+type Provider struct {
+}
+
+// RetrieverForChannel returns the retriever for the given channel
+func (p *Provider) RetrieverForChannel(channel string) offledger.Retriever {
+	return &retriever{}
+}
+
+type retriever struct {
+}
+
+// GetData gets data for the given key
+func (m *retriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return &storeapi.ExpiringValue{Value: []byte(key.Key)}, nil
+}
+
+// GetDataMultipleKeys gets data for multiple keys
+func (m *retriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = &storeapi.ExpiringValue{Value: []byte(k)}
+	}
+	return values, nil
+}
diff --git a/extensions/collections/offledger/olprovider.go b/extensions/collections/offledger/olprovider.go
new file mode 100644
index 00000000..4440030b
--- /dev/null
+++ b/extensions/collections/offledger/olprovider.go
@@ -0,0 +1,402 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package offledger
+
+import (
+	"context"
+	"sync"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	offledgerapi "github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/dissemination"
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/hyperledger/fabric/extensions/common/discovery"
+	"github.com/hyperledger/fabric/extensions/common/multirequest"
+	"github.com/hyperledger/fabric/extensions/common/requestmgr"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	cb "github.com/hyperledger/fabric/protos/common"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("offledgerstore")
+
+type support interface {
+	Config(channelID, ns, coll string) (*cb.StaticCollectionConfig, error)
+	Policy(channel, ns, collection string) (privdata.CollectionAccessPolicy, error)
+	BlockPublisher(channelID string) gossipapi.BlockPublisher
+}
+
+// Validator is a key/value validator
+type Validator func(txID, ns, coll, key string, value []byte) error
+
+// Provider is a collection data data provider.
+type Provider struct {
+	support
+	storeForChannel func(channelID string) offledgerapi.Store
+	gossipAdapter   func() supportapi.GossipAdapter
+	validators      map[cb.CollectionType]Validator
+}
+
+// Option is a provider option
+type Option func(p *Provider)
+
+// WithValidator sets the key/value validator
+func WithValidator(collType cb.CollectionType, validator Validator) Option {
+	return func(p *Provider) {
+		p.validators[collType] = validator
+	}
+}
+
+// NewProvider returns a new collection data provider
+func NewProvider(storeProvider func(channelID string) offledgerapi.Store, support support, gossipProvider func() supportapi.GossipAdapter, opts ...Option) offledgerapi.Provider {
+	p := &Provider{
+		support:         support,
+		storeForChannel: storeProvider,
+		gossipAdapter:   gossipProvider,
+		validators:      make(map[cb.CollectionType]Validator),
+	}
+
+	// Apply options
+	for _, opt := range opts {
+		opt(p)
+	}
+	return p
+}
+
+// RetrieverForChannel returns the collection data retriever for the given channel
+func (p *Provider) RetrieverForChannel(channelID string) offledgerapi.Retriever {
+	r := &retriever{
+		support:       p.support,
+		gossipAdapter: p.gossipAdapter(),
+		store:         p.storeForChannel(channelID),
+		channelID:     channelID,
+		reqMgr:        requestmgr.Get(channelID),
+		resolvers:     make(map[collKey]resolver),
+		validators:    p.validators,
+	}
+
+	// Add a handler so that we can remove the resolver for a chaincode that has been upgraded
+	p.support.BlockPublisher(channelID).AddCCUpgradeHandler(func(blockNum uint64, txID string, chaincodeID string) error {
+		logger.Infof("[%s] Chaincode [%s] has been upgraded. Clearing resolver cache for chaincode.", channelID, chaincodeID)
+		r.removeResolvers(chaincodeID)
+		return nil
+	})
+	return r
+}
+
+type resolver interface {
+	// ResolvePeersForRetrieval resolves to a set of peers from which data should be retrieved
+	ResolvePeersForRetrieval() discovery.PeerGroup
+}
+
+type collKey struct {
+	ns   string
+	coll string
+}
+
+func newCollKey(ns, coll string) collKey {
+	return collKey{ns: ns, coll: coll}
+}
+
+type retriever struct {
+	support
+	gossipAdapter supportapi.GossipAdapter
+	channelID     string
+	store         offledgerapi.Store
+	resolvers     map[collKey]resolver
+	lock          sync.RWMutex
+	reqMgr        requestmgr.RequestMgr
+	validators    map[cb.CollectionType]Validator
+}
+
+// GetData gets the values for the data item
+func (r *retriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	values, err := r.GetDataMultipleKeys(ctxt, storeapi.NewMultiKey(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Key))
+	if err != nil {
+		return nil, err
+	}
+
+	if values.Values().IsEmpty() {
+		return nil, nil
+	}
+
+	return values[0], nil
+}
+
+// GetDataMultipleKeys gets the values for the multiple data items in a single call
+func (r *retriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	authorized, err := r.isAuthorized(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, err
+	}
+	if !authorized {
+		logger.Infof("[%s] This peer does not have access to the collection [%s:%s]", r.channelID, key.Namespace, key.Collection)
+		return nil, nil
+	}
+
+	localValues, err := r.getMultipleKeysFromLocal(key)
+	if err != nil {
+		return nil, err
+	}
+	if localValues.Values().AllSet() {
+		err = r.validateValues(key, localValues)
+		if err != nil {
+			logger.Warningf(err.Error())
+			return nil, err
+		}
+		return localValues, nil
+	}
+
+	res, err := r.getResolver(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, errors.Wrapf(err, "unable to get resolver for channel [%s] and [%s:%s]", r.channelID, key.Namespace, key.Collection)
+	}
+
+	// Retrieve from the remote peers
+	cReq := multirequest.New()
+	for _, peer := range res.ResolvePeersForRetrieval() {
+		logger.Debugf("Adding request to get data for [%s] from [%s] ...", key, peer)
+		cReq.Add(peer.String(), r.getDataFromPeer(key, peer))
+	}
+
+	response := cReq.Execute(ctxt)
+
+	// Merge the values with the values received locally
+	values := storeapi.AsExpiringValues(localValues.Values().Merge(response.Values))
+
+	if err := r.validateValues(key, values); err != nil {
+		logger.Warningf(err.Error())
+		return nil, err
+	}
+
+	r.persistMissingKeys(key, localValues, values)
+
+	return values, nil
+}
+
+func (r *retriever) getMultipleKeysFromLocal(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	localValues := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		value, retrieveErr := r.store.GetData(storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, k))
+		if retrieveErr != nil {
+			logger.Warningf("[%s] Error getting data from local store for [%s]: %s", r.channelID, key, retrieveErr)
+			return nil, errors.Wrapf(retrieveErr, "unable to get data for channel [%s] and [%s]", r.channelID, key)
+		}
+		localValues[i] = value
+	}
+	return localValues, nil
+}
+
+func getMissingKeyIndexes(values []*storeapi.ExpiringValue) []int {
+	var missingIndexes []int
+	for i, v := range values {
+		if v == nil {
+			missingIndexes = append(missingIndexes, i)
+		}
+	}
+	return missingIndexes
+}
+
+func (r *retriever) persistMissingKeys(key *storeapi.MultiKey, localValues, values storeapi.ExpiringValues) {
+	// Persist the keys that were missing from the local store
+	for _, i := range getMissingKeyIndexes(localValues) {
+		v := values[i]
+		if v != nil {
+			k := storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Keys[i])
+			val := &storeapi.ExpiringValue{Value: v.Value, Expiry: v.Expiry}
+
+			logger.Debugf("Persisting key [%s] that was missing from the local store", k)
+			collConfig, err := r.Config(r.channelID, k.Namespace, k.Collection)
+			if err != nil {
+				logger.Warningf("Error persisting key [%s] that was missing from the local store: %s", k, err)
+			}
+			if err := r.store.PutData(collConfig, k, val); err != nil {
+				logger.Warningf("Error persisting key [%s] that was missing from the local store: %s", k, err)
+			}
+		}
+	}
+}
+
+func (r *retriever) validateValues(key *storeapi.MultiKey, values storeapi.ExpiringValues) error {
+	config, err := r.Config(r.channelID, key.Namespace, key.Collection)
+	if err != nil {
+		return err
+	}
+
+	validate, ok := r.validators[config.Type]
+	if !ok {
+		// No validator for config
+		return nil
+	}
+
+	for i, v := range values {
+		if v != nil && v.Value != nil {
+			err := validate(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Keys[i], v.Value)
+			if err != nil {
+				return err
+			}
+		}
+	}
+	return nil
+}
+
+func (r *retriever) getDataFromPeer(key *storeapi.MultiKey, endorser *discovery.Member) multirequest.Request {
+	return func(ctxt context.Context) (common.Values, error) {
+		logger.Debugf("Getting data for [%s] from [%s] ...", key, endorser)
+
+		values, err := r.getData(ctxt, key, endorser)
+		if err != nil {
+			if err == context.Canceled {
+				logger.Debugf("[%s] Request to get data from [%s] for [%s] was cancelled", r.channelID, endorser, key)
+			} else {
+				logger.Debugf("[%s] Error getting data from [%s] for [%s]: %s", r.channelID, endorser, key, err)
+			}
+			return nil, err
+		}
+
+		return values.Values(), nil
+	}
+}
+
+func (r *retriever) getResolver(ns, coll string) (resolver, error) {
+	key := newCollKey(ns, coll)
+
+	r.lock.RLock()
+	resolver, ok := r.resolvers[key]
+	r.lock.RUnlock()
+
+	if ok {
+		return resolver, nil
+	}
+
+	policy, err := r.Policy(r.channelID, ns, coll)
+	if err != nil {
+		return nil, err
+	}
+
+	resolver = dissemination.New(r.channelID, ns, coll, policy, r.gossipAdapter)
+
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	r.resolvers[key] = resolver
+
+	return resolver, nil
+}
+
+func (r *retriever) removeResolvers(ns string) {
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	for key := range r.resolvers {
+		if key.ns == ns {
+			logger.Debugf("[%s] Removing resolver [%s:%s] from cache", r.channelID, key.ns, key.coll)
+			delete(r.resolvers, key)
+		}
+	}
+}
+
+func (r *retriever) getData(ctxt context.Context, key *storeapi.MultiKey, peers ...*discovery.Member) (storeapi.ExpiringValues, error) {
+	logger.Debugf("[%s] Sending Gossip request to %s for data for [%s]", r.channelID, peers, key)
+
+	req := r.reqMgr.NewRequest()
+
+	logger.Debugf("[%s] Creating Gossip request %d for data for [%s]", r.channelID, req.ID(), key)
+	msg := r.createCollDataRequestMsg(req, key)
+
+	logger.Debugf("[%s] Sending Gossip request %d for data for [%s]", r.channelID, req.ID(), key)
+	r.gossipAdapter.Send(msg, asRemotePeers(peers)...)
+
+	logger.Debugf("[%s] Waiting for response for %d for data for [%s]", r.channelID, req.ID(), key)
+	res, err := req.GetResponse(ctxt)
+	if err != nil {
+		return nil, err
+	}
+
+	logger.Debugf("[%s] Got response for %d for data for [%s]", r.channelID, req.ID(), key)
+
+	data := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		d, ok := res.Data.Get(key.Namespace, key.Collection, k)
+		if !ok {
+			return nil, errors.Errorf("the response does not contain a value for key [%s:%s:%s]", key.Namespace, key.Collection, k)
+		}
+		if d.Value == nil {
+			data[i] = nil
+		} else {
+			data[i] = &storeapi.ExpiringValue{Value: d.Value, Expiry: d.Expiry}
+		}
+	}
+
+	return data, nil
+}
+
+// isAuthorized returns true if the local peer has access to the given collection
+func (r *retriever) isAuthorized(ns, coll string) (bool, error) {
+	policy, err := r.Policy(r.channelID, ns, coll)
+	if err != nil {
+		return false, errors.Wrapf(err, "unable to get policy for [%s:%s]", ns, coll)
+	}
+
+	localMSPID, err := getLocalMSPID()
+	if err != nil {
+		return false, errors.Wrap(err, "unable to get local MSP ID")
+	}
+
+	for _, mspID := range policy.MemberOrgs() {
+		if mspID == localMSPID {
+			return true, nil
+		}
+	}
+
+	return false, nil
+}
+
+func (r *retriever) createCollDataRequestMsg(req requestmgr.Request, key *storeapi.MultiKey) *gproto.GossipMessage {
+	var digests []*gproto.CollDataDigest
+	for _, k := range key.Keys {
+		digests = append(digests, &gproto.CollDataDigest{
+			Namespace:      key.Namespace,
+			Collection:     key.Collection,
+			Key:            k,
+			EndorsedAtTxID: key.EndorsedAtTxID,
+		})
+	}
+
+	return &gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(r.channelID),
+		Content: &gproto.GossipMessage_CollDataReq{
+			CollDataReq: &gproto.RemoteCollDataRequest{
+				Nonce:   req.ID(),
+				Digests: digests,
+			},
+		},
+	}
+}
+
+func asRemotePeers(members []*discovery.Member) []*comm.RemotePeer {
+	var peers []*comm.RemotePeer
+	for _, m := range members {
+		peers = append(peers, &comm.RemotePeer{
+			Endpoint: m.Endpoint,
+			PKIID:    m.PKIid,
+		})
+	}
+	return peers
+}
+
+// getLocalMSPID returns the MSP ID of the local peer. This variable may be overridden by unit tests.
+var getLocalMSPID = func() (string, error) {
+	return mspmgmt.GetLocalMSP().GetIdentifier()
+}
diff --git a/extensions/collections/offledger/olprovider_test.go b/extensions/collections/offledger/olprovider_test.go
new file mode 100644
index 00000000..c9d71915
--- /dev/null
+++ b/extensions/collections/offledger/olprovider_test.go
@@ -0,0 +1,341 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package offledger
+
+import (
+	"context"
+	"testing"
+	"time"
+
+	offledgerapi "github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	spmocks "github.com/hyperledger/fabric/extensions/collections/storeprovider/mocks"
+	kcommon "github.com/hyperledger/fabric/extensions/common"
+	"github.com/hyperledger/fabric/extensions/common/requestmgr"
+	"github.com/hyperledger/fabric/extensions/gossip/blockpublisher"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	ledgerconfig "github.com/hyperledger/fabric/extensions/roles"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	cb "github.com/hyperledger/fabric/protos/common"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+	ns1       = "chaincode1"
+	coll1     = "collection1"
+)
+
+var (
+	org1MSPID      = "Org1MSP"
+	p1Org1Endpoint = "p1.org1.com"
+	p1Org1PKIID    = gcommon.PKIidType("pkiid_P1O1")
+	p2Org1Endpoint = "p2.org1.com"
+	p2Org1PKIID    = gcommon.PKIidType("pkiid_P2O1")
+	p3Org1Endpoint = "p3.org1.com"
+	p3Org1PKIID    = gcommon.PKIidType("pkiid_P3O1")
+
+	org2MSPID      = "Org2MSP"
+	p1Org2Endpoint = "p1.org2.com"
+	p1Org2PKIID    = gcommon.PKIidType("pkiid_P1O2")
+	p2Org2Endpoint = "p2.org2.com"
+	p2Org2PKIID    = gcommon.PKIidType("pkiid_P2O2")
+	p3Org2Endpoint = "p3.org2.com"
+	p3Org2PKIID    = gcommon.PKIidType("pkiid_P3O2")
+
+	org3MSPID      = "Org3MSP"
+	p1Org3Endpoint = "p1.org3.com"
+	p1Org3PKIID    = gcommon.PKIidType("pkiid_P1O3")
+	p2Org3Endpoint = "p2.org3.com"
+	p2Org3PKIID    = gcommon.PKIidType("pkiid_P2O3")
+	p3Org3Endpoint = "p3.org3.com"
+	p3Org3PKIID    = gcommon.PKIidType("pkiid_P3O3")
+
+	committerRole = string(ledgerconfig.CommitterRole)
+	endorserRole  = string(ledgerconfig.EndorserRole)
+
+	respTimeout = 100 * time.Millisecond
+
+	value1 = &storeapi.ExpiringValue{Value: []byte("value1")}
+	value2 = &storeapi.ExpiringValue{Value: []byte("value2")}
+	value3 = &storeapi.ExpiringValue{Value: []byte("value3")}
+	value4 = &storeapi.ExpiringValue{Value: []byte("value4")}
+
+	key1 = kcommon.GetCASKey(value1.Value)
+	key2 = kcommon.GetCASKey(value2.Value)
+	key3 = kcommon.GetCASKey(value3.Value)
+	key4 = kcommon.GetCASKey(value4.Value)
+
+	txID = "tx1"
+)
+
+func TestProvider(t *testing.T) {
+	support := mocks.NewMockSupport().
+		CollectionPolicy(&mocks.MockAccessPolicy{
+			MaxPeerCount: 2,
+			Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+		}).
+		CollectionConfig(&cb.StaticCollectionConfig{
+			Type: cb.CollectionType_COL_OFFLEDGER,
+			Name: coll1,
+		})
+
+	getLocalMSPID = func() (string, error) { return org1MSPID, nil }
+
+	gossip := mocks.NewMockGossipAdapter()
+	gossip.Self(org1MSPID, mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, committerRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, committerRole)).
+		Member(org3MSPID, mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole))
+
+	localStore := spmocks.NewStore().
+		Data(storeapi.NewKey(txID, ns1, coll1, key1), value1)
+
+	storeProvider := func(channelID string) offledgerapi.Store { return localStore }
+	gossipProvider := func() supportapi.GossipAdapter { return gossip }
+
+	p := NewProvider(storeProvider, support, gossipProvider)
+
+	retriever := p.RetrieverForChannel(channelID)
+	require.NotNil(t, retriever)
+
+	t.Run("GetData - From local peer", func(t *testing.T) {
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key1))
+		require.NoError(t, err)
+		require.NotNil(t, value)
+		require.Equal(t, value1, value)
+	})
+
+	t.Run("GetData - From remote peer", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key2, value2).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key2))
+		require.NoError(t, err)
+		require.NotNil(t, value)
+		require.Equal(t, value2, value)
+	})
+
+	t.Run("GetData - No response from remote peer", func(t *testing.T) {
+		gossip.MessageHandler(func(msg *gproto.GossipMessage) {})
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key4))
+		require.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetData - Cancel request from remote peer", func(t *testing.T) {
+		gossip.MessageHandler(func(msg *gproto.GossipMessage) {})
+
+		ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
+		go func() {
+			time.Sleep(50 * time.Millisecond)
+			cancel()
+		}()
+
+		value, err := retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key4))
+		require.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetDataMultipleKeys -> success", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key2, value2).
+				Value(key3, value3).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		values, err := retriever.GetDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, key1, key2, key3))
+		require.NoError(t, err)
+		require.Equal(t, 3, len(values))
+		assert.Equal(t, value1, values[0])
+		assert.Equal(t, value2, values[1])
+		assert.Equal(t, value3, values[2])
+	})
+
+	t.Run("GetDataMultipleKeys - Key not found -> success", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key1, value1).
+				Value(key2, value2).
+				Value(key3, value3).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		values, err := retriever.GetDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, "xxx", key2, key3))
+		require.NoError(t, err)
+		require.Equal(t, 3, len(values))
+		assert.Nil(t, values[0])
+		assert.Equal(t, value2, values[1])
+		assert.Equal(t, value3, values[2])
+	})
+
+	t.Run("GetDataMultipleKeys - Timeout -> fail", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key1, value1).
+				Value(key2, value2).
+				Value(key3, value3).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), time.Microsecond)
+		values, err := retriever.GetDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, key1, key2, key3))
+		assert.NoError(t, err)
+		assert.Empty(t, values.Values().IsEmpty())
+	})
+
+	t.Run("Persist Missing -> success", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).Handle)
+
+		// Should not exist in local store
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key4))
+		require.NoError(t, err)
+		require.Nil(t, value)
+
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key4, value4).
+				Handle)
+
+		// Should retrieve from other peer and then persist to local store
+		value, err = retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key4))
+		require.NoError(t, err)
+		require.NotNil(t, value)
+
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).Handle)
+
+		// Should retrieve from local store
+		value, err = retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key4))
+		require.NoError(t, err)
+		require.NotNil(t, value)
+	})
+}
+
+func TestProvider_AccessDenied(t *testing.T) {
+	support := mocks.NewMockSupport().
+		CollectionPolicy(&mocks.MockAccessPolicy{
+			MaxPeerCount: 2,
+			Orgs:         []string{org1MSPID, org2MSPID},
+		})
+	support.Publisher = blockpublisher.New(channelID)
+
+	getLocalMSPID = func() (string, error) { return org3MSPID, nil }
+
+	gossip := mocks.NewMockGossipAdapter()
+	gossip.Self(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, committerRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole))
+
+	localStore := spmocks.NewStore().
+		Data(storeapi.NewKey(txID, ns1, coll1, key1), value1)
+
+	storeProvider := func(channelID string) offledgerapi.Store { return localStore }
+	gossipProvider := func() supportapi.GossipAdapter { return gossip }
+
+	p := NewProvider(storeProvider, support, gossipProvider)
+
+	retriever := p.RetrieverForChannel(channelID)
+	require.NotNil(t, retriever)
+
+	gossip.MessageHandler(
+		newMockGossipMsgHandler(channelID).
+			Value(key2, value2).
+			Handle)
+
+	t.Run("GetData - From remote peer -> nil", func(t *testing.T) {
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key2))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetDataMultipleKeys - From remote peer -> nil", func(t *testing.T) {
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		values, err := retriever.GetDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, "xxx", key2, key3))
+		assert.NoError(t, err)
+		assert.Nil(t, values)
+	})
+
+	t.Run("Missing from local", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key4, value4).
+				Handle)
+
+		// Shouldn't be authorized to retrieve from other peer
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetData(ctx, storeapi.NewKey(txID, ns1, coll1, key4))
+		require.NoError(t, err)
+		require.Nil(t, value)
+	})
+}
+
+type mockGossipMsgHandler struct {
+	channelID string
+	values    map[string]*storeapi.ExpiringValue
+}
+
+func newMockGossipMsgHandler(channelID string) *mockGossipMsgHandler {
+	return &mockGossipMsgHandler{
+		channelID: channelID,
+		values:    make(map[string]*storeapi.ExpiringValue),
+	}
+}
+
+func (m *mockGossipMsgHandler) Value(key string, value *storeapi.ExpiringValue) *mockGossipMsgHandler {
+	m.values[key] = value
+	return m
+}
+
+func (m *mockGossipMsgHandler) Handle(msg *gproto.GossipMessage) {
+	req := msg.GetCollDataReq()
+
+	res := &requestmgr.Response{
+		Endpoint:  "p1.org1",
+		MSPID:     "org1",
+		Identity:  []byte("p1.org1"),
+		Signature: []byte("sig"),
+	}
+
+	for _, d := range req.Digests {
+		e := &requestmgr.Element{
+			Namespace:  d.Namespace,
+			Collection: d.Collection,
+			Key:        d.Key,
+		}
+
+		v := m.values[d.Key]
+		if v != nil {
+			e.Value = v.Value
+			e.Expiry = v.Expiry
+		}
+
+		res.Data = append(res.Data, e)
+	}
+
+	requestmgr.Get(m.channelID).Respond(req.Nonce, res)
+}
diff --git a/extensions/collections/offledger/storeprovider/olstore.go b/extensions/collections/offledger/storeprovider/olstore.go
new file mode 100644
index 00000000..c3bc00a5
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/olstore.go
@@ -0,0 +1,316 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/cache"
+	"github.com/hyperledger/fabric/extensions/config"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	"github.com/hyperledger/fabric/protos/common"
+	cb "github.com/hyperledger/fabric/protos/common"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("offledgerstore")
+
+type store struct {
+	channelID   string
+	dbProvider  api.DBProvider
+	cache       *cache.Cache
+	collConfigs map[common.CollectionType]*collTypeConfig
+}
+
+func newStore(channelID string, dbProvider api.DBProvider, collConfigs map[common.CollectionType]*collTypeConfig) *store {
+	logger.Debugf("constructing collection data store")
+	return &store{
+		channelID:   channelID,
+		collConfigs: collConfigs,
+		dbProvider:  dbProvider,
+		cache:       cache.New(channelID, dbProvider, config.GetOLCollCacheSize()),
+	}
+}
+
+// Close closes the store
+func (s *store) Close() {
+	s.dbProvider.Close()
+}
+
+// Persist persists all data within the private data simulation results
+func (s *store) Persist(txID string, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	rwSet, err := rwsetutil.TxPvtRwSetFromProtoMsg(privateSimulationResultsWithConfig.PvtRwset)
+	if err != nil {
+		return errors.WithMessage(err, "error getting pvt RW set from bytes")
+	}
+
+	for _, nsRWSet := range rwSet.NsPvtRwSet {
+		for _, collRWSet := range nsRWSet.CollPvtRwSets {
+			if err := s.persistColl(txID, nsRWSet.NameSpace, privateSimulationResultsWithConfig.CollectionConfigs, collRWSet); err != nil {
+				return err
+			}
+		}
+	}
+
+	return nil
+}
+
+// PutData returns the  data for the given key
+func (s *store) PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error {
+	if value.Value == nil {
+		return errors.Errorf("attempt to put nil value for key [%s]", key)
+	}
+	if config.Name != key.Collection {
+		return errors.Errorf("invalid collection config for key [%s]", key)
+	}
+
+	key, value, err := s.decorate(config, key, value)
+	if err != nil {
+		return err
+	}
+
+	if !value.Expiry.IsZero() {
+		if value.Expiry.Before(time.Now()) {
+			// Already expired
+			logger.Debugf("[%s] Key [%s] already expired", s.channelID, key)
+			return nil
+		}
+	}
+
+	db, err := s.dbProvider.GetDB(key.Namespace, key.Collection)
+	if err != nil {
+		return err
+	}
+
+	logger.Debugf("[%s] Putting key [%s] to DB", s.channelID, key)
+	err = db.Put(api.NewKeyValue(key.Key, value.Value, key.EndorsedAtTxID, value.Expiry))
+	if err != nil {
+		return err
+	}
+
+	logger.Debugf("[%s] Putting key [%s] to cache", s.channelID, key)
+	s.cache.Put(key.Namespace, key.Collection, key.Key,
+		&api.Value{
+			Value:      value.Value,
+			TxID:       key.EndorsedAtTxID,
+			ExpiryTime: value.Expiry,
+		},
+	)
+
+	return nil
+}
+
+// GetData returns the  data for the given key
+func (s *store) GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return s.getData(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Key)
+}
+
+// tDataMultipleKeys returns the  data for the given keys
+func (s *store) GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return s.getDataMultipleKeys(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Keys...)
+}
+
+func (s *store) persistColl(txID string, ns string, collConfigPkgs map[string]*common.CollectionConfigPackage, collRWSet *rwsetutil.CollPvtRwSet) error {
+	config, exists := s.getCollectionConfig(collConfigPkgs, ns, collRWSet.CollectionName)
+	if !exists {
+		logger.Debugf("[%s]  config for collection [%s:%s] not found in config packages", s.channelID, ns, collRWSet.CollectionName)
+		return nil
+	}
+
+	authorized, err := s.isAuthorized(ns, config)
+	if err != nil {
+		return err
+	}
+	if !authorized {
+		logger.Infof("[%s] Will not store  collection [%s:%s] since local peer is not authorized.", s.channelID, ns, collRWSet.CollectionName)
+		return nil
+	}
+
+	logger.Debugf("[%s] Collection [%s:%s] is a  collection", s.channelID, ns, collRWSet.CollectionName)
+
+	expiryTime, err := s.getExpirationTime(config)
+	if err != nil {
+		return err
+	}
+
+	batch, err := s.createBatch(txID, ns, config, collRWSet, expiryTime)
+	if err != nil {
+		return err
+	}
+
+	db, err := s.dbProvider.GetDB(ns, collRWSet.CollectionName)
+	if err != nil {
+		return err
+	}
+
+	err = db.Put(batch...)
+	if err != nil {
+		return errors.Wrapf(err, "error persisting to [%s:%s]", ns, collRWSet.CollectionName)
+	}
+
+	for _, kv := range batch {
+		logger.Debugf("[%s] Putting key [%s:%s:%s] in Tx [%s]", s.channelID, ns, collRWSet.CollectionName, kv.Key, kv.TxID)
+		s.cache.Put(ns, collRWSet.CollectionName, kv.Key, kv.Value)
+	}
+
+	return nil
+}
+
+func (s *store) getData(txID, ns, coll, key string) (*storeapi.ExpiringValue, error) {
+	value, err := s.cache.Get(ns, coll, key)
+	if err != nil {
+		return nil, err
+	}
+
+	if value == nil {
+		return nil, nil
+	}
+
+	logger.Debugf("[%s] Got value for key [%s:%s:%s] which was persisted in transaction [%s]. Current tx [%s]", s.channelID, ns, coll, key, value.TxID, txID)
+	if value.TxID == txID {
+		logger.Debugf("[%s] Key [%s:%s:%s] was persisted in same transaction [%s] as caller. Returning nil.", s.channelID, ns, coll, key, txID)
+		return nil, nil
+	}
+
+	return &storeapi.ExpiringValue{Value: value.Value, Expiry: value.ExpiryTime}, nil
+}
+
+func (s *store) getDataMultipleKeys(txID, ns, coll string, keys ...string) (storeapi.ExpiringValues, error) {
+	values, err := s.cache.GetMultiple(ns, coll, keys...)
+	if err != nil {
+		return nil, err
+	}
+
+	if len(values) != len(keys) {
+		return nil, errors.New("not all of the values were returned for the set of keys")
+	}
+
+	var ret storeapi.ExpiringValues
+	for i, value := range values {
+		var v *storeapi.ExpiringValue
+		if value != nil {
+			logger.Debugf("[%s] Got value for key [%s:%s:%s] which was persisted in transaction [%s]. Current tx [%s]", s.channelID, ns, coll, keys[i], value.TxID, txID)
+			if value.TxID == txID {
+				logger.Debugf("[%s] Key [%s:%s:%s] was persisted in same transaction [%s] as caller. Returning nil.", s.channelID, ns, coll, keys[i], txID)
+			} else {
+				v = &storeapi.ExpiringValue{Value: value.Value, Expiry: value.ExpiryTime}
+			}
+		}
+		ret = append(ret, v)
+	}
+
+	return ret, nil
+}
+
+func (s *store) createBatch(txID, ns string, config *cb.StaticCollectionConfig, collRWSet *rwsetutil.CollPvtRwSet, expiryTime time.Time) ([]*api.KeyValue, error) {
+	var batch []*api.KeyValue
+	for _, wSet := range collRWSet.KvRwSet.Writes {
+		if wSet.IsDelete {
+			return nil, errors.Errorf("[%s] Attempt to delete key [%s] in collection [%s:%s]", s.channelID, wSet.Key, ns, collRWSet.CollectionName)
+		}
+
+		key := storeapi.NewKey(txID, ns, collRWSet.CollectionName, wSet.Key)
+		value := &storeapi.ExpiringValue{
+			Value:  wSet.Value,
+			Expiry: expiryTime,
+		}
+
+		key, value, err := s.decorate(config, key, value)
+		if err != nil {
+			return nil, err
+		}
+
+		batch = append(batch, api.NewKeyValue(key.Key, value.Value, txID, value.Expiry))
+	}
+	return batch, nil
+}
+
+func (s *store) isAuthorized(ns string, config *common.StaticCollectionConfig) (bool, error) {
+	policy, err := s.loadPolicy(ns, config)
+	if err != nil {
+		logger.Errorf("[%s] Error loading policy for collection [%s:%s]: %s", s.channelID, ns, config.Name, err)
+		return false, err
+	}
+
+	localMSPID, err := getLocalMSPID()
+	if err != nil {
+		logger.Errorf("[%s] Error getting local MSP ID: %s", s.channelID, err)
+		return false, err
+	}
+	for _, mspID := range policy.MemberOrgs() {
+		if mspID == localMSPID {
+			return true, nil
+		}
+	}
+	return false, nil
+}
+
+// TODO: Consider caching policies to avoid marshalling every time
+func (s *store) loadPolicy(ns string, config *common.StaticCollectionConfig) (privdata.CollectionAccessPolicy, error) {
+	logger.Debugf("[%s] Loading collection policy for [%s:%s]", s.channelID, ns, config.Name)
+
+	colAP := &privdata.SimpleCollection{}
+	err := colAP.Setup(config, mspmgmt.GetIdentityDeserializer(s.channelID))
+	if err != nil {
+		return nil, errors.Wrapf(err, "error setting up collection policy %s", config.Name)
+	}
+
+	return colAP, nil
+}
+
+func (s *store) getExpirationTime(config *common.StaticCollectionConfig) (time.Time, error) {
+	var expiryTime time.Time
+	if config.TimeToLive == "" {
+		return expiryTime, nil
+	}
+	ttl, e := time.ParseDuration(config.TimeToLive)
+	if e != nil {
+		// This shouldn't happen since the config was validated before being persisted
+		return expiryTime, errors.Wrapf(e, "error parsing time-to-live for collection [%s]", config.Name)
+	}
+	return time.Now().Add(ttl), nil
+}
+
+func (s *store) getCollectionConfig(collConfigPkgs map[string]*common.CollectionConfigPackage, namespace, collName string) (*common.StaticCollectionConfig, bool) {
+	collConfigPkg, ok := collConfigPkgs[namespace]
+	if !ok {
+		return nil, false
+	}
+
+	for _, collConfig := range collConfigPkg.Config {
+		config := collConfig.GetStaticCollectionConfig()
+		if config != nil && config.Name == collName && s.collTypeSupported(config.Type) {
+			return config, true
+		}
+	}
+
+	return nil, false
+}
+
+func (s *store) decorate(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) (*storeapi.Key, *storeapi.ExpiringValue, error) {
+	cfg, ok := s.collConfigs[config.Type]
+	if !ok || cfg.decorator == nil {
+		return key, value, nil
+	}
+	return cfg.decorator(key, value)
+}
+
+func (s *store) collTypeSupported(collType cb.CollectionType) bool {
+	_, ok := s.collConfigs[collType]
+	return ok
+}
+
+// getLocalMSPID returns the MSP ID of the local peer. This variable may be overridden by unit tests.
+var getLocalMSPID = func() (string, error) {
+	return mspmgmt.GetLocalMSP().GetIdentifier()
+}
diff --git a/extensions/collections/offledger/storeprovider/olstore_test.go b/extensions/collections/offledger/storeprovider/olstore_test.go
new file mode 100644
index 00000000..1d6b2948
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/olstore_test.go
@@ -0,0 +1,427 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"fmt"
+	"testing"
+	"time"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	olstoreapi "github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	olmocks "github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/mocks"
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+
+	txID1 = "txid1"
+	txID2 = "txid2"
+	txID3 = "txid3"
+
+	ns1 = "ns1"
+
+	coll0 = "coll0"
+	coll1 = "coll1"
+	coll2 = "coll2"
+	coll3 = "coll3"
+
+	org1MSP = "Org1MSP"
+	org2MSP = "Org2MSP"
+)
+
+var (
+	value1_1 = []byte("value1_1")
+	value1_2 = []byte("value1_2")
+	value2_1 = []byte("value2_1")
+	value3_1 = []byte("value3_1")
+	value4_1 = []byte("value4_1")
+
+	key1 = "key1"
+	key2 = "key2"
+	key3 = "key3"
+)
+
+func TestStore_Close(t *testing.T) {
+	collTypeConfig := map[cb.CollectionType]*collTypeConfig{cb.CollectionType_COL_OFFLEDGER: {}}
+	s := newStore(channelID, olmocks.NewDBProvider(), collTypeConfig)
+	require.NotNil(t, s)
+
+	s.Close()
+
+	// Multiple calls on Close are allowed
+	assert.NotPanics(t, func() {
+		s.Close()
+	})
+}
+
+func TestStore_PutAndGet(t *testing.T) {
+	collTypeConfig := map[cb.CollectionType]*collTypeConfig{cb.CollectionType_COL_OFFLEDGER: {}}
+	s := newStore(channelID, olmocks.NewDBProvider(), collTypeConfig)
+	require.NotNil(t, s)
+	defer s.Close()
+
+	getLocalMSPID = func() (string, error) { return org1MSP, nil }
+
+	b := mocks.NewPvtReadWriteSetBuilder()
+	ns1Builder := b.Namespace(ns1)
+	coll1Builder := ns1Builder.Collection(coll1)
+	coll1Builder.
+		OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, value1_1).
+		Write(key2, value1_2)
+	coll2Builder := ns1Builder.Collection(coll2)
+	coll2Builder.
+		OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "").
+		Write(key2, value2_1)
+	coll3Builder := ns1Builder.Collection(coll3)
+	coll3Builder.
+		StaticConfig("OR('Org1MSP.member')", 1, 2, 100).
+		Write(key1, value3_1)
+
+	err := s.Persist(txID1, b.Build())
+	assert.NoError(t, err)
+
+	t.Run("GetData invalid collection -> nil", func(t *testing.T) {
+		value, err := s.GetData(storeapi.NewKey(txID1, ns1, coll0, key1))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetData in same transaction -> nil", func(t *testing.T) {
+		value, err := s.GetData(storeapi.NewKey(txID1, ns1, coll1, key1))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetData in new transaction -> valid", func(t *testing.T) {
+		value, err := s.GetData(storeapi.NewKey(txID2, ns1, coll1, key1))
+		assert.NoError(t, err)
+		require.NotNil(t, value)
+		assert.Equal(t, value1_1, value.Value)
+
+		value, err = s.GetData(storeapi.NewKey(txID2, ns1, coll1, key2))
+		assert.NoError(t, err)
+		require.NotNil(t, value)
+		assert.Equal(t, value1_2, value.Value)
+	})
+
+	t.Run("GetData collection2 -> valid", func(t *testing.T) {
+		// Collection2
+		value, err := s.GetData(storeapi.NewKey(txID2, ns1, coll2, key2))
+		assert.NoError(t, err)
+		require.NotNil(t, value)
+		assert.Equal(t, value2_1, value.Value)
+	})
+
+	t.Run("GetData on non-off-ledger collection -> nil", func(t *testing.T) {
+		value, err := s.GetData(storeapi.NewKey(txID2, ns1, coll3, key1))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetDataMultipleKeys in same transaction -> nil", func(t *testing.T) {
+		values, err := s.GetDataMultipleKeys(storeapi.NewMultiKey(txID1, ns1, coll1, key1, key2))
+		assert.NoError(t, err)
+		require.Equal(t, 2, len(values))
+		assert.Nil(t, values[0])
+		assert.Nil(t, values[1])
+	})
+
+	t.Run("GetDataMultipleKeys in new transaction -> valid", func(t *testing.T) {
+		values, err := s.GetDataMultipleKeys(storeapi.NewMultiKey(txID2, ns1, coll1, key1, key2))
+		assert.NoError(t, err)
+		require.Equal(t, 2, len(values))
+		require.NotNil(t, values[0])
+		require.NotNil(t, values[1])
+		assert.Equal(t, value1_1, values[0].Value)
+		assert.Equal(t, value1_2, values[1].Value)
+	})
+
+	t.Run("Disallow delete data", func(t *testing.T) {
+		b := mocks.NewPvtReadWriteSetBuilder()
+		ns1Builder := b.Namespace(ns1)
+		coll1Builder := ns1Builder.Collection(coll1)
+		coll1Builder.
+			OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "1m").
+			Delete(key1)
+
+		err = s.Persist(txID2, b.Build())
+		assert.Error(t, err)
+	})
+
+	t.Run("Expire data", func(t *testing.T) {
+		b := mocks.NewPvtReadWriteSetBuilder()
+		ns1Builder := b.Namespace(ns1)
+		coll1Builder := ns1Builder.Collection(coll1)
+		coll1Builder.
+			OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "10ms").
+			Write(key3, value3_1)
+
+		err = s.Persist(txID2, b.Build())
+		assert.NoError(t, err)
+
+		value, err := s.GetData(storeapi.NewKey(txID3, ns1, coll1, key3))
+		assert.NoError(t, err)
+		require.NotNil(t, value)
+		assert.Equal(t, value3_1, value.Value)
+
+		time.Sleep(200 * time.Millisecond)
+
+		value, err = s.GetData(storeapi.NewKey(txID3, ns1, coll1, key3))
+		assert.NoError(t, err)
+		assert.Nilf(t, value, "expecting key to have expired")
+	})
+}
+
+func TestStore_LoadFromDB(t *testing.T) {
+	getLocalMSPID = func() (string, error) { return org1MSP, nil }
+
+	dbProvider := olmocks.NewDBProvider().
+		WithValue(ns1, coll1, key1, &olstoreapi.Value{Value: value1_1})
+
+	collTypeConfig := map[cb.CollectionType]*collTypeConfig{cb.CollectionType_COL_OFFLEDGER: {}}
+	s := newStore(channelID, dbProvider, collTypeConfig)
+	require.NotNil(t, s)
+	defer s.Close()
+
+	t.Run("GetData -> success", func(t *testing.T) {
+		value, err := s.GetData(storeapi.NewKey(txID2, ns1, coll1, key1))
+		assert.NoError(t, err)
+		assert.Equal(t, value1_1, value.Value)
+	})
+}
+
+func TestStore_PersistError(t *testing.T) {
+	getLocalMSPID = func() (string, error) { return org1MSP, nil }
+
+	collTypeConfig := map[cb.CollectionType]*collTypeConfig{cb.CollectionType_COL_OFFLEDGER: {}}
+	s := newStore(channelID, olmocks.NewDBProvider(), collTypeConfig)
+	require.NotNil(t, s)
+
+	defer s.Close()
+
+	t.Run("Persist marshal error -> error", func(t *testing.T) {
+		b := mocks.NewPvtReadWriteSetBuilder()
+		b.Namespace(ns1).
+			Collection(coll1).
+			OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "1m").
+			Write(key1, value1_1).
+			WithMarshalError()
+
+		err := s.Persist(txID1, b.Build())
+		assert.Errorf(t, err, "expecting marshal error")
+	})
+
+	t.Run("Persist invalid duration -> error", func(t *testing.T) {
+		b := mocks.NewPvtReadWriteSetBuilder()
+		ns1Builder := b.Namespace(ns1)
+		coll1Builder := ns1Builder.Collection(coll1)
+		coll1Builder.
+			OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "xxxxx")
+
+		err := s.Persist(txID1, b.Build())
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "invalid duration")
+	})
+}
+
+func TestStore_PutData(t *testing.T) {
+	getLocalMSPID = func() (string, error) { return org1MSP, nil }
+
+	collTypeConfig := map[cb.CollectionType]*collTypeConfig{cb.CollectionType_COL_OFFLEDGER: {}}
+	s := newStore(channelID, olmocks.NewDBProvider(), collTypeConfig)
+	require.NotNil(t, s)
+
+	defer s.Close()
+
+	collConfig := &cb.StaticCollectionConfig{
+		Type: cb.CollectionType_COL_OFFLEDGER,
+		Name: coll1,
+	}
+
+	t.Run("Valid key -> success", func(t *testing.T) {
+		err := s.PutData(
+			collConfig,
+			&storeapi.Key{
+				EndorsedAtTxID: txID1,
+				Namespace:      ns1,
+				Collection:     coll1,
+				Key:            common.GetCASKey(value1_1),
+			},
+			&storeapi.ExpiringValue{
+				Value: value1_1,
+			},
+		)
+		assert.NoError(t, err)
+
+		v, err := s.GetData(&storeapi.Key{EndorsedAtTxID: txID2, Namespace: ns1, Collection: coll1, Key: common.GetCASKey(value1_1)})
+		assert.NoError(t, err)
+		require.NotNil(t, v)
+		assert.Equal(t, value1_1, v.Value)
+	})
+
+	t.Run("Nil value -> error", func(t *testing.T) {
+		err := s.PutData(
+			collConfig,
+			&storeapi.Key{
+				EndorsedAtTxID: txID1,
+				Namespace:      ns1,
+				Collection:     coll1,
+			},
+			&storeapi.ExpiringValue{},
+		)
+		assert.Error(t, err)
+	})
+
+	t.Run("Already expired -> not added", func(t *testing.T) {
+		err := s.PutData(
+			collConfig,
+			&storeapi.Key{
+				EndorsedAtTxID: txID1,
+				Namespace:      ns1,
+				Collection:     coll1,
+			},
+			&storeapi.ExpiringValue{
+				Value:  value4_1,
+				Expiry: time.Now().Add(-1 * time.Second),
+			},
+		)
+		assert.NoError(t, err)
+
+		v, err := s.GetData(&storeapi.Key{EndorsedAtTxID: txID2, Namespace: ns1, Collection: coll1, Key: common.GetCASKey(value4_1)})
+		assert.NoError(t, err)
+		require.Nil(t, v)
+	})
+}
+
+func TestStore_DBError(t *testing.T) {
+	getLocalMSPID = func() (string, error) { return org1MSP, nil }
+
+	dbProvider := olmocks.NewDBProvider()
+	collTypeConfig := map[cb.CollectionType]*collTypeConfig{cb.CollectionType_COL_OFFLEDGER: {}}
+	s := newStore(channelID, dbProvider, collTypeConfig)
+	require.NotNil(t, s)
+
+	t.Run("GetData -> error", func(t *testing.T) {
+		key := storeapi.NewKey(txID1, ns1, coll1, key1)
+
+		expectedErr := fmt.Errorf("error getting DB")
+		dbProvider.WithError(expectedErr)
+		v, err := s.GetData(key)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		assert.Nil(t, v)
+
+		expectedErr = fmt.Errorf("error getting value")
+		dbProvider.WithError(nil).MockDB(ns1, coll1).WithError(expectedErr)
+		v, err = s.GetData(key)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		assert.Nil(t, v)
+	})
+
+	t.Run("GetDataMultipleKeys -> error", func(t *testing.T) {
+		key := storeapi.NewMultiKey(txID1, ns1, coll1, key1)
+
+		expectedErr := fmt.Errorf("error getting DB")
+		dbProvider.WithError(expectedErr)
+		v, err := s.GetDataMultipleKeys(key)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		assert.Nil(t, v)
+
+		expectedErr = fmt.Errorf("error getting value")
+		dbProvider.WithError(nil).MockDB(ns1, coll1).WithError(expectedErr)
+		v, err = s.GetDataMultipleKeys(key)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		assert.Nil(t, v)
+	})
+
+	t.Run("Persist -> error", func(t *testing.T) {
+		expectedErr := fmt.Errorf("error getting DB")
+		dbProvider.WithError(expectedErr)
+
+		b := mocks.NewPvtReadWriteSetBuilder()
+		b.Namespace(ns1).Collection(coll1).OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "1m")
+
+		err := s.Persist(txID1, b.Build())
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+
+		expectedErr = fmt.Errorf("error putting value")
+		dbProvider.WithError(nil).MockDB(ns1, coll1).WithError(expectedErr)
+
+		err = s.Persist(txID1, b.Build())
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+	})
+
+	t.Run("Persist -> error", func(t *testing.T) {
+		expectedErr := fmt.Errorf("error getting DB")
+		dbProvider.WithError(expectedErr)
+
+		collConfig := &cb.StaticCollectionConfig{
+			Type: cb.CollectionType_COL_OFFLEDGER,
+			Name: coll1,
+		}
+
+		key := &storeapi.Key{EndorsedAtTxID: txID1, Namespace: ns1, Collection: coll1}
+		value := &storeapi.ExpiringValue{Value: value1_1}
+
+		err := s.PutData(collConfig, key, value)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+
+		expectedErr = fmt.Errorf("error putting value")
+		dbProvider.WithError(nil).MockDB(ns1, coll1).WithError(expectedErr)
+
+		err = s.PutData(collConfig, key, value)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+	})
+}
+
+func TestStore_PersistNotAuthorized(t *testing.T) {
+	getLocalMSPID = func() (string, error) { return org2MSP, nil }
+
+	collTypeConfig := map[cb.CollectionType]*collTypeConfig{cb.CollectionType_COL_OFFLEDGER: {}}
+	s := newStore(channelID, olmocks.NewDBProvider(), collTypeConfig)
+	require.NotNil(t, s)
+	defer s.Close()
+
+	b := mocks.NewPvtReadWriteSetBuilder()
+	ns1Builder := b.Namespace(ns1)
+	coll1Builder := ns1Builder.Collection(coll1)
+	coll1Builder.
+		OffLedgerConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, value1_1)
+	coll2Builder := ns1Builder.Collection(coll2)
+	coll2Builder.
+		OffLedgerConfig("OR('Org1MSP.member','Org2MSP.member')", 1, 2, "").
+		Write(key2, value2_1)
+
+	err := s.Persist(txID1, b.Build())
+	assert.NoError(t, err)
+
+	// Key shouldn't have been persisted since org2 doesn't have access to collection1
+	value, err := s.GetData(storeapi.NewKey(txID2, ns1, coll1, key1))
+	assert.NoError(t, err)
+	assert.Nil(t, value)
+
+	// Key should have been persisted to collection2
+	value, err = s.GetData(storeapi.NewKey(txID2, ns1, coll2, key2))
+	assert.NoError(t, err)
+	require.NotNil(t, value)
+	assert.Equal(t, value2_1, value.Value)
+}
diff --git a/extensions/collections/offledger/storeprovider/olstoreprovider.go b/extensions/collections/offledger/storeprovider/olstoreprovider.go
new file mode 100644
index 00000000..6e6b593c
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/olstoreprovider.go
@@ -0,0 +1,108 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"sync"
+
+	"github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/couchdbstore"
+	"github.com/hyperledger/fabric/protos/common"
+)
+
+// Option is a store provider option
+type Option func(p *StoreProvider)
+
+// CollOption is a collection option
+type CollOption func(c *collTypeConfig)
+
+// WithCollectionType adds a collection type to the set of collection types supported by the off-ledger store
+// along with any options
+func WithCollectionType(collType common.CollectionType, opts ...CollOption) Option {
+	return func(p *StoreProvider) {
+		c := &collTypeConfig{}
+		p.collConfigs[collType] = c
+
+		for _, opt := range opts {
+			opt(c)
+		}
+	}
+}
+
+// Decorator allows the key/value to be modified before being persisted
+type Decorator func(key *storeapi.Key, value *storeapi.ExpiringValue) (*storeapi.Key, *storeapi.ExpiringValue, error)
+
+// WithDecorator sets a decorator for a collection type allowing the key/value to be modified before being persisted
+func WithDecorator(decorator Decorator) CollOption {
+	return func(c *collTypeConfig) {
+		c.decorator = decorator
+	}
+}
+
+type collTypeConfig struct {
+	decorator Decorator
+}
+
+// NewProviderFactory returns a store provider factory
+func NewProviderFactory(opts ...Option) *StoreProvider {
+	p := &StoreProvider{
+		stores:      make(map[string]offledger.Store),
+		dbProvider:  getDBProvider(),
+		collConfigs: make(map[common.CollectionType]*collTypeConfig),
+	}
+
+	// OFF_LEDGER collection type supported by default
+	opts = append(opts, WithCollectionType(common.CollectionType_COL_OFFLEDGER))
+
+	// Apply options
+	for _, opt := range opts {
+		opt(p)
+	}
+	return p
+}
+
+// StoreProvider is a store provider
+type StoreProvider struct {
+	stores map[string]offledger.Store
+	sync.RWMutex
+	dbProvider  api.DBProvider
+	collConfigs map[common.CollectionType]*collTypeConfig
+}
+
+// StoreForChannel returns the store for the given channel
+func (sp *StoreProvider) StoreForChannel(channelID string) offledger.Store {
+	sp.RLock()
+	defer sp.RUnlock()
+	return sp.stores[channelID]
+}
+
+// OpenStore opens the store for the given channel
+func (sp *StoreProvider) OpenStore(channelID string) (offledger.Store, error) {
+	sp.Lock()
+	defer sp.Unlock()
+
+	store, ok := sp.stores[channelID]
+	if !ok {
+		store = newStore(channelID, sp.dbProvider, sp.collConfigs)
+		sp.stores[channelID] = store
+	}
+	return store, nil
+}
+
+// Close shuts down all of the stores
+func (sp *StoreProvider) Close() {
+	for _, s := range sp.stores {
+		s.Close()
+	}
+}
+
+// getDBProvider returns the DB provider. This var may be overridden by unit tests
+var getDBProvider = func() api.DBProvider {
+	return couchdbstore.NewDBProvider()
+}
diff --git a/extensions/collections/offledger/storeprovider/olstoreprovider_test.go b/extensions/collections/offledger/storeprovider/olstoreprovider_test.go
new file mode 100644
index 00000000..a571e6fa
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/olstoreprovider_test.go
@@ -0,0 +1,90 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"fmt"
+	"os"
+	"reflect"
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/integration/runner"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestStore(t *testing.T) {
+	channel1 := "channel1"
+	channel2 := "channel2"
+
+	f := NewProviderFactory()
+	require.NotNil(t, f)
+
+	s1, err := f.OpenStore(channel1)
+	require.NotNil(t, s1)
+	assert.NoError(t, err)
+	assert.Equal(t, "*storeprovider.store", reflect.TypeOf(s1).String())
+	assert.Equal(t, s1, f.StoreForChannel(channel1))
+
+	s2, err := f.OpenStore(channel2)
+	require.NotNil(t, s2)
+	assert.NoError(t, err)
+	assert.NotEqual(t, s1, s2)
+	assert.Equal(t, s2, f.StoreForChannel(channel2))
+
+	assert.NotPanics(t, func() {
+		f.Close()
+	})
+}
+
+func TestMain(m *testing.M) {
+	viper.Set("offledger.cleanupExpired.Interval", "100ms")
+
+	// Switch to CouchDB
+	couchAddress, cleanup := couchDBSetup()
+	defer cleanup()
+
+	// CouchDB configuration
+	viper.Set("ledger.state.couchDBConfig.couchDBAddress", couchAddress)
+	viper.Set("ledger.state.couchDBConfig.username", "")
+	viper.Set("ledger.state.couchDBConfig.password", "")
+	viper.Set("ledger.state.couchDBConfig.maxRetries", 3)
+	viper.Set("ledger.state.couchDBConfig.maxRetriesOnStartup", 20)
+	viper.Set("ledger.state.couchDBConfig.requestTimeout", time.Second*35)
+	viper.Set("ledger.state.couchDBConfig.createGlobalChangesDB", true)
+
+	//set the logging level to DEBUG to test debug only code
+	flogging.ActivateSpec("couchdb=debug")
+	os.Exit(m.Run())
+}
+
+func removePath(t testing.TB, path string) {
+	if err := os.RemoveAll(path); err != nil {
+		t.Fatalf("Err: %s", err)
+	}
+}
+
+func couchDBSetup() (addr string, cleanup func()) {
+	externalCouch, set := os.LookupEnv("COUCHDB_ADDR")
+	if set {
+		return externalCouch, func() {}
+	}
+
+	couchDB := &runner.CouchDB{}
+	if err := couchDB.Start(); err != nil {
+		err := fmt.Errorf("Failed to start couchDB: %s", err)
+		panic(err)
+	}
+	return couchDB.Address(), func() {
+		if err := couchDB.Stop(); err != nil {
+			panic(err.Error())
+		}
+	}
+}
diff --git a/extensions/collections/offledger/storeprovider/store/api/api.go b/extensions/collections/offledger/storeprovider/store/api/api.go
new file mode 100644
index 00000000..e5fc5fda
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/api/api.go
@@ -0,0 +1,61 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	"time"
+)
+
+// Value is a data value
+type Value struct {
+	Value      []byte
+	TxID       string
+	ExpiryTime time.Time
+}
+
+// KeyValue is a struct to store a key value pair
+type KeyValue struct {
+	*Value
+	Key string
+}
+
+// NewKeyValue returns a new key
+func NewKeyValue(key string, value []byte, txID string, expiryTime time.Time) *KeyValue {
+	return &KeyValue{
+		Key:   key,
+		Value: &Value{Value: value, TxID: txID, ExpiryTime: expiryTime},
+	}
+}
+
+// String returns the string representation of the key
+func (k *KeyValue) String() string {
+	return k.Key
+}
+
+// DB persists collection data.
+type DB interface {
+	// Put stores the given set of keys/values. If expiry time is 0 then the data lives forever.
+	Put(keyVal ...*KeyValue) error
+
+	// Get returns the value for the given key or nil if the key doesn't exist
+	Get(key string) (*Value, error)
+
+	// Get returns the values for multiple keys. The values are returned in the same order as the keys.
+	GetMultiple(keys ...string) ([]*Value, error)
+
+	// DeleteExpiredKeys deletes all of the expired keys
+	DeleteExpiredKeys() error
+}
+
+// DBProvider returns the persister for the given namespace/collection
+type DBProvider interface {
+	// GetDB return the DB for the given namespace/collection
+	GetDB(ns, coll string) (DB, error)
+
+	// Close closes the DB provider
+	Close()
+}
diff --git a/extensions/collections/offledger/storeprovider/store/cache/cache.go b/extensions/collections/offledger/storeprovider/store/cache/cache.go
new file mode 100644
index 00000000..6797775f
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/cache/cache.go
@@ -0,0 +1,159 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cache
+
+import (
+	"fmt"
+	"time"
+
+	"github.com/bluele/gcache"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("offledger")
+
+// Cache implements a cache for collection data
+type Cache struct {
+	channelID  string
+	cache      gcache.Cache
+	dbProvider api.DBProvider
+}
+
+type cacheKey struct {
+	namespace  string
+	collection string
+	key        string
+}
+
+func (k cacheKey) String() string {
+	return fmt.Sprintf("%s:%s:%s", k.namespace, k.collection, k.key)
+}
+
+// New returns a new collection data cache
+func New(channelID string, dbProvider api.DBProvider, size int) *Cache {
+	c := &Cache{
+		channelID:  channelID,
+		dbProvider: dbProvider,
+	}
+	c.cache = gcache.New(size).ARC().LoaderExpireFunc(
+		func(k interface{}) (interface{}, *time.Duration, error) {
+			key := k.(cacheKey)
+			v, remainingTime, err := c.load(key)
+			if err != nil {
+				logger.Warningf("[%s] Error loading key [%s]: %s", c.channelID, key, err)
+				return nil, nil, err
+			}
+			return v, remainingTime, nil
+		}).Build()
+	return c
+}
+
+// Put adds the value for the given key.
+func (c *Cache) Put(ns, coll, key string, value *api.Value) {
+	cKey := cacheKey{
+		namespace:  ns,
+		collection: coll,
+		key:        key,
+	}
+
+	var err error
+	if value.ExpiryTime.IsZero() {
+		logger.Debugf("[%s] Putting key [%s]. Expires: NEVER", c.channelID, cKey)
+		err = c.cache.Set(cKey, value)
+	} else if value.ExpiryTime.Before(time.Now()) {
+		logger.Debugf("[%s] Expiry time for key [%s] occurs in the past. Value will not be added", c.channelID, cKey)
+	} else {
+		logger.Debugf("[%s] Putting key [%s]. Expires: %s", c.channelID, cKey, value.ExpiryTime)
+		err = c.cache.SetWithExpire(cKey, value, time.Until(value.ExpiryTime))
+	}
+
+	if err != nil {
+		panic("Set must never return an error")
+	}
+}
+
+// Get returns the values for the given keys
+func (c *Cache) Get(ns, coll, key string) (*api.Value, error) {
+	cKey := cacheKey{
+		namespace:  ns,
+		collection: coll,
+		key:        key,
+	}
+
+	value, err := c.cache.Get(cKey)
+	if err != nil {
+		logger.Warningf("[%s] Error getting key [%s]: %s", c.channelID, cKey, err)
+		return nil, err
+	}
+
+	if value == nil {
+		logger.Debugf("[%s] Key not found [%s]", c.channelID, cKey)
+		return nil, nil
+	}
+
+	v, ok := value.(*api.Value)
+	if !ok {
+		panic("Invalid value type!")
+	}
+	if v == nil {
+		logger.Debugf("[%s] Key not found [%s]", c.channelID, cKey)
+		return nil, nil
+	}
+
+	logger.Debugf("[%s] Got key [%s]. Expires: %s", c.channelID, cKey, v.ExpiryTime)
+	return v, nil
+}
+
+// GetMultiple returns the values for the given keys
+func (c *Cache) GetMultiple(ns, coll string, keys ...string) ([]*api.Value, error) {
+	values := make([]*api.Value, len(keys))
+	for i, key := range keys {
+		value, err := c.Get(ns, coll, key)
+		if err != nil {
+			return nil, err
+		}
+		values[i] = value
+	}
+	return values, nil
+}
+
+func (c *Cache) load(key cacheKey) (*api.Value, *time.Duration, error) {
+	db, err := c.dbProvider.GetDB(key.namespace, key.collection)
+	if err != nil {
+		return nil, nil, errors.WithMessage(err, "error getting database")
+	}
+
+	logger.Debugf("Loading value for key %s from DB", key)
+	v, err := db.Get(key.key)
+	if err != nil {
+		return nil, nil, errors.WithMessage(err, "error loading value")
+	}
+
+	logger.Debugf("Loaded value %v for key %s from DB", v, key)
+	if v == nil {
+		logger.Debugf("[%s] Value not found for key [%s]", c.channelID, key)
+		return nil, nil, nil
+	}
+
+	logger.Debugf("Checking expiry time for key %s", key)
+	if v.ExpiryTime.IsZero() {
+		logger.Debugf("[%s] Loaded key [%s]. Expires: NEVER", c.channelID, key)
+		return v, nil, nil
+
+	}
+	remainingTime := time.Until(v.ExpiryTime)
+	if remainingTime < 0 {
+		// Already expired
+		logger.Debugf("[%s] Loaded key [%s]. Expires: NOW", c.channelID, key)
+		return nil, nil, nil
+	}
+
+	logger.Debugf("[%s] Loaded key [%s]. Expires: %s", c.channelID, key, v.ExpiryTime)
+	return v, &remainingTime, nil
+}
diff --git a/extensions/collections/offledger/storeprovider/store/cache/cache_test.go b/extensions/collections/offledger/storeprovider/store/cache/cache_test.go
new file mode 100644
index 00000000..3c18fed3
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/cache/cache_test.go
@@ -0,0 +1,276 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cache
+
+import (
+	"fmt"
+	"math/rand"
+	"sync"
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/mocks"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+
+	ns1 = "ns1"
+
+	coll1 = "coll1"
+	coll2 = "coll2"
+
+	key1 = "key1"
+	key2 = "key2"
+	key3 = "key3"
+	key4 = "key4"
+	key5 = "key5"
+
+	txID1 = "tx1"
+)
+
+var (
+	value1 = []byte("value1")
+	value2 = []byte("value2")
+)
+
+func TestCache_PutAndGet(t *testing.T) {
+	now := time.Now()
+	v1 := &api.Value{Value: value1, TxID: txID1}
+	v2 := &api.Value{Value: value2, TxID: txID1}
+	v4 := &api.Value{Value: value2, TxID: txID1, ExpiryTime: now.Add(50 * time.Millisecond)}
+	v5 := &api.Value{Value: value2, TxID: txID1, ExpiryTime: now.Add(-1 * time.Minute)}
+
+	dbProvider := mocks.NewDBProvider()
+
+	c := New(channelID, dbProvider, 100)
+	require.NotNil(t, c)
+
+	c.Put(ns1, coll1, key1, v1)
+	c.Put(ns1, coll1, key2, v2)
+	c.Put(ns1, coll1, key4, v4)
+	c.Put(ns1, coll1, key5, v5)
+
+	v, err := c.Get(ns1, coll1, key1)
+	assert.NoError(t, err)
+	require.NotNil(t, v)
+	assert.Equal(t, v1, v)
+
+	values, err := c.GetMultiple(ns1, coll1, key1, key2, key3, key4, key5)
+	assert.NoError(t, err)
+	require.Equal(t, 5, len(values))
+	assert.Equal(t, v1, values[0])
+	assert.Equal(t, v2, values[1])
+	assert.Nil(t, values[2]) // Not added
+	assert.Equal(t, v4, values[3])
+	assert.Nil(t, values[4]) // Should not have been added
+
+	time.Sleep(100 * time.Millisecond)
+
+	v, err = c.Get(ns1, coll1, key4)
+	assert.NoError(t, err)
+	require.Nil(t, v) // Should have expired
+}
+
+func TestCache_LoadFromDB(t *testing.T) {
+	valueWithExpiry := &api.Value{
+		Value:      []byte("value1"),
+		ExpiryTime: time.Now().Add(time.Minute),
+	}
+	valueWithNoExpiry := &api.Value{
+		Value: []byte("value1"),
+	}
+	expiredValue := &api.Value{
+		Value:      []byte("value1"),
+		ExpiryTime: time.Now().Add(-1 * time.Minute),
+	}
+
+	dbProvider := mocks.NewDBProvider().
+		WithValue(ns1, coll1, key1, valueWithExpiry).
+		WithValue(ns1, coll1, key2, valueWithNoExpiry).
+		WithValue(ns1, coll1, key3, expiredValue)
+
+	c := New(channelID, dbProvider, 100)
+	require.NotNil(t, c)
+
+	// Not found
+	v, err := c.Get(ns1, coll2, key1)
+	assert.NoError(t, err)
+	assert.Nil(t, v)
+
+	// Found
+	v, err = c.Get(ns1, coll1, key1)
+	assert.NoError(t, err)
+	assert.Equal(t, valueWithExpiry, v)
+	v, err = c.Get(ns1, coll1, key2)
+	assert.NoError(t, err)
+	assert.Equal(t, valueWithNoExpiry, v)
+
+	// Expired
+	v, err = c.Get(ns1, coll1, key3)
+	assert.NoError(t, err)
+	assert.Nil(t, v)
+}
+
+func TestCache_LoadFromDBError(t *testing.T) {
+	t.Run("Error getting DB", func(t *testing.T) {
+		expectedErr := fmt.Errorf("get DB error")
+		dbProvider := mocks.NewDBProvider().
+			WithError(expectedErr)
+
+		c := New(channelID, dbProvider, 100)
+		require.NotNil(t, c)
+
+		v, err := c.Get(ns1, coll1, key1)
+		require.Error(t, err)
+		assert.Nil(t, v)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+	})
+
+	t.Run("Error loading key from DB", func(t *testing.T) {
+		expectedErr := fmt.Errorf("load key from DB error")
+		dbProvider := mocks.NewDBProvider()
+		dbProvider.MockDB(ns1, coll1).WithError(expectedErr)
+
+		c := New(channelID, dbProvider, 100)
+		require.NotNil(t, c)
+
+		v, err := c.Get(ns1, coll1, key1)
+		require.Error(t, err)
+		assert.Nil(t, v)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+
+		values, err := c.GetMultiple(ns1, coll1, key1, key2)
+		require.Error(t, err)
+		assert.Nil(t, values)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+	})
+}
+
+func TestCache_ReloadFromDB(t *testing.T) {
+	v1 := &api.Value{
+		Value:      []byte("value1"),
+		ExpiryTime: time.Now().Add(50 * time.Millisecond),
+	}
+	v2 := &api.Value{
+		Value:      []byte("value2"),
+		ExpiryTime: time.Now().Add(time.Minute),
+	}
+	v3 := &api.Value{
+		Value:      []byte("value3"),
+		ExpiryTime: time.Now().Add(time.Minute),
+	}
+	v4 := &api.Value{
+		Value:      []byte("value4"),
+		ExpiryTime: time.Now().Add(time.Minute),
+	}
+
+	dbProvider := mocks.NewDBProvider().
+		WithValue(ns1, coll1, key1, v1)
+
+	c := New(channelID, dbProvider, 1)
+	require.NotNil(t, c)
+
+	// v1 should be retrieved from the DB and cached
+	v, err := c.Get(ns1, coll1, key1)
+	assert.NoError(t, err)
+	assert.Equal(t, v1, v)
+
+	// Update the value in the DB
+	dbProvider.WithValue(ns1, coll1, key1, v2)
+
+	// v1 should still be retrieved from cache
+	v, err = c.Get(ns1, coll1, key1)
+	assert.NoError(t, err)
+	assert.Equal(t, v1, v)
+
+	time.Sleep(100 * time.Millisecond)
+
+	// key1 should have expired and v2 will be retrieved from the DB
+	v, err = c.Get(ns1, coll1, key1)
+	assert.NoError(t, err)
+	assert.Equal(t, v2, v)
+
+	// Update the value in the DB
+	dbProvider.WithValue(ns1, coll1, key1, v3)
+
+	// The following call should evict key1 from the cache
+	c.Put(ns1, coll1, key2, v4)
+
+	// key1 should have been evicted and v3 will be retrieved from the DB
+	v, err = c.Get(ns1, coll1, key1)
+	assert.NoError(t, err)
+	assert.Equal(t, v3, v)
+}
+
+func Test_Concurrency(t *testing.T) {
+	const (
+		nReaders = 10
+		nWriters = 3
+		nItems   = 1000
+		nReads   = 1000
+		txID1    = "tx1"
+	)
+
+	dbProvider := mocks.NewDBProvider()
+
+	c := New(channelID, dbProvider, 100)
+
+	var wWg sync.WaitGroup
+	wWg.Add(nWriters)
+
+	var rWg sync.WaitGroup
+	rWg.Add(nReaders)
+
+	for i := 0; i < nWriters; i++ {
+		go func() {
+			defer wWg.Done()
+
+			for j := 0; j < nItems; j++ {
+				key := fmt.Sprintf("k_%d", j)
+				expiry := time.Now().Add(getRandomDuration(100) * time.Millisecond)
+				c.Put(ns1, coll1, key, &api.Value{Value: []byte(key), TxID: txID1, ExpiryTime: expiry})
+				time.Sleep(time.Millisecond)
+			}
+		}()
+	}
+
+	var mutex sync.Mutex
+	var errors []error
+	for i := 0; i < nReaders; i++ {
+		go func() {
+			defer rWg.Done()
+
+			for j := 0; j < nReads; j++ {
+				_, err := c.Get(ns1, coll1, getRandomKey(nItems))
+				if err != nil {
+					mutex.Lock()
+					errors = append(errors, err)
+					mutex.Unlock()
+				}
+			}
+		}()
+	}
+
+	wWg.Wait()
+	rWg.Wait()
+
+	if len(errors) > 0 {
+		t.Errorf("Got errors: %s", errors)
+	}
+}
+
+func getRandomKey(max int32) string {
+	return fmt.Sprintf("k_%d", rand.Int31n(max))
+}
+
+func getRandomDuration(max int32) time.Duration {
+	return time.Duration(rand.Int31n(max))
+}
diff --git a/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore.go b/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore.go
new file mode 100644
index 00000000..586bac5c
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore.go
@@ -0,0 +1,212 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdbstore
+
+import (
+	"encoding/json"
+	"fmt"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/pkg/errors"
+)
+
+var (
+	logger          = flogging.MustGetLogger("offledgerstore")
+	compositeKeySep = "!"
+)
+
+const (
+	fetchExpiryDataQuery = `
+	{
+		"selector": {
+			"` + expiryField + `": {
+				"$lt": %v
+			}
+		},
+		"use_index": ["_design/` + expiryIndexDoc + `", "` + expiryIndexName + `"]
+	}`
+)
+
+// dataModel couch doc dataModel
+type dataModel struct {
+	ID      string `json:"_id"`
+	Rev     string `json:"_rev,omitempty"`
+	Data    string `json:"dataModel"`
+	TxnID   string `json:"txnID"`
+	Expiry  int64  `json:"expiry"`
+	Deleted bool   `json:"_deleted"`
+}
+
+type dbstore struct {
+	dbName string
+	db     *couchdb.CouchDatabase
+}
+
+// newDBStore constructs an instance of db store
+func newDBStore(db *couchdb.CouchDatabase, dbName string) *dbstore {
+	return &dbstore{dbName, db}
+}
+
+//-----------------Interface implementation functions--------------------//
+// AddKey adds dataModel to db
+func (s *dbstore) Put(keyVal ...*api.KeyValue) error {
+	docs := make([]*couchdb.CouchDoc, 0)
+	for _, kv := range keyVal {
+
+		dataDoc, err := createCouchDoc(string(encodeKey(kv.Key, time.Time{})), kv.Value)
+		if err != nil {
+			return err
+		}
+		if dataDoc != nil {
+			docs = append(docs, dataDoc)
+		}
+	}
+
+	_, err := s.db.BatchUpdateDocuments(docs)
+	if nil != err {
+		return errors.WithMessage(err, fmt.Sprintf("BatchUpdateDocuments failed for [%d] documents", len(docs)))
+	}
+
+	return nil
+}
+
+// GetKey get dataModel based on key from db
+func (s *dbstore) Get(key string) (*api.Value, error) {
+	data, err := fetchData(s.db, string(encodeKey(key, time.Time{})))
+	if err != nil {
+		return nil, errors.Wrapf(err, "Failed to load key [%s] from db", key)
+	}
+
+	if data != nil {
+		val := &api.Value{Value: []byte(data.Data), TxID: data.TxnID, ExpiryTime: time.Unix(0, data.Expiry)}
+		return val, nil
+	}
+
+	return nil, nil
+}
+
+// GetMultiple retrieves values for multiple keys at once
+func (s *dbstore) GetMultiple(keys ...string) ([]*api.Value, error) {
+	values := make([]*api.Value, len(keys))
+	for i, k := range keys {
+		v, err := s.Get(k)
+		if err != nil {
+			return nil, err
+		}
+		values[i] = v
+	}
+
+	return values, nil
+}
+
+// DeleteExpiredKeys delete expired keys from db
+func (s *dbstore) DeleteExpiredKeys() error {
+	data, err := fetchExpiryData(s.db, time.Now())
+	if err != nil {
+		return err
+	}
+	if len(data) == 0 {
+		logger.Debugf("No keys to delete from db")
+		return nil
+	}
+
+	docs := make([]*couchdb.CouchDoc, 0)
+	docIDs := make([]string, 0)
+	for _, doc := range data {
+		updateDoc := &dataModel{ID: doc.ID, Data: doc.Data, TxnID: doc.TxnID, Expiry: doc.Expiry, Rev: doc.Rev, Deleted: true}
+		jsonBytes, err := json.Marshal(updateDoc)
+		if err != nil {
+			return err
+		}
+		couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+		docs = append(docs, &couchDoc)
+		docIDs = append(docIDs, updateDoc.ID)
+	}
+
+	if len(docs) > 0 {
+		_, err := s.db.BatchUpdateDocuments(docs)
+		if err != nil {
+			return errors.WithMessage(err, fmt.Sprintf("BatchUpdateDocuments failed for [%d] documents", len(docs)))
+		}
+		logger.Debugf("Deleted expired keys %s from db", docIDs)
+	}
+
+	return nil
+}
+
+// Close db
+func (s *dbstore) Close() {
+}
+
+//-----------------helper functions--------------------//
+func encodeKey(key string, expiryTime time.Time) []byte {
+	var compositeKey []byte
+	if !expiryTime.IsZero() {
+		compositeKey = append(compositeKey, []byte(fmt.Sprintf("%d", expiryTime.UnixNano()/int64(time.Millisecond)))...)
+		compositeKey = append(compositeKey, compositeKeySep...)
+	}
+	compositeKey = append(compositeKey, []byte(key)...)
+	return compositeKey
+}
+
+//-----------------database helper functions--------------------//
+func fetchData(db *couchdb.CouchDatabase, key string) (*dataModel, error) {
+	doc, _, err := db.ReadDoc(key)
+	if err != nil {
+		return nil, err
+	}
+
+	if doc == nil {
+		return nil, nil
+	}
+
+	var data dataModel
+	err = json.Unmarshal(doc.JSONValue, &data)
+	if err != nil {
+		return nil, errors.Wrapf(err, "Result from DB is not JSON encoded")
+	}
+
+	return &data, nil
+}
+
+func createCouchDoc(key string, value *api.Value) (*couchdb.CouchDoc, error) {
+	data := &dataModel{ID: key, Data: string(value.Value), TxnID: value.TxID, Expiry: value.ExpiryTime.UnixNano() / int64(time.Millisecond)}
+
+	jsonBytes, err := json.Marshal(data)
+	if err != nil {
+		return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	return &couchDoc, nil
+}
+
+func fetchExpiryData(db *couchdb.CouchDatabase, expiry time.Time) ([]*dataModel, error) {
+	results, _, err := db.QueryDocuments(fmt.Sprintf(fetchExpiryDataQuery, expiry.UnixNano()/int64(time.Millisecond)))
+	if err != nil {
+		return nil, err
+	}
+
+	if len(results) == 0 {
+		return nil, nil
+	}
+
+	var responses []*dataModel
+	for _, result := range results {
+		var data dataModel
+		err = json.Unmarshal(result.Value, &data)
+		if err != nil {
+			return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+		}
+		responses = append(responses, &data)
+	}
+
+	return responses, nil
+}
diff --git a/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go b/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go
new file mode 100644
index 00000000..b31d7e2f
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_provider.go
@@ -0,0 +1,145 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdbstore
+
+import (
+	"fmt"
+	"sync"
+	"time"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/config"
+)
+
+const (
+	expiryField     = "expiry"
+	expiryIndexName = "by_expiry"
+	expiryIndexDoc  = "indexExpiry"
+	expiryIndexDef  = `
+	{
+		"index": {
+			"fields": ["` + expiryField + `"]
+		},
+		"name": "` + expiryIndexName + `",
+		"ddoc": "` + expiryIndexDoc + `",
+		"type": "json"
+	}`
+)
+
+// CouchDBProvider provides an handle to a db
+type CouchDBProvider struct {
+	couchInstance *couchdb.CouchInstance
+	stores        map[string]*dbstore
+	mutex         sync.RWMutex
+	done          chan struct{}
+	closed        bool
+}
+
+// NewDBProvider creates a CouchDB Provider
+func NewDBProvider() *CouchDBProvider {
+	couchDBDef := couchdb.GetCouchDBDefinition()
+
+	couchInstance, err := couchdb.CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	if err != nil {
+		logger.Error(err)
+		return nil
+	}
+
+	p := &CouchDBProvider{
+		couchInstance: couchInstance,
+		done:          make(chan struct{}),
+		stores:        make(map[string]*dbstore),
+	}
+
+	p.periodicPurge()
+
+	return p
+}
+
+//GetDB based on ns%coll
+func (p *CouchDBProvider) GetDB(ns, coll string) (api.DB, error) {
+	dbName := dbName(ns, coll)
+
+	p.mutex.RLock()
+	s, ok := p.stores[dbName]
+	p.mutex.RUnlock()
+
+	if ok {
+		return s, nil
+	}
+
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	if !ok {
+		db, err := couchdb.CreateCouchDatabase(p.couchInstance, dbName)
+		if nil != err {
+			logger.Error(err)
+			return nil, nil
+		}
+		s = newDBStore(db, dbName)
+
+		err = db.CreateNewIndexWithRetry(expiryIndexDef, expiryIndexDoc)
+		if err != nil {
+			return nil, err
+		}
+		p.stores[dbName] = s
+	}
+
+	return s, nil
+}
+
+// Close cleans up the Provider
+func (p *CouchDBProvider) Close() {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	if !p.closed {
+		p.done <- struct{}{}
+		p.closed = true
+	}
+}
+
+// periodicPurge goroutine to purge dataModel based on config interval time
+func (p *CouchDBProvider) periodicPurge() {
+	ticker := time.NewTicker(config.GetOLCollExpirationCheckInterval())
+	go func() {
+		defer ticker.Stop()
+		for {
+			select {
+			case <-ticker.C:
+				for _, s := range p.getStores() {
+					err := s.DeleteExpiredKeys()
+					if err != nil {
+						logger.Errorf("Error deleting expired keys for [%s]", s.dbName)
+					}
+				}
+			case <-p.done:
+				logger.Infof("Periodic purge is exiting")
+				return
+			}
+		}
+	}()
+}
+
+// getStores retrieves dbstores contained in the provider
+func (p *CouchDBProvider) getStores() []*dbstore {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	var stores []*dbstore
+	for _, s := range p.stores {
+		stores = append(stores, s)
+	}
+	return stores
+}
+
+func dbName(ns, coll string) string {
+	return fmt.Sprintf("%s$%s", ns, coll)
+}
diff --git a/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_test.go b/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_test.go
new file mode 100644
index 00000000..4ab7f04b
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/couchdbstore/dbstore_test.go
@@ -0,0 +1,134 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package couchdbstore
+
+import (
+	"os"
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	ledgertestutil "github.com/hyperledger/fabric/core/ledger/testutil"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/testutil"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	txID1 = "txid1"
+	txID2 = "txid2"
+	ns1   = "namespace1"
+	coll2 = "coll2"
+	coll1 = "coll1"
+	coll3 = "coll3"
+	key1  = "key1"
+	key2  = "key2"
+	key3  = "key3"
+	key4  = "key4"
+)
+
+var (
+	value1 = []byte("v111")
+	value2 = []byte("v222")
+)
+
+func TestMain(m *testing.M) {
+	os.Exit(testMain(m))
+}
+
+func testMain(m *testing.M) int {
+	// Read the core.yaml file for default config.
+	ledgertestutil.SetupCoreYAMLConfig()
+
+	// CouchDB configuration
+	_, _, stop := testutil.SetupExtTestEnv()
+
+	//set the logging level to DEBUG to test debug only code
+	flogging.ActivateSpec("couchdb=debug")
+
+	viper.Set("offledger.cleanupExpired.Interval", "500ms")
+
+	//run the tests
+	code := m.Run()
+
+	//stop couchdb
+	stop()
+
+	return code
+}
+
+func TestGetKeysFromDB(t *testing.T) {
+	provider := NewDBProvider()
+	defer provider.Close()
+
+	db1, err := provider.GetDB(ns1, coll1)
+	require.NoError(t, err)
+	require.NotNil(t, db1)
+
+	err = db1.Put(api.NewKeyValue(key1, value1, txID1, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	err = db1.Put(api.NewKeyValue(key2, value2, txID1, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	v, err := db1.Get(key1)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+	require.Equal(t, txID1, v.TxID)
+	require.Equal(t, value1, v.Value)
+
+	v, err = db1.Get(key2)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+	require.Equal(t, txID1, v.TxID)
+	require.Equal(t, value2, v.Value)
+
+	db2, err := provider.GetDB(ns1, coll2)
+	require.NoError(t, err)
+	require.NotNil(t, db2)
+
+	err = db2.Put(api.NewKeyValue(key1, value2, txID2, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	v, err = db2.Get(key1)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+	require.Equal(t, txID2, v.TxID)
+	require.Equal(t, value2, v.Value)
+
+	vals, err := db1.GetMultiple(key1, key2)
+	require.NoError(t, err)
+	require.Equal(t, 2, len(vals))
+	require.Equal(t, value1, vals[0].Value)
+	require.Equal(t, value2, vals[1].Value)
+}
+
+func TestDeleteExpiredKeysFromDB(t *testing.T) {
+	provider := NewDBProvider()
+	defer provider.Close()
+
+	db, err := provider.GetDB(ns1, coll3)
+	require.NoError(t, err)
+
+	err = db.Put(
+		api.NewKeyValue(key3, value1, txID1, time.Now().UTC().Add(-1*time.Minute)),
+		api.NewKeyValue(key4, value2, txID2, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	// Wait for the periodic purge
+	time.Sleep(1 * time.Second)
+
+	// Check if key is deleted from db
+	v, err := db.Get(key3)
+	require.NoError(t, err)
+	require.Nil(t, v)
+
+	// Check if k2 is still exist in db
+	v, err = db.Get(key4)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+}
diff --git a/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore.go b/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore.go
new file mode 100644
index 00000000..12616638
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore.go
@@ -0,0 +1,136 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package leveldbstore
+
+import (
+	"bytes"
+	"encoding/gob"
+	"fmt"
+	"strings"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("offledgerstore")
+
+var compositeKeySep = "!"
+
+type store struct {
+	db     *leveldbhelper.DBHandle
+	dbName string
+}
+
+// newDBStore constructs an instance of db store
+func newDBStore(db *leveldbhelper.DBHandle, dbName string) *store {
+	return &store{db, dbName}
+}
+
+// AddKey add cache key to db
+func (s *store) Put(keyVal ...*api.KeyValue) error {
+	batch := leveldbhelper.NewUpdateBatch()
+	for _, kv := range keyVal {
+		encodedVal, err := encodeVal(kv.Value)
+		if err != nil {
+			return errors.WithMessagef(err, "failed to encode value for key [%s]", kv.Value)
+		}
+		batch.Put(encodeKey(kv.Key, time.Time{}), encodedVal)
+
+		if !kv.ExpiryTime.IsZero() {
+			// put same previous key with prefix expiryTime so the clean up can remove all expired keys
+			err = s.db.Put(encodeKey(kv.Key, kv.ExpiryTime), []byte(""), true)
+			if err != nil {
+				return errors.Wrapf(err, "failed to save key [%s] in db", kv.Key)
+			}
+		}
+	}
+	return s.db.WriteBatch(batch, true)
+}
+
+// GetKey get cache key from db
+func (s *store) Get(key string) (*api.Value, error) {
+	logger.Debugf("load key [%s] from db", key)
+	value, err := s.db.Get(encodeKey(key, time.Time{}))
+	if err != nil {
+		return nil, errors.Wrapf(err, "failed to load key [%s] from db", key)
+	}
+	if value != nil {
+		val, err := decodeVal(value)
+		if err != nil {
+			return nil, errors.Wrapf(err, "failed to decode value [%s] for key [%s]", value, key)
+		}
+		return val, nil
+	}
+	return nil, nil
+}
+
+// GetMultiple retrieves values for multiple keys at once
+func (s *store) GetMultiple(keys ...string) ([]*api.Value, error) {
+	values := make([]*api.Value, len(keys))
+	for i, k := range keys {
+		v, err := s.Get(k)
+		if err != nil {
+			return nil, err
+		}
+		values[i] = v
+	}
+	return values, nil
+}
+
+// DeleteExpiredKeys delete expired keys from db
+func (s *store) DeleteExpiredKeys() error {
+	dbBatch := leveldbhelper.NewUpdateBatch()
+	itr := s.db.GetIterator(nil, []byte(fmt.Sprintf("%d%s", time.Now().UTC().UnixNano(), compositeKeySep)))
+	for itr.Next() {
+		key := string(itr.Key())
+		dbBatch.Delete([]byte(key))
+		dbBatch.Delete([]byte(key[strings.Index(key, compositeKeySep)+1:]))
+	}
+	if dbBatch.Len() > 0 {
+		err := s.db.WriteBatch(dbBatch, true)
+		if err != nil {
+			return errors.Errorf("failed to delete keys %s in db %s", dbBatch.KVs, err.Error())
+		}
+		logger.Debugf("delete expired keys %s from db", dbBatch.KVs)
+	}
+
+	return nil
+}
+
+// Close db
+func (s *store) Close() {
+}
+
+func encodeKey(key string, expiryTime time.Time) []byte {
+	var compositeKey []byte
+	if !expiryTime.IsZero() {
+		compositeKey = append(compositeKey, []byte(fmt.Sprintf("%d", expiryTime.UnixNano()))...)
+		compositeKey = append(compositeKey, compositeKeySep...)
+	}
+	compositeKey = append(compositeKey, []byte(key)...)
+	return compositeKey
+}
+
+func decodeVal(b []byte) (*api.Value, error) {
+	decoder := gob.NewDecoder(bytes.NewBuffer(b))
+	var v *api.Value
+	if err := decoder.Decode(&v); err != nil {
+		return nil, err
+	}
+	return v, nil
+}
+
+func encodeVal(v *api.Value) ([]byte, error) {
+	buf := bytes.NewBuffer(nil)
+	encoder := gob.NewEncoder(buf)
+	if err := encoder.Encode(v); err != nil {
+		return nil, err
+	}
+	return buf.Bytes(), nil
+}
diff --git a/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_provider.go b/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_provider.go
new file mode 100644
index 00000000..cebf4e46
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_provider.go
@@ -0,0 +1,117 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package leveldbstore
+
+import (
+	"fmt"
+	"sync"
+	"time"
+
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/config"
+)
+
+// LevelDBProvider provides a handle to a db
+type LevelDBProvider struct {
+	leveldbProvider *leveldbhelper.Provider
+	stores          map[string]*store
+	mutex           sync.RWMutex
+	done            chan struct{}
+	closed          bool
+}
+
+// NewDBProvider constructs new db provider
+func NewDBProvider() *LevelDBProvider {
+	dbPath := config.GetOLCollLevelDBPath()
+	logger.Debugf("constructing DBProvider dbPath=%s", dbPath)
+	p := &LevelDBProvider{
+		stores: make(map[string]*store),
+		done:   make(chan struct{}),
+		leveldbProvider: leveldbhelper.NewProvider(
+			&leveldbhelper.Conf{
+				DBPath: dbPath,
+			},
+		),
+	}
+
+	p.periodicPurge()
+
+	return p
+}
+
+// GetDB opens the db store
+func (p *LevelDBProvider) GetDB(ns, coll string) (api.DB, error) {
+	dbName := dbName(ns, coll)
+
+	p.mutex.RLock()
+	s, ok := p.stores[dbName]
+	p.mutex.RUnlock()
+
+	if ok {
+		return s, nil
+	}
+
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	s, ok = p.stores[dbName]
+	if !ok {
+		indexStore := p.leveldbProvider.GetDBHandle(dbName)
+		s = newDBStore(indexStore, dbName)
+		p.stores[dbName] = s
+	}
+
+	return s, nil
+}
+
+// Close cleans up the Provider
+func (p *LevelDBProvider) Close() {
+	p.leveldbProvider.Close()
+
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	if !p.closed {
+		p.done <- struct{}{}
+		p.closed = true
+	}
+}
+
+func (p *LevelDBProvider) getStores() []*store {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	var stores []*store
+	for _, s := range p.stores {
+		stores = append(stores, s)
+	}
+	return stores
+}
+
+func (p *LevelDBProvider) periodicPurge() {
+	ticker := time.NewTicker(config.GetOLCollExpirationCheckInterval())
+	go func() {
+		for {
+			select {
+			case <-ticker.C:
+				for _, s := range p.getStores() {
+					err := s.DeleteExpiredKeys()
+					if err != nil {
+						logger.Errorf("Error deleting expired keys for [%s]", s.dbName)
+					}
+				}
+			case <-p.done:
+				logger.Infof("Periodic purge is exiting")
+				return
+			}
+		}
+	}()
+}
+
+func dbName(ns, coll string) string {
+	return fmt.Sprintf("%s$%s", ns, coll)
+}
diff --git a/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_test.go b/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_test.go
new file mode 100644
index 00000000..a5012c6d
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/leveldbstore/dbstore_test.go
@@ -0,0 +1,127 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package leveldbstore
+
+import (
+	"os"
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	txID1 = "txid1"
+	txID2 = "txid2"
+	ns1   = "namespace1"
+	coll2 = "coll2"
+	coll1 = "coll1"
+	key1  = "key1"
+	key2  = "key2"
+)
+
+var (
+	value1 = []byte("v111")
+	value2 = []byte("v222")
+)
+
+func TestDeleteExpiredKeysFromDB(t *testing.T) {
+	removeDBPath(t)
+	defer removeDBPath(t)
+
+	provider := NewDBProvider()
+	defer provider.Close()
+
+	db, err := provider.GetDB(ns1, coll1)
+	require.NoError(t, err)
+
+	err = db.Put(
+		api.NewKeyValue(key1, value1, txID1, time.Now().UTC()),
+		api.NewKeyValue(key2, value2, txID2, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	// Wait for the periodic purge
+	time.Sleep(200 * time.Millisecond)
+
+	// Check if k1 is delete from db
+	v, err := db.Get(key1)
+	require.NoError(t, err)
+	require.Nil(t, v)
+
+	// Check if k2 is still exist in db
+	v, err = db.Get(key2)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+}
+
+func TestGetKeysFromDB(t *testing.T) {
+	defer removeDBPath(t)
+
+	provider := NewDBProvider()
+	defer provider.Close()
+
+	db1, err := provider.GetDB(ns1, coll1)
+	require.NoError(t, err)
+	require.NotNil(t, db1)
+
+	err = db1.Put(api.NewKeyValue(key1, value1, txID1, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	err = db1.Put(api.NewKeyValue(key2, value2, txID1, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	v, err := db1.Get(key1)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+	require.Equal(t, txID1, v.TxID)
+	require.Equal(t, value1, v.Value)
+
+	v, err = db1.Get(key2)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+	require.Equal(t, txID1, v.TxID)
+	require.Equal(t, value2, v.Value)
+
+	db2, err := provider.GetDB(ns1, coll2)
+	require.NoError(t, err)
+	require.NotNil(t, db2)
+
+	err = db2.Put(api.NewKeyValue(key1, value2, txID2, time.Now().UTC().Add(1*time.Minute)))
+	require.NoError(t, err)
+
+	v, err = db2.Get(key1)
+	require.NoError(t, err)
+	require.NotNil(t, v)
+	require.Equal(t, txID2, v.TxID)
+	require.Equal(t, value2, v.Value)
+
+	vals, err := db1.GetMultiple(key1, key2)
+	require.NoError(t, err)
+	require.Equal(t, 2, len(vals))
+	require.Equal(t, value1, vals[0].Value)
+	require.Equal(t, value2, vals[1].Value)
+}
+
+func TestMain(m *testing.M) {
+	removeDBPath(nil)
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/ledgertests/offledgerdb_89786")
+	viper.Set("offledger.cleanupExpired.Interval", "100ms")
+
+	os.Exit(m.Run())
+}
+
+func removeDBPath(t testing.TB) {
+	removePath(t, config.GetOLCollLevelDBPath())
+}
+
+func removePath(t testing.TB, path string) {
+	if err := os.RemoveAll(path); err != nil {
+		t.Fatalf("Err: %s", err)
+	}
+}
diff --git a/extensions/collections/offledger/storeprovider/store/mocks/mockdbprovider.go b/extensions/collections/offledger/storeprovider/store/mocks/mockdbprovider.go
new file mode 100644
index 00000000..defb75b9
--- /dev/null
+++ b/extensions/collections/offledger/storeprovider/store/mocks/mockdbprovider.go
@@ -0,0 +1,142 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"sync"
+
+	"github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider/store/api"
+)
+
+// DBProvider is a mock DB provider
+type DBProvider struct {
+	mutex sync.Mutex
+	dbs   map[string]*DB
+	err   error
+}
+
+// NewDBProvider returns a new mock DB provider
+func NewDBProvider() *DBProvider {
+	return &DBProvider{
+		dbs: make(map[string]*DB),
+	}
+}
+
+// WithValue sets a value for the given key
+func (m *DBProvider) WithValue(ns, coll, key string, value *api.Value) *DBProvider {
+	m.MockDB(ns, coll).WithValue(key, value)
+	return m
+}
+
+// WithError simulates an error on the provider
+func (m *DBProvider) WithError(err error) *DBProvider {
+	m.mutex.Lock()
+	defer m.mutex.Unlock()
+
+	m.err = err
+	return m
+}
+
+// MockDB is a mock database
+func (m *DBProvider) MockDB(ns, coll string) *DB {
+	m.mutex.Lock()
+	defer m.mutex.Unlock()
+
+	dbKey := ns + "_" + coll
+	db, ok := m.dbs[dbKey]
+	if !ok {
+		db = newMockDB()
+		m.dbs[dbKey] = db
+	}
+	return db
+}
+
+// GetDB returns a mock DB for the given namespace/collection
+func (m *DBProvider) GetDB(ns, coll string) (api.DB, error) {
+	if m.err != nil {
+		return nil, m.err
+	}
+	return m.MockDB(ns, coll), nil
+}
+
+// Close currently does nothing
+func (m *DBProvider) Close() {
+}
+
+// DB implements a mock DB
+type DB struct {
+	mutex sync.RWMutex
+	data  map[string]*api.Value
+	err   error
+}
+
+func newMockDB() *DB {
+	return &DB{
+		data: make(map[string]*api.Value),
+	}
+}
+
+// WithValue sets a value for the given key
+func (m *DB) WithValue(key string, value *api.Value) *DB {
+	m.mutex.Lock()
+	defer m.mutex.Unlock()
+
+	m.data[key] = value
+	return m
+}
+
+// WithError simulates an error on the DB
+func (m *DB) WithError(err error) *DB {
+	m.mutex.RLock()
+	defer m.mutex.RUnlock()
+
+	m.err = err
+	return m
+}
+
+// Put sets the given values
+func (m *DB) Put(keyVals ...*api.KeyValue) error {
+	m.mutex.Lock()
+	defer m.mutex.Unlock()
+
+	if m.err != nil {
+		return m.err
+	}
+
+	for _, kv := range keyVals {
+		m.data[kv.Key] = kv.Value
+	}
+
+	return nil
+}
+
+// Get retrieves the value for the given key
+func (m *DB) Get(key string) (*api.Value, error) {
+	m.mutex.RLock()
+	defer m.mutex.RUnlock()
+
+	return m.data[key], m.err
+}
+
+// GetMultiple retrieves multiple keys at once
+func (m *DB) GetMultiple(keys ...string) ([]*api.Value, error) {
+	m.mutex.RLock()
+	defer m.mutex.RUnlock()
+
+	values := make([]*api.Value, len(keys))
+	for i, k := range keys {
+		values[i] = m.data[k]
+	}
+	return values, m.err
+}
+
+// DeleteExpiredKeys currently does nothing
+func (m *DB) DeleteExpiredKeys() error {
+	m.mutex.RLock()
+	defer m.mutex.RUnlock()
+	return m.err
+}
diff --git a/extensions/collections/policy/validator.go b/extensions/collections/policy/validator.go
new file mode 100644
index 00000000..9f31fa03
--- /dev/null
+++ b/extensions/collections/policy/validator.go
@@ -0,0 +1,133 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package policy
+
+import (
+	"fmt"
+	"time"
+
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+// Validator is a collection policy validator
+type Validator struct {
+}
+
+// NewValidator returns a new collection policy validator
+func NewValidator() *Validator {
+	return &Validator{}
+}
+
+// Validate validates various collection config types
+func (v *Validator) Validate(collConfig *common.CollectionConfig) error {
+	config := collConfig.GetStaticCollectionConfig()
+	if config == nil {
+		return errors.New("unknown collection configuration type")
+	}
+
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		return v.validateTransientConfig(config)
+	case common.CollectionType_COL_DCAS:
+		fallthrough
+	case common.CollectionType_COL_OFFLEDGER:
+		return v.validateOffLedgerConfig(config)
+	default:
+		// Nothing to do
+		return nil
+	}
+}
+
+// ValidateNewCollectionConfigsAgainstOld validates updated collection configs
+func (v *Validator) ValidateNewCollectionConfigsAgainstOld(newCollectionConfigs []*common.CollectionConfig, oldCollectionConfigs []*common.CollectionConfig,
+) error {
+	newCollectionsMap := make(map[string]*common.StaticCollectionConfig, len(newCollectionConfigs))
+	for _, newCollectionConfig := range newCollectionConfigs {
+		newCollection := newCollectionConfig.GetStaticCollectionConfig()
+		newCollectionsMap[newCollection.GetName()] = newCollection
+	}
+
+	for _, oldCollConfig := range oldCollectionConfigs {
+		oldColl := oldCollConfig.GetStaticCollectionConfig()
+		if oldColl == nil {
+			// This shouldn't happen since we've already gone through the basic validation
+			return errors.New("invalid collection")
+		}
+		newColl, ok := newCollectionsMap[oldColl.Name]
+		if !ok {
+			continue
+		}
+
+		newCollType := getCollType(newColl)
+		oldCollType := getCollType(oldColl)
+		if newCollType != oldCollType {
+			return fmt.Errorf("collection-name: %s -- attempt to change collection type from [%s] to [%s]", oldColl.Name, oldCollType, newCollType)
+		}
+	}
+	return nil
+}
+
+func (v *Validator) validateTransientConfig(config *common.StaticCollectionConfig) error {
+	if config.RequiredPeerCount <= 0 {
+		return errors.Errorf("collection-name: %s -- required peer count must be greater than 0", config.Name)
+	}
+
+	if config.RequiredPeerCount > config.MaximumPeerCount {
+		return errors.Errorf("collection-name: %s -- maximum peer count (%d) must be greater than or equal to required peer count (%d)", config.Name, config.MaximumPeerCount, config.RequiredPeerCount)
+	}
+	if config.TimeToLive == "" {
+		return errors.Errorf("collection-name: %s -- time to live must be specified", config.Name)
+	}
+
+	if config.BlockToLive != 0 {
+		return errors.Errorf("collection-name: %s -- block-to-live not supported", config.Name)
+	}
+
+	_, err := time.ParseDuration(config.TimeToLive)
+	if err != nil {
+		return errors.Errorf("collection-name: %s -- invalid time format for time to live: %s", config.Name, err)
+	}
+
+	return nil
+}
+
+func (v *Validator) validateOffLedgerConfig(config *common.StaticCollectionConfig) error {
+	if config.RequiredPeerCount <= 0 {
+		return errors.Errorf("collection-name: %s -- required peer count must be greater than 0", config.Name)
+	}
+
+	if config.RequiredPeerCount > config.MaximumPeerCount {
+		return errors.Errorf("collection-name: %s -- maximum peer count (%d) must be greater than or equal to required peer count (%d)", config.Name, config.MaximumPeerCount, config.RequiredPeerCount)
+	}
+
+	if config.BlockToLive != 0 {
+		return errors.Errorf("collection-name: %s -- block-to-live not supported", config.Name)
+	}
+
+	if config.TimeToLive != "" {
+		_, err := time.ParseDuration(config.TimeToLive)
+		if err != nil {
+			return errors.Errorf("collection-name: %s -- invalid time format for time to live: %s", config.Name, err)
+		}
+	}
+
+	return nil
+}
+
+func getCollType(config *common.StaticCollectionConfig) common.CollectionType {
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		return common.CollectionType_COL_TRANSIENT
+	case common.CollectionType_COL_DCAS:
+		return common.CollectionType_COL_DCAS
+	case common.CollectionType_COL_PRIVATE:
+		fallthrough
+	default:
+		return common.CollectionType_COL_PRIVATE
+	}
+}
diff --git a/extensions/collections/policy/validator_test.go b/extensions/collections/policy/validator_test.go
new file mode 100644
index 00000000..ef394a3e
--- /dev/null
+++ b/extensions/collections/policy/validator_test.go
@@ -0,0 +1,217 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package policy
+
+import (
+	"strings"
+	"testing"
+
+	"github.com/hyperledger/fabric/common/cauthdsl"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestValidateTransientDataCollectionConfig(t *testing.T) {
+	coll1 := "mycollection"
+
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+
+	v := NewValidator()
+
+	t.Run("non-transient collection -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		collConfig := createStaticCollectionConfig(coll1, policyEnvelope, 1, 2, 1000)
+		err := v.Validate(collConfig)
+		assert.NoError(t, err)
+	})
+
+	t.Run("transient collection -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "1m"))
+		assert.NoError(t, err)
+	})
+
+	t.Run("transient collection req == 0 -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 0, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "required peer count must be greater than 0"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("transient collection req > max -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 3, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "maximum peer count (2) must be greater than or equal to required peer count (3)"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("transient collection no time-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, ""))
+		require.Error(t, err)
+		expectedErr := "time to live must be specified"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("transient collection invalid time-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "1k"))
+		require.Error(t, err)
+		expectedErr := "invalid time format for time to live"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("transient collection with blocks-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		config := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "1m")
+		config.GetStaticCollectionConfig().BlockToLive = 100
+		err := v.Validate(config)
+		require.Error(t, err)
+		expectedErr := "block-to-live not supported"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+}
+
+func TestValidateDCASCollectionConfig(t *testing.T) {
+	coll1 := "mycollection"
+
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+
+	v := NewValidator()
+
+	t.Run("DCAS collection -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 1, 2, "1m"))
+		assert.NoError(t, err)
+	})
+
+	t.Run("DCAS req == 0 -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 0, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "required peer count must be greater than 0"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("transient collection req > max -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createTransientCollectionConfig(coll1, policyEnvelope, 3, 2, "1m"))
+		require.Error(t, err)
+		expectedErr := "maximum peer count (2) must be greater than or equal to required peer count (3)"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("DCAS no time-to-live -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 1, 2, ""))
+		require.NoError(t, err)
+	})
+
+	t.Run("DCAS invalid time-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		err := v.Validate(createDCASCollectionConfig(coll1, policyEnvelope, 1, 2, "1k"))
+		require.Error(t, err)
+		expectedErr := "invalid time format for time to live"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+
+	t.Run("DCAS with blocks-to-live -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		config := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "1m")
+		config.GetStaticCollectionConfig().BlockToLive = 100
+		err := v.Validate(config)
+		require.Error(t, err)
+		expectedErr := "block-to-live not supported"
+		assert.Truef(t, strings.Contains(err.Error(), expectedErr), "Expected error to contain '%s' but got '%s'", expectedErr, err)
+	})
+}
+
+func TestValidateNewCollectionConfigAgainstOld(t *testing.T) {
+	coll1 := "mycollection"
+
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+
+	v := NewValidator()
+
+	t.Run("updated -> success", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		oldCollConfig := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "10m")
+		newCollConfig := createTransientCollectionConfig(coll1, policyEnvelope, 2, 3, "20m")
+		err := v.ValidateNewCollectionConfigsAgainstOld([]*common.CollectionConfig{newCollConfig}, []*common.CollectionConfig{oldCollConfig})
+		assert.NoError(t, err)
+	})
+
+	t.Run("private collection updated to transient -> error", func(t *testing.T) {
+		policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+		oldCollConfig := createStaticCollectionConfig(coll1, policyEnvelope, 1, 2, 1000)
+		newCollConfig := createTransientCollectionConfig(coll1, policyEnvelope, 1, 2, "10m")
+		err := v.ValidateNewCollectionConfigsAgainstOld([]*common.CollectionConfig{newCollConfig}, []*common.CollectionConfig{oldCollConfig})
+		assert.EqualError(t, err, "collection-name: mycollection -- attempt to change collection type from [COL_PRIVATE] to [COL_TRANSIENT]")
+	})
+}
+
+func createTransientCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, ttl string) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				Type:              common.CollectionType_COL_TRANSIENT,
+				MemberOrgsPolicy:  &common.CollectionPolicyConfig{Payload: signaturePolicy},
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+				TimeToLive:        ttl,
+			},
+		},
+	}
+}
+
+func createDCASCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, ttl string) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				Type:              common.CollectionType_COL_DCAS,
+				MemberOrgsPolicy:  &common.CollectionPolicyConfig{Payload: signaturePolicy},
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+				TimeToLive:        ttl,
+			},
+		},
+	}
+}
+
+func createStaticCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32, blockToLive uint64) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				MemberOrgsPolicy:  &common.CollectionPolicyConfig{Payload: signaturePolicy},
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+				BlockToLive:       blockToLive,
+			},
+		},
+	}
+}
diff --git a/extensions/collections/pvtdatahandler/pvtdatahandler.go b/extensions/collections/pvtdatahandler/pvtdatahandler.go
new file mode 100644
index 00000000..b93004e8
--- /dev/null
+++ b/extensions/collections/pvtdatahandler/pvtdatahandler.go
@@ -0,0 +1,156 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatahandler
+
+import (
+	"context"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/hyperledger/fabric/protos/common"
+)
+
+var logger = flogging.MustGetLogger("kevlar")
+
+// Handler handles the retrieval of kevlar-defined collection types
+type Handler struct {
+	channelID        string
+	collDataProvider storeapi.Provider
+}
+
+// New returns a new Handler
+func New(channelID string, collDataProvider storeapi.Provider) *Handler {
+	return &Handler{
+		channelID:        channelID,
+		collDataProvider: collDataProvider,
+	}
+}
+
+// HandleGetPrivateData if the collection is one of the custom Kevlar collections then the private data is returned
+func (h *Handler) HandleGetPrivateData(txID, ns string, config *common.StaticCollectionConfig, key string) ([]byte, bool, error) {
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		logger.Debugf("Collection [%s:%s] is of type TransientData. Returning transient data for key [%s]", ns, config.Name, key)
+		value, err := h.getTransientData(txID, ns, config.Name, key)
+		if err != nil {
+			return nil, true, err
+		}
+		return value, true, nil
+	case common.CollectionType_COL_DCAS:
+		fallthrough
+	case common.CollectionType_COL_OFFLEDGER:
+		logger.Debugf("Collection [%s:%s] is an off-ledger store. Returning data for key [%s]", ns, config.Name, key)
+		value, err := h.getData(txID, ns, config.Name, key)
+		if err != nil {
+			return nil, true, err
+		}
+		return value, true, nil
+	default:
+		return nil, false, nil
+	}
+}
+
+// HandleGetPrivateDataMultipleKeys if the collection is one of the custom Kevlar collections then the private data is returned
+func (h *Handler) HandleGetPrivateDataMultipleKeys(txID, ns string, config *common.StaticCollectionConfig, keys []string) ([][]byte, bool, error) {
+	switch config.Type {
+	case common.CollectionType_COL_TRANSIENT:
+		logger.Debugf("Collection [%s:%s] is of type TransientData. Returning transient data for keys [%s]", ns, config.Name, keys)
+		values, err := h.getTransientDataMultipleKeys(txID, ns, config.Name, keys)
+		if err != nil {
+			return nil, true, err
+		}
+		return values, true, nil
+	case common.CollectionType_COL_DCAS:
+		fallthrough
+	case common.CollectionType_COL_OFFLEDGER:
+		logger.Debugf("Collection [%s:%s] is of an off-ledger store. Returning data for keys [%s]", ns, config.Name, keys)
+		values, err := h.getDataMultipleKeys(txID, ns, config.Name, keys)
+		if err != nil {
+			return nil, true, err
+		}
+		return values, true, nil
+	default:
+		return nil, false, nil
+	}
+}
+
+func (h *Handler) getTransientData(txID, ns, coll, key string) ([]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetTransientDataPullTimeout())
+	defer cancel()
+
+	v, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetTransientData(ctxt, storeapi.NewKey(txID, ns, coll, key))
+	if err != nil {
+		return nil, err
+	}
+
+	if v == nil {
+		return nil, nil
+	}
+
+	return v.Value, nil
+}
+
+func (h *Handler) getTransientDataMultipleKeys(txID, ns, coll string, keys []string) ([][]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetTransientDataPullTimeout())
+	defer cancel()
+
+	vals, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetTransientDataMultipleKeys(ctxt, storeapi.NewMultiKey(txID, ns, coll, keys...))
+	if err != nil {
+		return nil, err
+	}
+
+	values := make([][]byte, len(vals))
+	for i, v := range vals {
+		if v == nil {
+			values[i] = nil
+		} else {
+			values[i] = v.Value
+		}
+	}
+	return values, nil
+}
+
+func (h *Handler) getData(txID, ns, coll, key string) ([]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetOLCollPullTimeout())
+	defer cancel()
+
+	v, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetData(ctxt, storeapi.NewKey(txID, ns, coll, key))
+	if err != nil {
+		return nil, err
+	}
+
+	if v == nil {
+		return nil, nil
+	}
+
+	return v.Value, nil
+}
+
+func (h *Handler) getDataMultipleKeys(txID, ns, coll string, keys []string) ([][]byte, error) {
+	ctxt, cancel := context.WithTimeout(context.Background(), config.GetOLCollPullTimeout())
+	defer cancel()
+
+	vals, err := h.collDataProvider.RetrieverForChannel(h.channelID).
+		GetDataMultipleKeys(ctxt, storeapi.NewMultiKey(txID, ns, coll, keys...))
+	if err != nil {
+		return nil, err
+	}
+
+	values := make([][]byte, len(vals))
+	for i, v := range vals {
+		if v == nil {
+			values[i] = nil
+		} else {
+			values[i] = v.Value
+		}
+	}
+	return values, nil
+}
diff --git a/extensions/collections/pvtdatahandler/pvtdatahandler_test.go b/extensions/collections/pvtdatahandler/pvtdatahandler_test.go
new file mode 100644
index 00000000..b6142df1
--- /dev/null
+++ b/extensions/collections/pvtdatahandler/pvtdatahandler_test.go
@@ -0,0 +1,104 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatahandler
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+	tx1       = "tx1"
+	ns1       = "ns1"
+	key1      = "key1"
+	key2      = "key2"
+)
+
+func TestHandler_HandleGetPrivateData(t *testing.T) {
+	storeProvider := mocks.NewDataProvider()
+
+	h := New(channelID, storeProvider)
+	require.NotNil(t, h)
+
+	t.Run("Transient Data", func(t *testing.T) {
+		config := &common.StaticCollectionConfig{
+			Type: common.CollectionType_COL_TRANSIENT,
+		}
+
+		value, handled, err := h.HandleGetPrivateData(tx1, ns1, config, key1)
+		assert.NoError(t, err)
+		assert.True(t, handled)
+		assert.NotNil(t, value)
+	})
+
+	t.Run("Off-ledger Data", func(t *testing.T) {
+		config := &common.StaticCollectionConfig{
+			Type: common.CollectionType_COL_OFFLEDGER,
+		}
+
+		value, handled, err := h.HandleGetPrivateData(tx1, ns1, config, key1)
+		assert.NoError(t, err)
+		assert.True(t, handled)
+		assert.NotNil(t, value)
+	})
+
+	t.Run("DCAS Data", func(t *testing.T) {
+		config := &common.StaticCollectionConfig{
+			Type: common.CollectionType_COL_DCAS,
+		}
+
+		value, handled, err := h.HandleGetPrivateData(tx1, ns1, config, key1)
+		assert.NoError(t, err)
+		assert.True(t, handled)
+		assert.NotNil(t, value)
+	})
+}
+
+func TestHandler_HandleGetPrivateDataMultipleKeys(t *testing.T) {
+	storeProvider := mocks.NewDataProvider()
+
+	h := New(channelID, storeProvider)
+	require.NotNil(t, h)
+
+	t.Run("Transient Data", func(t *testing.T) {
+		config := &common.StaticCollectionConfig{
+			Type: common.CollectionType_COL_TRANSIENT,
+		}
+
+		value, handled, err := h.HandleGetPrivateDataMultipleKeys(tx1, ns1, config, []string{key1, key2})
+		assert.NoError(t, err)
+		assert.True(t, handled)
+		assert.NotNil(t, value)
+	})
+
+	t.Run("Off-ledger Data", func(t *testing.T) {
+		config := &common.StaticCollectionConfig{
+			Type: common.CollectionType_COL_OFFLEDGER,
+		}
+
+		value, handled, err := h.HandleGetPrivateDataMultipleKeys(tx1, ns1, config, []string{key1, key2})
+		assert.NoError(t, err)
+		assert.True(t, handled)
+		assert.NotNil(t, value)
+	})
+
+	t.Run("DCAS Data", func(t *testing.T) {
+		config := &common.StaticCollectionConfig{
+			Type: common.CollectionType_COL_DCAS,
+		}
+
+		value, handled, err := h.HandleGetPrivateDataMultipleKeys(tx1, ns1, config, []string{key1, key2})
+		assert.NoError(t, err)
+		assert.True(t, handled)
+		assert.NotNil(t, value)
+	})
+}
diff --git a/extensions/collections/retriever/retriever.go b/extensions/collections/retriever/retriever.go
new file mode 100644
index 00000000..a2bd1bce
--- /dev/null
+++ b/extensions/collections/retriever/retriever.go
@@ -0,0 +1,115 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	"context"
+	"sync"
+
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	olapi "github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	tdataapi "github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	"github.com/hyperledger/fabric/extensions/collections/offledger"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/dcas"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata"
+	supp "github.com/hyperledger/fabric/extensions/common/support"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	cb "github.com/hyperledger/fabric/protos/common"
+)
+
+// Provider is a transient data provider.
+type Provider struct {
+	transientDataProvider tdataapi.Provider
+	offLedgerProvider     olapi.Provider
+	retrievers            map[string]*retriever
+	mutex                 sync.RWMutex
+}
+
+// NewProvider returns a new transient data provider
+func NewProvider(
+	storeProvider func(channelID string) storeapi.Store,
+	ledgerProvider func(channelID string) ledger.PeerLedger,
+	gossipProvider func() supportapi.GossipAdapter,
+	blockPublisherProvider func(channelID string) gossipapi.BlockPublisher) storeapi.Provider {
+
+	support := supp.New(ledgerProvider, blockPublisherProvider)
+
+	tdataStoreProvider := func(channelID string) tdataapi.Store { return storeProvider(channelID) }
+	offLedgerStoreProvider := func(channelID string) olapi.Store { return storeProvider(channelID) }
+
+	return &Provider{
+		transientDataProvider: getTransientDataProvider(tdataStoreProvider, support, gossipProvider),
+		offLedgerProvider:     getOffLedgerProvider(offLedgerStoreProvider, support, gossipProvider),
+		retrievers:            make(map[string]*retriever),
+	}
+}
+
+// RetrieverForChannel returns the collection retriever for the given channel
+func (p *Provider) RetrieverForChannel(channelID string) storeapi.Retriever {
+	p.mutex.RLock()
+	r, ok := p.retrievers[channelID]
+	p.mutex.RUnlock()
+
+	if !ok {
+		p.mutex.Lock()
+		defer p.mutex.Unlock()
+
+		r, ok = p.retrievers[channelID]
+		if !ok {
+			r = &retriever{
+				transientDataRetriever: p.transientDataProvider.RetrieverForChannel(channelID),
+				dcasRetriever:          p.offLedgerProvider.RetrieverForChannel(channelID),
+			}
+			p.retrievers[channelID] = r
+		}
+	}
+
+	return r
+}
+
+type retriever struct {
+	transientDataRetriever tdataapi.Retriever
+	dcasRetriever          olapi.Retriever
+}
+
+func (r *retriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return r.transientDataRetriever.GetTransientData(ctxt, key)
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (r *retriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return r.transientDataRetriever.GetTransientDataMultipleKeys(ctxt, key)
+}
+
+// GetData gets the value for the given data item
+func (r *retriever) GetData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return r.dcasRetriever.GetData(ctxt, key)
+}
+
+// GetDataMultipleKeys gets the values for the multiple data items in a single call
+func (r *retriever) GetDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return r.dcasRetriever.GetDataMultipleKeys(ctxt, key)
+}
+
+type support interface {
+	Config(channelID, ns, coll string) (*cb.StaticCollectionConfig, error)
+	Policy(channel, ns, collection string) (privdata.CollectionAccessPolicy, error)
+	BlockPublisher(channelID string) gossipapi.BlockPublisher
+}
+
+var getTransientDataProvider = func(storeProvider func(channelID string) tdataapi.Store, support support, gossipProvider func() supportapi.GossipAdapter) tdataapi.Provider {
+	return transientdata.NewProvider(storeProvider, support, gossipProvider)
+}
+
+var getOffLedgerProvider = func(storeProvider func(channelID string) olapi.Store, support support, gossipProvider func() supportapi.GossipAdapter) olapi.Provider {
+	return offledger.NewProvider(storeProvider, support, gossipProvider,
+		offledger.WithValidator(cb.CollectionType_COL_DCAS, dcas.Validator),
+	)
+}
diff --git a/extensions/collections/retriever/retriever_test.go b/extensions/collections/retriever/retriever_test.go
new file mode 100644
index 00000000..1e1a869b
--- /dev/null
+++ b/extensions/collections/retriever/retriever_test.go
@@ -0,0 +1,63 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package retriever
+
+import (
+	"context"
+	"testing"
+
+	olapi "github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	tdataapi "github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	olmocks "github.com/hyperledger/fabric/extensions/collections/offledger/mocks"
+	tdatamocks "github.com/hyperledger/fabric/extensions/collections/transientdata/mocks"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+)
+
+func TestRetriever(t *testing.T) {
+	getTransientDataProvider = func(storeProvider func(channelID string) tdataapi.Store, support support, gossipProvider func() supportapi.GossipAdapter) tdataapi.Provider {
+		return &tdatamocks.TransientDataProvider{}
+	}
+
+	getOffLedgerProvider = func(storeProvider func(channelID string) olapi.Store, support support, gossipProvider func() supportapi.GossipAdapter) olapi.Provider {
+		return &olmocks.Provider{}
+	}
+
+	p := NewProvider(nil, nil, nil, nil)
+	require.NotNil(t, p)
+
+	retriever := p.RetrieverForChannel(channelID)
+	require.NotNil(t, retriever)
+
+	const key1 = "key1"
+
+	v, err := retriever.GetTransientData(context.Background(), &storeapi.Key{Key: key1})
+	assert.NoError(t, err)
+	require.NotNil(t, v)
+	assert.Equal(t, []byte(key1), v.Value)
+
+	vals, err := retriever.GetTransientDataMultipleKeys(context.Background(), &storeapi.MultiKey{Keys: []string{key1}})
+	assert.NoError(t, err)
+	require.Equal(t, 1, len(vals))
+	assert.Equal(t, []byte(key1), vals[0].Value)
+
+	v, err = retriever.GetData(context.Background(), &storeapi.Key{Key: key1})
+	assert.NoError(t, err)
+	require.NotNil(t, v)
+	assert.Equal(t, []byte(key1), v.Value)
+
+	vals, err = retriever.GetDataMultipleKeys(context.Background(), &storeapi.MultiKey{Keys: []string{key1}})
+	assert.NoError(t, err)
+	require.Equal(t, 1, len(vals))
+	assert.Equal(t, []byte(key1), vals[0].Value)
+}
diff --git a/extensions/collections/storeprovider/mocks/mockolstore.go b/extensions/collections/storeprovider/mocks/mockolstore.go
new file mode 100644
index 00000000..23139866
--- /dev/null
+++ b/extensions/collections/storeprovider/mocks/mockolstore.go
@@ -0,0 +1,129 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	offledgerapi "github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	cb "github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// StoreProvider implements a mock transient data store provider
+type StoreProvider struct {
+	store *Store
+	err   error
+}
+
+// NewStoreProvider returns a new  provider
+func NewStoreProvider() *StoreProvider {
+	return &StoreProvider{
+		store: NewStore(),
+	}
+}
+
+// Data invokes the Data of the store for the given key and value
+func (p *StoreProvider) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *StoreProvider {
+	p.store.Data(key, value)
+	return p
+}
+
+// Error stores the error
+func (p *StoreProvider) Error(err error) *StoreProvider {
+	p.err = err
+	return p
+}
+
+// StoreError stores the storeError
+func (p *StoreProvider) StoreError(err error) *StoreProvider {
+	p.store.Error(err)
+	return p
+}
+
+// StoreForChannel returns the transient data store for the given channel
+func (p *StoreProvider) StoreForChannel(channelID string) offledgerapi.Store {
+	return p.store
+}
+
+// OpenStore opens the transient data store for the given channel
+func (p *StoreProvider) OpenStore(channelID string) (offledgerapi.Store, error) {
+	return p.store, p.err
+}
+
+// Close closes the  store for the given channel
+func (p *StoreProvider) Close() {
+	p.store.Close()
+}
+
+// IsStoreClosed indicates whether the  store is closed
+func (p *StoreProvider) IsStoreClosed() bool {
+	return p.store.closed
+}
+
+// Store implements a mock store
+type Store struct {
+	data   map[storeapi.Key]*storeapi.ExpiringValue
+	err    error
+	closed bool
+}
+
+// NewStore returns a mock transient data store
+func NewStore() *Store {
+	return &Store{
+		data: make(map[storeapi.Key]*storeapi.ExpiringValue),
+	}
+}
+
+// Data sets the data for the given key
+func (m *Store) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *Store {
+	m.data[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return m
+}
+
+// Error sets an err
+func (m *Store) Error(err error) *Store {
+	m.err = err
+	return m
+}
+
+// Persist stores the private write set of a transaction along with the collection config
+// in the transient store based on txid and the block height the private data was received at
+func (m *Store) Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error {
+	return m.err
+}
+
+// PutData stores the key/value
+func (m *Store) PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error {
+	if m.err != nil {
+		return m.err
+	}
+	m.data[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return nil
+}
+
+// GetData gets the value for the given item
+func (m *Store) GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return m.data[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}], m.err
+}
+
+// GetDataMultipleKeys gets the values for the multiple items in a single call
+func (m *Store) GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	var values storeapi.ExpiringValues
+	for _, k := range key.Keys {
+		value, err := m.GetData(&storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: k})
+		if err != nil {
+			return nil, err
+		}
+		values = append(values, value)
+	}
+	return values, m.err
+}
+
+// Close closes the store
+func (m *Store) Close() {
+	m.closed = true
+}
diff --git a/extensions/collections/storeprovider/mocks/mocktransientdatastore.go b/extensions/collections/storeprovider/mocks/mocktransientdatastore.go
new file mode 100644
index 00000000..f9304a8e
--- /dev/null
+++ b/extensions/collections/storeprovider/mocks/mocktransientdatastore.go
@@ -0,0 +1,119 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	tdataapi "github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	proto "github.com/hyperledger/fabric/protos/transientstore"
+)
+
+// TransientDataStoreProvider implements a mock transient data store provider
+type TransientDataStoreProvider struct {
+	store *TransientDataStore
+	err   error
+}
+
+// NewTransientDataStoreProvider creates a new provider
+func NewTransientDataStoreProvider() *TransientDataStoreProvider {
+	return &TransientDataStoreProvider{
+		store: NewTransientDataStore(),
+	}
+}
+
+// Data stores key value
+func (p *TransientDataStoreProvider) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *TransientDataStoreProvider {
+	p.store.Data(key, value)
+	return p
+}
+
+// Error stores the error
+func (p *TransientDataStoreProvider) Error(err error) *TransientDataStoreProvider {
+	p.err = err
+	return p
+}
+
+// StoreError stores the StoreError
+func (p *TransientDataStoreProvider) StoreError(err error) *TransientDataStoreProvider {
+	p.store.Error(err)
+	return p
+}
+
+// StoreForChannel returns the transient data store for the given channel
+func (p *TransientDataStoreProvider) StoreForChannel(channelID string) tdataapi.Store {
+	return p.store
+}
+
+// OpenStore opens the transient data store for the given channel
+func (p *TransientDataStoreProvider) OpenStore(channelID string) (tdataapi.Store, error) {
+	return p.store, p.err
+}
+
+// Close closes the transient data store for the given channel
+func (p *TransientDataStoreProvider) Close() {
+	p.store.Close()
+}
+
+// IsStoreClosed indicates whether the transient data store is closed
+func (p *TransientDataStoreProvider) IsStoreClosed() bool {
+	return p.store.closed
+}
+
+// TransientDataStore implements a mock transient data store
+type TransientDataStore struct {
+	transientData map[storeapi.Key]*storeapi.ExpiringValue
+	err           error
+	closed        bool
+}
+
+// NewTransientDataStore returns a mock transient data store
+func NewTransientDataStore() *TransientDataStore {
+	return &TransientDataStore{
+		transientData: make(map[storeapi.Key]*storeapi.ExpiringValue),
+	}
+}
+
+// Data sets the transient data for the given key
+func (m *TransientDataStore) Data(key *storeapi.Key, value *storeapi.ExpiringValue) *TransientDataStore {
+	m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}] = value
+	return m
+}
+
+// Error sets an err
+func (m *TransientDataStore) Error(err error) *TransientDataStore {
+	m.err = err
+	return m
+}
+
+// Persist stores the private write set of a transaction along with the collection config
+// in the transient store based on txid and the block height the private data was received at
+func (m *TransientDataStore) Persist(txid string, privateSimulationResultsWithConfig *proto.TxPvtReadWriteSetWithConfigInfo) error {
+	return m.err
+}
+
+// GetTransientData gets the value for the given transient data item
+func (m *TransientDataStore) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return m.transientData[storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: key.Key}], m.err
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (m *TransientDataStore) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	var values storeapi.ExpiringValues
+	for _, k := range key.Keys {
+		value, err := m.GetTransientData(&storeapi.Key{Namespace: key.Namespace, Collection: key.Collection, Key: k})
+		if err != nil {
+			return nil, err
+		}
+		values = append(values, value)
+	}
+	return values, m.err
+}
+
+// Close closes the store
+func (m *TransientDataStore) Close() {
+	m.closed = true
+}
diff --git a/extensions/collections/storeprovider/store.go b/extensions/collections/storeprovider/store.go
new file mode 100644
index 00000000..c130bf2a
--- /dev/null
+++ b/extensions/collections/storeprovider/store.go
@@ -0,0 +1,75 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	cb "github.com/hyperledger/fabric/protos/common"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+)
+
+type store struct {
+	channelID          string
+	transientDataStore transientdata.Store
+	offLedgerStore     offledger.Store
+}
+
+func newDelegatingStore(channelID string, transientDataStore transientdata.Store, offLedgerStore offledger.Store) *store {
+	return &store{
+		channelID:          channelID,
+		transientDataStore: transientDataStore,
+		offLedgerStore:     offLedgerStore,
+	}
+}
+
+// Persist persists all transient data within the private data simulation results
+func (d *store) Persist(txID string, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	if err := d.transientDataStore.Persist(txID, privateSimulationResultsWithConfig); err != nil {
+		return errors.WithMessage(err, "error persisting transient data")
+	}
+
+	// Off-ledger data should only be persisted on committers
+	if err := d.offLedgerStore.Persist(txID, privateSimulationResultsWithConfig); err != nil {
+		return errors.WithMessage(err, "error persisting off-ledger data")
+	}
+
+	return nil
+}
+
+// GetTransientData returns the transient data for the given key
+func (d *store) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return d.transientDataStore.GetTransientData(key)
+}
+
+// GetTransientData returns the transient data for the given keys
+func (d *store) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return d.transientDataStore.GetTransientDataMultipleKeys(key)
+}
+
+// GetData gets the value for the given key
+func (d *store) GetData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return d.offLedgerStore.GetData(key)
+}
+
+// PutData stores the key/value.
+func (d *store) PutData(config *cb.StaticCollectionConfig, key *storeapi.Key, value *storeapi.ExpiringValue) error {
+	return d.offLedgerStore.PutData(config, key, value)
+}
+
+// GetDataMultipleKeys gets the values for multiple keys in a single call
+func (d *store) GetDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return d.offLedgerStore.GetDataMultipleKeys(key)
+}
+
+// Close closes all of the stores store
+func (d *store) Close() {
+	d.transientDataStore.Close()
+	d.offLedgerStore.Close()
+}
diff --git a/extensions/collections/storeprovider/storeprovider.go b/extensions/collections/storeprovider/storeprovider.go
new file mode 100644
index 00000000..54ea105c
--- /dev/null
+++ b/extensions/collections/storeprovider/storeprovider.go
@@ -0,0 +1,87 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"sync"
+
+	olapi "github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	tdataapi "github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	"github.com/hyperledger/fabric/extensions/collections/offledger/dcas"
+	offledgerstoreprovider "github.com/hyperledger/fabric/extensions/collections/offledger/storeprovider"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider"
+	cb "github.com/hyperledger/fabric/protos/common"
+)
+
+// NewProviderFactory returns a new store provider factory
+func NewProviderFactory() *StoreProvider {
+	return &StoreProvider{
+		transientDataProvider: newTransientDataProviderFactory(),
+		olProvider:            newOffLedgerProviderFactory(),
+		stores:                make(map[string]*store),
+	}
+}
+
+// StoreProvider is a store provider that creates delegating stores.
+// A delegating store delegates requests to collection-specific store.
+// For example, transient data store, Off-ledger store, etc.
+type StoreProvider struct {
+	transientDataProvider tdataapi.StoreProvider
+	olProvider            olapi.StoreProvider
+	stores                map[string]*store
+	sync.RWMutex
+}
+
+// StoreForChannel returns the store for the given channel
+func (sp *StoreProvider) StoreForChannel(channelID string) storeapi.Store {
+	sp.RLock()
+	defer sp.RUnlock()
+	return sp.stores[channelID]
+}
+
+// OpenStore opens the store for the given channel
+func (sp *StoreProvider) OpenStore(channelID string) (storeapi.Store, error) {
+	sp.Lock()
+	defer sp.Unlock()
+
+	store, ok := sp.stores[channelID]
+	if !ok {
+		tdataStore, err := sp.transientDataProvider.OpenStore(channelID)
+		if err != nil {
+			return nil, err
+		}
+		olStore, err := sp.olProvider.OpenStore(channelID)
+		if err != nil {
+			return nil, err
+		}
+		store = newDelegatingStore(channelID, tdataStore, olStore)
+		sp.stores[channelID] = store
+	}
+	return store, nil
+}
+
+// Close shuts down all of the stores
+func (sp *StoreProvider) Close() {
+	for _, s := range sp.stores {
+		s.Close()
+	}
+}
+
+// newTransientDataProviderFactory may be overridden in unit tests
+var newTransientDataProviderFactory = func() tdataapi.StoreProvider {
+	return storeprovider.NewProviderFactory()
+}
+
+// newOffLedgerProviderFactory may be overridden in unit tests
+var newOffLedgerProviderFactory = func() olapi.StoreProvider {
+	return offledgerstoreprovider.NewProviderFactory(
+		offledgerstoreprovider.WithCollectionType(
+			cb.CollectionType_COL_DCAS, offledgerstoreprovider.WithDecorator(dcas.Decorator),
+		),
+	)
+}
diff --git a/extensions/collections/storeprovider/storeprovider_test.go b/extensions/collections/storeprovider/storeprovider_test.go
new file mode 100644
index 00000000..4d16e07a
--- /dev/null
+++ b/extensions/collections/storeprovider/storeprovider_test.go
@@ -0,0 +1,175 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"testing"
+
+	offledgerapi "github.com/hyperledger/fabric/extensions/collections/api/offledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	tdataapi "github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	"github.com/hyperledger/fabric/extensions/collections/storeprovider/mocks"
+	kmocks "github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestStoreProvider(t *testing.T) {
+	tdataProvider := mocks.NewTransientDataStoreProvider()
+	olProvider := mocks.NewStoreProvider()
+
+	newTransientDataProviderFactory = func() tdataapi.StoreProvider {
+		return tdataProvider
+	}
+
+	newOffLedgerProviderFactory = func() offledgerapi.StoreProvider {
+		return olProvider
+	}
+
+	t.Run("OpenStore - success", func(t *testing.T) {
+		p := NewProviderFactory()
+		require.NotNil(t, p)
+
+		s, err := p.OpenStore("testchannel")
+		require.NoError(t, err)
+		require.NotNil(t, s)
+
+		s2 := p.StoreForChannel("testchannel")
+		require.Equal(t, s, s2)
+	})
+
+	t.Run("OpenStore - transient data error", func(t *testing.T) {
+		p := NewProviderFactory()
+		require.NotNil(t, p)
+
+		expectedErr := errors.New("transientdata error")
+		tdataProvider.Error(expectedErr)
+		defer tdataProvider.Error(nil)
+
+		s, err := p.OpenStore("testchannel")
+		assert.EqualError(t, err, expectedErr.Error())
+		require.Nil(t, s)
+	})
+
+	t.Run("OpenStore - DCAS data error", func(t *testing.T) {
+		p := NewProviderFactory()
+		require.NotNil(t, p)
+
+		expectedErr := errors.New("dcas error")
+		olProvider.Error(expectedErr)
+		defer olProvider.Error(nil)
+
+		s, err := p.OpenStore("testchannel")
+		assert.EqualError(t, err, expectedErr.Error())
+		require.Nil(t, s)
+	})
+
+	t.Run("Close - success", func(t *testing.T) {
+		p := NewProviderFactory()
+		require.NotNil(t, p)
+
+		s, err := p.OpenStore("testchannel")
+		require.NoError(t, err)
+		require.NotNil(t, s)
+
+		assert.False(t, tdataProvider.IsStoreClosed())
+		assert.False(t, olProvider.IsStoreClosed())
+
+		p.Close()
+
+		assert.True(t, tdataProvider.IsStoreClosed())
+		assert.True(t, olProvider.IsStoreClosed())
+	})
+}
+
+func TestStore_PutAndGetData(t *testing.T) {
+	const (
+		tx1   = "tx1"
+		ns1   = "ns1"
+		coll1 = "coll1"
+		coll2 = "coll2"
+		key1  = "key1"
+		key2  = "key2"
+	)
+
+	k1 := storeapi.NewKey(tx1, ns1, coll1, key1)
+	k2 := storeapi.NewKey(tx1, ns1, coll1, key2)
+	k3 := storeapi.NewKey(tx1, ns1, coll2, key1)
+
+	v1 := &storeapi.ExpiringValue{Value: []byte("value1")}
+	v2 := &storeapi.ExpiringValue{Value: []byte("value1")}
+
+	tdataProvider := mocks.NewTransientDataStoreProvider()
+	olProvider := mocks.NewStoreProvider()
+
+	newTransientDataProviderFactory = func() tdataapi.StoreProvider {
+		return tdataProvider.Data(k1, v1).Data(k2, v2)
+	}
+
+	newOffLedgerProviderFactory = func() offledgerapi.StoreProvider {
+		return olProvider.Data(k1, v2).Data(k2, v1)
+	}
+
+	p := NewProviderFactory()
+	require.NotNil(t, p)
+
+	s, err := p.OpenStore("testchannel")
+	require.NoError(t, err)
+	require.NotNil(t, s)
+
+	t.Run("GetTransientData", func(t *testing.T) {
+		value, err := s.GetTransientData(k1)
+		require.NoError(t, err)
+		require.NotNil(t, value)
+
+		values, err := s.GetTransientDataMultipleKeys(storeapi.NewMultiKey(tx1, ns1, coll1, key1, key2))
+		require.NoError(t, err)
+		assert.Equal(t, 2, len(values))
+	})
+
+	t.Run("GetData", func(t *testing.T) {
+		value, err := s.GetData(k1)
+		require.NoError(t, err)
+		require.NotNil(t, value)
+
+		values, err := s.GetDataMultipleKeys(storeapi.NewMultiKey(tx1, ns1, coll1, key1, key2))
+		require.NoError(t, err)
+		assert.Equal(t, 2, len(values))
+	})
+
+	t.Run("PutData", func(t *testing.T) {
+		collConfig := &common.StaticCollectionConfig{
+			Type: common.CollectionType_COL_DCAS,
+			Name: coll2,
+		}
+		err := s.PutData(collConfig, k3, v1)
+		require.NoError(t, err)
+	})
+
+	t.Run("Persist", func(t *testing.T) {
+		isCommitter = func() bool { return true }
+
+		err := s.Persist(tx1, kmocks.NewPvtReadWriteSetBuilder().Build())
+		assert.NoError(t, err)
+
+		expectedErr := errors.New("transient data error")
+		tdataProvider.StoreError(expectedErr)
+		err = s.Persist(tx1, kmocks.NewPvtReadWriteSetBuilder().Build())
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		tdataProvider.StoreError(nil)
+
+		expectedErr = errors.New("DCAS error")
+		olProvider.StoreError(expectedErr)
+		err = s.Persist(tx1, kmocks.NewPvtReadWriteSetBuilder().Build())
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		olProvider.StoreError(nil)
+	})
+}
diff --git a/extensions/collections/transientdata/dissemination/disseminationplan.go b/extensions/collections/transientdata/dissemination/disseminationplan.go
new file mode 100644
index 00000000..594ccd37
--- /dev/null
+++ b/extensions/collections/transientdata/dissemination/disseminationplan.go
@@ -0,0 +1,94 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	protobuf "github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/dissemination"
+	"github.com/hyperledger/fabric/extensions/common/discovery"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/common"
+	gdiscovery "github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/gossip"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	"github.com/pkg/errors"
+	"github.com/spf13/viper"
+)
+
+type gossipAdapter interface {
+	PeersOfChannel(common.ChainID) []gdiscovery.NetworkMember
+	SelfMembershipInfo() gdiscovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+// ComputeDisseminationPlan returns the dissemination plan for transient data
+func ComputeDisseminationPlan(
+	channelID, ns string,
+	rwSet *rwset.CollectionPvtReadWriteSet,
+	colAP privdata.CollectionAccessPolicy,
+	pvtDataMsg *proto.SignedGossipMessage,
+	gossipAdapter gossipAdapter) ([]*dissemination.Plan, bool, error) {
+	logger.Debugf("Computing transient data dissemination plan for [%s:%s]", ns, rwSet.CollectionName)
+
+	disseminator := New(channelID, ns, rwSet.CollectionName, colAP, gossipAdapter)
+
+	kvRwSet := &kvrwset.KVRWSet{}
+	if err := protobuf.Unmarshal(rwSet.Rwset, kvRwSet); err != nil {
+		return nil, true, errors.WithMessage(err, "error unmarshalling KV read/write set for transient data")
+	}
+
+	// FIXME: This logic should be optimized by taking apart the rw-set and computing a plan for each key, since each key
+	// 	will be going to different peers.
+	var endorsers discovery.PeerGroup
+	for _, kvWrite := range kvRwSet.Writes {
+		if kvWrite.IsDelete {
+			continue
+		}
+		endorsersForKey, err := disseminator.ResolveEndorsers(kvWrite.Key)
+		if err != nil {
+			return nil, true, errors.WithMessage(err, "error resolving endorsers for transient data")
+		}
+
+		logger.Debugf("Endorsers for key [%s:%s:%s]: %s", ns, rwSet.CollectionName, kvWrite.Key, endorsersForKey)
+
+		for _, endorser := range endorsersForKey {
+			if endorser.Local {
+				logger.Debugf("Not adding local endorser for key [%s:%s:%s]", ns, rwSet.CollectionName, kvWrite.Key)
+				continue
+			}
+			endorsers = discovery.Merge(endorsers, endorser)
+		}
+	}
+
+	logger.Debugf("Endorsers for collection [%s:%s]: %s", ns, rwSet.CollectionName, endorsers)
+
+	routingFilter := func(member gdiscovery.NetworkMember) bool {
+		if endorsers.ContainsPeer(member.Endpoint) {
+			logger.Debugf("Peer [%s] is an endorser for [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+			return true
+		}
+
+		logger.Debugf("Peer [%s] is NOT an endorser for [%s:%s]", member.Endpoint, ns, rwSet.CollectionName)
+		return false
+	}
+
+	sc := gossip.SendCriteria{
+		Timeout:    viper.GetDuration("peer.gossip.pvtData.pushAckTimeout"),
+		Channel:    common.ChainID(channelID),
+		MaxPeers:   colAP.MaximumPeerCount(),
+		MinAck:     colAP.RequiredPeerCount(),
+		IsEligible: routingFilter,
+	}
+
+	return []*dissemination.Plan{{
+		Criteria: sc,
+		Msg:      pvtDataMsg,
+	}}, true, nil
+}
diff --git a/extensions/collections/transientdata/dissemination/disseminator.go b/extensions/collections/transientdata/dissemination/disseminator.go
new file mode 100644
index 00000000..c0b1c91a
--- /dev/null
+++ b/extensions/collections/transientdata/dissemination/disseminator.go
@@ -0,0 +1,130 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"hash/fnv"
+	"sort"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/common/discovery"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("transientdata")
+
+// Disseminator disseminates transient data to a deterministic set of endorsers
+type Disseminator struct {
+	*discovery.Discovery
+	namespace  string
+	collection string
+	policy     privdata.CollectionAccessPolicy
+}
+
+// New returns a new transient data disseminator
+func New(channelID, namespace, collection string, policy privdata.CollectionAccessPolicy, gossip gossipAdapter) *Disseminator {
+	return &Disseminator{
+		Discovery:  discovery.New(channelID, gossip),
+		namespace:  namespace,
+		collection: collection,
+		policy:     policy,
+	}
+}
+
+// ResolveEndorsers resolves to a set of endorsers to which transient data should be disseminated
+func (d *Disseminator) ResolveEndorsers(key string) (discovery.PeerGroup, error) {
+	h, err := getHash32(key)
+	if err != nil {
+		return nil, errors.WithMessage(err, "error computing int32 hash of key")
+	}
+
+	orgs := d.chooseOrgs(h)
+
+	logger.Debugf("[%s] Chosen orgs: %s", d.ChannelID(), orgs)
+
+	endorsers := d.chooseEndorsers(h, orgs)
+
+	logger.Debugf("[%s] Chosen endorsers from orgs %s: %s", d.ChannelID(), orgs, endorsers)
+	return endorsers, nil
+}
+
+func (d *Disseminator) chooseEndorsers(h uint32, orgs []string) discovery.PeerGroup {
+	var endorsers discovery.PeerGroup
+
+	for i := 0; i < d.policy.MaximumPeerCount(); i++ {
+		for _, org := range orgs {
+			if len(endorsers) == d.policy.MaximumPeerCount() {
+				// We have enough endorsers
+				break
+			}
+
+			// Get a sorted list of endorsers for the org
+			endorsersForOrg := d.getEndorsers(org).Sort()
+			if len(endorsersForOrg) == 0 {
+				logger.Debugf("[%s] There are no endorsers in org [%s]", d.ChannelID(), org)
+				continue
+			}
+
+			logger.Debugf("[%s] Endorsers for [%s]: %s", d.ChannelID(), org, endorsersForOrg)
+
+			// Deterministically choose an endorser
+			endorserForOrg := endorsersForOrg[(int(h)+i)%len(endorsersForOrg)]
+			if endorsers.Contains(endorserForOrg) {
+				logger.Debugf("[%s] Will not add endorser [%s] from org [%s] since it is already added", d.ChannelID(), endorserForOrg, org)
+				continue
+			}
+
+			endorsers = append(endorsers, endorserForOrg)
+		}
+	}
+	return endorsers
+}
+
+func (d *Disseminator) getEndorsers(mspID string) discovery.PeerGroup {
+	return d.GetMembers(func(m *discovery.Member) bool {
+		if m.MSPID != mspID {
+			logger.Debugf("[%s] Not adding peer [%s] as an endorser since it is not in org [%s]", d.ChannelID(), m.Endpoint, mspID)
+			return false
+		}
+		return true
+	})
+
+}
+
+func (d *Disseminator) chooseOrgs(h uint32) []string {
+	memberOrgs := d.policy.MemberOrgs()
+	numOrgs := min(d.policy.MaximumPeerCount(), len(memberOrgs))
+
+	// Copy and sort the orgs
+	var sortedOrgs []string
+	sortedOrgs = append(sortedOrgs, memberOrgs...)
+	sort.Strings(sortedOrgs)
+
+	var chosenOrgs []string
+	for i := 0; i < numOrgs; i++ {
+		chosenOrgs = append(chosenOrgs, sortedOrgs[(int(h)+i)%len(sortedOrgs)])
+	}
+
+	return chosenOrgs
+}
+
+func getHash32(key string) (uint32, error) {
+	h := fnv.New32a()
+	_, err := h.Write([]byte(key))
+	if err != nil {
+		return 0, err
+	}
+	return h.Sum32(), nil
+}
+
+func min(i, j int) int {
+	if i < j {
+		return i
+	}
+	return j
+}
diff --git a/extensions/collections/transientdata/dissemination/disseminator_test.go b/extensions/collections/transientdata/dissemination/disseminator_test.go
new file mode 100644
index 00000000..e699cf4c
--- /dev/null
+++ b/extensions/collections/transientdata/dissemination/disseminator_test.go
@@ -0,0 +1,317 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dissemination
+
+import (
+	"os"
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/mocks"
+	ledgerconfig "github.com/hyperledger/fabric/extensions/roles"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+var (
+	ns1   = "chaincode1"
+	ns2   = "chaincode2"
+	coll1 = "collection1"
+	coll2 = "collection2"
+	key1  = "key1"
+	key2  = "key2"
+
+	org1MSPID      = "Org1MSP"
+	p1Org1Endpoint = "p1.org1.com"
+	p1Org1PKIID    = gcommon.PKIidType("pkiid_P1O1")
+	p2Org1Endpoint = "p2.org1.com"
+	p2Org1PKIID    = gcommon.PKIidType("pkiid_P2O1")
+	p3Org1Endpoint = "p3.org1.com"
+	p3Org1PKIID    = gcommon.PKIidType("pkiid_P3O1")
+
+	org2MSPID      = "Org2MSP"
+	p1Org2Endpoint = "p1.org2.com"
+	p1Org2PKIID    = gcommon.PKIidType("pkiid_P1O2")
+	p2Org2Endpoint = "p2.org2.com"
+	p2Org2PKIID    = gcommon.PKIidType("pkiid_P2O2")
+	p3Org2Endpoint = "p3.org2.com"
+	p3Org2PKIID    = gcommon.PKIidType("pkiid_P3O2")
+
+	org3MSPID      = "Org3MSP"
+	p1Org3Endpoint = "p1.org3.com"
+	p1Org3PKIID    = gcommon.PKIidType("pkiid_P1O3")
+	p2Org3Endpoint = "p2.org3.com"
+	p2Org3PKIID    = gcommon.PKIidType("pkiid_P2O3")
+	p3Org3Endpoint = "p3.org3.com"
+	p3Org3PKIID    = gcommon.PKIidType("pkiid_P3O3")
+
+	validatorRole = string(ledgerconfig.ValidatorRole)
+	endorserRole  = string(ledgerconfig.EndorserRole)
+)
+
+func TestDissemination(t *testing.T) {
+	channelID := "testchannel"
+
+	gossip := mocks.NewMockGossipAdapter().
+		Self(org1MSPID, mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, validatorRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, validatorRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, validatorRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, validatorRole)).
+		Member(org3MSPID, mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole))
+
+	t.Run("2 peers", func(t *testing.T) {
+		maxPeers := 2
+
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				MaxPeerCount: maxPeers,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		// key1
+		endorsers, err := d.ResolveEndorsers(key1)
+		require.NoError(t, err)
+		require.Equal(t, maxPeers, len(endorsers))
+
+		t.Logf("Endorsers: %s", endorsers)
+
+		assert.Equal(t, p3Org3Endpoint, endorsers[0].Endpoint)
+		assert.Equal(t, p1Org1Endpoint, endorsers[1].Endpoint)
+
+		// key2
+		endorsers, err = d.ResolveEndorsers(key2)
+		require.NoError(t, err)
+		require.Equal(t, maxPeers, len(endorsers))
+
+		t.Logf("Endorsers: %s", endorsers)
+
+		assert.Equal(t, p1Org2Endpoint, endorsers[0].Endpoint)
+		assert.Equal(t, p1Org3Endpoint, endorsers[1].Endpoint)
+	})
+
+	t.Run("5 peers", func(t *testing.T) {
+		maxPeers := 5
+
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				MaxPeerCount: maxPeers,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		// key1
+		endorsers, err := d.ResolveEndorsers(key1)
+		require.NoError(t, err)
+		require.Equal(t, maxPeers, len(endorsers))
+
+		t.Logf("Endorsers: %s", endorsers)
+
+		assert.Equal(t, p3Org3Endpoint, endorsers[0].Endpoint)
+		assert.Equal(t, p1Org1Endpoint, endorsers[1].Endpoint)
+		assert.Equal(t, p3Org2Endpoint, endorsers[2].Endpoint)
+		assert.Equal(t, p1Org3Endpoint, endorsers[3].Endpoint)
+		assert.Equal(t, p1Org2Endpoint, endorsers[4].Endpoint)
+	})
+
+	t.Run("Not enough peers", func(t *testing.T) {
+		maxPeers := 6
+
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				MaxPeerCount: maxPeers,
+				Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+			}, gossip)
+
+		// key1
+		endorsers, err := d.ResolveEndorsers(key1)
+		require.NoError(t, err)
+		require.Equal(t, 5, len(endorsers))
+
+		t.Logf("Endorsers: %s", endorsers)
+
+		assert.Equal(t, p3Org3Endpoint, endorsers[0].Endpoint)
+		assert.Equal(t, p1Org1Endpoint, endorsers[1].Endpoint)
+		assert.Equal(t, p3Org2Endpoint, endorsers[2].Endpoint)
+		assert.Equal(t, p1Org3Endpoint, endorsers[3].Endpoint)
+		assert.Equal(t, p1Org2Endpoint, endorsers[4].Endpoint)
+	})
+
+	t.Run("Subset of orgs", func(t *testing.T) {
+		maxPeers := 3
+
+		d := New(channelID, ns1, coll1,
+			&mocks.MockAccessPolicy{
+				ReqPeerCount: 1,
+				MaxPeerCount: maxPeers,
+				Orgs:         []string{org2MSPID, org3MSPID},
+			}, gossip)
+
+		// key1
+		endorsers, err := d.ResolveEndorsers(key1)
+		require.NoError(t, err)
+		require.Equal(t, maxPeers, len(endorsers))
+
+		t.Logf("Endorsers: %s", endorsers)
+
+		assert.Equal(t, p3Org3Endpoint, endorsers[0].Endpoint)
+		assert.Equal(t, p3Org2Endpoint, endorsers[1].Endpoint)
+		assert.Equal(t, p1Org3Endpoint, endorsers[2].Endpoint)
+	})
+}
+
+func TestComputeDisseminationPlan(t *testing.T) {
+	channelID := "testchannel"
+
+	p1Org1 := mocks.NewMember(p1Org1Endpoint, p1Org1PKIID, endorserRole)
+	p2Org1 := mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, endorserRole)
+	p3Org1 := mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, validatorRole)
+	p1Org2 := mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)
+	p2Org2 := mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, validatorRole)
+	p3Org2 := mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)
+	p1Org3 := mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)
+	p2Org3 := mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, validatorRole)
+	p3Org3 := mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole)
+
+	gossip := mocks.NewMockGossipAdapter().
+		Self(org1MSPID, p1Org1).
+		Member(org1MSPID, p2Org1).
+		Member(org1MSPID, p3Org1).
+		Member(org2MSPID, p1Org2).
+		Member(org2MSPID, p2Org2).
+		Member(org2MSPID, p3Org2).
+		Member(org3MSPID, p1Org3).
+		Member(org3MSPID, p2Org3).
+		Member(org3MSPID, p3Org3)
+
+	t.Run("Orgs: 2, Max Peers: 2, Keys: 1", func(t *testing.T) {
+		maxPeers := 2
+
+		colAP := &mocks.MockAccessPolicy{
+			ReqPeerCount: 1,
+			MaxPeerCount: maxPeers,
+			Orgs:         []string{org2MSPID, org3MSPID},
+		}
+
+		coll1Builder := mocks.NewPvtReadWriteSetCollectionBuilder(coll1)
+		coll1Builder.
+			Write(key1, []byte("value1")).
+			Delete(key2) // Deletes should be ignored
+
+		rwSet := coll1Builder.Build()
+
+		dPlan, handled, err := ComputeDisseminationPlan(channelID, ns1, rwSet, colAP, nil, gossip)
+		require.NoError(t, err)
+		require.True(t, handled)
+		require.Equal(t, 1, len(dPlan))
+
+		criteria := dPlan[0].Criteria
+
+		assert.Equal(t, maxPeers, criteria.MaxPeers)
+
+		assert.False(t, criteria.IsEligible(p1Org1))
+		assert.False(t, criteria.IsEligible(p2Org1))
+		assert.False(t, criteria.IsEligible(p3Org1))
+		assert.False(t, criteria.IsEligible(p1Org2))
+		assert.False(t, criteria.IsEligible(p2Org2))
+		assert.True(t, criteria.IsEligible(p3Org2))
+		assert.False(t, criteria.IsEligible(p1Org3))
+		assert.False(t, criteria.IsEligible(p2Org3))
+		assert.True(t, criteria.IsEligible(p3Org3))
+	})
+
+	t.Run("Orgs: 3, Max Peers: 3, Keys: 1", func(t *testing.T) {
+		maxPeers := 3
+
+		colAP := &mocks.MockAccessPolicy{
+			ReqPeerCount: 1,
+			MaxPeerCount: maxPeers,
+			Orgs:         []string{org2MSPID, org3MSPID},
+		}
+
+		coll1Builder := mocks.NewPvtReadWriteSetCollectionBuilder(coll1)
+		coll1Builder.
+			Write(key1, []byte("value1"))
+
+		rwSet := coll1Builder.Build()
+
+		dPlan, handled, err := ComputeDisseminationPlan(channelID, ns1, rwSet, colAP, nil, gossip)
+		require.NoError(t, err)
+		require.True(t, handled)
+		require.Equal(t, 1, len(dPlan))
+
+		criteria := dPlan[0].Criteria
+
+		assert.Equal(t, maxPeers, criteria.MaxPeers)
+
+		// The transient data should be stored to p1Org1 and p3Org3 but, since p1Org1 is
+		// a local peer, it is not included in the dissemination plan.
+		assert.False(t, criteria.IsEligible(p1Org1))
+		assert.False(t, criteria.IsEligible(p2Org1))
+		assert.False(t, criteria.IsEligible(p3Org1))
+		assert.False(t, criteria.IsEligible(p1Org2))
+		assert.False(t, criteria.IsEligible(p2Org2))
+		assert.True(t, criteria.IsEligible(p3Org2))
+		assert.True(t, criteria.IsEligible(p1Org3))
+		assert.False(t, criteria.IsEligible(p2Org3))
+		assert.True(t, criteria.IsEligible(p3Org3))
+	})
+
+	t.Run("Orgs: 3, Max Peers: 2, Keys: 2", func(t *testing.T) {
+		maxPeers := 2
+
+		colAP := &mocks.MockAccessPolicy{
+			ReqPeerCount: 1,
+			MaxPeerCount: maxPeers,
+			Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+		}
+
+		coll1Builder := mocks.NewPvtReadWriteSetCollectionBuilder(coll1)
+		coll1Builder.
+			Write(key1, []byte("value1")).
+			Write(key2, []byte("value2"))
+
+		rwSet := coll1Builder.Build()
+
+		dPlan, handled, err := ComputeDisseminationPlan(channelID, ns1, rwSet, colAP, nil, gossip)
+		require.NoError(t, err)
+		require.True(t, handled)
+		require.Equal(t, 1, len(dPlan))
+
+		criteria := dPlan[0].Criteria
+
+		assert.Equal(t, maxPeers, criteria.MaxPeers)
+
+		// The transient data for:
+		// - key1: p2Org1, p3Org3
+		// - key2: p1Org2, p1Org3
+		// Since p1Org1 is a local peer, it's not included in the dissemination plan.
+		// So the data should be disseminated to p2Org1, p3Org3, p1Org2, and p1Org3
+		assert.False(t, criteria.IsEligible(p1Org1))
+		assert.True(t, criteria.IsEligible(p2Org1))
+		assert.False(t, criteria.IsEligible(p3Org1))
+		assert.True(t, criteria.IsEligible(p1Org2))
+		assert.False(t, criteria.IsEligible(p2Org2))
+		assert.False(t, criteria.IsEligible(p3Org2))
+		assert.True(t, criteria.IsEligible(p1Org3))
+		assert.False(t, criteria.IsEligible(p2Org3))
+		assert.True(t, criteria.IsEligible(p3Org3))
+	})
+}
+
+func TestMain(m *testing.M) {
+	// The local peer's roles are retrieved from ledgerconfig
+	viper.SetDefault("ledger.roles", "committer,endorser")
+
+	os.Exit(m.Run())
+}
diff --git a/extensions/collections/transientdata/mocks/mockprovider.go b/extensions/collections/transientdata/mocks/mockprovider.go
new file mode 100644
index 00000000..f0e68aea
--- /dev/null
+++ b/extensions/collections/transientdata/mocks/mockprovider.go
@@ -0,0 +1,40 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"context"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+)
+
+// TransientDataProvider is a mock transient data provider
+type TransientDataProvider struct {
+}
+
+// RetrieverForChannel returns a provider for the given channel
+func (p *TransientDataProvider) RetrieverForChannel(channel string) transientdata.Retriever {
+	return &transientDataRetriever{}
+}
+
+type transientDataRetriever struct {
+}
+
+// GetTransientData returns the transientData
+func (m *transientDataRetriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return &storeapi.ExpiringValue{Value: []byte(key.Key)}, nil
+}
+
+// GetTransientDataMultipleKeys returns the data with multiple keys
+func (m *transientDataRetriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		values[i] = &storeapi.ExpiringValue{Value: []byte(k)}
+	}
+	return values, nil
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/api/api.go b/extensions/collections/transientdata/storeprovider/store/api/api.go
new file mode 100644
index 00000000..7c091717
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/api/api.go
@@ -0,0 +1,30 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	"fmt"
+	"time"
+)
+
+// Key is a transient data key
+type Key struct {
+	Namespace  string
+	Collection string
+	Key        string
+}
+
+func (k Key) String() string {
+	return fmt.Sprintf("%s:%s:%s", k.Namespace, k.Collection, k.Key)
+}
+
+// Value is a transient data value
+type Value struct {
+	Value      []byte
+	TxID       string
+	ExpiryTime time.Time
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/cache/cache.go b/extensions/collections/transientdata/storeprovider/store/cache/cache.go
new file mode 100644
index 00000000..4c859f89
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/cache/cache.go
@@ -0,0 +1,151 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cache
+
+import (
+	"fmt"
+	"time"
+
+	"github.com/bluele/gcache"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/config"
+)
+
+var logger = flogging.MustGetLogger("memtransientdatastore")
+
+// Cache is an in-memory key-value cache
+type Cache struct {
+	cache   gcache.Cache
+	ticker  *time.Ticker
+	dbstore transientDB
+}
+
+// transientDB - an interface for persisting and retrieving keys
+type transientDB interface {
+	AddKey(api.Key, *api.Value) error
+	DeleteExpiredKeys() error
+	GetKey(key api.Key) (*api.Value, error)
+	Close()
+}
+
+// New return a new in-memory key-value cache
+func New(size int, dbstore transientDB) *Cache {
+	cache := gcache.New(size).LoaderExpireFunc(func(key interface{}) (interface{}, *time.Duration, error) {
+		logger.Debugf("LoaderExpireFunc for key %s", key)
+		value, err := dbstore.GetKey(key.(api.Key))
+		if value == nil || err != nil {
+			if err != nil {
+				logger.Error(err.Error())
+			}
+			logger.Debugf("Key [%s] not found in DB", key)
+			return nil, nil, gcache.KeyNotFoundError
+		}
+		isExpired, diff := checkExpiryTime(value.ExpiryTime)
+		if isExpired {
+			logger.Debugf("Key [%s] from DB has expired", key)
+			return nil, nil, gcache.KeyNotFoundError
+		}
+		logger.Debugf("Loaded key [%s] from DB", key)
+		return value, &diff, nil
+
+	}).
+		EvictedFunc(func(key, value interface{}) {
+			logger.Debugf("EvictedFunc for key %s", key)
+			if value != nil {
+				k := key.(api.Key)
+				v := value.(*api.Value)
+				isExpired, _ := checkExpiryTime(v.ExpiryTime)
+				if !isExpired {
+					dbstoreErr := dbstore.AddKey(k, v)
+					if dbstoreErr != nil {
+						logger.Error(dbstoreErr.Error())
+					} else {
+						logger.Debugf("Key [%s] offloaded to DB", key)
+					}
+				}
+			}
+
+		}).ARC().Build()
+
+	// cleanup expired data in db
+	ticker := time.NewTicker(config.GetTransientDataExpiredIntervalTime())
+	go func() {
+		for range ticker.C {
+			dbstoreErr := dbstore.DeleteExpiredKeys()
+			if dbstoreErr != nil {
+				logger.Error(dbstoreErr.Error())
+			}
+		}
+	}()
+
+	return &Cache{
+		cache:   cache,
+		ticker:  ticker,
+		dbstore: dbstore,
+	}
+}
+
+// Close closes the cache
+func (c *Cache) Close() {
+	c.cache.Purge()
+	c.ticker.Stop()
+}
+
+// Put adds the transient value for the given key.
+// Returns the previous value (if any)
+func (c *Cache) Put(key api.Key, value []byte, txID string) {
+	if err := c.cache.Set(key,
+		&api.Value{
+			Value: value,
+			TxID:  txID,
+		}); err != nil {
+		panic("Set must never return an error")
+	}
+}
+
+// PutWithExpire adds the transient value for the given key.
+// Returns the previous value (if any)
+func (c *Cache) PutWithExpire(key api.Key, value []byte, txID string, expiry time.Duration) {
+	if err := c.cache.SetWithExpire(key,
+		&api.Value{
+			Value:      value,
+			TxID:       txID,
+			ExpiryTime: time.Now().UTC().Add(expiry),
+		}, expiry); err != nil {
+		panic("Set must never return an error")
+	}
+}
+
+// Get returns the transient value for the given key
+func (c *Cache) Get(key api.Key) *api.Value {
+	value, err := c.cache.Get(key)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get must never return an error other than KeyNotFoundError err:%s", err))
+		}
+		return nil
+	}
+
+	return value.(*api.Value)
+}
+
+func checkExpiryTime(expiryTime time.Time) (bool, time.Duration) {
+	if expiryTime.IsZero() {
+		return false, 0
+	}
+	timeNow := time.Now().UTC()
+	logger.Debugf("time now %s", timeNow)
+	logger.Debugf("expiry time %s", expiryTime)
+	diff := expiryTime.Sub(timeNow)
+	logger.Debugf("diff time %s", diff)
+
+	if diff <= 0 {
+		return true, diff
+	}
+	return false, diff
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/cache/cache_test.go b/extensions/collections/transientdata/storeprovider/store/cache/cache_test.go
new file mode 100644
index 00000000..a499bd5c
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/cache/cache_test.go
@@ -0,0 +1,320 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cache
+
+import (
+	"fmt"
+	"os"
+	"sync"
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/dbstore"
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	txID1 = "txid1"
+	txID2 = "txid2"
+
+	ns1 = "namespace1"
+	ns2 = "namespace2"
+
+	coll1 = "coll1"
+	coll2 = "coll2"
+
+	key1 = "key1"
+	key2 = "key2"
+	key3 = "key3"
+)
+
+var (
+	k1 = api.Key{
+		Namespace:  ns1,
+		Collection: coll1,
+		Key:        key1,
+	}
+	k2 = api.Key{
+		Namespace:  ns2,
+		Collection: coll2,
+		Key:        key2,
+	}
+	k3 = api.Key{
+		Namespace:  ns1,
+		Collection: coll2,
+		Key:        key2,
+	}
+	k4 = api.Key{
+		Namespace:  ns1,
+		Collection: coll2,
+		Key:        key3,
+	}
+	v1 = []byte("v1")
+	v2 = []byte("v2")
+)
+
+func TestCleanupExpiredTransientDataFromDB(t *testing.T) {
+	defer removeDBPath(t)
+
+	p := dbstore.NewDBProvider()
+	require.NotNil(t, p)
+	defer p.Close()
+
+	db, err := p.OpenDBStore("testchannel")
+	require.NoError(t, err)
+	require.NotNil(t, db)
+
+	cache := New(1, db)
+	require.NotNil(t, cache)
+
+	cache.PutWithExpire(k1, v1, txID1, 2*time.Second)
+	cache.PutWithExpire(k2, v1, txID1, 1*time.Second)
+	cache.Put(k3, v1, txID1)
+	cache.Put(k4, v1, txID1)
+
+	// wait one second and half to confirm cleanup remove k2
+	time.Sleep(1500 * time.Millisecond)
+
+	// check data exist
+	v, err := cache.dbstore.GetKey(k1)
+	require.Nil(t, err)
+	require.NotNil(t, v)
+	// check data not exist because it's expired
+	v, err = cache.dbstore.GetKey(k2)
+	require.Nil(t, err)
+	require.Nil(t, v)
+	// check data exist
+	v, err = cache.dbstore.GetKey(k3)
+	require.Nil(t, err)
+	require.NotNil(t, v)
+
+	// wait one more second and half to confirm cleanup remove k1
+	time.Sleep(1500 * time.Millisecond)
+	// check data not exist because it's expired
+	v, err = cache.dbstore.GetKey(k1)
+	require.Nil(t, err)
+	require.Nil(t, v)
+	// check data exist
+	v, err = cache.dbstore.GetKey(k3)
+	require.Nil(t, err)
+	require.NotNil(t, v)
+
+}
+
+func TestRetrieveTransientDataFromDB(t *testing.T) {
+	t.Run("getAfterPurgeFromCache", func(t *testing.T) {
+		defer removeDBPath(t)
+		p := dbstore.NewDBProvider()
+		require.NotNil(t, p)
+		defer p.Close()
+
+		db, err := p.OpenDBStore("testchannel")
+		require.NoError(t, err)
+		require.NotNil(t, db)
+
+		cache := New(1, db)
+		require.NotNil(t, cache)
+		v := cache.Get(k1)
+		require.Nil(t, v)
+
+		cache.Put(k1, v1, txID1)
+		// Get k1 from cache
+		v = cache.Get(k1)
+		require.NotNil(t, v)
+		require.Equal(t, txID1, v.TxID)
+		require.Equal(t, v1, v.Value)
+
+		// After put the k2 the k1 will be purged from cache
+		cache.Put(k2, v2, txID2)
+
+		// k1 will not be found in cache so will get it from db
+		v = cache.Get(k1)
+		require.NotNil(t, v)
+		require.Equal(t, txID1, v.TxID)
+		require.Equal(t, v1, v.Value)
+
+	})
+
+	t.Run("getAfterPurgeFromCacheForNotExpiredData", func(t *testing.T) {
+		defer removeDBPath(t)
+		p := dbstore.NewDBProvider()
+		require.NotNil(t, p)
+		defer p.Close()
+
+		db, err := p.OpenDBStore("testchannel")
+		require.NoError(t, err)
+		require.NotNil(t, db)
+
+		cache := New(1, db)
+		require.NotNil(t, cache)
+		v := cache.Get(k1)
+		require.Nil(t, v)
+
+		cache.PutWithExpire(k1, v1, txID1, 5*time.Second)
+		// Get k1 from cache
+		v = cache.Get(k1)
+		require.NotNil(t, v)
+
+		// After put the k2 the k1 will be purged from cache
+		cache.PutWithExpire(k2, v2, txID2, 5*time.Second)
+
+		// k1 will not be found in cache so will get it from db
+		v = cache.Get(k1)
+		require.NotNil(t, v)
+
+	})
+
+	t.Run("getAfterPurgeFromCacheForExpiredData", func(t *testing.T) {
+		defer removeDBPath(t)
+		p := dbstore.NewDBProvider()
+		require.NotNil(t, p)
+		defer p.Close()
+
+		db, err := p.OpenDBStore("testchannel")
+		require.NoError(t, err)
+		require.NotNil(t, db)
+
+		cache := New(1, db)
+		require.NotNil(t, cache)
+		v := cache.Get(k1)
+		require.Nil(t, v)
+
+		cache.PutWithExpire(k1, v1, txID1, 2*time.Second)
+		// Get k1 from cache
+		v = cache.Get(k1)
+		require.NotNil(t, v)
+
+		// After put the k2 the k1 will be purged from cache
+		cache.PutWithExpire(k2, v2, txID2, 5*time.Second)
+
+		// k1 will not be found in cache so will get it from db
+		time.Sleep(1 * time.Second)
+		v = cache.Get(k1)
+		require.NotNil(t, v)
+
+		// k1 will not be found in db because it's already expired
+		time.Sleep(1 * time.Second)
+		v = cache.Get(k1)
+		require.Nil(t, v)
+
+	})
+
+}
+
+func TestTransientDataCache(t *testing.T) {
+	defer removeDBPath(t)
+	p := dbstore.NewDBProvider()
+	require.NotNil(t, p)
+	defer p.Close()
+
+	db, err := p.OpenDBStore("testchannel")
+	require.NoError(t, err)
+	require.NotNil(t, db)
+
+	cache := New(1, db)
+	require.NotNil(t, cache)
+	defer cache.Close()
+
+	t.Run("Key", func(t *testing.T) {
+		require.Equal(t, "namespace1:coll1:key1", k1.String())
+	})
+
+	t.Run("GetAndPut", func(t *testing.T) {
+		v := cache.Get(k1)
+		require.Nil(t, v)
+
+		cache.Put(k1, v1, txID1)
+		cache.Put(k2, v2, txID2)
+
+		v = cache.Get(k1)
+		require.NotNil(t, v)
+		require.Equal(t, txID1, v.TxID)
+		require.Equal(t, v1, v.Value)
+
+		v = cache.Get(k2)
+		require.NotNil(t, v)
+		require.Equal(t, txID2, v.TxID)
+		require.Equal(t, v2, v.Value)
+	})
+
+	t.Run("Expire", func(t *testing.T) {
+		expiration := 10 * time.Millisecond
+		cache.PutWithExpire(k3, v1, txID1, expiration)
+
+		v := cache.Get(k3)
+		require.NotNil(t, v)
+
+		time.Sleep(100 * time.Millisecond)
+		v = cache.Get(k3)
+		require.Nil(t, v)
+	})
+}
+
+func TestTransientDataCacheConcurrency(t *testing.T) {
+	defer removeDBPath(t)
+	p := dbstore.NewDBProvider()
+	require.NotNil(t, p)
+	defer p.Close()
+
+	db, err := p.OpenDBStore("testchannel")
+	require.NoError(t, err)
+	require.NotNil(t, db)
+
+	cache := New(1, db)
+	require.NotNil(t, cache)
+	defer cache.Close()
+
+	n := 100
+
+	var wg sync.WaitGroup
+	wg.Add(n)
+
+	for i := 0; i < n; i++ {
+		k := api.Key{
+			Namespace:  ns1,
+			Collection: coll1,
+			Key:        fmt.Sprintf("k_%d", i),
+		}
+		v := []byte(fmt.Sprintf("v_%d", i))
+
+		go func() {
+			defer wg.Done()
+
+			cache.Put(k, v, txID1)
+			val := cache.Get(k)
+			if val == nil {
+				panic("Unable to get value for key")
+			}
+			if string(val.Value) != string(v) {
+				panic("Unable to get value for key")
+			}
+		}()
+	}
+
+	wg.Wait()
+}
+
+func TestMain(m *testing.M) {
+	removeDBPath(nil)
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/ledgertests/transientdatadb")
+	viper.Set("ledger.transientdata.cleanupExpired.Interval", "50ms")
+
+	os.Exit(m.Run())
+}
+
+func removeDBPath(t testing.TB) {
+	removePath(t, config.GetTransientDataLevelDBPath())
+}
+
+func removePath(t testing.TB, path string) {
+	if err := os.RemoveAll(path); err != nil {
+		t.Fatalf("Err: %s", err)
+	}
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore.go b/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore.go
new file mode 100644
index 00000000..129b92e4
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore.go
@@ -0,0 +1,130 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dbstore
+
+import (
+	"bytes"
+	"encoding/gob"
+	"fmt"
+	"strings"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/api"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("transientdb")
+
+var compositeKeySep = "!"
+
+// DBStore holds the db handle and the db name
+type DBStore struct {
+	db     *leveldbhelper.DBHandle
+	dbName string
+}
+
+// newDBStore constructs an instance of db store
+func newDBStore(db *leveldbhelper.DBHandle, dbName string) *DBStore {
+	return &DBStore{db, dbName}
+}
+
+// AddKey add cache key to db
+func (s *DBStore) AddKey(key api.Key, value *api.Value) error {
+	encodeVal, err := encodeCacheVal(value)
+	if err != nil {
+		return errors.Wrapf(err, "failed to encode transientdata value %s", value)
+	}
+	// put key in db
+	err = s.db.Put(encodeCacheKey(key, time.Time{}), encodeVal, true)
+	if err != nil {
+		return errors.Wrapf(err, "failed to save transientdata key %s in db", key)
+	}
+
+	if !value.ExpiryTime.IsZero() {
+		// put same previous key with prefix expiryTime so the clean up can remove all expired keys
+		err = s.db.Put(encodeCacheKey(key, value.ExpiryTime), []byte(""), true)
+		if err != nil {
+			return errors.Wrapf(err, "failed to save transientdata key %s in db", key)
+		}
+	}
+
+	return nil
+}
+
+// GetKey get cache key from db
+func (s *DBStore) GetKey(key api.Key) (*api.Value, error) {
+	logger.Debugf("load transientdata key %s from db", key)
+	value, err := s.db.Get(encodeCacheKey(key, time.Time{}))
+	if err != nil {
+		return nil, errors.Wrapf(err, "failed to load transientdata key %s from db", key)
+	}
+	if value != nil {
+		val, err := decodeCacheVal(value)
+		if err != nil {
+			return nil, errors.Wrapf(err, "failed to decode transientdata value %s", value)
+		}
+		return val, nil
+	}
+	return nil, nil
+}
+
+// DeleteExpiredKeys delete expired keys from db
+func (s *DBStore) DeleteExpiredKeys() error {
+	dbBatch := leveldbhelper.NewUpdateBatch()
+	itr := s.db.GetIterator(nil, []byte(fmt.Sprintf("%d%s", time.Now().UTC().UnixNano(), compositeKeySep)))
+	for itr.Next() {
+		key := string(itr.Key())
+		dbBatch.Delete([]byte(key))
+		dbBatch.Delete([]byte(key[strings.Index(key, compositeKeySep)+1:]))
+	}
+	if dbBatch.Len() > 0 {
+		err := s.db.WriteBatch(dbBatch, true)
+		if err != nil {
+			return errors.Errorf("failed to delete transient data keys %s in db %s", dbBatch.KVs, err.Error())
+		}
+		logger.Debugf("delete expired keys %s from db", dbBatch.KVs)
+	}
+
+	return nil
+}
+
+// Close db
+func (s *DBStore) Close() {
+}
+
+func encodeCacheKey(key api.Key, expiryTime time.Time) []byte {
+	var compositeKey []byte
+	if !expiryTime.IsZero() {
+		compositeKey = append(compositeKey, []byte(fmt.Sprintf("%d", expiryTime.UnixNano()))...)
+		compositeKey = append(compositeKey, compositeKeySep...)
+	}
+	compositeKey = append(compositeKey, []byte(key.Namespace)...)
+	compositeKey = append(compositeKey, compositeKeySep...)
+	compositeKey = append(compositeKey, []byte(key.Collection)...)
+	compositeKey = append(compositeKey, compositeKeySep...)
+	compositeKey = append(compositeKey, []byte(key.Key)...)
+	return compositeKey
+}
+
+func decodeCacheVal(b []byte) (*api.Value, error) {
+	decoder := gob.NewDecoder(bytes.NewBuffer(b))
+	var v *api.Value
+	if err := decoder.Decode(&v); err != nil {
+		return nil, err
+	}
+	return v, nil
+}
+
+func encodeCacheVal(v *api.Value) ([]byte, error) {
+	buf := bytes.NewBuffer(nil)
+	encoder := gob.NewEncoder(buf)
+	if err := encoder.Encode(v); err != nil {
+		return nil, err
+	}
+	return buf.Bytes(), nil
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go b/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go
new file mode 100644
index 00000000..8ab8254c
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_provider.go
@@ -0,0 +1,41 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dbstore
+
+import (
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/extensions/config"
+)
+
+// DBProvider provides an handle to a transientdata db
+type DBProvider interface {
+	OpenDBStore(id string) (DBStore, error)
+	Close()
+}
+
+// LevelDBProvider provides an handle to a transientdata db
+type LevelDBProvider struct {
+	leveldbProvider *leveldbhelper.Provider
+}
+
+// NewDBProvider constructs new db provider
+func NewDBProvider() *LevelDBProvider {
+	dbPath := config.GetTransientDataLevelDBPath()
+	logger.Debugf("constructing DBProvider dbPath=%s", dbPath)
+	return &LevelDBProvider{leveldbhelper.NewProvider(&leveldbhelper.Conf{DBPath: dbPath})}
+
+}
+
+// OpenDBStore opens the db store
+func (p *LevelDBProvider) OpenDBStore(dbName string) (*DBStore, error) {
+	indexStore := p.leveldbProvider.GetDBHandle(dbName)
+	return newDBStore(indexStore, dbName), nil
+}
+
+// Close cleans up the Provider
+func (p *LevelDBProvider) Close() {
+	p.leveldbProvider.Close()
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_test.go b/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_test.go
new file mode 100644
index 00000000..def31ee8
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/dbstore/dbstore_test.go
@@ -0,0 +1,113 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dbstore
+
+import (
+	"os"
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	txID1 = "txid1"
+	txID2 = "txid2"
+	ns1   = "namespace1"
+	ns2   = "namespace2"
+	coll2 = "coll2"
+	coll1 = "coll1"
+	key1  = "key1"
+	key2  = "key2"
+)
+
+var (
+	k1 = api.Key{
+		Namespace:  ns1,
+		Collection: coll1,
+		Key:        key1,
+	}
+	v1 = &api.Value{TxID: txID1, Value: value1, ExpiryTime: time.Now().UTC()}
+
+	k2 = api.Key{
+		Namespace:  ns2,
+		Collection: coll2,
+		Key:        key2,
+	}
+	v2 = &api.Value{TxID: txID2, Value: value2, ExpiryTime: time.Now().UTC().Add(1 * time.Minute)}
+
+	value1 = []byte("v1")
+	value2 = []byte("v2")
+)
+
+func TestDeleteExpiredKeysFromDB(t *testing.T) {
+	removeDBPath(t)
+	defer removeDBPath(t)
+
+	p := NewDBProvider()
+	db, err := p.OpenDBStore("testchannel")
+	require.NoError(t, err)
+	defer p.Close()
+
+	err = db.AddKey(k1, v1)
+	require.Nil(t, err)
+
+	err = db.AddKey(k2, v2)
+	require.Nil(t, err)
+
+	// delete expired keys
+	require.NoError(t, db.DeleteExpiredKeys())
+
+	// Check if k1 is delete from db
+	v, err := db.GetKey(k1)
+	require.Nil(t, err)
+	require.Nil(t, v)
+
+	// Check if k2 is still exist in db
+	v, err = db.GetKey(k2)
+	require.Nil(t, err)
+	require.NotNil(t, v)
+
+}
+
+func TestAddRetrieveKeysFromDB(t *testing.T) {
+	defer removeDBPath(t)
+	p := NewDBProvider()
+	db, err := p.OpenDBStore("testchannel")
+	require.NoError(t, err)
+	defer p.Close()
+
+	err = db.AddKey(k1, v1)
+	require.Nil(t, err)
+
+	v, err := db.GetKey(k1)
+	require.Nil(t, err)
+	require.NotNil(t, v)
+	require.Equal(t, txID1, v.TxID)
+	require.Equal(t, value1, v.Value)
+
+}
+
+func TestMain(m *testing.M) {
+	removeDBPath(nil)
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/ledgertests/transientdatadb")
+	viper.Set("ledger.transientdata.cleanupExpired.Interval", "100ms")
+
+	os.Exit(m.Run())
+}
+
+func removeDBPath(t testing.TB) {
+	removePath(t, config.GetTransientDataLevelDBPath())
+}
+
+func removePath(t testing.TB, path string) {
+	if err := os.RemoveAll(path); err != nil {
+		t.Fatalf("Err: %s", err)
+	}
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/memstore.go b/extensions/collections/transientdata/storeprovider/store/memstore.go
new file mode 100644
index 00000000..6379b2bb
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/memstore.go
@@ -0,0 +1,189 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package store
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/api"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/cache"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store/dbstore"
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("memtransientdatastore")
+
+// Provider implements an in-memory transient data storage provider
+type Provider struct {
+	dbProvider *dbstore.LevelDBProvider
+}
+
+// NewProvider returns an in-memory transient data storage provider
+func NewProvider() *Provider {
+	logger.Debugf("constructing mem transient data storage provider")
+	return &Provider{dbProvider: dbstore.NewDBProvider()}
+}
+
+// OpenStore creates a handle to the transient data store for the given channel ID
+func (p *Provider) OpenStore(channelID string) (transientdata.Store, error) {
+	cacheSize := config.GetTransientDataCacheSize()
+	transientDB, err := p.dbProvider.OpenDBStore(channelID)
+	if err != nil {
+		return nil, err
+	}
+	return newStore(channelID, cacheSize, transientDB), nil
+}
+
+// Close closes the provider
+func (p *Provider) Close() {
+	p.dbProvider.Close()
+}
+
+type store struct {
+	channelID string
+	cache     *cache.Cache
+}
+
+func newStore(channelID string, cacheSize int, transientDB *dbstore.DBStore) *store {
+	logger.Debugf("[%s] Creating new store - cacheSize=%d", channelID, cacheSize)
+	return &store{
+		channelID: channelID,
+		cache:     cache.New(cacheSize, transientDB),
+	}
+}
+
+// Persist persists all transient data within the private data simulation results
+func (s *store) Persist(txID string, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	rwSet, err := rwsetutil.TxPvtRwSetFromProtoMsg(privateSimulationResultsWithConfig.PvtRwset)
+	if err != nil {
+		return errors.WithMessage(err, "error getting pvt RW set from bytes")
+	}
+
+	for _, nsRWSet := range rwSet.NsPvtRwSet {
+		for _, collRWSet := range nsRWSet.CollPvtRwSets {
+			if err := s.persistColl(txID, nsRWSet.NameSpace, privateSimulationResultsWithConfig.CollectionConfigs, collRWSet); err != nil {
+				return err
+			}
+		}
+	}
+
+	return nil
+}
+
+// GetTransientData returns the transient data for the given key
+func (s *store) GetTransientData(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	return s.getTransientData(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Key), nil
+}
+
+// GetTransientData returns the transient data for the given keys
+func (s *store) GetTransientDataMultipleKeys(key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	return s.getTransientDataMultipleKeys(key), nil
+}
+
+// Close closes the transient data store
+func (s *store) Close() {
+	if s.cache != nil {
+		logger.Debugf("[%s] Closing cache", s.channelID)
+		s.cache.Close()
+		s.cache = nil
+	}
+}
+
+func (s *store) persistColl(txID string, ns string, collConfigPkgs map[string]*common.CollectionConfigPackage, collRWSet *rwsetutil.CollPvtRwSet) error {
+	config, exists := getCollectionConfig(collConfigPkgs, ns, collRWSet.CollectionName)
+	if !exists {
+		logger.Debugf("[%s] Config for collection [%s:%s] not found in config packages", s.channelID, ns, collRWSet.CollectionName)
+		return nil
+	}
+
+	ttl, err := time.ParseDuration(config.TimeToLive)
+	if err != nil {
+		// This shouldn't happen since the config was validated before being persisted
+		return errors.Wrapf(err, "error parsing time-to-live for collection [%s]", collRWSet.CollectionName)
+	}
+
+	logger.Debugf("[%s] Collection [%s:%s] is a transient data collection", s.channelID, ns, collRWSet.CollectionName)
+
+	for _, wSet := range collRWSet.KvRwSet.Writes {
+		s.persistKVWrite(txID, ns, collRWSet.CollectionName, wSet, ttl)
+	}
+
+	return nil
+}
+
+func (s *store) persistKVWrite(txID, ns, coll string, w *kvrwset.KVWrite, ttl time.Duration) {
+	if w.IsDelete {
+		logger.Debugf("[%s] Skipping key [%s] in collection [%s] in private data rw-set since it was deleted", s.channelID, w.Key, coll)
+		return
+	}
+
+	key := api.Key{
+		Namespace:  ns,
+		Collection: coll,
+		Key:        w.Key,
+	}
+
+	if s.cache.Get(key) != nil {
+		logger.Warningf("[%s] Attempt to update transient data key [%s] in collection [%s]", s.channelID, w.Key, coll)
+		return
+	}
+
+	s.cache.PutWithExpire(key, w.Value, txID, ttl)
+}
+
+func (s *store) getTransientData(txID, ns, coll, key string) *storeapi.ExpiringValue {
+	value := s.cache.Get(api.Key{Namespace: ns, Collection: coll, Key: key})
+	if value == nil {
+		logger.Debugf("[%s] Key [%s] not found in transient store", s.channelID, key)
+		return nil
+	}
+
+	// Check if the data was stored in the current transaction. If so, ignore it or else an endorsement mismatch may result.
+	if value.TxID == txID {
+		logger.Debugf("[%s] Key [%s] skipped since it was stored in the current transaction", s.channelID, key)
+		return nil
+	}
+
+	logger.Debugf("[%s] Key [%s] found in transient store", s.channelID, key)
+
+	return &storeapi.ExpiringValue{Value: value.Value, Expiry: value.ExpiryTime}
+}
+
+func (s *store) getTransientDataMultipleKeys(mkey *storeapi.MultiKey) storeapi.ExpiringValues {
+	var values storeapi.ExpiringValues
+	for _, key := range mkey.Keys {
+		value := s.getTransientData(mkey.EndorsedAtTxID, mkey.Namespace, mkey.Collection, key)
+		if value != nil {
+			values = append(values, value)
+		}
+	}
+	return values
+}
+
+func getCollectionConfig(collConfigPkgs map[string]*common.CollectionConfigPackage, namespace, collName string) (*common.StaticCollectionConfig, bool) {
+	collConfigPkg, ok := collConfigPkgs[namespace]
+	if !ok {
+		return nil, false
+	}
+
+	for _, collConfig := range collConfigPkg.Config {
+		transientConfig := collConfig.GetStaticCollectionConfig()
+		if transientConfig != nil && transientConfig.Type == common.CollectionType_COL_TRANSIENT && transientConfig.Name == collName {
+			return transientConfig, true
+		}
+	}
+
+	return nil, false
+}
diff --git a/extensions/collections/transientdata/storeprovider/store/memstore_test.go b/extensions/collections/transientdata/storeprovider/store/memstore_test.go
new file mode 100644
index 00000000..18bf6dde
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/store/memstore_test.go
@@ -0,0 +1,218 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package store
+
+import (
+	"os"
+	"testing"
+	"time"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+
+	txID1 = "txid1"
+	txID2 = "txid2"
+	txID3 = "txid3"
+
+	ns1 = "ns1"
+
+	coll0 = "coll0"
+	coll1 = "coll1"
+	coll2 = "coll2"
+	coll3 = "coll3"
+
+	key1 = "key1"
+	key2 = "key2"
+	key3 = "key3"
+)
+
+func TestMemStore(t *testing.T) {
+	removeDBPath(t)
+	defer removeDBPath(t)
+	p := NewProvider()
+	require.NotNil(t, p)
+	defer p.Close()
+
+	s, err := p.OpenStore(channelID)
+	assert.NoError(t, err)
+	require.NotNil(t, s)
+
+	s.Close()
+
+	// Multiple calls on Close are allowed
+	assert.NotPanics(t, func() {
+		s.Close()
+	})
+
+	// Calls on closed store should panic
+	assert.Panics(t, func() {
+		s.GetTransientData(storeapi.NewKey("txid", "ns", "coll", "key"))
+	})
+}
+
+func TestMemStorePutAndGet(t *testing.T) {
+	defer removeDBPath(t)
+	p := NewProvider()
+	require.NotNil(t, p)
+	defer p.Close()
+
+	s, err := p.OpenStore(channelID)
+	assert.NoError(t, err)
+	require.NotNil(t, s)
+
+	value1_1 := []byte("value1_1")
+	value1_2 := []byte("value1_2")
+
+	value2_1 := []byte("value2_1")
+	value3_1 := []byte("value3_1")
+
+	b := mocks.NewPvtReadWriteSetBuilder()
+	ns1Builder := b.Namespace(ns1)
+	coll1Builder := ns1Builder.Collection(coll1)
+	coll1Builder.
+		TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, value1_1).
+		Write(key2, value1_2)
+	coll2Builder := ns1Builder.Collection(coll2)
+	coll2Builder.
+		TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, value2_1).
+		Delete(key2)
+	coll3Builder := ns1Builder.Collection(coll3)
+	coll3Builder.
+		StaticConfig("OR('Org1MSP.member')", 1, 2, 100).
+		Write(key1, value3_1)
+
+	err = s.Persist(txID1, b.Build())
+	assert.NoError(t, err)
+
+	t.Run("GetTransientData invalid collection -> nil", func(t *testing.T) {
+		value, err := s.GetTransientData(storeapi.NewKey(txID1, ns1, coll0, key1))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetTransientData in same transaction -> nil", func(t *testing.T) {
+		value, err := s.GetTransientData(storeapi.NewKey(txID1, ns1, coll1, key1))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetTransientData in new transaction -> valid", func(t *testing.T) {
+		value, err := s.GetTransientData(storeapi.NewKey(txID2, ns1, coll1, key1))
+		assert.NoError(t, err)
+		assert.Equal(t, value1_1, value.Value)
+
+		value, err = s.GetTransientData(storeapi.NewKey(txID2, ns1, coll1, key2))
+		assert.NoError(t, err)
+		assert.Equal(t, value1_2, value.Value)
+	})
+
+	t.Run("GetTransientData collection2 -> valid", func(t *testing.T) {
+		// Collection2
+		value, err := s.GetTransientData(storeapi.NewKey(txID2, ns1, coll2, key1))
+		assert.NoError(t, err)
+		assert.Equal(t, value2_1, value.Value)
+	})
+
+	t.Run("GetTransientData on non-transient collection -> nil", func(t *testing.T) {
+		value, err := s.GetTransientData(storeapi.NewKey(txID2, ns1, coll3, key1))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetTransientDataMultipleKeys -> valid", func(t *testing.T) {
+		values, err := s.GetTransientDataMultipleKeys(storeapi.NewMultiKey(txID2, ns1, coll1, key1, key2))
+		assert.NoError(t, err)
+		require.Equal(t, 2, len(values))
+		assert.Equal(t, value1_1, values[0].Value)
+		assert.Equal(t, value1_2, values[1].Value)
+	})
+
+	t.Run("Disallow update transient data", func(t *testing.T) {
+		b := mocks.NewPvtReadWriteSetBuilder()
+		ns1Builder := b.Namespace(ns1)
+		coll1Builder := ns1Builder.Collection(coll1)
+		coll1Builder.
+			TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+			Write(key1, value2_1)
+
+		err = s.Persist(txID2, b.Build())
+		assert.NoError(t, err)
+
+		value, err := s.GetTransientData(storeapi.NewKey(txID3, ns1, coll1, key1))
+		assert.NoError(t, err)
+		assert.Equalf(t, value1_1, value.Value, "expecting transient data to not have been updated")
+	})
+
+	t.Run("Expire transient data", func(t *testing.T) {
+		b := mocks.NewPvtReadWriteSetBuilder()
+		ns1Builder := b.Namespace(ns1)
+		coll1Builder := ns1Builder.Collection(coll1)
+		coll1Builder.
+			TransientConfig("OR('Org1MSP.member')", 1, 2, "10ms").
+			Write(key3, value1_1)
+
+		err = s.Persist(txID2, b.Build())
+		assert.NoError(t, err)
+
+		value, err := s.GetTransientData(storeapi.NewKey(txID3, ns1, coll1, key3))
+		assert.NoError(t, err)
+		assert.Equal(t, value1_1, value.Value)
+
+		time.Sleep(15 * time.Millisecond)
+
+		value, err = s.GetTransientData(storeapi.NewKey(txID3, ns1, coll1, key3))
+		assert.NoError(t, err)
+		assert.Nilf(t, value, "expecting key to have expired")
+	})
+}
+
+func TestMemStoreInvalidRWSet(t *testing.T) {
+	defer removeDBPath(t)
+	p := NewProvider()
+	require.NotNil(t, p)
+	defer p.Close()
+
+	s, err := p.OpenStore(channelID)
+	assert.NoError(t, err)
+	require.NotNil(t, s)
+
+	b := mocks.NewPvtReadWriteSetBuilder()
+	b.Namespace(ns1).
+		Collection(coll1).
+		TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, []byte("value")).
+		WithMarshalError()
+
+	err = s.Persist(txID1, b.Build())
+	assert.Errorf(t, err, "expecting marshal error")
+}
+
+func TestMain(m *testing.M) {
+	removeDBPath(nil)
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/ledgertests/transientdatadb")
+	os.Exit(m.Run())
+}
+
+func removeDBPath(t testing.TB) {
+	removePath(t, config.GetTransientDataLevelDBPath())
+}
+
+func removePath(t testing.TB, path string) {
+	if err := os.RemoveAll(path); err != nil {
+		t.Fatalf("Err: %s", err)
+	}
+}
diff --git a/extensions/collections/transientdata/storeprovider/transientdatastore.go b/extensions/collections/transientdata/storeprovider/transientdatastore.go
new file mode 100644
index 00000000..cc6e926d
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/transientdatastore.go
@@ -0,0 +1,56 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"sync"
+
+	"github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/storeprovider/store"
+)
+
+// NewProviderFactory returns a new transient data store provider factory
+func NewProviderFactory() *StoreProvider {
+	return &StoreProvider{
+		stores: make(map[string]transientdata.Store),
+	}
+}
+
+// StoreProvider is a transient data store provider
+type StoreProvider struct {
+	stores map[string]transientdata.Store
+	transientdata.StoreProvider
+	sync.RWMutex
+}
+
+// StoreForChannel returns the transient data store for the given channel
+func (sp *StoreProvider) StoreForChannel(channelID string) transientdata.Store {
+	sp.RLock()
+	defer sp.RUnlock()
+	return sp.stores[channelID]
+}
+
+// OpenStore opens the transient data store for the given channel
+func (sp *StoreProvider) OpenStore(channelID string) (transientdata.Store, error) {
+	sp.Lock()
+	defer sp.Unlock()
+	if sp.StoreProvider == nil {
+		sp.StoreProvider = store.NewProvider()
+	}
+	store, err := sp.StoreProvider.OpenStore(channelID)
+	if err == nil {
+		sp.stores[channelID] = store
+	}
+	return store, err
+}
+
+// Close shuts down all of the stores
+func (sp *StoreProvider) Close() {
+	for _, s := range sp.stores {
+		s.Close()
+	}
+}
diff --git a/extensions/collections/transientdata/storeprovider/transientdatastore_test.go b/extensions/collections/transientdata/storeprovider/transientdatastore_test.go
new file mode 100644
index 00000000..7e649d9b
--- /dev/null
+++ b/extensions/collections/transientdata/storeprovider/transientdatastore_test.go
@@ -0,0 +1,62 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package storeprovider
+
+import (
+	"os"
+	"reflect"
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestTransientDataStore(t *testing.T) {
+	defer removeDBPath(t)
+	channel1 := "channel1"
+	channel2 := "channel2"
+
+	f := NewProviderFactory()
+	require.NotNil(t, f)
+	removeDBPath(t)
+
+	s1, err := f.OpenStore(channel1)
+	require.NotNil(t, s1)
+	assert.NoError(t, err)
+	assert.Equal(t, "*store.store", reflect.TypeOf(s1).String())
+	assert.Equal(t, s1, f.StoreForChannel(channel1))
+	removeDBPath(t)
+
+	s2, err := f.OpenStore(channel2)
+	require.NotNil(t, s2)
+	assert.NoError(t, err)
+	assert.NotEqual(t, s1, s2)
+	assert.Equal(t, s2, f.StoreForChannel(channel2))
+
+	assert.NotPanics(t, func() {
+		f.Close()
+	})
+}
+
+func TestMain(m *testing.M) {
+	removeDBPath(nil)
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/ledgertests/transientdatadb")
+	os.Exit(m.Run())
+}
+
+func removeDBPath(t testing.TB) {
+	removePath(t, config.GetTransientDataLevelDBPath())
+}
+
+func removePath(t testing.TB, path string) {
+	if err := os.RemoveAll(path); err != nil {
+		t.Fatalf("Err: %s", err)
+		t.FailNow()
+	}
+}
diff --git a/extensions/collections/transientdata/transientdataprovider.go b/extensions/collections/transientdata/transientdataprovider.go
new file mode 100644
index 00000000..850b496b
--- /dev/null
+++ b/extensions/collections/transientdata/transientdataprovider.go
@@ -0,0 +1,371 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientdata
+
+import (
+	"context"
+	"sync"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	"github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	"github.com/hyperledger/fabric/extensions/collections/transientdata/dissemination"
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/hyperledger/fabric/extensions/common/discovery"
+	"github.com/hyperledger/fabric/extensions/common/multirequest"
+	"github.com/hyperledger/fabric/extensions/common/requestmgr"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("transientdata")
+
+type support interface {
+	Policy(channel, ns, collection string) (privdata.CollectionAccessPolicy, error)
+	BlockPublisher(channelID string) gossipapi.BlockPublisher
+}
+
+// Provider is a transient data provider.
+type Provider struct {
+	support
+	storeForChannel func(channelID string) transientdata.Store
+	gossipAdapter   func() supportapi.GossipAdapter
+}
+
+// NewProvider returns a new transient data provider
+func NewProvider(storeProvider func(channelID string) transientdata.Store, support support, gossipProvider func() supportapi.GossipAdapter) transientdata.Provider {
+	return &Provider{
+		support:         support,
+		storeForChannel: storeProvider,
+		gossipAdapter:   gossipProvider,
+	}
+}
+
+// RetrieverForChannel returns the transient data dataRetriever for the given channel
+func (p *Provider) RetrieverForChannel(channelID string) transientdata.Retriever {
+	r := &retriever{
+		support:       p.support,
+		gossipAdapter: p.gossipAdapter(),
+		store:         p.storeForChannel(channelID),
+		channelID:     channelID,
+		reqMgr:        requestmgr.Get(channelID),
+		resolvers:     make(map[collKey]resolver),
+	}
+
+	// Add a handler so that we can remove the resolver for a chaincode that has been upgraded
+	p.support.BlockPublisher(channelID).AddCCUpgradeHandler(func(blockNum uint64, txID string, chaincodeID string) error {
+		logger.Infof("[%s] Chaincode [%s] has been upgraded. Clearing resolver cache for chaincode.", channelID, chaincodeID)
+		r.removeResolvers(chaincodeID)
+		return nil
+	})
+
+	return r
+}
+
+// ResolveEndorsers resolves to a set of endorsers to which transient data should be disseminated
+type resolver interface {
+	ResolveEndorsers(key string) (discovery.PeerGroup, error)
+}
+
+type collKey struct {
+	ns   string
+	coll string
+}
+
+func newCollKey(ns, coll string) collKey {
+	return collKey{ns: ns, coll: coll}
+}
+
+type retriever struct {
+	support
+	gossipAdapter supportapi.GossipAdapter
+	channelID     string
+	store         transientdata.Store
+	resolvers     map[collKey]resolver
+	lock          sync.RWMutex
+	reqMgr        requestmgr.RequestMgr
+}
+
+func (r *retriever) GetTransientData(ctxt context.Context, key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	authorized, err := r.isAuthorized(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, err
+	}
+	if !authorized {
+		logger.Infof("[%s] This peer does not have access to the collection [%s:%s]", r.channelID, key.Namespace, key.Collection)
+		return nil, nil
+	}
+
+	// Find the endorser which has the transient data
+	res, err := r.getResolver(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, errors.Wrapf(err, "unable to get resolver for channel [%s] and [%s:%s]", r.channelID, key.Namespace, key.Collection)
+	}
+
+	endorsers, err := res.ResolveEndorsers(key.Key)
+	if err != nil {
+		return nil, errors.Wrapf(err, "unable to get resolve endorsers for channel [%s] and [%s]", r.channelID, key)
+	}
+
+	logger.Debugf("[%s] Endorsers for [%s]: %s", r.channelID, key, endorsers)
+
+	if endorsers.ContainsLocal() {
+		value, ok, err := r.getTransientDataFromLocal(key)
+		if err != nil {
+			return nil, err
+		}
+		if ok {
+			return value, nil
+		}
+	}
+
+	// Retrieve from the remote peers
+	cReq := multirequest.New()
+	for _, endorser := range endorsers.Remote() {
+		logger.Debugf("Adding request to get transient data for [%s] from [%s] ...", key, endorser)
+		cReq.Add(endorser.String(), r.getTransientDataFromEndorser(key, endorser))
+	}
+
+	response := cReq.Execute(ctxt)
+
+	if response.Values.IsEmpty() {
+		logger.Debugf("Got empty transient data response for [%s] ...", key)
+		return nil, nil
+	}
+
+	logger.Debugf("Got non-nil transient data response for [%s] ...", key)
+	return response.Values[0].(*storeapi.ExpiringValue), nil
+}
+
+type valueResp struct {
+	value *storeapi.ExpiringValue
+	err   error
+}
+
+// GetTransientDataMultipleKeys gets the values for the multiple transient data items in a single call
+func (r *retriever) GetTransientDataMultipleKeys(ctxt context.Context, key *storeapi.MultiKey) (storeapi.ExpiringValues, error) {
+	if len(key.Keys) == 0 {
+		return nil, errors.New("at least one key must be specified")
+	}
+
+	if len(key.Keys) == 1 {
+		value, err := r.GetTransientData(ctxt, storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, key.Keys[0]))
+		if err != nil {
+			return nil, err
+		}
+		return storeapi.ExpiringValues{value}, nil
+	}
+
+	var wg sync.WaitGroup
+	wg.Add(len(key.Keys))
+	var mutex sync.Mutex
+
+	// TODO: This can be optimized by sending one request to endorsers that have multiple keys, as opposed to one request per key.
+	responses := make(map[string]*valueResp)
+	for _, k := range key.Keys {
+		go func(key *storeapi.Key) {
+			cctxt, cancel := context.WithCancel(ctxt)
+			defer cancel()
+
+			value, err := r.GetTransientData(cctxt, key)
+			mutex.Lock()
+			responses[key.Key] = &valueResp{value: value, err: err}
+			logger.Debugf("Got response for [%s]: %s, Err: %s", key.Key, value, err)
+			mutex.Unlock()
+			wg.Done()
+		}(storeapi.NewKey(key.EndorsedAtTxID, key.Namespace, key.Collection, k))
+	}
+
+	wg.Wait()
+
+	// Return the responses in the order of the requested keys
+	values := make(storeapi.ExpiringValues, len(key.Keys))
+	for i, k := range key.Keys {
+		r, ok := responses[k]
+		if !ok {
+			return nil, errors.Errorf("no response for key [%s:%s:%s]", key.Namespace, key.Collection, k)
+		}
+		if r.err != nil {
+			return nil, r.err
+		}
+		values[i] = r.value
+	}
+
+	return values, nil
+}
+
+func (r *retriever) getTransientDataFromLocal(key *storeapi.Key) (*storeapi.ExpiringValue, bool, error) {
+	value, retrieveErr := r.store.GetTransientData(key)
+	if retrieveErr != nil {
+		logger.Debugf("[%s] Error getting transient data from local store for [%s]: %s", r.channelID, key, retrieveErr)
+		return nil, false, errors.Wrapf(retrieveErr, "unable to get transient data for channel [%s] and [%s]", r.channelID, key)
+	}
+
+	if value != nil && len(value.Value) > 0 {
+		logger.Debugf("[%s] Got transient data from local store for [%s]", r.channelID, key)
+		return value, true, nil
+	}
+
+	logger.Debugf("[%s] nil transient data in local store for [%s]. Will try to pull from remote peer(s).", r.channelID, key)
+	return nil, false, nil
+}
+
+func (r *retriever) getTransientDataFromEndorser(key *storeapi.Key, endorser *discovery.Member) multirequest.Request {
+	return func(ctxt context.Context) (common.Values, error) {
+		logger.Debugf("Getting transient data for [%s] from [%s] ...", key, endorser)
+
+		value, err := r.getTransientData(ctxt, key, endorser)
+		if err != nil {
+			if err == context.Canceled {
+				logger.Debugf("[%s] Request to get transient data from [%s] for [%s] was cancelled", r.channelID, endorser, key)
+			} else {
+				logger.Debugf("[%s] Error getting transient data from [%s] for [%s]: %s", r.channelID, endorser, key, err)
+			}
+			return common.Values{nil}, err
+		}
+
+		if value == nil {
+			logger.Debugf("[%s] Transient data not found on [%s] for [%s]", r.channelID, endorser, key)
+		} else {
+			logger.Debugf("[%s] Got transient data from [%s] for [%s]", r.channelID, endorser, key)
+		}
+
+		return common.Values{value}, nil
+	}
+}
+
+func (r *retriever) getResolver(ns, coll string) (resolver, error) {
+	key := newCollKey(ns, coll)
+
+	r.lock.RLock()
+	resolver, ok := r.resolvers[key]
+	r.lock.RUnlock()
+
+	if ok {
+		return resolver, nil
+	}
+
+	policy, err := r.Policy(r.channelID, ns, coll)
+	if err != nil {
+		return nil, err
+	}
+
+	resolver = dissemination.New(r.channelID, ns, coll, policy, r.gossipAdapter)
+
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	r.resolvers[key] = resolver
+
+	return resolver, nil
+}
+
+func (r *retriever) removeResolvers(ns string) {
+	r.lock.Lock()
+	defer r.lock.Unlock()
+
+	for key := range r.resolvers {
+		if key.ns == ns {
+			logger.Debugf("[%s] Removing resolver [%s:%s] from cache", r.channelID, key.ns, key.coll)
+			delete(r.resolvers, key)
+		}
+	}
+}
+
+func (r *retriever) getTransientData(ctxt context.Context, key *storeapi.Key, endorsers ...*discovery.Member) (*storeapi.ExpiringValue, error) {
+	logger.Debugf("[%s] Sending Gossip request to %s for transient data for [%s]", r.channelID, endorsers, key)
+
+	req := r.reqMgr.NewRequest()
+
+	logger.Debugf("[%s] Creating Gossip request %d for transient data for [%s]", r.channelID, req.ID(), key)
+	msg := r.createCollDataRequestMsg(req, key)
+
+	logger.Debugf("[%s] Sending Gossip request %d for transient data for [%s]", r.channelID, req.ID(), key)
+	r.gossipAdapter.Send(msg, asRemotePeers(endorsers)...)
+
+	logger.Debugf("[%s] Waiting for response for %d for transient data for [%s]", r.channelID, req.ID(), key)
+	res, err := req.GetResponse(ctxt)
+	if err != nil {
+		return nil, err
+	}
+
+	logger.Debugf("[%s] Got response for %d for transient data for [%s]", r.channelID, req.ID(), key)
+
+	for _, e := range res.Data {
+		if e.Namespace == key.Namespace && e.Collection == key.Collection && e.Key == key.Key {
+			logger.Debugf("[%s] Got response for transient data for [%s]", r.channelID, key)
+			if e.Value == nil {
+				return nil, nil
+			}
+			return &storeapi.ExpiringValue{Value: e.Value, Expiry: e.Expiry}, nil
+		}
+	}
+
+	return nil, errors.Errorf("expecting a response to a transient data request for [%s] but got a response for another key", key)
+}
+
+func (r *retriever) createCollDataRequestMsg(req requestmgr.Request, key *storeapi.Key) *gproto.GossipMessage {
+	return &gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(r.channelID),
+		Content: &gproto.GossipMessage_CollDataReq{
+			CollDataReq: &gproto.RemoteCollDataRequest{
+				Nonce: req.ID(),
+				Digests: []*gproto.CollDataDigest{
+					{
+						Namespace:      key.Namespace,
+						Collection:     key.Collection,
+						Key:            key.Key,
+						EndorsedAtTxID: key.EndorsedAtTxID,
+					},
+				},
+			},
+		},
+	}
+}
+
+// isAuthorized returns true if the local peer has access to the given collection
+func (r *retriever) isAuthorized(ns, coll string) (bool, error) {
+	policy, err := r.Policy(r.channelID, ns, coll)
+	if err != nil {
+		return false, errors.Wrapf(err, "unable to get policy for [%s:%s]", ns, coll)
+	}
+
+	localMSPID, err := getLocalMSPID()
+	if err != nil {
+		return false, errors.Wrap(err, "unable to get local MSP ID")
+	}
+
+	for _, mspID := range policy.MemberOrgs() {
+		if mspID == localMSPID {
+			return true, nil
+		}
+	}
+
+	return false, nil
+}
+
+func asRemotePeers(members []*discovery.Member) []*comm.RemotePeer {
+	var peers []*comm.RemotePeer
+	for _, m := range members {
+		peers = append(peers, &comm.RemotePeer{
+			Endpoint: m.Endpoint,
+			PKIID:    m.PKIid,
+		})
+	}
+	return peers
+}
+
+// getLocalMSPID returns the MSP ID of the local peer. This variable may be overridden by unit tests.
+var getLocalMSPID = func() (string, error) {
+	return mspmgmt.GetLocalMSP().GetIdentifier()
+}
diff --git a/extensions/collections/transientdata/transientdataprovider_test.go b/extensions/collections/transientdata/transientdataprovider_test.go
new file mode 100644
index 00000000..0b628786
--- /dev/null
+++ b/extensions/collections/transientdata/transientdataprovider_test.go
@@ -0,0 +1,311 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientdata
+
+import (
+	"context"
+	"testing"
+	"time"
+
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	tdataapi "github.com/hyperledger/fabric/extensions/collections/api/transientdata"
+	spmocks "github.com/hyperledger/fabric/extensions/collections/storeprovider/mocks"
+	"github.com/hyperledger/fabric/extensions/common/requestmgr"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	ledgerconfig "github.com/hyperledger/fabric/extensions/roles"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+	ns1       = "chaincode1"
+	coll1     = "collection1"
+	key1      = "key1"
+	key2      = "key2"
+	key3      = "key3"
+	txID      = "tx1"
+)
+
+var (
+	org1MSPID      = "Org1MSP"
+	p1Org1Endpoint = "p1.org1.com"
+	p1Org1PKIID    = gcommon.PKIidType("pkiid_P1O1")
+	p2Org1Endpoint = "p2.org1.com"
+	p2Org1PKIID    = gcommon.PKIidType("pkiid_P2O1")
+	p3Org1Endpoint = "p3.org1.com"
+	p3Org1PKIID    = gcommon.PKIidType("pkiid_P3O1")
+
+	org2MSPID      = "Org2MSP"
+	p1Org2Endpoint = "p1.org2.com"
+	p1Org2PKIID    = gcommon.PKIidType("pkiid_P1O2")
+	p2Org2Endpoint = "p2.org2.com"
+	p2Org2PKIID    = gcommon.PKIidType("pkiid_P2O2")
+	p3Org2Endpoint = "p3.org2.com"
+	p3Org2PKIID    = gcommon.PKIidType("pkiid_P3O2")
+
+	org3MSPID      = "Org3MSP"
+	p1Org3Endpoint = "p1.org3.com"
+	p1Org3PKIID    = gcommon.PKIidType("pkiid_P1O3")
+	p2Org3Endpoint = "p2.org3.com"
+	p2Org3PKIID    = gcommon.PKIidType("pkiid_P2O3")
+	p3Org3Endpoint = "p3.org3.com"
+	p3Org3PKIID    = gcommon.PKIidType("pkiid_P3O3")
+
+	validatorRole = string(ledgerconfig.ValidatorRole)
+	endorserRole  = string(ledgerconfig.EndorserRole)
+
+	respTimeout = 100 * time.Millisecond
+)
+
+func TestTransientDataProvider(t *testing.T) {
+	value1 := &storeapi.ExpiringValue{Value: []byte("value1")}
+	value2 := &storeapi.ExpiringValue{Value: []byte("value2")}
+	value3 := &storeapi.ExpiringValue{Value: []byte("value3")}
+
+	support := mocks.NewMockSupport().
+		CollectionPolicy(&mocks.MockAccessPolicy{
+			MaxPeerCount: 2,
+			Orgs:         []string{org1MSPID, org2MSPID, org3MSPID},
+		})
+
+	getLocalMSPID = func() (string, error) { return org1MSPID, nil }
+
+	gossip := mocks.NewMockGossipAdapter()
+	gossip.Self(org1MSPID, mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, validatorRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, validatorRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, validatorRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, validatorRole)).
+		Member(org3MSPID, mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole)).IdentityInfo()
+
+	localStore := spmocks.NewTransientDataStore().
+		Data(storeapi.NewKey(txID, ns1, coll1, key1), value1)
+
+	p := &Provider{
+		support:         support,
+		storeForChannel: func(channelID string) tdataapi.Store { return localStore },
+		gossipAdapter:   func() supportapi.GossipAdapter { return gossip },
+	}
+
+	retriever := p.RetrieverForChannel(channelID)
+	require.NotNil(t, retriever)
+
+	t.Run("GetTransientData - From local peer", func(t *testing.T) {
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetTransientData(ctx, storeapi.NewKey(txID, ns1, coll1, key1))
+		require.NoError(t, err)
+		require.NotNil(t, value)
+		require.Equal(t, value1, value)
+	})
+
+	t.Run("GetTransientData - From remote peer", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key2, value2).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetTransientData(ctx, storeapi.NewKey(txID, ns1, coll1, key2))
+		require.NoError(t, err)
+		require.NotNil(t, value)
+		require.Equal(t, value2, value)
+	})
+
+	t.Run("GetTransientData - No response from remote peer", func(t *testing.T) {
+		gossip.MessageHandler(func(msg *gproto.GossipMessage) {})
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetTransientData(ctx, storeapi.NewKey(txID, ns1, coll1, key2))
+		require.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetTransientData - Cancel request from remote peer", func(t *testing.T) {
+		gossip.MessageHandler(func(msg *gproto.GossipMessage) {})
+
+		ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
+		go func() {
+			time.Sleep(50 * time.Millisecond)
+			cancel()
+		}()
+
+		value, err := retriever.GetTransientData(ctx, storeapi.NewKey(txID, ns1, coll1, key2))
+		require.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetTransientDataMultipleKeys - 1 key -> success", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key1, value2).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		values, err := retriever.GetTransientDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, key1))
+		require.NoError(t, err)
+		require.Equal(t, 1, len(values))
+		assert.Equal(t, value1, values[0])
+	})
+
+	t.Run("GetTransientDataMultipleKeys - 3 keys -> success", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key1, value1).
+				Value(key2, value2).
+				Value(key3, value3).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		values, err := retriever.GetTransientDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, key1, key2, key3))
+		require.NoError(t, err)
+		require.Equal(t, 3, len(values))
+		assert.Equal(t, value1, values[0])
+		assert.Equal(t, value2, values[1])
+		assert.Equal(t, value3, values[2])
+	})
+
+	t.Run("GetTransientDataMultipleKeys - Key not found -> success", func(t *testing.T) {
+		gossip.MessageHandler(
+			newMockGossipMsgHandler(channelID).
+				Value(key1, value1).
+				Value(key2, value2).
+				Value(key3, value3).
+				Handle)
+
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		values, err := retriever.GetTransientDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, "xxx", key2, key3))
+		require.NoError(t, err)
+		require.Equal(t, 3, len(values))
+		assert.Nil(t, values[0])
+		assert.Equal(t, value2, values[1])
+		assert.Equal(t, value3, values[2])
+	})
+
+	t.Run("GetTransientDataMultipleKeys - Timeout -> fail", func(t *testing.T) {
+		handler := newMockGossipMsgHandler(channelID).
+			Value(key1, value1).
+			Value(key2, value2).
+			Value(key3, value3)
+		gossip.MessageHandler(
+			func(msg *gproto.GossipMessage) {
+				time.Sleep(10 * time.Millisecond)
+				handler.Handle(msg)
+			},
+		)
+
+		ctx, _ := context.WithTimeout(context.Background(), time.Microsecond)
+		values, err := retriever.GetTransientDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, key2, key3))
+		require.NoError(t, err)
+		assert.True(t, values.Values().IsEmpty())
+	})
+}
+
+func TestTransientDataProvider_AccessDenied(t *testing.T) {
+	value1 := &storeapi.ExpiringValue{Value: []byte("value1")}
+	value2 := &storeapi.ExpiringValue{Value: []byte("value2")}
+
+	getLocalMSPID = func() (string, error) { return org3MSPID, nil }
+
+	gossip := mocks.NewMockGossipAdapter()
+	gossip.Self(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, validatorRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, validatorRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, validatorRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole))
+
+	gossip.MessageHandler(
+		newMockGossipMsgHandler(channelID).
+			Value(key2, value2).
+			Handle)
+
+	localStore := spmocks.NewTransientDataStore().
+		Data(storeapi.NewKey(txID, ns1, coll1, key1), value1)
+
+	support := mocks.NewMockSupport().
+		CollectionPolicy(&mocks.MockAccessPolicy{
+			MaxPeerCount: 2,
+			Orgs:         []string{org1MSPID, org2MSPID},
+		})
+
+	p := &Provider{
+		support:         support,
+		storeForChannel: func(channelID string) tdataapi.Store { return localStore },
+		gossipAdapter:   func() supportapi.GossipAdapter { return gossip },
+	}
+
+	retriever := p.RetrieverForChannel(channelID)
+	require.NotNil(t, retriever)
+
+	t.Run("GetTransientData - From remote peer -> nil (not authorized)", func(t *testing.T) {
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		value, err := retriever.GetTransientData(ctx, storeapi.NewKey(txID, ns1, coll1, key2))
+		assert.NoError(t, err)
+		assert.Nil(t, value)
+	})
+
+	t.Run("GetTransientDataMultipleKeys - From remote peer -> nil (not authorized)", func(t *testing.T) {
+		ctx, _ := context.WithTimeout(context.Background(), respTimeout)
+		values, err := retriever.GetTransientDataMultipleKeys(ctx, storeapi.NewMultiKey(txID, ns1, coll1, key2))
+		assert.NoError(t, err)
+		require.Equal(t, 1, len(values))
+		assert.Nil(t, values[0])
+	})
+}
+
+type mockGossipMsgHandler struct {
+	channelID string
+	values    map[string]*storeapi.ExpiringValue
+}
+
+func newMockGossipMsgHandler(channelID string) *mockGossipMsgHandler {
+	return &mockGossipMsgHandler{
+		channelID: channelID,
+		values:    make(map[string]*storeapi.ExpiringValue),
+	}
+}
+
+func (m *mockGossipMsgHandler) Value(key string, value *storeapi.ExpiringValue) *mockGossipMsgHandler {
+	m.values[key] = value
+	return m
+}
+
+func (m *mockGossipMsgHandler) Handle(msg *gproto.GossipMessage) {
+	req := msg.GetCollDataReq()
+
+	res := &requestmgr.Response{
+		Endpoint:  "p1.org1",
+		MSPID:     "org1",
+		Identity:  []byte("p1.org1"),
+		Signature: []byte("sig"),
+	}
+
+	for _, d := range req.Digests {
+		e := &requestmgr.Element{
+			Namespace:  d.Namespace,
+			Collection: d.Collection,
+			Key:        d.Key,
+		}
+
+		v := m.values[d.Key]
+		if v != nil {
+			e.Value = v.Value
+			e.Expiry = v.Expiry
+		}
+
+		res.Data = append(res.Data, e)
+	}
+
+	requestmgr.Get(m.channelID).Respond(req.Nonce, res)
+}
diff --git a/extensions/common/cas.go b/extensions/common/cas.go
new file mode 100644
index 00000000..80990c97
--- /dev/null
+++ b/extensions/common/cas.go
@@ -0,0 +1,41 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"crypto"
+	"encoding/base64"
+
+	"github.com/btcsuite/btcutil/base58"
+)
+
+// GetCASKey returns the content-addressable key for the given content.
+func GetCASKey(content []byte) string {
+	address := calculateAddress(content)
+
+	// Address above is as per CAS spec(sha256 hash + base64 URL encoding),
+	// however since fabric/couchdb doesn't support keys that start with _
+	// we have to do additional transformation
+	return base58.Encode(address)
+}
+
+func calculateAddress(content []byte) []byte {
+	hash := getHash(content)
+	buf := make([]byte, base64.URLEncoding.EncodedLen(len(hash)))
+	base64.URLEncoding.Encode(buf, hash)
+
+	return buf
+}
+
+// getHash will compute the hash for the supplied bytes using SHA256
+func getHash(bytes []byte) []byte {
+	h := crypto.SHA256.New()
+	// added no lint directive because there's no error from source code
+	// error cannot be produced, checked google source
+	h.Write(bytes) //nolint
+	return h.Sum(nil)
+}
diff --git a/extensions/common/cas_test.go b/extensions/common/cas_test.go
new file mode 100644
index 00000000..76afbf36
--- /dev/null
+++ b/extensions/common/cas_test.go
@@ -0,0 +1,18 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+)
+
+func TestGetCASKey(t *testing.T) {
+	k := GetCASKey([]byte("value1"))
+	assert.NotEmpty(t, k)
+}
diff --git a/extensions/common/common.go b/extensions/common/common.go
new file mode 100644
index 00000000..1e338ab3
--- /dev/null
+++ b/extensions/common/common.go
@@ -0,0 +1,88 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"reflect"
+	"time"
+
+	"github.com/golang/protobuf/ptypes/timestamp"
+)
+
+// Values contains a slice of values
+type Values []interface{}
+
+// IsEmpty returns true if all of the values are nil
+func (v Values) IsEmpty() bool {
+	for _, value := range v {
+		if !IsNil(value) {
+			return false
+		}
+	}
+	return true
+}
+
+// AllSet returns true if all of the values are not nil
+func (v Values) AllSet() bool {
+	for _, value := range v {
+		if IsNil(value) {
+			return false
+		}
+	}
+	return true
+}
+
+// Merge merges this set of values with the given set
+// and returns the new set
+func (v Values) Merge(other Values) Values {
+	var max int
+	if len(other) < len(v) {
+		max = len(v)
+	} else {
+		max = len(other)
+	}
+
+	retVal := make(Values, max)
+	copy(retVal, v)
+
+	for i, o := range other {
+		if IsNil(retVal[i]) {
+			retVal[i] = o
+		}
+	}
+
+	return retVal
+}
+
+// IsNil returns true if the given value is nil
+func IsNil(p interface{}) bool {
+	if p == nil {
+		return true
+	}
+
+	v := reflect.ValueOf(p)
+
+	switch {
+	case v.Kind() == reflect.Ptr:
+		return v.IsNil()
+	case v.Kind() == reflect.Array || v.Kind() == reflect.Slice:
+		return v.Len() == 0
+	default:
+		return false
+	}
+}
+
+// ToTimestamp converts the Time into Timestamp
+func ToTimestamp(t time.Time) *timestamp.Timestamp {
+	now := time.Now().UTC()
+	return &(timestamp.Timestamp{Seconds: now.Unix()})
+}
+
+// FromTimestamp converts the Timestamp into Time
+func FromTimestamp(ts *timestamp.Timestamp) time.Time {
+	return time.Unix(ts.Seconds, 0)
+}
diff --git a/extensions/common/common_test.go b/extensions/common/common_test.go
new file mode 100644
index 00000000..3c42ca77
--- /dev/null
+++ b/extensions/common/common_test.go
@@ -0,0 +1,110 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"testing"
+	"time"
+
+	"github.com/stretchr/testify/assert"
+)
+
+type iface interface {
+}
+
+type test struct {
+}
+
+func Test_IsNil(t *testing.T) {
+	assert.True(t, IsNil(nil))
+
+	var b []byte
+	assert.True(t, IsNil(b))
+
+	var v iface
+	assert.True(t, IsNil(v))
+	v = &test{}
+	assert.False(t, IsNil(v))
+
+	v = nil
+	v2 := v
+	assert.True(t, IsNil(v2))
+
+	assert.False(t, IsNil(test{}))
+}
+
+func TestValues_IsEmpty(t *testing.T) {
+	v1 := []byte("v1")
+	v2 := []byte("v1")
+	v3 := []byte("v1")
+
+	values := Values{v1, v2, v3}
+	assert.False(t, values.IsEmpty())
+
+	values = Values{v1, nil, v3}
+	assert.False(t, values.IsEmpty())
+
+	values = Values{nil, nil, nil}
+	assert.True(t, values.IsEmpty())
+}
+
+func TestValues_AllSet(t *testing.T) {
+	v1 := []byte("v1")
+	v2 := []byte("v1")
+	v3 := []byte("v1")
+
+	values := Values{v1, v2, v3}
+	assert.True(t, values.AllSet())
+
+	values = Values{v1, nil, v3}
+	assert.False(t, values.AllSet())
+}
+
+func TestValues_Merge(t *testing.T) {
+	v1 := []byte("v1")
+	v2 := []byte("v1")
+	v3 := []byte("v1")
+
+	t.Run("Merge same size", func(t *testing.T) {
+		values1 := Values{v1, nil, v3}
+		values2 := Values{v3, v2, v1}
+
+		values := values1.Merge(values2)
+		assert.True(t, values.AllSet())
+		assert.Equal(t, v1, values[0])
+		assert.Equal(t, v2, values[1])
+		assert.Equal(t, v3, values[2])
+	})
+
+	t.Run("Merge source size bigger", func(t *testing.T) {
+		values1 := Values{v1, nil, v3}
+		values2 := Values{v3, v2}
+
+		values := values1.Merge(values2)
+		assert.True(t, values.AllSet())
+		assert.Equal(t, v1, values[0])
+		assert.Equal(t, v2, values[1])
+		assert.Equal(t, v3, values[2])
+	})
+
+	t.Run("Merge dest size bigger", func(t *testing.T) {
+		values1 := Values{v1, nil}
+		values2 := Values{v3, v2, v3}
+
+		values := values1.Merge(values2)
+		assert.True(t, values.AllSet())
+		assert.Equal(t, v1, values[0])
+		assert.Equal(t, v2, values[1])
+		assert.Equal(t, v3, values[2])
+	})
+}
+
+func TestTimestamp(t *testing.T) {
+	tim := time.Now()
+	tim2 := FromTimestamp(ToTimestamp(tim))
+	assert.Equal(t, tim.Unix(), tim2.Unix())
+}
diff --git a/extensions/common/discovery/discovery.go b/extensions/common/discovery/discovery.go
new file mode 100644
index 00000000..86c129c3
--- /dev/null
+++ b/extensions/common/discovery/discovery.go
@@ -0,0 +1,120 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"sync"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+)
+
+var logger = flogging.MustGetLogger("discovery")
+
+// Discovery provides functions to retrieve info about the local peer and other peers in a given channel
+type Discovery struct {
+	channelID string
+	gossip    gossipAdapter
+	self      *Member
+	selfInit  sync.Once
+}
+
+// New returns a new Discovery
+func New(channelID string, gossip gossipAdapter) *Discovery {
+	return &Discovery{
+		channelID: channelID,
+		gossip:    gossip,
+	}
+}
+
+type filter func(m *Member) bool
+
+type gossipAdapter interface {
+	PeersOfChannel(common.ChainID) []discovery.NetworkMember
+	SelfMembershipInfo() discovery.NetworkMember
+	IdentityInfo() api.PeerIdentitySet
+}
+
+// Self returns the local peer
+func (r *Discovery) Self() *Member {
+	r.selfInit.Do(func() {
+		r.self = getSelf(r.channelID, r.gossip)
+	})
+	return r.self
+}
+
+// ChannelID returns the channel ID
+func (r *Discovery) ChannelID() string {
+	return r.channelID
+}
+
+// GetMembers returns members filtered by the given filter
+func (r *Discovery) GetMembers(accept filter) []*Member {
+	identityInfo := r.gossip.IdentityInfo()
+	mapByID := identityInfo.ByID()
+
+	var peers []*Member
+	for _, m := range r.gossip.PeersOfChannel(common.ChainID(r.channelID)) {
+		identity, ok := mapByID[string(m.PKIid)]
+		if !ok {
+			logger.Warningf("[%s] Not adding peer [%s] as a validator since unable to determine MSP ID from PKIID for [%s]", r.channelID, m.Endpoint)
+			continue
+		}
+
+		m := &Member{
+			NetworkMember: m,
+			MSPID:         string(identity.Organization),
+		}
+
+		if accept(m) {
+			peers = append(peers, m)
+		}
+	}
+
+	if accept(r.Self()) {
+		peers = append(peers, r.Self())
+	}
+
+	return peers
+}
+
+// GetMSPID gets the MSP id
+func (r *Discovery) GetMSPID(pkiID common.PKIidType) (string, bool) {
+	identityInfo := r.gossip.IdentityInfo()
+	mapByID := identityInfo.ByID()
+
+	identity, ok := mapByID[string(pkiID)]
+	if !ok {
+		return "", false
+	}
+	return string(identity.Organization), true
+}
+
+func getSelf(channelID string, gossip gossipAdapter) *Member {
+	self := gossip.SelfMembershipInfo()
+	self.Properties = &proto.Properties{}
+
+	identityInfo := gossip.IdentityInfo()
+	mapByID := identityInfo.ByID()
+
+	var mspID string
+	selfIdentity, ok := mapByID[string(self.PKIid)]
+	if ok {
+		mspID = string(selfIdentity.Organization)
+	} else {
+		logger.Warningf("[%s] Unable to determine MSP ID from PKIID for self", channelID)
+	}
+
+	return &Member{
+		NetworkMember: self,
+		MSPID:         mspID,
+		Local:         true,
+	}
+}
diff --git a/extensions/common/discovery/discovery_test.go b/extensions/common/discovery/discovery_test.go
new file mode 100644
index 00000000..11e5c8fe
--- /dev/null
+++ b/extensions/common/discovery/discovery_test.go
@@ -0,0 +1,134 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/mocks"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	"github.com/stretchr/testify/assert"
+)
+
+const (
+	channelID = "testchannel"
+
+	ns1   = "chaincode1"
+	ns2   = "chaincode2"
+	coll1 = "collection1"
+	coll2 = "collection2"
+	key1  = "key1"
+	key2  = "key2"
+)
+
+var (
+	org1MSPID      = "Org1MSP"
+	p1Org1Endpoint = "p1.org1.com"
+	p1Org1PKIID    = gcommon.PKIidType("pkiid_P1O1")
+	p2Org1Endpoint = "p2.org1.com"
+	p2Org1PKIID    = gcommon.PKIidType("pkiid_P2O1")
+	p3Org1Endpoint = "p3.org1.com"
+	p3Org1PKIID    = gcommon.PKIidType("pkiid_P3O1")
+
+	org2MSPID      = "Org2MSP"
+	p1Org2Endpoint = "p1.org2.com"
+	p1Org2PKIID    = gcommon.PKIidType("pkiid_P1O2")
+	p2Org2Endpoint = "p2.org2.com"
+	p2Org2PKIID    = gcommon.PKIidType("pkiid_P2O2")
+	p3Org2Endpoint = "p3.org2.com"
+	p3Org2PKIID    = gcommon.PKIidType("pkiid_P3O2")
+
+	org3MSPID      = "Org3MSP"
+	p1Org3Endpoint = "p1.org3.com"
+	p1Org3PKIID    = gcommon.PKIidType("pkiid_P1O3")
+	p2Org3Endpoint = "p2.org3.com"
+	p2Org3PKIID    = gcommon.PKIidType("pkiid_P2O3")
+	p3Org3Endpoint = "p3.org3.com"
+	p3Org3PKIID    = gcommon.PKIidType("pkiid_P3O3")
+)
+
+func TestDiscovery(t *testing.T) {
+	p1Org1 := mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)
+	p2Org1 := mocks.NewMember(p2Org1Endpoint, p2Org1PKIID)
+	p3Org1 := mocks.NewMember(p3Org1Endpoint, p3Org1PKIID)
+	p1Org2 := mocks.NewMember(p1Org2Endpoint, p1Org2PKIID)
+	p2Org2 := mocks.NewMember(p2Org2Endpoint, p2Org2PKIID)
+	p3Org2 := mocks.NewMember(p3Org2Endpoint, p3Org2PKIID)
+	p1Org3 := mocks.NewMember(p1Org3Endpoint, p1Org3PKIID)
+	p2Org3 := mocks.NewMember(p2Org3Endpoint, p2Org3PKIID)
+	p3Org3 := mocks.NewMember(p3Org3Endpoint, p3Org3PKIID)
+	pInvalid := mocks.NewMember("invalid", gcommon.PKIidType("invalid"))
+
+	gossip := mocks.NewMockGossipAdapter().
+		Self(org1MSPID, p1Org1).
+		Member(org1MSPID, p2Org1).
+		Member(org1MSPID, p3Org1).
+		Member(org2MSPID, p1Org2).
+		Member(org2MSPID, p2Org2).
+		Member(org2MSPID, p3Org2).
+		Member(org3MSPID, p1Org3).
+		Member(org3MSPID, p2Org3).
+		Member(org3MSPID, p3Org3).
+		MemberWithNoPKIID(org3MSPID, pInvalid) // Should be ignored
+
+	d := New(channelID, gossip)
+
+	t.Run("ChannelID", func(t *testing.T) {
+		assert.Equal(t, channelID, d.ChannelID())
+	})
+
+	t.Run("Self", func(t *testing.T) {
+		s := d.Self()
+		assert.NotNil(t, s)
+		assert.Equal(t, p1Org1.Endpoint, s.Endpoint)
+	})
+
+	t.Run("GetMembers", func(t *testing.T) {
+		f := func(m *Member) bool {
+			e := m.Endpoint
+			if e == p1Org1.Endpoint ||
+				e == p2Org1.Endpoint ||
+				e == p1Org2.Endpoint ||
+				e == p3Org2.Endpoint ||
+				e == p1Org3.Endpoint ||
+				e == p3Org3.Endpoint {
+				return true
+			}
+			return false
+		}
+		members := d.GetMembers(f)
+		assert.Equal(t, 6, len(members))
+
+		expectedEndpoints := []string{p1Org1Endpoint, p2Org1Endpoint, p1Org2Endpoint, p3Org2Endpoint, p1Org3Endpoint, p3Org3Endpoint}
+
+		for _, m := range members {
+			assert.True(t, contains(expectedEndpoints, m.Endpoint))
+		}
+
+		memberEndpoints := asEndpoints(members...)
+		for _, e := range expectedEndpoints {
+			assert.True(t, contains(memberEndpoints, e))
+		}
+	})
+}
+
+func contains(endpoints []string, endpoint string) bool {
+	for _, e := range endpoints {
+		if endpoint == e {
+			return true
+		}
+	}
+	return false
+}
+
+func asEndpoints(members ...*Member) []string {
+	endpoints := make([]string, len(members))
+	for i, m := range members {
+		endpoints[i] = m.Endpoint
+	}
+	return endpoints
+}
diff --git a/extensions/common/discovery/member.go b/extensions/common/discovery/member.go
new file mode 100644
index 00000000..631bd670
--- /dev/null
+++ b/extensions/common/discovery/member.go
@@ -0,0 +1,23 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"github.com/hyperledger/fabric/gossip/discovery"
+)
+
+// Member wraps a NetworkMember and provides additional info
+type Member struct {
+	discovery.NetworkMember
+	ChannelID string
+	MSPID     string
+	Local     bool // Indicates whether this member is the local peer
+}
+
+func (m *Member) String() string {
+	return m.Endpoint
+}
diff --git a/extensions/common/discovery/member_test.go b/extensions/common/discovery/member_test.go
new file mode 100644
index 00000000..d01d0f3b
--- /dev/null
+++ b/extensions/common/discovery/member_test.go
@@ -0,0 +1,98 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"fmt"
+	"testing"
+
+	"github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	org1MSP = "org1MSP"
+
+	p0Endpoint = "p0.org1.com"
+	p1Endpoint = "p1.org1.com"
+	p2Endpoint = "p2.org1.com"
+	p3Endpoint = "p3.org1.com"
+	p4Endpoint = "p4.org1.com"
+	p5Endpoint = "p5.org1.com"
+	p6Endpoint = "p6.org1.com"
+	p7Endpoint = "p7.org1.com"
+)
+
+var (
+	p1 = newMember(org1MSP, p1Endpoint, false)
+	p2 = newMember(org1MSP, p2Endpoint, false)
+	p3 = newMember(org1MSP, p3Endpoint, false)
+	p4 = newMember(org1MSP, p4Endpoint, false)
+	p5 = newMember(org1MSP, p5Endpoint, true)
+	p6 = newMember(org1MSP, p6Endpoint, false)
+	p7 = newMember(org1MSP, p7Endpoint, false)
+)
+
+type testCase struct {
+	endpoint string
+	member   *Member
+}
+
+func TestMember(t *testing.T) {
+	testCases := []testCase{
+		{
+			p1Endpoint,
+			p1,
+		},
+		{
+			p2Endpoint,
+			p2,
+		},
+		{
+			p3Endpoint,
+			p3,
+		},
+		{
+			p4Endpoint,
+			p4,
+		},
+		{
+			p5Endpoint,
+			p5,
+		},
+		{
+			p6Endpoint,
+			p6,
+		},
+		{
+			p7Endpoint,
+			p7,
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(fmt.Sprintf("Testing Member %s", tc.member), func(t *testing.T) {
+			require.Equal(t, tc.endpoint, fmt.Sprintf("%s", tc.member), "Member doesn't match Endpoint assigned")
+			if tc.member == p5 {
+				require.True(t, tc.member.Local)
+			} else {
+				require.False(t, tc.member.Local)
+			}
+		})
+	}
+}
+
+func newMember(mspID, endpoint string, local bool) *Member {
+	m := &Member{
+		NetworkMember: discovery.NetworkMember{
+			Endpoint: endpoint,
+		},
+		MSPID: mspID,
+		Local: local,
+	}
+	return m
+}
diff --git a/extensions/common/discovery/peergroup.go b/extensions/common/discovery/peergroup.go
new file mode 100644
index 00000000..944996f5
--- /dev/null
+++ b/extensions/common/discovery/peergroup.go
@@ -0,0 +1,136 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"math/rand"
+	"sort"
+)
+
+// PeerGroup is a group of peers
+type PeerGroup []*Member
+
+// ContainsLocal return true if one of the peers in the group is the local peer
+func (g PeerGroup) ContainsLocal() bool {
+	for _, p := range g {
+		if p.Local {
+			return true
+		}
+	}
+	return false
+}
+
+func (g PeerGroup) String() string {
+	s := "["
+	for i, p := range g {
+		s += p.String()
+		if i+1 < len(g) {
+			s += ", "
+		}
+	}
+	s += "]"
+	return s
+}
+
+// Sort sorts the peer group by endpoint
+func (g PeerGroup) Sort() PeerGroup {
+	sort.Sort(g)
+	return g
+}
+
+// ContainsAll returns true if ALL of the peers within the given peer group are contained within this peer group
+func (g PeerGroup) ContainsAll(peerGroup PeerGroup) bool {
+	for _, p := range peerGroup {
+		if !g.Contains(p) {
+			return false
+		}
+	}
+	return true
+}
+
+// ContainsAny returns true if ANY of the peers within the given peer group are contained within this peer group
+func (g PeerGroup) ContainsAny(peerGroup PeerGroup) bool {
+	for _, p := range peerGroup {
+		if g.Contains(p) {
+			return true
+		}
+	}
+	return false
+}
+
+// Contains returns true if the given peer is contained within this peer group
+func (g PeerGroup) Contains(peer *Member) bool {
+	for _, p := range g {
+		if p.Endpoint == peer.Endpoint {
+			return true
+		}
+	}
+	return false
+}
+
+// ContainsPeer returns true if the given peer is contained within this peer group
+func (g PeerGroup) ContainsPeer(endpoint string) bool {
+	for _, p := range g {
+		if p.Endpoint == endpoint {
+			return true
+		}
+	}
+	return false
+}
+
+func (g PeerGroup) Len() int {
+	return len(g)
+}
+
+func (g PeerGroup) Less(i, j int) bool {
+	return g[i].Endpoint < g[j].Endpoint
+}
+
+func (g PeerGroup) Swap(i, j int) {
+	g[i], g[j] = g[j], g[i]
+}
+
+// Local returns the local peer from the group
+func (g PeerGroup) Local() (*Member, bool) {
+	for _, p := range g {
+		if p.Local {
+			return p, true
+		}
+	}
+	return nil, false
+}
+
+// Remote returns a PeerGroup subset that only includes remote peers
+func (g PeerGroup) Remote() PeerGroup {
+	var pg PeerGroup
+	for _, p := range g {
+		if !p.Local {
+			pg = append(pg, p)
+		}
+	}
+	return pg
+}
+
+// Shuffle returns a randomly shuffled PeerGroup
+func (g PeerGroup) Shuffle() PeerGroup {
+	var pg PeerGroup
+	for _, i := range rand.Perm(len(g)) {
+		pg = append(pg, g[i])
+	}
+
+	return pg
+}
+
+// Merge adds the given peers to the group if they don't already exist
+func Merge(g PeerGroup, peers ...*Member) PeerGroup {
+	for _, p := range peers {
+		if !g.Contains(p) {
+			g = append(g, p)
+		}
+	}
+	return g
+}
diff --git a/extensions/common/discovery/peergroup_test.go b/extensions/common/discovery/peergroup_test.go
new file mode 100644
index 00000000..b6e201ef
--- /dev/null
+++ b/extensions/common/discovery/peergroup_test.go
@@ -0,0 +1,103 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+)
+
+func TestPeerGroup_String(t *testing.T) {
+	pg := PeerGroup{p3, p1, p6, p5}
+	assert.Equal(t, "[p3.org1.com, p1.org1.com, p6.org1.com, p5.org1.com]", pg.String())
+}
+
+func TestPeerGroup_Sort(t *testing.T) {
+	pg := PeerGroup{p3, p1, p6, p5}
+	pg.Sort()
+
+	assert.Equal(t, p1, pg[0])
+	assert.Equal(t, p3, pg[1])
+	assert.Equal(t, p5, pg[2])
+	assert.Equal(t, p6, pg[3])
+}
+
+func TestPeerGroup_Contains(t *testing.T) {
+	pg1 := PeerGroup{p1, p2, p3}
+	pg2 := PeerGroup{p2, p3}
+	pg3 := PeerGroup{p2, p3, p4}
+	pg4 := PeerGroup{p4, p5}
+
+	assert.True(t, pg1.Contains(p1))
+	assert.True(t, pg1.Contains(p2))
+	assert.False(t, pg1.Contains(p4))
+	assert.True(t, pg1.ContainsAll(pg2))
+	assert.False(t, pg1.ContainsAll(pg3))
+	assert.True(t, pg1.ContainsAny(pg3))
+	assert.False(t, pg1.ContainsAny(pg4))
+}
+
+func TestPeerGroup_ContainsLocal(t *testing.T) {
+	pLocal := newMember(org1MSP, p0Endpoint, true)
+
+	pg1 := PeerGroup{p1, p2, p3, pLocal}
+	pg2 := PeerGroup{p2, p3, p4}
+
+	assert.True(t, pg1.ContainsLocal())
+	assert.False(t, pg2.ContainsLocal())
+}
+
+func TestPeerGroup_ContainsPeer(t *testing.T) {
+	pg := PeerGroup{p1, p2, p3}
+	assert.False(t, pg.ContainsPeer(p0Endpoint))
+	assert.True(t, pg.ContainsPeer(p1Endpoint))
+	assert.True(t, pg.ContainsPeer(p2Endpoint))
+	assert.True(t, pg.ContainsPeer(p3Endpoint))
+}
+
+func TestPeerGroup_Local(t *testing.T) {
+	pg := PeerGroup{p1, p2, p3, p4, p5, p6}
+	p, ok := pg.Local()
+	assert.True(t, ok)
+	assert.Equal(t, p5, p)
+
+	pg = PeerGroup{p1, p2, p3, p4, p6}
+	p, ok = pg.Local()
+	assert.False(t, ok)
+	assert.Nil(t, p)
+}
+
+func TestPeerGroup_Remote(t *testing.T) {
+	pg := PeerGroup{p1, p2, p3, p4, p5, p6}
+	for _, pg := range pg.Remote() {
+		assert.False(t, pg.Local)
+	}
+}
+
+func TestMerge(t *testing.T) {
+	pg1 := PeerGroup{p1, p2, p3}
+	pg := Merge(pg1, p2, p3, p4)
+	assert.Equal(t, p1, pg[0])
+	assert.Equal(t, p2, pg[1])
+	assert.Equal(t, p3, pg[2])
+	assert.Equal(t, p4, pg[3])
+}
+
+func TestPeerGroup_Shuffle(t *testing.T) {
+	pg := PeerGroup{p1, p2, p3, p4, p5, p6}
+
+	chosen := make(map[string]struct{})
+
+	for i := 0; i < 10; i++ {
+		pgShuffled := pg.Shuffle()
+		assert.True(t, pgShuffled.ContainsAll(pg))
+		chosen[pgShuffled[0].Endpoint] = struct{}{}
+	}
+
+	assert.Truef(t, len(chosen) > 1, "Expecting that the first peer is not always the same one.")
+}
diff --git a/extensions/common/discovery/peergroups.go b/extensions/common/discovery/peergroups.go
new file mode 100644
index 00000000..2ed7e761
--- /dev/null
+++ b/extensions/common/discovery/peergroups.go
@@ -0,0 +1,80 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"sort"
+)
+
+// PeerGroups is a group of peers
+type PeerGroups []PeerGroup
+
+func (g PeerGroups) String() string {
+	s := "("
+	for i, p := range g {
+		s += p.String()
+		if i+1 < len(g) {
+			s += ", "
+		}
+	}
+	s += ")"
+	return s
+}
+
+// Sort sorts the peer group by endpoint
+func (g PeerGroups) Sort() PeerGroups {
+	// First sort each peer group
+	for _, pg := range g {
+		pg.Sort()
+	}
+	// Now sort the peer groups
+	sort.Sort(g)
+
+	return g
+}
+
+// Contains returns true if the given peer is contained within any of the peer groups
+func (g PeerGroups) Contains(p *Member) bool {
+	for _, pg := range g {
+		if pg.Contains(p) {
+			return true
+		}
+	}
+	return false
+}
+
+// ContainsAll returns true if all of the peers within the given peer group are contained within the peer groups
+func (g PeerGroups) ContainsAll(peerGroup PeerGroup) bool {
+	for _, p := range peerGroup {
+		if !g.Contains(p) {
+			return false
+		}
+	}
+	return true
+}
+
+// ContainsAny returns true if any of the peers within the given peer group are contained within the peer groups
+func (g PeerGroups) ContainsAny(peerGroup PeerGroup) bool {
+	for _, pg := range g {
+		if pg.ContainsAny(peerGroup) {
+			return true
+		}
+	}
+	return false
+}
+
+func (g PeerGroups) Len() int {
+	return len(g)
+}
+
+func (g PeerGroups) Less(i, j int) bool {
+	return g[i].String() < g[j].String()
+}
+
+func (g PeerGroups) Swap(i, j int) {
+	g[i], g[j] = g[j], g[i]
+}
diff --git a/extensions/common/discovery/peergroups_test.go b/extensions/common/discovery/peergroups_test.go
new file mode 100644
index 00000000..4a5abeba
--- /dev/null
+++ b/extensions/common/discovery/peergroups_test.go
@@ -0,0 +1,61 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+)
+
+func TestPeerGroupsSort(t *testing.T) {
+	pg1 := PeerGroup{p2, p1, p3}
+	pg2 := PeerGroup{p5, p4}
+	pg3 := PeerGroup{p7, p6}
+
+	pgs := PeerGroups{pg3, pg1, pg2}
+
+	pgs.Sort()
+
+	// The peer groups should be sorted
+	assert.Equal(t, pg1, pgs[0])
+	assert.Equal(t, pg2, pgs[1])
+	assert.Equal(t, pg3, pgs[2])
+
+	// Each peer group should be sorted
+	assert.Equal(t, p1, pg1[0])
+	assert.Equal(t, p2, pg1[1])
+	assert.Equal(t, p3, pg1[2])
+
+	assert.Equal(t, p4, pg2[0])
+	assert.Equal(t, p5, pg2[1])
+
+	assert.Equal(t, p6, pg3[0])
+	assert.Equal(t, p7, pg3[1])
+}
+
+func TestPeerGroupsContains(t *testing.T) {
+	pg1 := PeerGroup{p1, p2, p3}
+	pg2 := PeerGroup{p3, p4, p5}
+	pg3 := PeerGroup{p1, p4}
+	pg4 := PeerGroup{p2, p5, p6}
+	pg5 := PeerGroup{p6, p7}
+
+	pgs1 := PeerGroups{pg1, pg2}
+
+	assert.True(t, pgs1.Contains(p1))
+	assert.True(t, pgs1.Contains(p2))
+	assert.True(t, pgs1.Contains(p3))
+	assert.True(t, pgs1.Contains(p4))
+	assert.True(t, pgs1.Contains(p5))
+	assert.False(t, pgs1.Contains(p6))
+
+	assert.True(t, pgs1.ContainsAll(pg3))
+	assert.False(t, pgs1.ContainsAll(pg4))
+	assert.True(t, pgs1.ContainsAny(pg4))
+	assert.False(t, pgs1.ContainsAny(pg5))
+}
diff --git a/extensions/common/discovery/peergroupsiterator.go b/extensions/common/discovery/peergroupsiterator.go
new file mode 100644
index 00000000..65abbf34
--- /dev/null
+++ b/extensions/common/discovery/peergroupsiterator.go
@@ -0,0 +1,42 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+// Iterator is a peer group iterator
+type Iterator struct {
+	peerGroups    PeerGroups
+	startingIndex int
+	index         int
+	done          bool
+}
+
+// NewIterator returns a new peer group iterator
+func NewIterator(peerGroups PeerGroups, index int) *Iterator {
+	return &Iterator{
+		peerGroups:    peerGroups,
+		startingIndex: index,
+		index:         index,
+	}
+}
+
+// Next returns the next peer group
+func (it *Iterator) Next() PeerGroup {
+	if it.done || len(it.peerGroups) == 0 {
+		return nil
+	}
+
+	pg := it.peerGroups[it.index]
+	it.index++
+	if it.index >= len(it.peerGroups) {
+		it.index = 0
+	}
+	if it.index == it.startingIndex {
+		it.done = true
+	}
+
+	return pg
+}
diff --git a/extensions/common/discovery/peergroupsiterator_test.go b/extensions/common/discovery/peergroupsiterator_test.go
new file mode 100644
index 00000000..d99607e2
--- /dev/null
+++ b/extensions/common/discovery/peergroupsiterator_test.go
@@ -0,0 +1,35 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+)
+
+var (
+	pg1 = PeerGroup{}
+	pg2 = PeerGroup{}
+	pg3 = PeerGroup{}
+	pg4 = PeerGroup{}
+)
+
+func TestIterator(t *testing.T) {
+
+	peerGroups := PeerGroups{pg1, pg2, pg3, pg4}
+
+	startingIndex := 2
+
+	it := NewIterator(peerGroups, startingIndex)
+
+	assert.Equal(t, pg3, it.Next())
+	assert.Equal(t, pg4, it.Next())
+	assert.Equal(t, pg1, it.Next())
+	assert.Equal(t, pg2, it.Next())
+	assert.Nil(t, it.Next())
+}
diff --git a/extensions/common/discovery/permutations.go b/extensions/common/discovery/permutations.go
new file mode 100644
index 00000000..b4c2201c
--- /dev/null
+++ b/extensions/common/discovery/permutations.go
@@ -0,0 +1,69 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"fmt"
+
+	"github.com/hyperledger/fabric/common/graph"
+)
+
+// Permuations calculates the permutations of the peer groups.
+type Permuations struct {
+	vr    *graph.TreeVertex
+	index int
+}
+
+// NewPermutations returns a new Permutations struct
+func NewPermutations() *Permuations {
+	return &Permuations{
+		vr: graph.NewTreeVertex("root", nil),
+	}
+}
+
+// Groups adds the given peer groups
+func (p *Permuations) Groups(peerGroups ...PeerGroup) *Permuations {
+	for _, pg := range peerGroups {
+		p.addGroup(pg)
+	}
+	return p
+}
+
+// Evaluate calculates the permutations of the peer groups.
+// For example:
+//   If the given peer groups are [(p1,p2), (p3,p4)] then the return value
+//   will be [(p1,p3),(p1,p4),(p2,p3),(p2,p4)]
+func (p *Permuations) Evaluate() PeerGroups {
+	var groups PeerGroups
+	for _, permutation := range p.vr.ToTree().Permute() {
+		groups = append(groups, combinations(permutation.BFS()))
+	}
+	return groups
+}
+
+func (p *Permuations) addGroup(pg PeerGroup) *Permuations {
+	p.index++
+	p.vr.Threshold = p.index
+
+	gvr := p.vr.AddDescendant(graph.NewTreeVertex(fmt.Sprintf("%d", p.index), nil))
+	gvr.Threshold = 1
+
+	for _, p := range pg {
+		gvr.AddDescendant(graph.NewTreeVertex(p.Endpoint, p))
+	}
+	return p
+}
+
+func combinations(it graph.Iterator) PeerGroup {
+	var peerGroup PeerGroup
+	for v := it.Next(); v != nil; v = it.Next() {
+		if v.Data != nil {
+			peerGroup = append(peerGroup, v.Data.(*Member))
+		}
+	}
+	return peerGroup
+}
diff --git a/extensions/common/discovery/permutations_test.go b/extensions/common/discovery/permutations_test.go
new file mode 100644
index 00000000..b186c6aa
--- /dev/null
+++ b/extensions/common/discovery/permutations_test.go
@@ -0,0 +1,85 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package discovery
+
+import (
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestPermutations(t *testing.T) {
+	p1Org1 := newMember(org1MSPID, p1Org1Endpoint, false)
+	p2Org1 := newMember(org1MSPID, p2Org1Endpoint, false)
+	p3Org1 := newMember(org1MSPID, p3Org1Endpoint, false)
+
+	p1Org2 := newMember(org2MSPID, p1Org2Endpoint, false)
+	p2Org2 := newMember(org2MSPID, p2Org2Endpoint, false)
+
+	p1Org3 := newMember(org3MSPID, p1Org3Endpoint, false)
+	p2Org3 := newMember(org3MSPID, p2Org3Endpoint, false)
+
+	t.Run("1 Of", func(t *testing.T) {
+		perm := NewPermutations().
+			Groups(NewPeerGroup(p2Org1, p1Org1))
+
+		peerGroups := perm.Evaluate().Sort()
+		require.Equal(t, 2, len(peerGroups))
+
+		t.Logf("%s", peerGroups)
+
+		assert.Equal(t, asEndpoints(p1Org1), asEndpoints(peerGroups[0]...))
+		assert.Equal(t, asEndpoints(p2Org1), asEndpoints(peerGroups[1]...))
+	})
+
+	t.Run("2 Of", func(t *testing.T) {
+		perm := NewPermutations().
+			Groups(
+				NewPeerGroup(p1Org2, p2Org2),
+				NewPeerGroup(p2Org1, p1Org1),
+			)
+
+		peerGroups := perm.Evaluate().Sort()
+		require.Equal(t, 4, len(peerGroups))
+
+		t.Logf("%s", peerGroups)
+
+		assert.Equal(t, asEndpoints(p1Org1, p1Org2), asEndpoints(peerGroups[0]...))
+		assert.Equal(t, asEndpoints(p1Org1, p2Org2), asEndpoints(peerGroups[1]...))
+		assert.Equal(t, asEndpoints(p1Org2, p2Org1), asEndpoints(peerGroups[2]...))
+		assert.Equal(t, asEndpoints(p2Org1, p2Org2), asEndpoints(peerGroups[3]...))
+	})
+
+	t.Run("3 Of", func(t *testing.T) {
+		perm := NewPermutations().
+			Groups(NewPeerGroup(p1Org2)).
+			Groups(NewPeerGroup(p2Org1, p1Org1, p3Org1)).
+			Groups(NewPeerGroup(p2Org3, p1Org3))
+
+		peerGroups := perm.Evaluate().Sort()
+		require.Equal(t, 6, len(peerGroups))
+
+		t.Logf("%s", peerGroups)
+
+		assert.Equal(t, asEndpoints(p1Org1, p1Org2, p1Org3), asEndpoints(peerGroups[0]...))
+		assert.Equal(t, asEndpoints(p1Org1, p1Org2, p2Org3), asEndpoints(peerGroups[1]...))
+		assert.Equal(t, asEndpoints(p1Org2, p1Org3, p2Org1), asEndpoints(peerGroups[2]...))
+		assert.Equal(t, asEndpoints(p1Org2, p1Org3, p3Org1), asEndpoints(peerGroups[3]...))
+		assert.Equal(t, asEndpoints(p1Org2, p2Org1, p2Org3), asEndpoints(peerGroups[4]...))
+		assert.Equal(t, asEndpoints(p1Org2, p2Org3, p3Org1), asEndpoints(peerGroups[5]...))
+	})
+
+}
+
+func NewPeerGroup(members ...*Member) PeerGroup {
+	var peerGroup PeerGroup
+	for _, member := range members {
+		peerGroup = append(peerGroup, member)
+	}
+	return peerGroup
+}
diff --git a/extensions/common/multirequest/multirequest.go b/extensions/common/multirequest/multirequest.go
new file mode 100644
index 00000000..a675a132
--- /dev/null
+++ b/extensions/common/multirequest/multirequest.go
@@ -0,0 +1,97 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package multirequest
+
+import (
+	"context"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/extensions/common"
+)
+
+var logger = flogging.MustGetLogger("kevlar-common")
+
+// Request is the request to execute
+type Request func(ctxt context.Context) (common.Values, error)
+
+type req struct {
+	id      string
+	execute Request
+}
+
+type res struct {
+	id     string
+	values common.Values
+	err    error
+}
+
+// Response contains the response for a given request ID
+type Response struct {
+	RequestID string
+	Values    common.Values
+}
+
+// MultiRequest executes multiple requests and returns the first, non-error response
+type MultiRequest struct {
+	requests []*req
+}
+
+// New returns a new MultiRequest
+func New() *MultiRequest {
+	return &MultiRequest{}
+}
+
+// Add adds a request function
+func (r *MultiRequest) Add(id string, execute Request) {
+	r.requests = append(r.requests, &req{id: id, execute: execute})
+}
+
+// Execute executes the requests concurrently and returns the responses.
+func (r *MultiRequest) Execute(ctxt context.Context) *Response {
+	respChan := make(chan *res, len(r.requests)+1)
+
+	cctxt, cancel := context.WithCancel(ctxt)
+	defer cancel()
+
+	for _, request := range r.requests {
+		go func(r *req) {
+			ccctxt, cancelReq := context.WithCancel(cctxt)
+			defer cancelReq()
+			values, err := r.execute(ccctxt)
+			respChan <- &res{id: r.id, values: values, err: err}
+		}(request)
+	}
+
+	resp := &Response{}
+	done := false
+
+	// Wait for all responses
+	for range r.requests {
+		response := <-respChan
+
+		if done {
+			continue
+		}
+
+		if response.err != nil {
+			logger.Debugf("Error response was received from [%s]: %s", response.id, response.err)
+		} else {
+			if response.values.AllSet() {
+				logger.Debugf("All values were received from [%s]", response.id)
+				// Cancel all outstanding requests since we've got our response
+				cancel()
+				done = true
+			} else {
+				logger.Debugf("One or more values are missing in response from [%s]", response.id)
+			}
+			resp.RequestID = response.id
+			resp.Values = resp.Values.Merge(response.values)
+		}
+	}
+
+	return resp
+}
diff --git a/extensions/common/multirequest/multirequest_test.go b/extensions/common/multirequest/multirequest_test.go
new file mode 100644
index 00000000..4d325ddc
--- /dev/null
+++ b/extensions/common/multirequest/multirequest_test.go
@@ -0,0 +1,154 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package multirequest
+
+import (
+	"context"
+	"errors"
+	"testing"
+	"time"
+
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+type requestTest struct {
+	id     string
+	err    error
+	values [][]byte
+}
+
+func TestAllSet(t *testing.T) {
+	t.Parallel()
+
+	re := New()
+
+	value1 := []byte("value1")
+	value2 := []byte("value2")
+	value3 := []byte("value3")
+
+	requests := []requestTest{
+		{id: "Request1", values: [][]byte{value1, value2, value3}},
+		{id: "Request2", values: [][]byte{value1, value2, value3}},
+		{id: "Request3", values: [][]byte{value1, value2, value3}},
+	}
+
+	for _, r := range requests {
+		re.Add(r.id, getRequestFunc(r))
+	}
+
+	result := re.Execute(context.Background())
+	require.NotNil(t, result)
+	require.Equal(t, 3, len(result.Values))
+	assert.Equal(t, value1, result.Values[0])
+	assert.Equal(t, value2, result.Values[1])
+	assert.Equal(t, value3, result.Values[2])
+}
+
+func TestMerge(t *testing.T) {
+	t.Parallel()
+
+	re := New()
+
+	value1 := []byte("value1")
+	value2 := []byte("value2")
+	value3 := []byte("value3")
+
+	requests := []requestTest{
+		{id: "Request1", values: [][]byte{nil, value2, value3}},
+		{id: "Request2", values: [][]byte{value1, nil, value3}},
+		{id: "Request3", values: [][]byte{value1, value2, nil}},
+	}
+
+	for _, r := range requests {
+		re.Add(r.id, getRequestFunc(r))
+	}
+
+	result := re.Execute(context.Background())
+	require.NotNil(t, result)
+	require.Equal(t, 3, len(result.Values))
+
+	assert.Equal(t, value1, result.Values[0])
+	assert.Equal(t, value2, result.Values[1])
+	assert.Equal(t, value3, result.Values[2])
+}
+
+func TestTimeoutOrCancel(t *testing.T) {
+	t.Parallel()
+
+	re := New()
+
+	requests := []requestTest{
+		{id: "Request1"},
+		{id: "Request2"},
+		{id: "Request3"},
+	}
+
+	for _, r := range requests {
+		re.Add(r.id, getRequestFunc(r))
+	}
+
+	t.Run("Timeout", func(t *testing.T) {
+		t.Parallel()
+		ctxt, _ := context.WithTimeout(context.Background(), 1*time.Microsecond)
+		result := re.Execute(ctxt)
+		assert.True(t, result.Values.IsEmpty())
+	})
+
+	t.Run("Cancelled", func(t *testing.T) {
+		t.Parallel()
+		ctxt, cancel := context.WithCancel(context.Background())
+		go cancel()
+		result := re.Execute(ctxt)
+		assert.True(t, result.Values.IsEmpty())
+	})
+}
+
+func TestSomeFailed(t *testing.T) {
+	t.Parallel()
+
+	re := New()
+
+	value1 := []byte("value1")
+	value3 := []byte("value3")
+
+	requests := []requestTest{
+		{id: "Request1", err: errors.New("error for request1")},
+		{id: "Request2", values: [][]byte{value1, nil, value3}},
+		{id: "Request3", err: errors.New("error for request3")},
+	}
+
+	for _, r := range requests {
+		re.Add(r.id, getRequestFunc(r))
+	}
+
+	result := re.Execute(context.Background())
+	require.Equal(t, 3, len(result.Values))
+	assert.Equal(t, value1, result.Values[0])
+	assert.Nil(t, result.Values[1])
+	assert.Equal(t, value3, result.Values[2])
+}
+
+func getRequestFunc(r requestTest) Request {
+	return func(ctxt context.Context) (common.Values, error) {
+		select {
+		case <-time.After(5 * time.Millisecond):
+			return asValues(r.values), r.err
+		case <-ctxt.Done():
+			return nil, ctxt.Err()
+		}
+	}
+}
+
+func asValues(bv [][]byte) common.Values {
+	vals := make(common.Values, len(bv))
+	for i, v := range bv {
+		vals[i] = v
+	}
+	return vals
+}
diff --git a/extensions/common/requestmgr/requestmgr.go b/extensions/common/requestmgr/requestmgr.go
new file mode 100644
index 00000000..5ccef31b
--- /dev/null
+++ b/extensions/common/requestmgr/requestmgr.go
@@ -0,0 +1,207 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package requestmgr
+
+import (
+	"context"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("kevlar-common")
+
+// Element contains transient data for a single key
+type Element struct {
+	Namespace  string
+	Collection string
+	Key        string
+	Value      []byte
+	Expiry     time.Time
+}
+
+// Elements is a slice of Element
+type Elements []*Element
+
+// Get returns the Element matching the given namespace, collection, and key.
+func (e Elements) Get(ns, coll, key string) (*Element, bool) {
+	for _, element := range e {
+		if element.Namespace == ns && element.Collection == coll && element.Key == key {
+			return element, true
+		}
+	}
+	return nil, false
+}
+
+// Response is the response from a remote peer for a collection of transient data keys
+type Response struct {
+	Endpoint  string   // The endpoint of the peer that sent the response
+	MSPID     string   // The MSP ID of the peer that sent the response
+	Signature []byte   // The signature of the peer that provided the data
+	Identity  []byte   // The identity of the peer that sent the response
+	Data      Elements // The transient data
+}
+
+// Request is an interface to get the response
+type Request interface {
+	ID() uint64
+	GetResponse(context context.Context) (*Response, error)
+	Cancel()
+}
+
+// RequestMgr is an interface to create a new request and to respond to the request
+type RequestMgr interface {
+	Respond(reqID uint64, response *Response)
+	NewRequest() Request
+}
+
+type requestMgr struct {
+	mutex sync.RWMutex
+	mgrs  map[string]*channelMgr
+}
+
+var mgr = newRequestMgr()
+
+// Get returns the RequestMgr for the given channel
+func Get(channelID string) RequestMgr {
+	return mgr.forChannel(channelID)
+}
+
+func newRequestMgr() *requestMgr {
+	return &requestMgr{
+		mgrs: make(map[string]*channelMgr),
+	}
+}
+
+func (m *requestMgr) forChannel(channelID string) RequestMgr {
+	m.mutex.RLock()
+	cm, ok := m.mgrs[channelID]
+	m.mutex.RUnlock()
+
+	if ok {
+		return cm
+	}
+
+	m.mutex.Lock()
+	defer m.mutex.Unlock()
+
+	cm = newChannelMgr(channelID)
+	m.mgrs[channelID] = cm
+	return cm
+}
+
+type channelMgr struct {
+	mutex         sync.RWMutex
+	channelID     string
+	requests      map[uint64]*request
+	nextRequestID uint64
+}
+
+func newChannelMgr(channelID string) *channelMgr {
+	logger.Debugf("[%s] Creating new channel request manager", channelID)
+	return &channelMgr{
+		channelID:     channelID,
+		requests:      make(map[uint64]*request),
+		nextRequestID: 1000000000,
+	}
+}
+
+func (c *channelMgr) NewRequest() Request {
+	c.mutex.Lock()
+	defer c.mutex.Unlock()
+
+	reqID := c.newRequestID()
+
+	logger.Debugf("[%s] Subscribing to transient data request %d", c.channelID, reqID)
+
+	s := newRequest(reqID, c.channelID, c.remove)
+	c.requests[reqID] = s
+
+	return s
+}
+
+func (c *channelMgr) Respond(reqID uint64, response *Response) {
+	c.mutex.RLock()
+	defer c.mutex.RUnlock()
+
+	s, ok := c.requests[reqID]
+	if !ok {
+		logger.Debugf("[%s] No transient data requests for %d", c.channelID, reqID)
+		return
+	}
+
+	logger.Debugf("[%s] Publishing transient data response %d", c.channelID, reqID)
+	s.respond(response)
+}
+
+func (c *channelMgr) remove(reqID uint64) {
+	c.mutex.Lock()
+	defer c.mutex.Unlock()
+
+	if _, ok := c.requests[reqID]; !ok {
+		return
+	}
+
+	delete(c.requests, reqID)
+	logger.Debugf("[%s] Unsubscribed from transient data request %d", c.channelID, reqID)
+}
+
+func (c *channelMgr) newRequestID() uint64 {
+	return atomic.AddUint64(&c.nextRequestID, 1)
+}
+
+type request struct {
+	reqID     uint64
+	channelID string
+	remove    func(reqID uint64)
+	respChan  chan *Response
+	done      bool
+}
+
+func newRequest(reqID uint64, channelID string, remove func(reqID uint64)) *request {
+	return &request{
+		reqID:     reqID,
+		channelID: channelID,
+		respChan:  make(chan *Response, 1),
+		remove:    remove,
+	}
+}
+
+func (r *request) ID() uint64 {
+	return r.reqID
+}
+
+func (r *request) GetResponse(ctxt context.Context) (*Response, error) {
+	if r.done {
+		return nil, errors.New("request has already completed")
+	}
+
+	logger.Debugf("[%s] Waiting for response to request %d", r.channelID, r.ID())
+
+	defer r.Cancel()
+
+	select {
+	case <-ctxt.Done():
+		logger.Debugf("[%s] Request %d was timed out or cancelled", r.channelID, r.ID())
+		return nil, ctxt.Err()
+	case item := <-r.respChan:
+		logger.Debugf("[%s] Got response for request %d", r.channelID, r.ID())
+		return item, nil
+	}
+}
+
+func (r *request) Cancel() {
+	r.remove(r.reqID)
+	r.done = true
+}
+
+func (r *request) respond(res *Response) {
+	r.respChan <- res
+}
diff --git a/extensions/common/requestmgr/requestmgr_test.go b/extensions/common/requestmgr/requestmgr_test.go
new file mode 100644
index 00000000..ae6916cc
--- /dev/null
+++ b/extensions/common/requestmgr/requestmgr_test.go
@@ -0,0 +1,153 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package requestmgr
+
+import (
+	"context"
+	"testing"
+	"time"
+
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channel1 = "channel1"
+	channel2 = "channel2"
+
+	timeout = 200 * time.Millisecond
+)
+
+func TestElements_Get(t *testing.T) {
+	const (
+		ns1   = "ns1"
+		coll1 = "coll1"
+		coll2 = "coll2"
+		key1  = "key1"
+		key2  = "key2"
+		key3  = "key3"
+	)
+
+	e1 := &Element{Namespace: ns1, Collection: coll1, Key: key1, Expiry: time.Now().Add(time.Minute)}
+	e2 := &Element{Namespace: ns1, Collection: coll2, Key: key1, Expiry: time.Now().Add(time.Minute)}
+	e3 := &Element{Namespace: ns1, Collection: coll1, Key: key2, Expiry: time.Now().Add(time.Minute)}
+
+	elements := Elements{e1, e2, e3}
+
+	e, ok := elements.Get(ns1, coll1, key1)
+	assert.True(t, ok)
+	require.NotNil(t, e)
+	assert.Equal(t, e1, e)
+
+	e, ok = elements.Get(ns1, coll2, key1)
+	assert.True(t, ok)
+	require.NotNil(t, e)
+	assert.Equal(t, e2, e)
+
+	e, ok = elements.Get(ns1, coll1, key3)
+	assert.False(t, ok)
+	require.Nil(t, e)
+}
+
+func TestResponseMgr(t *testing.T) {
+	t.Parallel()
+
+	m1 := Get(channel1)
+	require.NotNil(t, m1)
+
+	m2 := Get(channel2)
+	require.NotNil(t, m2)
+	require.False(t, m1 == m2)
+
+	t.Run("Response received - channel1", func(t *testing.T) {
+		t.Parallel()
+
+		req1 := m1.NewRequest()
+		require.NotNil(t, req1)
+
+		req2 := m1.NewRequest()
+		require.NotNil(t, req2)
+
+		expect1 := &Response{Endpoint: "p1"}
+		expect2 := &Response{Endpoint: "p2"}
+
+		go func() {
+			m1.Respond(req1.ID(), expect1)
+			m1.Respond(req2.ID(), expect2)
+		}()
+
+		res1, err := req1.GetResponse(context.Background())
+		require.NoError(t, err)
+		assert.Equal(t, expect1, res1)
+
+		res2, err := req2.GetResponse(context.Background())
+		require.NoError(t, err)
+		assert.Equal(t, expect2, res2)
+	})
+
+	t.Run("Response received - channel2", func(t *testing.T) {
+		t.Parallel()
+
+		req3 := m2.NewRequest()
+		require.NotNil(t, req3)
+
+		expect3 := &Response{Endpoint: "p3"}
+
+		go func() {
+			m2.Respond(req3.ID(), expect3)
+		}()
+
+		res3, err := req3.GetResponse(context.Background())
+		require.NoError(t, err)
+		assert.Equal(t, expect3, res3)
+	})
+
+	t.Run("No response", func(t *testing.T) {
+		t.Parallel()
+
+		req4 := m2.NewRequest()
+		require.NotNil(t, req4)
+
+		// Should time out since we never publish a response
+		ctxt, _ := context.WithTimeout(context.Background(), timeout)
+		_, err := req4.GetResponse(ctxt)
+		require.Error(t, err)
+	})
+
+	t.Run("Context cancelled", func(t *testing.T) {
+		t.Parallel()
+
+		req5 := m2.NewRequest()
+		require.NotNil(t, req5)
+
+		ctxt, cancel := context.WithTimeout(context.Background(), timeout)
+
+		go func() {
+			cancel()
+			time.Sleep(10 * time.Millisecond)
+			m2.Respond(req5.ID(), &Response{}) // No subscribers since the context was cancelled
+		}()
+
+		// Should get error that the context was cancelled
+		_, err := req5.GetResponse(ctxt)
+		require.Error(t, err)
+		assert.EqualError(t, err, "context canceled")
+	})
+
+	t.Run("Request cancelled", func(t *testing.T) {
+		t.Parallel()
+
+		req := m2.NewRequest()
+		require.NotNil(t, req)
+		req.Cancel()
+
+		// Should get error that the request was cancelled
+		_, err := req.GetResponse(context.Background())
+		require.Error(t, err)
+		assert.EqualError(t, err, "request has already completed")
+	})
+}
diff --git a/extensions/common/support/collconfigretriever.go b/extensions/common/support/collconfigretriever.go
new file mode 100644
index 00000000..e5a19151
--- /dev/null
+++ b/extensions/common/support/collconfigretriever.go
@@ -0,0 +1,206 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	"fmt"
+	"reflect"
+
+	"github.com/bluele/gcache"
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	mspmgmt "github.com/hyperledger/fabric/msp/mgmt"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/pkg/errors"
+)
+
+type peerLedger interface {
+	// NewQueryExecutor gives handle to a query executor.
+	// A client can obtain more than one 'QueryExecutor's for parallel execution.
+	// Any synchronization should be performed at the implementation level if required
+	NewQueryExecutor() (ledger.QueryExecutor, error)
+}
+
+// CollectionConfigRetriever loads and caches collection configuration and policies
+type CollectionConfigRetriever struct {
+	channelID string
+	ledger    peerLedger
+	cache     gcache.Cache
+}
+
+type blockPublisher interface {
+	AddCCUpgradeHandler(handler gossipapi.ChaincodeUpgradeHandler)
+}
+
+// NewCollectionConfigRetriever returns a new collection configuration retriever
+func NewCollectionConfigRetriever(channelID string, ledger peerLedger, blockPublisher blockPublisher) *CollectionConfigRetriever {
+	r := &CollectionConfigRetriever{
+		channelID: channelID,
+		ledger:    ledger,
+	}
+
+	r.cache = gcache.New(0).Simple().LoaderFunc(
+		func(key interface{}) (interface{}, error) {
+			ccID := key.(string)
+			configs, err := r.loadConfigAndPolicy(ccID)
+			if err != nil {
+				logger.Debugf("Error loading collection configs for chaincode [%s]: %s", ccID, err)
+				return nil, err
+			}
+			return configs, nil
+		}).Build()
+
+	// Add a handler to remove the collection configs from cache when the chaincode is upgraded
+	blockPublisher.AddCCUpgradeHandler(func(blockNum uint64, txID string, chaincodeName string) error {
+		if r.cache.Remove(chaincodeName) {
+			logger.Infof("Chaincode [%s] was upgraded. Removed collection configs from cache.", chaincodeName)
+		}
+		return nil
+	})
+
+	return r
+}
+
+type cacheItem struct {
+	config *common.StaticCollectionConfig
+	policy privdata.CollectionAccessPolicy
+}
+
+type cacheItems []*cacheItem
+
+func (c cacheItems) get(coll string) (*cacheItem, error) {
+	for _, item := range c {
+		if item.config.Name == coll {
+			return item, nil
+		}
+	}
+	return nil, errors.Errorf("configuration not found for collection [%s]", coll)
+}
+
+func (c cacheItems) config(coll string) (*common.StaticCollectionConfig, error) {
+	item, err := c.get(coll)
+	if err != nil {
+		return nil, err
+	}
+	return item.config, nil
+}
+
+func (c cacheItems) policy(coll string) (privdata.CollectionAccessPolicy, error) {
+	item, err := c.get(coll)
+	if err != nil {
+		return nil, err
+	}
+	return item.policy, nil
+}
+
+// Config returns the configuration for the given collection
+func (s *CollectionConfigRetriever) Config(ns, coll string) (*common.StaticCollectionConfig, error) {
+	logger.Debugf("[%s] Retrieving collection configuration for chaincode [%s]", s.channelID, ns)
+	item, err := s.cache.Get(ns)
+	if err != nil {
+		return nil, err
+	}
+
+	configs, ok := item.(cacheItems)
+	if !ok {
+		panic(fmt.Sprintf("unexpected type in cache: %s", reflect.TypeOf(item)))
+	}
+
+	return configs.config(coll)
+}
+
+// Policy returns the collection access policy
+func (s *CollectionConfigRetriever) Policy(ns, coll string) (privdata.CollectionAccessPolicy, error) {
+	logger.Debugf("[%s] Retrieving collection policy for chaincode [%s]", s.channelID, ns)
+	item, err := s.cache.Get(ns)
+	if err != nil {
+		return nil, err
+	}
+
+	configs, ok := item.(cacheItems)
+	if !ok {
+		panic(fmt.Sprintf("unexpected type in cache: %s", reflect.TypeOf(item)))
+	}
+
+	return configs.policy(coll)
+}
+
+func (s *CollectionConfigRetriever) loadConfigAndPolicy(ns string) (cacheItems, error) {
+	configs, err := s.loadConfigs(ns)
+	if err != nil {
+		return nil, err
+	}
+
+	var items []*cacheItem
+	for _, config := range configs {
+		policy, err := s.loadPolicy(ns, config)
+		if err != nil {
+			return nil, err
+		}
+		items = append(items, &cacheItem{
+			config: config,
+			policy: policy,
+		})
+	}
+
+	return items, nil
+}
+
+func (s *CollectionConfigRetriever) loadConfigs(ns string) ([]*common.StaticCollectionConfig, error) {
+	logger.Debugf("[%s] Loading collection configs for chaincode [%s]", s.channelID, ns)
+
+	cpBytes, err := s.getCCPBytes(ns)
+	if err != nil {
+		return nil, errors.Wrapf(err, "error retrieving collection config for chaincode [%s]", ns)
+	}
+	if cpBytes == nil {
+		return nil, errors.Errorf("no collection config for chaincode [%s]", ns)
+	}
+
+	cp := &common.CollectionConfigPackage{}
+	err = proto.Unmarshal(cpBytes, cp)
+	if err != nil {
+		return nil, errors.Wrapf(err, "invalid collection configuration for [%s]", ns)
+	}
+
+	var configs []*common.StaticCollectionConfig
+	for _, collConfig := range cp.Config {
+		config := collConfig.GetStaticCollectionConfig()
+		logger.Debugf("[%s] Checking collection config for [%s:%+v]", s.channelID, ns, config)
+		if config == nil {
+			logger.Warningf("[%s] No config found for a collection in namespace [%s]", s.channelID, ns)
+			continue
+		}
+		configs = append(configs, config)
+	}
+
+	return configs, nil
+}
+
+func (s *CollectionConfigRetriever) loadPolicy(ns string, config *common.StaticCollectionConfig) (privdata.CollectionAccessPolicy, error) {
+	logger.Debugf("[%s] Loading collection policy for [%s:%s]", s.channelID, ns, config.Name)
+
+	colAP := &privdata.SimpleCollection{}
+	err := colAP.Setup(config, mspmgmt.GetIdentityDeserializer(s.channelID))
+	if err != nil {
+		return nil, errors.Wrapf(err, "error setting up collection policy %s", config.Name)
+	}
+
+	return colAP, nil
+}
+
+func (s *CollectionConfigRetriever) getCCPBytes(ns string) ([]byte, error) {
+	qe, err := s.ledger.NewQueryExecutor()
+	if err != nil {
+		return nil, err
+	}
+	defer qe.Done()
+
+	return qe.GetState("lscc", privdata.BuildCollectionKVSKey(ns))
+}
diff --git a/extensions/common/support/collconfigretriever_test.go b/extensions/common/support/collconfigretriever_test.go
new file mode 100644
index 00000000..a8811eee
--- /dev/null
+++ b/extensions/common/support/collconfigretriever_test.go
@@ -0,0 +1,171 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	"fmt"
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/hyperledger/fabric/protos/common"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	coll3 = "collection3"
+)
+
+func TestConfigRetriever(t *testing.T) {
+	const channelID = "testchannel"
+	const lscc = "lscc"
+
+	nsBuilder := mocks.NewNamespaceBuilder(ns1)
+	nsBuilder.Collection(coll1).StaticConfig("OR ('Org1.member','Org2.member')", 3, 3, 100)
+	nsBuilder.Collection(coll2).TransientConfig("OR ('Org1.member','Org2.member')", 3, 3, "1m")
+
+	configPkgBytes, err := proto.Marshal(nsBuilder.BuildCollectionConfig())
+	require.NoError(t, err)
+
+	state := make(map[string]map[string][]byte)
+	state[lscc] = make(map[string][]byte)
+	state[lscc][privdata.BuildCollectionKVSKey(ns1)] = configPkgBytes
+
+	blockPublisher := &mockBlockPublisher{}
+
+	r := NewCollectionConfigRetriever(channelID, &mocks.Ledger{
+		QueryExecutor: mocks.NewQueryExecutor(state),
+	}, blockPublisher)
+	require.NotNil(t, r)
+
+	t.Run("Policy", func(t *testing.T) {
+		policy, err := r.Policy(ns1, coll2)
+		require.NoError(t, err)
+		require.NotNil(t, policy)
+		assert.Equal(t, 2, len(policy.MemberOrgs()))
+
+		policy2, err := r.Policy(ns1, coll2)
+		require.NoError(t, err)
+		assert.Truef(t, policy == policy2, "expecting policy to be retrieved from cache")
+
+		policy, err = r.Policy(ns1, coll1)
+		require.NoError(t, err)
+		require.NotNil(t, policy)
+		assert.Equal(t, 2, len(policy.MemberOrgs()))
+	})
+
+	t.Run("Config", func(t *testing.T) {
+		config, err := r.Config(ns1, coll2)
+		require.NoError(t, err)
+		assert.Equal(t, coll2, config.Name)
+		assert.Equal(t, int32(3), config.RequiredPeerCount)
+		assert.Equal(t, common.CollectionType_COL_TRANSIENT, config.Type)
+		assert.Equal(t, "1m", config.TimeToLive)
+
+		config2, err := r.Config(ns1, coll2)
+		require.NoError(t, err)
+		assert.Truef(t, config == config2, "expecting config to be retrieved from cache")
+	})
+
+	t.Run("Config not found -> error", func(t *testing.T) {
+		config, err := r.Config(ns1, coll3)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), "configuration not found")
+		assert.Nil(t, config)
+	})
+
+	t.Run("Chaincode updated", func(t *testing.T) {
+		nsBuilder := mocks.NewNamespaceBuilder(ns1)
+		nsBuilder.Collection(coll1).StaticConfig("OR ('Org1.member','Org2.member','Org3.member')", 3, 3, 100)
+		nsBuilder.Collection(coll2).TransientConfig("OR ('Org1.member','Org2.member','Org3.member')", 4, 3, "10m")
+
+		configPkgBytes, err := proto.Marshal(nsBuilder.BuildCollectionConfig())
+		require.NoError(t, err)
+
+		state[lscc][privdata.BuildCollectionKVSKey(ns1)] = configPkgBytes
+
+		err = blockPublisher.handleUpgrade(1001, "tx1", ns1)
+		assert.NoError(t, err)
+
+		// Make sure the new config is loaded
+		config, err := r.Config(ns1, coll2)
+		require.NoError(t, err)
+		assert.Equal(t, coll2, config.Name)
+		assert.Equal(t, int32(4), config.RequiredPeerCount)
+		assert.Equal(t, common.CollectionType_COL_TRANSIENT, config.Type)
+		assert.Equal(t, "10m", config.TimeToLive)
+
+		policy, err := r.Policy(ns1, coll2)
+		require.NoError(t, err)
+		require.NotNil(t, policy)
+		assert.Equal(t, 3, len(policy.MemberOrgs()))
+
+		policy, err = r.Policy(ns1, coll1)
+		require.NoError(t, err)
+		require.NotNil(t, policy)
+		assert.Equal(t, 3, len(policy.MemberOrgs()))
+	})
+}
+
+func TestConfigRetrieverError(t *testing.T) {
+	const channelID = "testchannel"
+
+	state := make(map[string]map[string][]byte)
+
+	blockPublisher := &mockBlockPublisher{}
+
+	expectedErr := fmt.Errorf("injected error")
+	r := NewCollectionConfigRetriever(channelID, &mocks.Ledger{
+		QueryExecutor: mocks.NewQueryExecutor(state).WithError(expectedErr),
+	}, blockPublisher)
+	require.NotNil(t, r)
+
+	t.Run("Policy", func(t *testing.T) {
+		policy, err := r.Policy(ns1, coll2)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		assert.Nil(t, policy)
+	})
+
+	t.Run("Config", func(t *testing.T) {
+		config, err := r.Config(ns1, coll2)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+		assert.Nil(t, config)
+	})
+}
+
+type mockBlockPublisher struct {
+	handleUpgrade gossipapi.ChaincodeUpgradeHandler
+}
+
+func (m *mockBlockPublisher) AddCCUpgradeHandler(handler gossipapi.ChaincodeUpgradeHandler) {
+	m.handleUpgrade = handler
+}
+
+func (m *mockBlockPublisher) AddConfigUpdateHandler(handler gossipapi.ConfigUpdateHandler) {
+	panic("not implemented")
+}
+
+func (m *mockBlockPublisher) AddWriteHandler(handler gossipapi.WriteHandler) {
+	panic("not implemented")
+}
+
+func (m *mockBlockPublisher) AddReadHandler(handler gossipapi.ReadHandler) {
+	panic("not implemented")
+}
+
+func (m *mockBlockPublisher) AddCCEventHandler(handler gossipapi.ChaincodeEventHandler) {
+	panic("not implemented")
+}
+
+func (m *mockBlockPublisher) Publish(block *cb.Block) {
+}
diff --git a/extensions/common/support/support.go b/extensions/common/support/support.go
new file mode 100644
index 00000000..68d1b71b
--- /dev/null
+++ b/extensions/common/support/support.go
@@ -0,0 +1,66 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	"github.com/bluele/gcache"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/protos/common"
+)
+
+var logger = flogging.MustGetLogger("kevlar-common")
+
+type ledgerProvider func(channelID string) ledger.PeerLedger
+type blockPublisherProvider func(channelID string) gossipapi.BlockPublisher
+
+// Support holds the ledger provider and the cache
+type Support struct {
+	getLedger              ledgerProvider
+	configRetrieverCache   gcache.Cache
+	blockPublisherProvider blockPublisherProvider
+}
+
+// New creates a new Support using the ledger provider
+func New(ledgerProvider ledgerProvider, blockPublisherProvider blockPublisherProvider) *Support {
+	s := &Support{
+		getLedger:              ledgerProvider,
+		blockPublisherProvider: blockPublisherProvider,
+	}
+	s.configRetrieverCache = gcache.New(0).Simple().LoaderFunc(
+		func(key interface{}) (interface{}, error) {
+			channelID := key.(string)
+			logger.Debugf("[%s] Creating collection config retriever", channelID)
+			return NewCollectionConfigRetriever(channelID, s.getLedger(channelID), blockPublisherProvider(channelID)), nil
+		}).Build()
+	return s
+}
+
+// Config returns the configuration for the given collection
+func (s *Support) Config(channelID, ns, coll string) (*common.StaticCollectionConfig, error) {
+	ccRetriever, err := s.configRetrieverCache.Get(channelID)
+	if err != nil {
+		return nil, err
+	}
+	return ccRetriever.(*CollectionConfigRetriever).Config(ns, coll)
+}
+
+// Policy returns the collection access policy for the given collection
+func (s *Support) Policy(channelID, ns, coll string) (privdata.CollectionAccessPolicy, error) {
+	ccRetriever, err := s.configRetrieverCache.Get(channelID)
+	if err != nil {
+		return nil, err
+	}
+	return ccRetriever.(*CollectionConfigRetriever).Policy(ns, coll)
+}
+
+// BlockPublisher returns the block publisher for the given channel
+func (s *Support) BlockPublisher(channelID string) gossipapi.BlockPublisher {
+	return s.blockPublisherProvider(channelID)
+}
diff --git a/extensions/common/support/support_test.go b/extensions/common/support/support_test.go
new file mode 100644
index 00000000..4f1a2313
--- /dev/null
+++ b/extensions/common/support/support_test.go
@@ -0,0 +1,75 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package support
+
+import (
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	gossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	ns1   = "chaincode1"
+	coll1 = "collection1"
+	coll2 = "collection2"
+)
+
+func TestSupport(t *testing.T) {
+	const channelID = "testchannel"
+	const lscc = "lscc"
+
+	nsBuilder := mocks.NewNamespaceBuilder(ns1)
+	nsBuilder.Collection(coll1).StaticConfig("OR ('Org1.member','Org2.member')", 3, 3, 100)
+	nsBuilder.Collection(coll2).TransientConfig("OR ('Org1.member','Org2.member')", 3, 3, "1m")
+
+	configPkgBytes, err := proto.Marshal(nsBuilder.BuildCollectionConfig())
+	require.NoError(t, err)
+
+	state := make(map[string]map[string][]byte)
+	state[lscc] = make(map[string][]byte)
+	state[lscc][privdata.BuildCollectionKVSKey(ns1)] = configPkgBytes
+
+	ledgerProvider := func(channelID string) ledger.PeerLedger {
+		return &mocks.Ledger{
+			QueryExecutor: mocks.NewQueryExecutor(state),
+		}
+	}
+
+	blockPublisherProvider := func(channelID string) gossipapi.BlockPublisher {
+		return &mockBlockPublisher{}
+	}
+
+	s := New(ledgerProvider, blockPublisherProvider)
+	require.NotNil(t, s)
+
+	t.Run("Policy", func(t *testing.T) {
+		policy, err := s.Policy(channelID, ns1, coll2)
+		require.NoError(t, err)
+		require.NotNil(t, policy)
+	})
+
+	t.Run("Config", func(t *testing.T) {
+		config, err := s.Config(channelID, ns1, coll2)
+		require.NoError(t, err)
+		assert.Equal(t, coll2, config.Name)
+		assert.Equal(t, int32(3), config.RequiredPeerCount)
+		assert.Equal(t, common.CollectionType_COL_TRANSIENT, config.Type)
+		assert.Equal(t, "1m", config.TimeToLive)
+	})
+
+	t.Run("BlockPublisher", func(t *testing.T) {
+		p := s.BlockPublisher(channelID)
+		require.NotNil(t, p)
+	})
+}
diff --git a/extensions/config/config.go b/extensions/config/config.go
new file mode 100644
index 00000000..b023d345
--- /dev/null
+++ b/extensions/config/config.go
@@ -0,0 +1,397 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package config
+
+import (
+	"path/filepath"
+	"time"
+
+	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	"github.com/spf13/viper"
+)
+
+const confCouchDB = "CouchDB"
+const confGoleveldb = "goleveldb"
+const confMissingPvtdataKeysStore = "missingPvtdataKeysStore"
+const confStateKeyleveldb = "stateKeyLeveldb"
+const confBlockCacheSize = "ledger.blockchain.blockCacheSize"
+const confKVCacheSize = "ledger.blockchain.kvCacheSize"
+const confPvtDataCacheSize = "ledger.blockchain.pvtDataStorage.cacheSize"
+const confKVCacheBlocksToLive = "ledger.blockchain.kvCacheBlocksToLive"
+const confKVCacheNonDurableSize = "ledger.blockchain.kvCacheNonDurableSize"
+const confBlockStorage = "ledger.blockchain.blockStorage"
+const confHistoryStorage = "ledger.state.historyStorage"
+const confTransientStorage = "ledger.blockchain.transientStorage"
+const confConfigHistoryStorage = "ledger.blockchain.configHistoryStorage"
+const confRoles = "ledger.roles"
+const confConcurrentBlockWrites = "ledger.concurrentBlockWrites"
+const confValidationMinWaitTime = "ledger.blockchain.validation.minwaittime"
+const confTransientDataLeveldb = "transientDataLeveldb"
+const confTransientDataCleanupIntervalTime = "ledger.transientdata.cleanupExpired.Interval"
+const confTransientDataCacheSize = "ledger.transientdata.cacheSize"
+const confTransientDataPullTimeout = "peer.gossip.transientData.pullTimeout"
+const confOLCollLeveldb = "offLedgerLeveldb"
+const confOLCollCleanupIntervalTime = "offledger.cleanupExpired.Interval"
+const confOLCollMaxPeersForRetrieval = "offledger.maxpeers"
+const confOLCollPullTimeout = "offledger.gossip.pullTimeout"
+const confOLCollCacheSize = "offledger.cacheSize"
+
+// TODO: Dynamic configuration
+// Configure channels that will be observed for sidetree txn
+const confObserverChannels = "ledger.sidetree.observer.channels"
+
+// TODO: couchDB config should be in a common section rather than being under state.
+const confCouchDBMaxIdleConns = "ledger.state.couchDBConfig.maxIdleConns"
+const confCouchDBMaxIdleConnsPerHost = "ledger.state.couchDBConfig.maxIdleConnsPerHost"
+const confCouchDBIdleConnTimeout = "ledger.state.couchDBConfig.idleConnTimeout"
+const confCouchDBKeepAliveTimeout = "ledger.state.couchDBConfig.keepAliveTimeout"
+const confCouchDBHTTPTraceEnabled = "ledger.state.couchDBConfig.httpTraceEnabled"
+const confPeerRestServerPort = "peer.restserver.port"
+const confPeerIdentityHubEnabled = "peer.identityhub.enabled"
+
+//default values
+const defaultValidationMinWaitTime = 50 * time.Millisecond
+const defaultTransientDataCleanupIntervalTime = 5 * time.Second
+const defaultTransientDataCacheSize = 100000
+const defaultTransientDataPullTimeout = 5 * time.Second
+const defaultOLCollCleanupIntervalTime = 5 * time.Second
+const defaultOLCollMaxPeersForRetrieval = 2
+const defaultOLCollPullTimeout = 5 * time.Second
+const defaultOLCollCacheSize = 10000
+const defaultRestServerPort = 443
+const defaultIdentityHubEnabled = false
+
+const confBlockStorageAttachTxn = "ledger.blockchain.blockStorage.attachTransaction"
+
+// BlockStorageProvider holds the configuration names of the available storage providers
+type BlockStorageProvider int
+
+const (
+	// FilesystemLedgerStorage stores blocks in a raw file with a LevelDB index (default)
+	FilesystemLedgerStorage BlockStorageProvider = iota
+	// CouchDBLedgerStorage stores blocks in CouchDB
+	CouchDBLedgerStorage
+)
+
+// PvtDataStorageProvider holds the configuration names of the available storage providers
+type PvtDataStorageProvider int
+
+const (
+	// LevelDBPvtDataStorage stores private data in LevelDB (default)
+	LevelDBPvtDataStorage PvtDataStorageProvider = iota
+	// CouchDBPvtDataStorage stores private data in CouchDB
+	CouchDBPvtDataStorage
+)
+
+// HistoryStorageProvider holds the configuration names of the available history storage providers
+type HistoryStorageProvider int
+
+const (
+	// LevelDBHistoryStorage stores history in LevelDB (default)
+	LevelDBHistoryStorage HistoryStorageProvider = iota
+	// CouchDBHistoryStorage stores history in CouchDB
+	CouchDBHistoryStorage
+)
+
+// TransientStorageProvider holds the configuration names of the available transient storage providers
+type TransientStorageProvider int
+
+const (
+	// LevelDBTransientStorage stores transient data in LevelDB (default)
+	LevelDBTransientStorage TransientStorageProvider = iota
+	// CouchDBTransientStorage stores transient data in CouchDB
+	CouchDBTransientStorage
+	// MemoryTransientStorage stores transient data in Memory
+	MemoryTransientStorage
+)
+
+// ConfigurationHistoryStorageProvider holds the configuration names of the available config history storage providers
+type ConfigurationHistoryStorageProvider int
+
+const (
+	// LevelDBConfigHistoryStorage stores config history data in LevelDB (default)
+	LevelDBConfigHistoryStorage ConfigurationHistoryStorageProvider = iota
+	// CouchDBConfigHistoryStorage stores config history data in CouchDB
+	CouchDBConfigHistoryStorage
+)
+
+// GetStateKeyLevelDBPath returns the filesystem path that is used to maintain the state key level db
+func GetStateKeyLevelDBPath() string {
+	return filepath.Join(ledgerconfig.GetRootPath(), confStateKeyleveldb)
+}
+
+// GetMissingPvtdataKeysStorePath returns the filesystem path that is used for permanent storage of private write-sets
+func GetMissingPvtdataKeysStorePath() string {
+	return filepath.Join(ledgerconfig.GetRootPath(), confMissingPvtdataKeysStore)
+}
+
+// GetCouchDBMaxIdleConns returns the number of idle connections to hold in the connection pool for couchDB.
+func GetCouchDBMaxIdleConns() int {
+	// TODO: this probably be the default golang version (100)
+	const defaultMaxIdleConns = 1000
+	if !viper.IsSet(confCouchDBMaxIdleConns) {
+		return defaultMaxIdleConns
+	}
+	return viper.GetInt(confCouchDBMaxIdleConns)
+}
+
+// GetCouchDBMaxIdleConnsPerHost returns the number of idle connections to allow per host in the connection pool for couchDB.
+func GetCouchDBMaxIdleConnsPerHost() int {
+	// TODO: this probably be the default golang version (http.DefaultMaxIdleConnsPerHost)
+	const defaultMaxIdleConnsPerHost = 100
+	if !viper.IsSet(confCouchDBMaxIdleConnsPerHost) {
+		return defaultMaxIdleConnsPerHost
+	}
+	return viper.GetInt(confCouchDBMaxIdleConnsPerHost)
+}
+
+// GetCouchDBIdleConnTimeout returns the duration before closing an idle connection.
+func GetCouchDBIdleConnTimeout() time.Duration {
+	const defaultIdleConnTimeout = 90 * time.Second
+	if !viper.IsSet(confCouchDBIdleConnTimeout) {
+		return defaultIdleConnTimeout
+	}
+	return viper.GetDuration(confCouchDBIdleConnTimeout)
+}
+
+// GetCouchDBKeepAliveTimeout returns the duration for keep alive.
+func GetCouchDBKeepAliveTimeout() time.Duration {
+	const defaultKeepAliveTimeout = 30 * time.Second
+	if !viper.IsSet(confCouchDBKeepAliveTimeout) {
+		return defaultKeepAliveTimeout
+	}
+	return viper.GetDuration(confCouchDBKeepAliveTimeout)
+}
+
+// GetBlockStoreProvider returns the block storage provider specified in the configuration
+func GetBlockStoreProvider() BlockStorageProvider {
+	blockStorageConfig := viper.GetString(confBlockStorage)
+	switch blockStorageConfig {
+	case confCouchDB:
+		return CouchDBLedgerStorage
+	default:
+		fallthrough
+	case "filesystem":
+		return FilesystemLedgerStorage
+	}
+}
+
+// GetBlockCacheSize returns the number of blocks to keep the in the LRU cache
+func GetBlockCacheSize() int {
+	blockCacheSize := viper.GetInt(confBlockCacheSize)
+	if !viper.IsSet(confBlockCacheSize) {
+		blockCacheSize = 5
+	}
+	return blockCacheSize
+}
+
+// GetPvtDataCacheSize returns the number of pvt data per block to keep the in the LRU cache
+func GetPvtDataCacheSize() int {
+	pvtDataCacheSize := viper.GetInt(confPvtDataCacheSize)
+	if !viper.IsSet(confPvtDataCacheSize) {
+		pvtDataCacheSize = 10
+	}
+	return pvtDataCacheSize
+}
+
+// GetKVCacheSize returns the cache size
+func GetKVCacheSize() int {
+	kvCacheSize := viper.GetInt(confKVCacheSize)
+	if !viper.IsSet(confKVCacheSize) {
+		kvCacheSize = 64 * 1024
+	}
+	return kvCacheSize
+}
+
+// GetKVCacheBlocksToLive returns the cache blocks to live
+func GetKVCacheBlocksToLive() uint64 {
+	if !viper.IsSet(confKVCacheBlocksToLive) {
+		return 120
+	}
+	return uint64(viper.GetInt(confKVCacheBlocksToLive))
+}
+
+// GetKVCacheNonDurableSize returns the cache non durable size
+func GetKVCacheNonDurableSize() int {
+	if !viper.IsSet(confKVCacheNonDurableSize) {
+		return 64 * 1024
+	}
+	return viper.GetInt(confKVCacheNonDurableSize)
+}
+
+// GetTransientStoreProvider returns the transient storage provider specified in the configuration
+func GetTransientStoreProvider() TransientStorageProvider {
+	transientStorageConfig := viper.GetString(confTransientStorage)
+	switch transientStorageConfig {
+	case confCouchDB:
+		return CouchDBTransientStorage
+	case "Memory":
+		return MemoryTransientStorage
+	default:
+		fallthrough
+	case confGoleveldb:
+		return LevelDBTransientStorage
+	}
+}
+
+// GetConfigHistoryStoreProvider returns the config history storage provider specified in the configuration
+func GetConfigHistoryStoreProvider() ConfigurationHistoryStorageProvider {
+	configHistoryStorageConfig := viper.GetString(confConfigHistoryStorage)
+	switch configHistoryStorageConfig {
+	case confCouchDB:
+		return CouchDBConfigHistoryStorage
+	default:
+		fallthrough
+	case confGoleveldb:
+		return LevelDBConfigHistoryStorage
+	}
+}
+
+// GetHistoryStoreProvider returns the history storage provider specified in the configuration
+func GetHistoryStoreProvider() HistoryStorageProvider {
+	historyStorageConfig := viper.GetString(confHistoryStorage)
+	switch historyStorageConfig {
+	case confCouchDB:
+		return CouchDBHistoryStorage
+	default:
+		fallthrough
+	case confGoleveldb:
+		return LevelDBHistoryStorage
+	}
+}
+
+// CouchDBHTTPTraceEnabled returns true if HTTP tracing is enabled for Couch DB
+func CouchDBHTTPTraceEnabled() bool {
+	return viper.GetBool(confCouchDBHTTPTraceEnabled)
+}
+
+// GetValidationMinWaitTime is used by the committer in distributed validation and is the minimum
+// time to wait for Tx validation responses from other validators.
+func GetValidationMinWaitTime() time.Duration {
+	timeout := viper.GetDuration(confValidationMinWaitTime)
+	if timeout == 0 {
+		return defaultValidationMinWaitTime
+	}
+	return timeout
+}
+
+// GetConcurrentBlockWrites is how many concurrent writes to db
+func GetConcurrentBlockWrites() int {
+	concurrentWrites := viper.GetInt(confConcurrentBlockWrites)
+	if !viper.IsSet(confConcurrentBlockWrites) {
+		return 1
+	}
+	return concurrentWrites
+}
+
+// GetTransientDataLevelDBPath returns the filesystem path that is used to maintain the transient data level db
+func GetTransientDataLevelDBPath() string {
+	return filepath.Join(ledgerconfig.GetRootPath(), confTransientDataLeveldb)
+}
+
+// GetTransientDataExpiredIntervalTime is time when background routine check expired transient data in db to cleanup.
+func GetTransientDataExpiredIntervalTime() time.Duration {
+	timeout := viper.GetDuration(confTransientDataCleanupIntervalTime)
+	if timeout == 0 {
+		return defaultTransientDataCleanupIntervalTime
+	}
+	return timeout
+}
+
+// GetTransientDataCacheSize returns the size of the transient data cache
+func GetTransientDataCacheSize() int {
+	size := viper.GetInt(confTransientDataCacheSize)
+	if size <= 0 {
+		return defaultTransientDataCacheSize
+	}
+	return size
+}
+
+// GetRoles returns the roles of the peer. Empty return value indicates that the peer has all roles.
+func GetRoles() string {
+	return viper.GetString(confRoles)
+}
+
+// GetRestServerPort returns the port used for the rest server in the peer
+func GetRestServerPort() int {
+	if !viper.IsSet(confPeerRestServerPort) {
+		return defaultRestServerPort
+	}
+	return viper.GetInt(confPeerRestServerPort)
+}
+
+// GetIdentityHubEnabled returns whether the identity hub is enabled in this peer
+func GetIdentityHubEnabled() bool {
+	if !viper.IsSet(confPeerIdentityHubEnabled) {
+		return defaultIdentityHubEnabled
+	}
+	return viper.GetBool(confPeerIdentityHubEnabled)
+}
+
+// GetObserverChannels returns the channels that will be observed for sidetree txn.
+func GetObserverChannels() string {
+	return viper.GetString(confObserverChannels)
+}
+
+// GetOLCollLevelDBPath returns the filesystem path that is used to maintain the off-ledger level db
+func GetOLCollLevelDBPath() string {
+	return filepath.Join(ledgerconfig.GetRootPath(), confOLCollLeveldb)
+}
+
+// GetOLCollExpirationCheckInterval is time when background routine check expired collection data in db to cleanup.
+func GetOLCollExpirationCheckInterval() time.Duration {
+	timeout := viper.GetDuration(confOLCollCleanupIntervalTime)
+	if timeout == 0 {
+		return defaultOLCollCleanupIntervalTime
+	}
+	return timeout
+}
+
+// GetOLCollMaxPeersForRetrieval returns the number of peers that should be messaged
+// to retrieve collection data that is not stored locally.
+func GetOLCollMaxPeersForRetrieval() int {
+	maxPeers := viper.GetInt(confOLCollMaxPeersForRetrieval)
+	if maxPeers <= 0 {
+		maxPeers = defaultOLCollMaxPeersForRetrieval
+	}
+	return maxPeers
+}
+
+// GetTransientDataPullTimeout is the amount of time a peer waits for a response from another peer for transient data.
+func GetTransientDataPullTimeout() time.Duration {
+	timeout := viper.GetDuration(confTransientDataPullTimeout)
+	if timeout == 0 {
+		timeout = defaultTransientDataPullTimeout
+	}
+	return timeout
+}
+
+// GetOLCollPullTimeout is the amount of time a peer waits for a response from another peer for transient data.
+func GetOLCollPullTimeout() time.Duration {
+	timeout := viper.GetDuration(confOLCollPullTimeout)
+	if timeout == 0 {
+		timeout = defaultOLCollPullTimeout
+	}
+	return timeout
+}
+
+// GetOLCollCacheSize returns the size of the off-ledger cache
+func GetOLCollCacheSize() int {
+	size := viper.GetInt(confOLCollCacheSize)
+	if size <= 0 {
+		return defaultOLCollCacheSize
+	}
+	return size
+}
+
+// GetBlockStorageAttachTxn returns whether or not the block storage provider should attach a copy
+// of the transaction to the transaction ID index.
+// TODO: this was made configurable to make it easier to measure the performance & storage differences.
+// TODO: based on the analysis, we might remove this configuration.
+func GetBlockStorageAttachTxn() bool {
+	return viper.GetBool(confBlockStorageAttachTxn)
+}
diff --git a/extensions/config/config_test.go b/extensions/config/config_test.go
new file mode 100644
index 00000000..87dff386
--- /dev/null
+++ b/extensions/config/config_test.go
@@ -0,0 +1,472 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package config
+
+import (
+	"os"
+	"testing"
+	"time"
+
+	ledgertestutil "github.com/hyperledger/fabric/core/ledger/testutil"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestGetBlockStoreProviderDefault(t *testing.T) {
+
+	oldVal := viper.Get(confBlockStorage)
+	defer viper.Set(confBlockStorage, oldVal)
+
+	viper.Set(confBlockStorage, "")
+	provider := GetBlockStoreProvider()
+	assert.Equal(t, provider, FilesystemLedgerStorage)
+
+	viper.Set(confBlockStorage, "CouchDB")
+	provider = GetBlockStoreProvider()
+	assert.Equal(t, provider, CouchDBLedgerStorage)
+
+	viper.Set(confBlockStorage, "filesystem")
+	provider = GetBlockStoreProvider()
+	assert.Equal(t, provider, FilesystemLedgerStorage)
+}
+
+func TestGetStateKeyLevelDBPath(t *testing.T) {
+	oldVal := viper.Get("peer.fileSystemPath")
+	defer viper.Set("peer.fileSystemPath", oldVal)
+
+	expected := "xyz123/ledgersData/stateKeyLeveldb"
+	viper.Set("peer.fileSystemPath", "xyz123")
+	val := GetStateKeyLevelDBPath()
+	assert.Equal(t, expected, val)
+
+}
+
+func TestGetMissingPvtdataKeysStorePatht(t *testing.T) {
+	oldVal := viper.Get("peer.fileSystemPath")
+	defer viper.Set("peer.fileSystemPath", oldVal)
+
+	expected := "xyz123/ledgersData/missingPvtdataKeysStore"
+	viper.Set("peer.fileSystemPath", "xyz123")
+	val := GetMissingPvtdataKeysStorePath()
+	assert.Equal(t, expected, val)
+
+}
+
+func TestGetCouchDBMaxIdleConns(t *testing.T) {
+	oldVal := viper.Get(confCouchDBMaxIdleConns)
+	defer viper.Set(confCouchDBMaxIdleConns, oldVal)
+
+	val := GetCouchDBMaxIdleConns()
+	assert.Equal(t, val, 1000)
+
+	viper.Set(confCouchDBMaxIdleConns, 2000)
+	val = GetCouchDBMaxIdleConns()
+	assert.Equal(t, val, 2000)
+
+}
+
+func TestGetCouchDBMaxIdleConnsPerHost(t *testing.T) {
+	oldVal := viper.Get(confCouchDBMaxIdleConnsPerHost)
+	defer viper.Set(confCouchDBMaxIdleConnsPerHost, oldVal)
+
+	val := GetCouchDBMaxIdleConnsPerHost()
+	assert.Equal(t, val, 100)
+
+	viper.Set(confCouchDBMaxIdleConnsPerHost, 200)
+	val = GetCouchDBMaxIdleConnsPerHost()
+	assert.Equal(t, val, 200)
+
+}
+
+func TestGetCouchDBIdleConnTimeout(t *testing.T) {
+	oldVal := viper.Get(confCouchDBIdleConnTimeout)
+	defer viper.Set(confCouchDBIdleConnTimeout, oldVal)
+
+	val := GetCouchDBIdleConnTimeout()
+	assert.Equal(t, val, 90*time.Second)
+
+	viper.Set(confCouchDBIdleConnTimeout, 99*time.Second)
+	val = GetCouchDBIdleConnTimeout()
+	assert.Equal(t, val, 99*time.Second)
+
+}
+
+func TestGetCouchDBKeepAliveTimeout(t *testing.T) {
+	oldVal := viper.Get(confCouchDBKeepAliveTimeout)
+	defer viper.Set(confCouchDBKeepAliveTimeout, oldVal)
+
+	val := GetCouchDBKeepAliveTimeout()
+	assert.Equal(t, val, 30*time.Second)
+
+	viper.Set(confCouchDBKeepAliveTimeout, 33*time.Second)
+	val = GetCouchDBKeepAliveTimeout()
+	assert.Equal(t, val, 33*time.Second)
+
+}
+
+func TestGetBlockStoreProviderFilesystem(t *testing.T) {
+	setUpCoreYAMLConfig()
+	defer ledgertestutil.ResetConfigToDefaultValues()
+	viper.Set("ledger.blockchain.blockStorage", "filesystem")
+	provider := GetBlockStoreProvider()
+	assert.Equal(t, provider, FilesystemLedgerStorage)
+}
+
+func TestGetBlockStoreProviderCouchDB(t *testing.T) {
+	setUpCoreYAMLConfig()
+	defer ledgertestutil.ResetConfigToDefaultValues()
+	viper.Set("ledger.blockchain.blockStorage", "CouchDB")
+	provider := GetBlockStoreProvider()
+	assert.Equal(t, provider, CouchDBLedgerStorage)
+}
+
+func TestGetBlockCacheSize(t *testing.T) {
+	oldVal := viper.Get(confBlockCacheSize)
+	defer viper.Set(confBlockCacheSize, oldVal)
+
+	val := GetBlockCacheSize()
+	assert.Equal(t, val, 5)
+
+	viper.Set(confBlockCacheSize, 9)
+	val = GetBlockCacheSize()
+	assert.Equal(t, val, 9)
+
+}
+
+func TestGetPvtDataCacheSize(t *testing.T) {
+	oldVal := viper.Get(confPvtDataCacheSize)
+	defer viper.Set(confPvtDataCacheSize, oldVal)
+
+	val := GetPvtDataCacheSize()
+	assert.Equal(t, val, 10)
+
+	viper.Set(confPvtDataCacheSize, 99)
+	val = GetPvtDataCacheSize()
+	assert.Equal(t, val, 99)
+
+}
+
+func TestGetKVCacheSize(t *testing.T) {
+	oldVal := viper.Get(confKVCacheSize)
+	defer viper.Set(confKVCacheSize, oldVal)
+
+	val := GetKVCacheSize()
+	assert.Equal(t, val, 64*1024)
+
+	viper.Set(confKVCacheSize, 128*1024)
+	val = GetKVCacheSize()
+	assert.Equal(t, val, 128*1024)
+
+}
+
+func TestGetKVCacheBlocksToLive(t *testing.T) {
+	oldVal := viper.Get(confKVCacheBlocksToLive)
+	defer viper.Set(confKVCacheBlocksToLive, oldVal)
+
+	val := GetKVCacheBlocksToLive()
+	assert.Equal(t, val, uint64(120))
+
+	viper.Set(confKVCacheBlocksToLive, 60)
+	val = GetKVCacheBlocksToLive()
+	assert.Equal(t, val, uint64(60))
+
+}
+
+func TestGetKVCacheNonDurableSize(t *testing.T) {
+	oldVal := viper.Get(confKVCacheNonDurableSize)
+	defer viper.Set(confKVCacheNonDurableSize, oldVal)
+
+	val := GetKVCacheNonDurableSize()
+	assert.Equal(t, val, 64*1024)
+
+	viper.Set(confKVCacheNonDurableSize, 128*1024)
+	val = GetKVCacheNonDurableSize()
+	assert.Equal(t, val, 128*1024)
+
+}
+
+func TestGetConfigHistoryStoreProvider(t *testing.T) {
+	oldVal := viper.Get(confConfigHistoryStorage)
+	defer viper.Set(confConfigHistoryStorage, oldVal)
+
+	viper.Set(confConfigHistoryStorage, "CouchDB")
+	val := GetConfigHistoryStoreProvider()
+	assert.Equal(t, CouchDBConfigHistoryStorage, val)
+
+	viper.Set(confConfigHistoryStorage, "goleveldb")
+	val = GetConfigHistoryStoreProvider()
+	assert.Equal(t, LevelDBConfigHistoryStorage, val)
+
+	viper.Set(confConfigHistoryStorage, "INVALID")
+	val = GetConfigHistoryStoreProvider()
+	assert.Equal(t, LevelDBConfigHistoryStorage, val)
+
+}
+
+func TestGetTransientStoreProvider(t *testing.T) {
+	oldVal := viper.Get(confTransientStorage)
+	defer viper.Set(confTransientStorage, oldVal)
+
+	viper.Set(confTransientStorage, "CouchDB")
+	val := GetTransientStoreProvider()
+	assert.Equal(t, CouchDBTransientStorage, val)
+
+	viper.Set(confTransientStorage, "Memory")
+	val = GetTransientStoreProvider()
+	assert.Equal(t, MemoryTransientStorage, val)
+
+	viper.Set(confTransientStorage, "goleveldb")
+	val = GetTransientStoreProvider()
+	assert.Equal(t, LevelDBTransientStorage, val)
+
+	viper.Set(confTransientStorage, "INVALID")
+	val = GetTransientStoreProvider()
+	assert.Equal(t, LevelDBTransientStorage, val)
+
+}
+
+func TestGetHistoryStoreProvider(t *testing.T) {
+	oldVal := viper.Get(confHistoryStorage)
+	defer viper.Set(confHistoryStorage, oldVal)
+
+	viper.Set(confHistoryStorage, "CouchDB")
+	val := GetHistoryStoreProvider()
+	assert.Equal(t, CouchDBHistoryStorage, val)
+
+	viper.Set(confHistoryStorage, "goleveldb")
+	val = GetHistoryStoreProvider()
+	assert.Equal(t, LevelDBHistoryStorage, val)
+
+	viper.Set(confHistoryStorage, "INVALID")
+	val = GetHistoryStoreProvider()
+	assert.Equal(t, LevelDBHistoryStorage, val)
+
+}
+
+func TestGetHistoryStoreProviderDefault(t *testing.T) {
+	provider := GetHistoryStoreProvider()
+	assert.Equal(t, provider, LevelDBHistoryStorage)
+}
+
+func TestGetHistoryStoreProviderLevelDB(t *testing.T) {
+	setUpCoreYAMLConfig()
+	defer ledgertestutil.ResetConfigToDefaultValues()
+	viper.Set("ledger.state.historyStorage", "goleveldb")
+	provider := GetHistoryStoreProvider()
+	assert.Equal(t, provider, LevelDBHistoryStorage)
+}
+
+func TestGetHistoryStoreProviderCouchDB(t *testing.T) {
+	setUpCoreYAMLConfig()
+	defer ledgertestutil.ResetConfigToDefaultValues()
+	viper.Set("ledger.state.historyStorage", "CouchDB")
+	provider := GetHistoryStoreProvider()
+	assert.Equal(t, provider, CouchDBHistoryStorage)
+}
+
+func TestCouchDBHTTPTraceEnabled(t *testing.T) {
+	oldVal := viper.Get(confCouchDBHTTPTraceEnabled)
+	defer viper.Set(confCouchDBHTTPTraceEnabled, oldVal)
+
+	viper.Set(confCouchDBHTTPTraceEnabled, true)
+	assert.True(t, CouchDBHTTPTraceEnabled())
+
+	viper.Set(confCouchDBHTTPTraceEnabled, false)
+	assert.False(t, CouchDBHTTPTraceEnabled())
+
+}
+
+func TestGetValidationMinWaitTime(t *testing.T) {
+	oldVal := viper.Get(confValidationMinWaitTime)
+	defer viper.Set(confValidationMinWaitTime, oldVal)
+
+	viper.Set(confValidationMinWaitTime, 0)
+	assert.Equal(t, defaultValidationMinWaitTime, GetValidationMinWaitTime())
+
+	viper.Set(confValidationMinWaitTime, 10*time.Second)
+	assert.Equal(t, 10*time.Second, GetValidationMinWaitTime())
+}
+
+func TestGetConcurrentBlockWrites(t *testing.T) {
+	oldVal := viper.Get(confConcurrentBlockWrites)
+	defer viper.Set(confConcurrentBlockWrites, oldVal)
+
+	assert.Equal(t, 1, GetConcurrentBlockWrites())
+	viper.Set(confConcurrentBlockWrites, 7)
+	assert.Equal(t, 7, GetConcurrentBlockWrites())
+}
+
+func TestGetTransientDataCacheSize(t *testing.T) {
+	oldVal := viper.Get(confTransientDataCacheSize)
+	defer viper.Set(confTransientDataCacheSize, oldVal)
+
+	os.Setenv("CORE_LEDGER_TRANSIENTDATA_CACHESIZE", "27")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, 27, GetTransientDataCacheSize())
+
+	viper.Set(confTransientDataCacheSize, 0)
+	assert.Equal(t, defaultTransientDataCacheSize, GetTransientDataCacheSize())
+
+	viper.Set(confTransientDataCacheSize, 10)
+	assert.Equal(t, 10, GetTransientDataCacheSize())
+}
+
+func TestGetTransientDataPullTimeout(t *testing.T) {
+	oldVal := viper.Get(confTransientDataPullTimeout)
+	defer viper.Set(confTransientDataPullTimeout, oldVal)
+
+	os.Setenv("CORE_PEER_GOSSIP_TRANSIENTDATA_PULLTIMEOUT", "11s")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, 11*time.Second, GetTransientDataPullTimeout())
+
+	viper.Set(confTransientDataPullTimeout, "")
+	assert.Equal(t, defaultTransientDataPullTimeout, GetTransientDataPullTimeout())
+
+	viper.Set(confTransientDataPullTimeout, 111*time.Second)
+	assert.Equal(t, 111*time.Second, GetTransientDataPullTimeout())
+}
+
+func TestGetTransientDataExpiredIntervalTime(t *testing.T) {
+	oldVal := viper.Get(confTransientDataCleanupIntervalTime)
+	defer viper.Set(confTransientDataCleanupIntervalTime, oldVal)
+
+	os.Setenv("CORE_LEDGER_TRANSIENTDATA_CLEANUPEXPIRED_INTERVAL", "11s")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, 11*time.Second, GetTransientDataExpiredIntervalTime())
+
+	viper.Set(confTransientDataCleanupIntervalTime, "")
+	assert.Equal(t, defaultTransientDataCleanupIntervalTime, GetTransientDataExpiredIntervalTime())
+
+	viper.Set(confTransientDataCleanupIntervalTime, 111*time.Second)
+	assert.Equal(t, 111*time.Second, GetTransientDataExpiredIntervalTime())
+}
+
+func TestGetDCASExpiredIntervalTime(t *testing.T) {
+	oldVal := viper.Get(confOLCollCleanupIntervalTime)
+	defer viper.Set(confOLCollCleanupIntervalTime, oldVal)
+
+	os.Setenv("CORE_OFFLEDGER_CLEANUPEXPIRED_INTERVAL", "11s")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, 11*time.Second, GetOLCollExpirationCheckInterval())
+
+	viper.Set(confOLCollCleanupIntervalTime, "")
+	assert.Equal(t, defaultOLCollCleanupIntervalTime, GetOLCollExpirationCheckInterval())
+
+	viper.Set(confOLCollCleanupIntervalTime, 111*time.Second)
+	assert.Equal(t, 111*time.Second, GetOLCollExpirationCheckInterval())
+}
+
+func TestGetDCASLevelDBPath(t *testing.T) {
+	oldVal := viper.Get("peer.fileSystemPath")
+	defer viper.Set("peer.fileSystemPath", oldVal)
+
+	viper.Set("peer.fileSystemPath", "/tmp123")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, "/tmp123/ledgersData/offLedgerLeveldb", GetOLCollLevelDBPath())
+}
+
+func TestGetDCASPullTimeout(t *testing.T) {
+	oldVal := viper.Get(confOLCollPullTimeout)
+	defer viper.Set(confOLCollPullTimeout, oldVal)
+
+	os.Setenv("CORE_OFFLEDGER_GOSSIP_PULLTIMEOUT", "11s")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, 11*time.Second, GetOLCollPullTimeout())
+
+	viper.Set(confOLCollPullTimeout, "")
+	assert.Equal(t, defaultOLCollPullTimeout, GetOLCollPullTimeout())
+
+	viper.Set(confOLCollPullTimeout, 111*time.Second)
+	assert.Equal(t, 111*time.Second, GetOLCollPullTimeout())
+}
+
+func TestGetDCASMaxPeersForRetrieval(t *testing.T) {
+	oldVal := viper.Get(confOLCollMaxPeersForRetrieval)
+	defer viper.Set(confOLCollMaxPeersForRetrieval, oldVal)
+
+	os.Setenv("CORE_OFFLEDGER_MAXPEERS", "3")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, 3, GetOLCollMaxPeersForRetrieval())
+
+	viper.Set(confOLCollMaxPeersForRetrieval, "")
+	assert.Equal(t, defaultOLCollMaxPeersForRetrieval, GetOLCollMaxPeersForRetrieval())
+
+	viper.Set(confOLCollMaxPeersForRetrieval, 7)
+	assert.Equal(t, 7, GetOLCollMaxPeersForRetrieval())
+}
+
+func TestGetDCASCacheSize(t *testing.T) {
+	oldVal := viper.Get(confOLCollCacheSize)
+	defer viper.Set(confOLCollCacheSize, oldVal)
+
+	os.Setenv("CORE_OFFLEDGER_CACHESIZE", "27")
+	setUpCoreYAMLConfig()
+
+	assert.Equal(t, 27, GetOLCollCacheSize())
+
+	viper.Set(confOLCollCacheSize, 0)
+	assert.Equal(t, defaultOLCollCacheSize, GetOLCollCacheSize())
+
+	viper.Set(confOLCollCacheSize, 10)
+	assert.Equal(t, 10, GetOLCollCacheSize())
+}
+
+func TestGetRestServerPort(t *testing.T) {
+	oldVal := viper.Get(confPeerRestServerPort)
+	defer viper.Set(confPeerRestServerPort, oldVal)
+
+	viper.Set(confPeerRestServerPort, 443)
+	val := GetRestServerPort()
+	assert.Equal(t, defaultRestServerPort, val)
+
+	viper.Set(confPeerRestServerPort, 3000)
+	val = GetRestServerPort()
+	assert.Equal(t, val, 3000)
+}
+
+func TestGetIdentityHubEnabled(t *testing.T) {
+	oldVal := viper.Get(confPeerIdentityHubEnabled)
+	defer viper.Set(confPeerIdentityHubEnabled, oldVal)
+
+	viper.Set(confPeerIdentityHubEnabled, false)
+	val := GetIdentityHubEnabled()
+	assert.Equal(t, defaultIdentityHubEnabled, val)
+
+	viper.Set(confPeerIdentityHubEnabled, true)
+	val = GetIdentityHubEnabled()
+	assert.True(t, val)
+}
+
+func TestGetRoles(t *testing.T) {
+	oldVal := viper.Get(confRoles)
+	defer viper.Set(confRoles, oldVal)
+
+	roles := "endorser,observer"
+	viper.Set(confRoles, roles)
+	assert.Equal(t, roles, GetRoles())
+}
+
+func TestGetObservingChannels(t *testing.T) {
+	oldVal := viper.Get(confObserverChannels)
+	defer viper.Set(confObserverChannels, oldVal)
+
+	channels := "ch1,ch2"
+	viper.Set(confObserverChannels, channels)
+	assert.Equal(t, channels, GetObserverChannels())
+}
+
+func setUpCoreYAMLConfig() {
+	//call a helper method to load the core.yaml
+	ledgertestutil.SetupCoreYAMLConfig()
+}
diff --git a/extensions/gossip/api/gossipapi.go b/extensions/gossip/api/gossipapi.go
new file mode 100644
index 00000000..6b0b88f9
--- /dev/null
+++ b/extensions/gossip/api/gossipapi.go
@@ -0,0 +1,44 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package api
+
+import (
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+)
+
+// ConfigUpdateHandler handles a config update
+type ConfigUpdateHandler func(blockNum uint64, configUpdate *cb.ConfigUpdate) error
+
+// WriteHandler handles a KV write
+type WriteHandler func(blockNum uint64, txID string, namespace string, kvWrite *kvrwset.KVWrite) error
+
+// ReadHandler handles a KV read
+type ReadHandler func(blockNum uint64, txID string, namespace string, kvRead *kvrwset.KVRead) error
+
+// ChaincodeEventHandler handles a chaincode event
+type ChaincodeEventHandler func(blockNum uint64, txID string, event *pb.ChaincodeEvent) error
+
+// ChaincodeUpgradeHandler handles chaincode upgrade events
+type ChaincodeUpgradeHandler func(blockNum uint64, txID string, chaincodeName string) error
+
+// BlockPublisher allows clients to add handlers for various block events
+type BlockPublisher interface {
+	// AddCCUpgradeHandler adds a handler for chaincode upgrades
+	AddCCUpgradeHandler(handler ChaincodeUpgradeHandler)
+	// AddConfigUpdateHandler adds a handler for config updates
+	AddConfigUpdateHandler(handler ConfigUpdateHandler)
+	// AddWriteHandler adds a handler for KV writes
+	AddWriteHandler(handler WriteHandler)
+	// AddReadHandler adds a handler for KV reads
+	AddReadHandler(handler ReadHandler)
+	// AddCCEventHandler adds a handler for chaincode events
+	AddCCEventHandler(handler ChaincodeEventHandler)
+	// Publish traverses the block and invokes all applicable handlers
+	Publish(block *cb.Block)
+}
diff --git a/extensions/gossip/blockpublisher/blockpublisher.go b/extensions/gossip/blockpublisher/blockpublisher.go
new file mode 100644
index 00000000..cae53904
--- /dev/null
+++ b/extensions/gossip/blockpublisher/blockpublisher.go
@@ -0,0 +1,511 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package blockpublisher
+
+import (
+	"sync"
+	"sync/atomic"
+
+	"github.com/bluele/gcache"
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	ledgerutil "github.com/hyperledger/fabric/core/ledger/util"
+	"github.com/hyperledger/fabric/extensions/gossip/api"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/hyperledger/fabric/protos/utils"
+	"github.com/pkg/errors"
+)
+
+const (
+	lsccID       = "lscc"
+	upgradeEvent = "upgrade"
+)
+
+var logger = flogging.MustGetLogger("kevlar_gossip")
+
+type write struct {
+	blockNum  uint64
+	txID      string
+	namespace string
+	w         *kvrwset.KVWrite
+}
+
+type read struct {
+	blockNum  uint64
+	txID      string
+	namespace string
+	r         *kvrwset.KVRead
+}
+
+type ccEvent struct {
+	blockNum uint64
+	txID     string
+	event    *pb.ChaincodeEvent
+}
+
+type configUpdate struct {
+	blockNum     uint64
+	configUpdate *cb.ConfigUpdate
+}
+
+// Provider maintains a cache of Block Publishers - one per channel
+type Provider struct {
+	cache gcache.Cache
+}
+
+// NewProvider returns a new block publisher provider
+func NewProvider() *Provider {
+	return &Provider{
+		cache: gcache.New(0).LoaderFunc(func(channelID interface{}) (interface{}, error) {
+			return New(channelID.(string)), nil
+		}).Build(),
+	}
+}
+
+// ForChannel returns the block publisher for the given channel
+func (p *Provider) ForChannel(channelID string) api.BlockPublisher {
+	publisher, err := p.cache.Get(channelID)
+	if err != nil {
+		// This should never happen
+		panic(err.Error())
+	}
+	return publisher.(*Publisher)
+}
+
+// Close closes all block publishers
+func (p *Provider) Close() {
+	for _, publisher := range p.cache.GetALL() {
+		publisher.(*Publisher).Close()
+	}
+}
+
+// Publisher traverses a block and publishes KV read, KV write, and chaincode events to registered handlers
+type Publisher struct {
+	channelID            string
+	writeHandlers        []api.WriteHandler
+	readHandlers         []api.ReadHandler
+	ccEventHandlers      []api.ChaincodeEventHandler
+	configUpdateHandlers []api.ConfigUpdateHandler
+	mutex                sync.RWMutex
+	blockChan            chan *cb.Block
+	wChan                chan *write
+	rChan                chan *read
+	ccEvtChan            chan *ccEvent
+	configUpdateChan     chan *configUpdate
+	doneChan             chan struct{}
+	closed               uint32
+}
+
+// New returns a new block Publisher for the given channel
+func New(channelID string) *Publisher {
+	p := &Publisher{
+		channelID:        channelID,
+		blockChan:        make(chan *cb.Block, 10), // FIXME: Make buffer configurable
+		wChan:            make(chan *write, 10),    // FIXME: Make buffer configurable
+		rChan:            make(chan *read, 10),
+		ccEvtChan:        make(chan *ccEvent, 10),
+		configUpdateChan: make(chan *configUpdate, 10),
+		doneChan:         make(chan struct{}),
+	}
+	go p.listen()
+	return p
+}
+
+// Close releases all resources associated with the Publisher. Calling this function
+// multiple times has no effect.
+func (p *Publisher) Close() {
+	if atomic.CompareAndSwapUint32(&p.closed, 0, 1) {
+		p.doneChan <- struct{}{}
+	} else {
+		logger.Debugf("[%s] Block Publisher already closed", p.channelID)
+	}
+}
+
+// AddConfigUpdateHandler adds a handler for config update events
+func (p *Publisher) AddConfigUpdateHandler(handler api.ConfigUpdateHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding config update", p.channelID)
+	p.configUpdateHandlers = append(p.configUpdateHandlers, handler)
+}
+
+// AddWriteHandler adds a new handler for KV writes
+func (p *Publisher) AddWriteHandler(handler api.WriteHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding write", p.channelID)
+	p.writeHandlers = append(p.writeHandlers, handler)
+}
+
+// AddReadHandler adds a new handler for KV reads
+func (p *Publisher) AddReadHandler(handler api.ReadHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding read", p.channelID)
+	p.readHandlers = append(p.readHandlers, handler)
+}
+
+// AddCCEventHandler adds a new handler for chaincode events
+func (p *Publisher) AddCCEventHandler(handler api.ChaincodeEventHandler) {
+	p.mutex.Lock()
+	defer p.mutex.Unlock()
+
+	logger.Debugf("[%s] Adding chaincode event", p.channelID)
+	p.ccEventHandlers = append(p.ccEventHandlers, handler)
+}
+
+// AddCCUpgradeHandler adds a handler for chaincode upgrade events
+func (p *Publisher) AddCCUpgradeHandler(handler api.ChaincodeUpgradeHandler) {
+	logger.Debugf("[%s] Adding chaincode upgrade", p.channelID)
+	p.AddCCEventHandler(newChaincodeUpgradeHandler(p.channelID, handler))
+}
+
+// Publish publishes a block
+func (p *Publisher) Publish(block *cb.Block) {
+	newBlockEvent(p.channelID, block, p.wChan, p.rChan, p.ccEvtChan, p.configUpdateChan).publish()
+}
+
+func (p *Publisher) listen() {
+	for {
+		select {
+		case w := <-p.wChan:
+			p.handleWrite(w)
+		case r := <-p.rChan:
+			p.handleRead(r)
+		case ccEvt := <-p.ccEvtChan:
+			p.handleCCEvent(ccEvt)
+		case cu := <-p.configUpdateChan:
+			p.handleConfigUpdate(cu)
+		case <-p.doneChan:
+			logger.Debugf("[%s] Exiting block Publisher", p.channelID)
+			return
+		}
+	}
+}
+
+func (p *Publisher) handleRead(r *read) {
+	logger.Debugf("[%s] Handling read: [%s]", p.channelID, r)
+	for _, handleRead := range p.getReadHandlers() {
+		if err := handleRead(r.blockNum, r.txID, r.namespace, r.r); err != nil {
+			logger.Warningf("[%s] Error returned from KV read handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) handleWrite(w *write) {
+	logger.Debugf("[%s] Handling write: [%s]", p.channelID, w)
+	for _, handleWrite := range p.getWriteHandlers() {
+		if err := handleWrite(w.blockNum, w.txID, w.namespace, w.w); err != nil {
+			logger.Warningf("[%s] Error returned from KV write handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) handleCCEvent(event *ccEvent) {
+	logger.Debugf("[%s] Handling chaincode event: [%s]", p.channelID, event)
+	for _, handleCCEvent := range p.getCCEventHandlers() {
+		if err := handleCCEvent(event.blockNum, event.txID, event.event); err != nil {
+			logger.Warningf("[%s] Error returned from CC event handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) handleConfigUpdate(cu *configUpdate) {
+	logger.Debugf("[%s] Handling config update [%s]", p.channelID, cu)
+	for _, handleConfigUpdate := range p.getConfigUpdateHandlers() {
+		if err := handleConfigUpdate(cu.blockNum, cu.configUpdate); err != nil {
+			logger.Warningf("[%s] Error returned from config update handler: %s", p.channelID, err)
+		}
+	}
+}
+
+func (p *Publisher) getReadHandlers() []api.ReadHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.ReadHandler, len(p.readHandlers))
+	copy(handlers, p.readHandlers)
+	return handlers
+}
+
+func (p *Publisher) getWriteHandlers() []api.WriteHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.WriteHandler, len(p.writeHandlers))
+	copy(handlers, p.writeHandlers)
+	return handlers
+}
+
+func (p *Publisher) getCCEventHandlers() []api.ChaincodeEventHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.ChaincodeEventHandler, len(p.ccEventHandlers))
+	copy(handlers, p.ccEventHandlers)
+	return handlers
+}
+
+func (p *Publisher) getConfigUpdateHandlers() []api.ConfigUpdateHandler {
+	p.mutex.RLock()
+	defer p.mutex.RUnlock()
+
+	handlers := make([]api.ConfigUpdateHandler, len(p.configUpdateHandlers))
+	copy(handlers, p.configUpdateHandlers)
+	return handlers
+}
+
+type blockEvent struct {
+	channelID        string
+	block            *cb.Block
+	wChan            chan<- *write
+	rChan            chan<- *read
+	ccEvtChan        chan<- *ccEvent
+	configUpdateChan chan<- *configUpdate
+}
+
+func newBlockEvent(channelID string, block *cb.Block, wChan chan<- *write, rChan chan<- *read, ccEvtChan chan<- *ccEvent, configUpdateChan chan<- *configUpdate) *blockEvent {
+	return &blockEvent{
+		channelID:        channelID,
+		block:            block,
+		wChan:            wChan,
+		rChan:            rChan,
+		ccEvtChan:        ccEvtChan,
+		configUpdateChan: configUpdateChan,
+	}
+}
+
+func (p *blockEvent) publish() {
+	logger.Debugf("[%s] Publishing block #%d", p.channelID, p.block.Header.Number)
+	for i := range p.block.Data.Data {
+		envelope, err := utils.ExtractEnvelope(p.block, i)
+		if err != nil {
+			logger.Warningf("[%s] Error extracting envelope at index %d in block %d: %s", p.channelID, i, p.block.Header.Number, err)
+		} else {
+			err = p.visitEnvelope(i, envelope)
+			if err != nil {
+				logger.Warningf("[%s] Error checking envelope at index %d in block %d: %s", p.channelID, i, p.block.Header.Number, err)
+			}
+		}
+	}
+}
+
+func (p *blockEvent) visitEnvelope(i int, envelope *cb.Envelope) error {
+	payload, err := utils.ExtractPayload(envelope)
+	if err != nil {
+		return err
+	}
+
+	chdr, err := utils.UnmarshalChannelHeader(payload.Header.ChannelHeader)
+	if err != nil {
+		return err
+	}
+
+	if cb.HeaderType(chdr.Type) == cb.HeaderType_ENDORSER_TRANSACTION {
+		txFilter := ledgerutil.TxValidationFlags(p.block.Metadata.Metadata[cb.BlockMetadataIndex_TRANSACTIONS_FILTER])
+		code := txFilter.Flag(i)
+		if code != pb.TxValidationCode_VALID {
+			logger.Debugf("[%s] Transaction at index %d in block %d is not valid. Status code: %s", p.channelID, i, p.block.Header.Number, code)
+			return nil
+		}
+		tx, err := utils.GetTransaction(payload.Data)
+		if err != nil {
+			return err
+		}
+		newTxEvent(p.channelID, p.block.Header.Number, chdr.TxId, tx, p.wChan, p.rChan, p.ccEvtChan).publish()
+		return nil
+	}
+
+	if cb.HeaderType(chdr.Type) == cb.HeaderType_CONFIG_UPDATE {
+		envelope := &cb.ConfigUpdateEnvelope{}
+		if err := proto.Unmarshal(payload.Data, envelope); err != nil {
+			return err
+		}
+		newConfigUpdateEvent(p.channelID, p.block.Header.Number, envelope, p.configUpdateChan).publish()
+		return nil
+	}
+
+	return nil
+}
+
+type txEvent struct {
+	channelID string
+	blockNum  uint64
+	txID      string
+	tx        *pb.Transaction
+	wChan     chan<- *write
+	rChan     chan<- *read
+	ccEvtChan chan<- *ccEvent
+}
+
+func newTxEvent(channelID string, blockNum uint64, txID string, tx *pb.Transaction, wChan chan<- *write, rChan chan<- *read, ccEvtChan chan<- *ccEvent) *txEvent {
+	return &txEvent{
+		channelID: channelID,
+		blockNum:  blockNum,
+		txID:      txID,
+		tx:        tx,
+		wChan:     wChan,
+		rChan:     rChan,
+		ccEvtChan: ccEvtChan,
+	}
+}
+
+func (p *txEvent) publish() {
+	logger.Debugf("[%s] Publishing Tx %s in block #%d", p.channelID, p.txID, p.blockNum)
+	for i, action := range p.tx.Actions {
+		err := p.visitTXAction(action)
+		if err != nil {
+			logger.Warningf("[%s] Error checking TxAction at index %d: %s", p.channelID, i, err)
+		}
+	}
+}
+
+func (p *txEvent) visitTXAction(action *pb.TransactionAction) error {
+	chaPayload, err := utils.GetChaincodeActionPayload(action.Payload)
+	if err != nil {
+		return err
+	}
+	return p.visitChaincodeActionPayload(chaPayload)
+}
+
+func (p *txEvent) visitChaincodeActionPayload(chaPayload *pb.ChaincodeActionPayload) error {
+	cpp := &pb.ChaincodeProposalPayload{}
+	err := proto.Unmarshal(chaPayload.ChaincodeProposalPayload, cpp)
+	if err != nil {
+		return err
+	}
+
+	return p.visitAction(chaPayload.Action)
+}
+
+func (p *txEvent) visitAction(action *pb.ChaincodeEndorsedAction) error {
+	prp := &pb.ProposalResponsePayload{}
+	err := proto.Unmarshal(action.ProposalResponsePayload, prp)
+	if err != nil {
+		return err
+	}
+	return p.visitProposalResponsePayload(prp)
+}
+
+func (p *txEvent) visitProposalResponsePayload(prp *pb.ProposalResponsePayload) error {
+	chaincodeAction := &pb.ChaincodeAction{}
+	err := proto.Unmarshal(prp.Extension, chaincodeAction)
+	if err != nil {
+		return err
+	}
+	return p.visitChaincodeAction(chaincodeAction)
+}
+
+func (p *txEvent) visitChaincodeAction(chaincodeAction *pb.ChaincodeAction) error {
+	if len(chaincodeAction.Results) > 0 {
+		txRWSet := &rwsetutil.TxRwSet{}
+		if err := txRWSet.FromProtoBytes(chaincodeAction.Results); err != nil {
+			return err
+		}
+		p.visitTxReadWriteSet(txRWSet)
+	}
+
+	if len(chaincodeAction.Events) > 0 {
+		evt := &pb.ChaincodeEvent{}
+		if err := proto.Unmarshal(chaincodeAction.Events, evt); err != nil {
+			logger.Warningf("[%s] Invalid chaincode event for chaincode [%s]", p.channelID, chaincodeAction.ChaincodeId)
+			return errors.Wrapf(err, "invalid chaincode event for chaincode [%s]", chaincodeAction.ChaincodeId)
+		}
+		p.ccEvtChan <- &ccEvent{
+			blockNum: p.blockNum,
+			txID:     p.txID,
+			event:    evt,
+		}
+	}
+
+	return nil
+}
+
+func (p *txEvent) visitTxReadWriteSet(txRWSet *rwsetutil.TxRwSet) {
+	for _, nsRWSet := range txRWSet.NsRwSets {
+		p.visitNsReadWriteSet(nsRWSet)
+	}
+}
+
+func (p *txEvent) visitNsReadWriteSet(nsRWSet *rwsetutil.NsRwSet) {
+	for _, r := range nsRWSet.KvRwSet.Reads {
+		p.rChan <- &read{
+			blockNum:  p.blockNum,
+			txID:      p.txID,
+			namespace: nsRWSet.NameSpace,
+			r:         r,
+		}
+	}
+	for _, w := range nsRWSet.KvRwSet.Writes {
+		p.wChan <- &write{
+			blockNum:  p.blockNum,
+			txID:      p.txID,
+			namespace: nsRWSet.NameSpace,
+			w:         w,
+		}
+	}
+}
+
+type configUpdateEvent struct {
+	channelID        string
+	blockNum         uint64
+	envelope         *cb.ConfigUpdateEnvelope
+	configUpdateChan chan<- *configUpdate
+}
+
+func newConfigUpdateEvent(channelID string, blockNum uint64, envelope *cb.ConfigUpdateEnvelope, configUpdateChan chan<- *configUpdate) *configUpdateEvent {
+	return &configUpdateEvent{
+		channelID:        channelID,
+		blockNum:         blockNum,
+		envelope:         envelope,
+		configUpdateChan: configUpdateChan,
+	}
+}
+
+func (p *configUpdateEvent) publish() {
+	cu := &cb.ConfigUpdate{}
+	if err := proto.Unmarshal(p.envelope.ConfigUpdate, cu); err != nil {
+		logger.Warningf("[%s] Error unmarshalling config update: %s", p.channelID, err)
+		return
+	}
+
+	logger.Debugf("[%s] Publishing Config Update [%s] in block #%d", p.channelID, cu, p.blockNum)
+
+	p.configUpdateChan <- &configUpdate{
+		blockNum:     p.blockNum,
+		configUpdate: cu,
+	}
+}
+
+func newChaincodeUpgradeHandler(channelID string, handleUpgrade api.ChaincodeUpgradeHandler) api.ChaincodeEventHandler {
+	return func(blockNum uint64, txID string, event *pb.ChaincodeEvent) error {
+		logger.Debugf("[%s] Handling chaincode event: %s", channelID, event)
+		if event.ChaincodeId != lsccID {
+			logger.Debugf("[%s] Chaincode event is not from 'lscc'", channelID)
+			return nil
+		}
+		if event.EventName != upgradeEvent {
+			logger.Debugf("[%s] Chaincode event from 'lscc' is not an upgrade event", channelID)
+			return nil
+		}
+
+		ccData := &pb.LifecycleEvent{}
+		err := proto.Unmarshal(event.Payload, ccData)
+		if err != nil {
+			return errors.WithMessage(err, "error unmarshalling chaincode upgrade event")
+		}
+
+		logger.Debugf("[%s] Handling chaincode upgrade of chaincode [%s]", channelID, ccData.ChaincodeName)
+		return handleUpgrade(blockNum, txID, ccData.ChaincodeName)
+	}
+}
diff --git a/extensions/gossip/blockpublisher/blockpublisher_test.go b/extensions/gossip/blockpublisher/blockpublisher_test.go
new file mode 100644
index 00000000..97719aba
--- /dev/null
+++ b/extensions/gossip/blockpublisher/blockpublisher_test.go
@@ -0,0 +1,310 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package blockpublisher
+
+import (
+	"fmt"
+	"sync/atomic"
+	"testing"
+	"time"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	cb "github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
+	pb "github.com/hyperledger/fabric/protos/peer"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+
+	txID1 = "tx1"
+	txID2 = "tx2"
+	txID3 = "tx3"
+
+	ccID1 = "cc1"
+	ccID2 = "cc2"
+
+	coll1 = "collection1"
+	coll2 = "collection2"
+
+	key1 = "key1"
+	key2 = "key2"
+	key3 = "key3"
+
+	ccEvent1 = "ccevent1"
+
+	hName1 = "handler1"
+	hName2 = "handler2"
+	hName3 = "handler3"
+)
+
+func TestPublisher_Get(t *testing.T) {
+	p := New("mychannel")
+	require.NotNil(t, p)
+	p.Close()
+}
+
+func TestPublisher_Close(t *testing.T) {
+	p := New(channelID)
+	require.NotNil(t, p)
+
+	p.Close()
+
+	assert.NotPanics(t, func() {
+		p.Close()
+	}, "Expecting Close to not panic when called multiple times")
+}
+
+func TestPublisher_PublishEndorsementEvents(t *testing.T) {
+	var (
+		value1 = []byte("value1")
+		value2 = []byte("value2")
+		value3 = []byte("value3")
+
+		v1 = &kvrwset.Version{
+			BlockNum: 1000,
+			TxNum:    0,
+		}
+		v2 = &kvrwset.Version{
+			BlockNum: 1001,
+			TxNum:    1,
+		}
+	)
+
+	p := New(channelID)
+	require.NotNil(t, p)
+	defer p.Close()
+
+	handler1 := newMockBlockHandler(hName1)
+	p.AddReadHandler(handler1.HandleRead)
+	p.AddWriteHandler(handler1.HandleWrite)
+
+	handler2 := newMockBlockHandler(hName2)
+	p.AddReadHandler(handler2.HandleRead)
+	p.AddCCEventHandler(handler2.HandleChaincodeEvent)
+
+	handler3 := newMockBlockHandler(hName3)
+	p.AddCCUpgradeHandler(handler3.HandleChaincodeUpgradeEvent)
+
+	b := mocks.NewBlockBuilder(channelID, 1100)
+
+	tb1 := b.Transaction(txID1, pb.TxValidationCode_VALID)
+	tb1.ChaincodeAction(ccID1).
+		Write(key1, value1).
+		Read(key1, v1).
+		ChaincodeEvent(ccEvent1, []byte("ccpayload"))
+	tb1.ChaincodeAction(ccID2).
+		Write(key2, value2).
+		Read(key2, v2)
+
+	tb2 := b.Transaction(txID2, pb.TxValidationCode_VALID)
+	cc2_1 := tb2.ChaincodeAction(ccID1).
+		Write(key2, value2)
+	cc2_1.Collection(coll1).
+		Write(key1, value2)
+	cc2_1.Collection(coll2).
+		Delete(key1)
+
+	// This transaction should not be published
+	tb3 := b.Transaction(txID3, pb.TxValidationCode_MVCC_READ_CONFLICT)
+	tb3.ChaincodeAction(ccID1).
+		Write(key3, value3).
+		ChaincodeEvent(ccEvent1, []byte("ccpayload"))
+
+	lceBytes, err := proto.Marshal(&pb.LifecycleEvent{ChaincodeName: ccID2})
+	require.NoError(t, err)
+	require.NotNil(t, lceBytes)
+
+	b.Transaction(txID1, pb.TxValidationCode_VALID).
+		ChaincodeAction(lsccID).
+		ChaincodeEvent(upgradeEvent, lceBytes)
+	tb4 := b.Transaction(txID2, pb.TxValidationCode_VALID)
+	tb4.ChaincodeAction(lsccID).
+		ChaincodeEvent(ccEvent1, nil)
+
+	p.Publish(b.Build())
+
+	time.Sleep(time.Second)
+
+	assert.Equal(t, 2, handler1.NumReads())
+	assert.Equal(t, 5, handler1.NumWrites())
+	assert.Equal(t, 0, handler1.NumCCEvents())
+	assert.Equal(t, 0, handler1.NumCCUpgradeEvents())
+
+	assert.Equal(t, 2, handler2.NumReads())
+	assert.Equal(t, 0, handler2.NumWrites())
+	assert.Equal(t, 3, handler2.NumCCEvents())
+	assert.Equal(t, 0, handler2.NumCCUpgradeEvents())
+
+	assert.Equal(t, 0, handler3.NumReads())
+	assert.Equal(t, 0, handler3.NumWrites())
+	assert.Equal(t, 0, handler3.NumCCEvents())
+	assert.Equal(t, 1, handler3.NumCCUpgradeEvents())
+}
+
+func TestPublisher_PublishConfigUpdateEvents(t *testing.T) {
+	p := New(channelID)
+	require.NotNil(t, p)
+	defer p.Close()
+
+	handler := newMockBlockHandler(hName1)
+	p.AddConfigUpdateHandler(handler.HandleConfigUpdate)
+
+	b := mocks.NewBlockBuilder(channelID, 1100)
+	b.ConfigUpdate()
+
+	p.Publish(b.Build())
+
+	time.Sleep(time.Second)
+
+	assert.Equal(t, 1, handler.NumConfigUpdates())
+}
+
+func TestPublisher_Error(t *testing.T) {
+	var (
+		value1 = []byte("value1")
+		value2 = []byte("value2")
+
+		v1 = &kvrwset.Version{
+			BlockNum: 1000,
+			TxNum:    3,
+		}
+		v2 = &kvrwset.Version{
+			BlockNum: 1001,
+			TxNum:    5,
+		}
+	)
+
+	p := New(channelID)
+	require.NotNil(t, p)
+	defer p.Close()
+
+	expectedErr := fmt.Errorf("injected error")
+
+	handler1 := newMockBlockHandler(hName1).WithError(expectedErr)
+	p.AddReadHandler(handler1.HandleRead)
+	p.AddWriteHandler(handler1.HandleWrite)
+	p.AddCCEventHandler(handler1.HandleChaincodeEvent)
+	p.AddCCUpgradeHandler(handler1.HandleChaincodeUpgradeEvent)
+
+	b := mocks.NewBlockBuilder(channelID, 1100)
+
+	tb1 := b.Transaction(txID1, pb.TxValidationCode_VALID)
+	tb1.ChaincodeAction(ccID1).
+		Write(key1, value1).
+		Read(key1, v1).
+		ChaincodeEvent(ccEvent1, []byte("ccpayload"))
+	tb1.ChaincodeAction(ccID2).
+		Write(key2, value2).
+		Read(key2, v2)
+
+	tb2 := b.Transaction(txID2, pb.TxValidationCode_VALID)
+	cc2_1 := tb2.ChaincodeAction(ccID1).
+		Write(key2, value2)
+	cc2_1.Collection(coll1).
+		Write(key1, value2)
+	cc2_1.Collection(coll2).
+		Delete(key1)
+
+	lceBytes, err := proto.Marshal(&pb.LifecycleEvent{ChaincodeName: ccID2})
+	require.NoError(t, err)
+	require.NotNil(t, lceBytes)
+
+	b.Transaction(txID1, pb.TxValidationCode_VALID).
+		ChaincodeAction(lsccID).
+		ChaincodeEvent(upgradeEvent, lceBytes)
+	tb4 := b.Transaction(txID2, pb.TxValidationCode_VALID)
+	tb4.ChaincodeAction(lsccID).
+		ChaincodeEvent(ccEvent1, nil)
+
+	p.Publish(b.Build())
+
+	time.Sleep(time.Second)
+
+	assert.Equal(t, 2, handler1.NumReads())
+	assert.Equal(t, 5, handler1.NumWrites())
+	assert.Equal(t, 3, handler1.NumCCEvents())
+	assert.Equal(t, 1, handler1.NumCCUpgradeEvents())
+}
+
+type mockBlockHandler struct {
+	name               string
+	numReads           int32
+	numWrites          int32
+	numCCEvents        int32
+	numCCUpgradeEvents int32
+	numConfigUpdates   int32
+	err                error
+}
+
+func newMockBlockHandler(name string) *mockBlockHandler {
+	return &mockBlockHandler{
+		name: name,
+	}
+}
+
+func (m *mockBlockHandler) WithError(err error) *mockBlockHandler {
+	m.err = err
+	return m
+}
+
+func (m *mockBlockHandler) Name() string {
+	return m.name
+}
+
+func (m *mockBlockHandler) NumReads() int {
+	return int(atomic.LoadInt32(&m.numReads))
+}
+
+func (m *mockBlockHandler) NumWrites() int {
+	return int(atomic.LoadInt32(&m.numWrites))
+}
+
+func (m *mockBlockHandler) NumCCEvents() int {
+	return int(atomic.LoadInt32(&m.numCCEvents))
+}
+
+func (m *mockBlockHandler) NumCCUpgradeEvents() int {
+	return int(atomic.LoadInt32(&m.numCCUpgradeEvents))
+}
+
+func (m *mockBlockHandler) NumConfigUpdates() int {
+	return int(atomic.LoadInt32(&m.numConfigUpdates))
+}
+
+func (m *mockBlockHandler) HandleRead(blockNum uint64, txID string, namespace string, kvRead *kvrwset.KVRead) error {
+	atomic.AddInt32(&m.numReads, 1)
+	fmt.Printf("[%s] Got read in block [%d] at Tx [%s] for [%s]: %s\n", m.name, blockNum, txID, namespace, kvRead)
+	return m.err
+}
+
+func (m *mockBlockHandler) HandleWrite(blockNum uint64, txID string, namespace string, kvWrite *kvrwset.KVWrite) error {
+	atomic.AddInt32(&m.numWrites, 1)
+	fmt.Printf("[%s] Got write in block [%d] at Tx [%s] for [%s]: %s\n", m.name, blockNum, txID, namespace, kvWrite)
+	return m.err
+}
+
+func (m *mockBlockHandler) HandleChaincodeEvent(blockNum uint64, txID string, event *pb.ChaincodeEvent) error {
+	atomic.AddInt32(&m.numCCEvents, 1)
+	fmt.Printf("[%s] Got CC event in block [%d] at Tx [%s]: %s\n", m.name, blockNum, txID, event)
+	return m.err
+}
+
+func (m *mockBlockHandler) HandleChaincodeUpgradeEvent(blockNum uint64, txID string, chaincodeName string) error {
+	atomic.AddInt32(&m.numCCUpgradeEvents, 1)
+	fmt.Printf("[%s] Got CC upgrade event in block [%d] at Tx [%s] for CC [%s]\n", m.name, blockNum, txID, chaincodeName)
+	return m.err
+}
+
+func (m *mockBlockHandler) HandleConfigUpdate(blockNum uint64, configUpdate *cb.ConfigUpdate) error {
+	atomic.AddInt32(&m.numConfigUpdates, 1)
+	fmt.Printf("[%s] Got config update in block [%d]: %s\n", m.name, blockNum, configUpdate)
+	return m.err
+}
diff --git a/extensions/gossip/coordinator/coordinator.go b/extensions/gossip/coordinator/coordinator.go
new file mode 100644
index 00000000..3c6250e2
--- /dev/null
+++ b/extensions/gossip/coordinator/coordinator.go
@@ -0,0 +1,159 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package coordinator
+
+import (
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+)
+
+var logger = flogging.MustGetLogger("kevlar_gossip_state")
+
+type transientStore interface {
+	// PersistWithConfig stores the private write set of a transaction along with the collection config
+	// in the transient store based on txid and the block height the private data was received at
+	PersistWithConfig(txid string, blockHeight uint64, privateSimulationResultsWithConfig *transientstore.TxPvtReadWriteSetWithConfigInfo) error
+}
+
+// Coordinator is the Kevlar Gossip coordinator
+type Coordinator struct {
+	channelID      string
+	transientStore transientStore
+	collDataStore  storeapi.Store
+}
+
+// New returns a new Coordinator
+func New(channelID string, transientStore transientStore, collDataStore storeapi.Store) *Coordinator {
+	return &Coordinator{
+		channelID:      channelID,
+		transientStore: transientStore,
+		collDataStore:  collDataStore,
+	}
+}
+
+// StorePvtData used to persist private date into transient store
+func (c *Coordinator) StorePvtData(txID string, privData *transientstore.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error {
+	transientRWSet, err := c.getTransientRWSets(txID, blkHeight, privData)
+	if err != nil {
+		logger.Errorf("[%s:%d:%s] Unable to extract transient r/w set from private data: %s", c.channelID, blkHeight, txID, err)
+		return err
+	}
+
+	// Some of the write-sets may be transient and some not - need to invoke persist on both transient data and regular private data
+	err = c.collDataStore.Persist(txID, privData)
+	if err != nil {
+		logger.Errorf("[%s:%d:%s] Unable to persist collection data: %s", c.channelID, blkHeight, txID, err)
+		return err
+	}
+
+	if transientRWSet == nil {
+		logger.Debugf("[%s:%d:%s] Nothing to persist to transient store", c.channelID, blkHeight, txID)
+		return nil
+	}
+
+	logger.Debugf("[%s:%d:%s] Persisting private data to transient store", c.channelID, blkHeight, txID)
+
+	return c.transientStore.PersistWithConfig(
+		txID, blkHeight,
+		&transientstore.TxPvtReadWriteSetWithConfigInfo{
+			PvtRwset:          transientRWSet,
+			CollectionConfigs: privData.CollectionConfigs,
+			EndorsedAt:        privData.EndorsedAt,
+		})
+}
+
+func (c *Coordinator) getTransientRWSets(txID string, blkHeight uint64, privData *transientstore.TxPvtReadWriteSetWithConfigInfo) (*rwset.TxPvtReadWriteSet, error) {
+	txPvtRWSet, err := rwsetutil.TxPvtRwSetFromProtoMsg(privData.PvtRwset)
+	if err != nil {
+		return nil, errors.New("error getting pvt RW set from bytes")
+	}
+
+	nsPvtRwSet, modified, err := c.extractTransientRWSets(txPvtRWSet.NsPvtRwSet, privData.CollectionConfigs, txID, blkHeight)
+	if err != nil {
+		return nil, err
+	}
+	if !modified {
+		logger.Debugf("[%s:%d:%s] Rewrite to NsPvtRwSet not required for transient store", c.channelID, blkHeight, txID)
+		return privData.PvtRwset, nil
+	}
+	if len(nsPvtRwSet) == 0 {
+		logger.Debugf("[%s:%d:%s] Didn't find any private data to persist to transient store", c.channelID, blkHeight, txID)
+		return nil, nil
+	}
+
+	logger.Debugf("[%s:%d:%s] Rewriting NsPvtRwSet for transient store", c.channelID, blkHeight, txID)
+	txPvtRWSet.NsPvtRwSet = nsPvtRwSet
+	newPvtRwset, err := txPvtRWSet.ToProtoMsg()
+	if err != nil {
+		return nil, errors.WithMessage(err, "error marshalling private data r/w set")
+	}
+
+	logger.Debugf("[%s:%d:%s] Rewriting private data r/w set since it was modified", c.channelID, blkHeight, txID)
+	return newPvtRwset, nil
+}
+
+func (c *Coordinator) extractTransientRWSets(srcNsPvtRwSets []*rwsetutil.NsPvtRwSet, collConfigs map[string]*common.CollectionConfigPackage, txID string, blkHeight uint64) ([]*rwsetutil.NsPvtRwSet, bool, error) {
+	modified := false
+	var nsPvtRwSet []*rwsetutil.NsPvtRwSet
+	for _, nsRWSet := range srcNsPvtRwSets {
+		var collPvtRwSets []*rwsetutil.CollPvtRwSet
+		for _, collRWSet := range nsRWSet.CollPvtRwSets {
+			ok, e := isPvtData(collConfigs, nsRWSet.NameSpace, collRWSet.CollectionName)
+			if e != nil {
+				return nil, false, errors.New("error in collection config")
+			}
+			if !ok {
+				logger.Debugf("[%s:%d:%s] Not persisting collection [%s:%s] in transient store", c.channelID, blkHeight, txID, nsRWSet.NameSpace, collRWSet.CollectionName)
+				modified = true
+				continue
+			}
+			logger.Debugf("[%s:%d:%s] Persisting collection [%s:%s] in transient store", c.channelID, blkHeight, txID, nsRWSet.NameSpace, collRWSet.CollectionName)
+			collPvtRwSets = append(collPvtRwSets, collRWSet)
+		}
+		if len(collPvtRwSets) > 0 {
+			if len(collPvtRwSets) != len(nsRWSet.CollPvtRwSets) {
+				logger.Debugf("[%s:%d:%s] Rewriting collections for [%s] in transient store", c.channelID, blkHeight, txID, nsRWSet.NameSpace)
+				nsRWSet.CollPvtRwSets = collPvtRwSets
+				modified = true
+			} else {
+				logger.Debugf("[%s:%d:%s] Not touching collections for [%s] in transient store", c.channelID, blkHeight, txID, nsRWSet.NameSpace)
+			}
+			logger.Debugf("[%s:%d:%s] Adding NsPvtRwSet for [%s] in transient store", c.channelID, blkHeight, txID, nsRWSet.NameSpace)
+			nsPvtRwSet = append(nsPvtRwSet, nsRWSet)
+		} else {
+			logger.Debugf("[%s:%d:%s] NOT adding NsPvtRwSet for [%s] in transient store since no private data collections found", c.channelID, blkHeight, txID, nsRWSet.NameSpace)
+		}
+	}
+	return nsPvtRwSet, modified, nil
+}
+
+func isPvtData(collConfigs map[string]*common.CollectionConfigPackage, ns, coll string) (bool, error) {
+	pkg, ok := collConfigs[ns]
+	if !ok {
+		return false, errors.Errorf("could not find collection configs for namespace [%s]", ns)
+	}
+
+	var config *common.StaticCollectionConfig
+	for _, c := range pkg.Config {
+		staticConfig := c.GetStaticCollectionConfig()
+		if staticConfig.Name == coll {
+			config = staticConfig
+			break
+		}
+	}
+
+	if config == nil {
+		return false, errors.Errorf("could not find collection config for collection [%s:%s]", ns, coll)
+	}
+
+	return config.Type == common.CollectionType_COL_UNKNOWN || config.Type == common.CollectionType_COL_PRIVATE, nil
+}
diff --git a/extensions/gossip/coordinator/coordinator_test.go b/extensions/gossip/coordinator/coordinator_test.go
new file mode 100644
index 00000000..c8c88754
--- /dev/null
+++ b/extensions/gossip/coordinator/coordinator_test.go
@@ -0,0 +1,176 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package coordinator
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	channelID = "testchannel"
+
+	ns1 = "ns1"
+	ns2 = "ns2"
+
+	coll1 = "coll1"
+	coll2 = "coll2"
+	coll3 = "coll3"
+
+	policy1 = "OR('Org1MSP.member','Org2MSP.member')"
+)
+
+func TestCoordinator_StorePvtData(t *testing.T) {
+	collStore := mocks.NewDataStore()
+
+	t.Run("Just transient", func(t *testing.T) {
+		tStore := &mockTransientStore{}
+
+		c := New(channelID, tStore, collStore)
+		require.NotNil(t, c)
+
+		b := mocks.NewPvtReadWriteSetBuilder()
+		b.Namespace(ns1).Collection(coll1).StaticConfig(policy1, 2, 5, 1000)
+		pvtData := b.Build()
+
+		err := c.StorePvtData("tx1", pvtData, 1000)
+		assert.NoError(t, err)
+		require.NotNil(t, tStore.persistedData)
+		assert.Equal(t, pvtData.PvtRwset, tStore.persistedData.PvtRwset)
+	})
+
+	t.Run("No transient", func(t *testing.T) {
+		tStore := &mockTransientStore{}
+
+		c := New(channelID, tStore, collStore)
+		require.NotNil(t, c)
+
+		b := mocks.NewPvtReadWriteSetBuilder()
+		b.Namespace(ns1).Collection(coll2).DCASConfig(policy1, 2, 5, "10m")
+		pvtData := b.Build()
+
+		err := c.StorePvtData("tx1", pvtData, 1000)
+		assert.NoError(t, err)
+		assert.Nil(t, tStore.persistedData)
+	})
+
+	t.Run("Mixed types", func(t *testing.T) {
+		tStore := &mockTransientStore{}
+
+		c := New(channelID, tStore, collStore)
+		require.NotNil(t, c)
+
+		b := mocks.NewPvtReadWriteSetBuilder()
+		nsb := b.Namespace(ns1)
+		nsb.Collection(coll1).StaticConfig(policy1, 2, 5, 1000)
+		nsb.Collection(coll2).DCASConfig(policy1, 2, 5, "10m")
+		pvtData := b.Build()
+
+		err := c.StorePvtData("tx1", pvtData, 1000)
+		assert.NoError(t, err)
+		require.NotNil(t, tStore.persistedData)
+		require.NotNil(t, tStore.persistedData.PvtRwset)
+		assert.NotEqual(t, pvtData.PvtRwset, tStore.persistedData.PvtRwset)
+
+		// The original collections set should have two entries: DCAS and regular private data
+		require.Equal(t, 1, len(pvtData.PvtRwset.NsPvtRwset))
+		assert.Equal(t, 2, len(pvtData.PvtRwset.NsPvtRwset[0].CollectionPvtRwset))
+
+		// The rewritten collections set should have only one entry: regular private data
+		require.Equal(t, 1, len(tStore.persistedData.PvtRwset.NsPvtRwset))
+		assert.Equal(t, 1, len(tStore.persistedData.PvtRwset.NsPvtRwset[0].CollectionPvtRwset))
+	})
+}
+
+func TestCoordinator_StorePvtData_error(t *testing.T) {
+	t.Run("Marshal error", func(t *testing.T) {
+		tStore := &mockTransientStore{}
+		collStore := mocks.NewDataStore()
+
+		c := New(channelID, tStore, collStore)
+		require.NotNil(t, c)
+
+		b := mocks.NewPvtReadWriteSetBuilder()
+		b.Namespace(ns1).Collection(coll1).StaticConfig(policy1, 2, 5, 1000).WithMarshalError()
+		pvtData := b.Build()
+
+		err := c.StorePvtData("tx1", pvtData, 1000)
+		assert.Error(t, err)
+		assert.Nil(t, tStore.persistedData)
+	})
+
+	t.Run("CollStore error", func(t *testing.T) {
+		tStore := &mockTransientStore{}
+		expectedErr := errors.New("test coll error")
+		collStore := mocks.NewDataStore().Error(expectedErr)
+
+		c := New(channelID, tStore, collStore)
+		require.NotNil(t, c)
+
+		b := mocks.NewPvtReadWriteSetBuilder()
+		b.Namespace(ns1).Collection(coll1).StaticConfig(policy1, 2, 5, 1000)
+		pvtData := b.Build()
+
+		err := c.StorePvtData("tx1", pvtData, 1000)
+		require.Error(t, err)
+		assert.Contains(t, err.Error(), expectedErr.Error())
+	})
+}
+
+func TestIsPvtData(t *testing.T) {
+	pvtColl := &common.CollectionConfig_StaticCollectionConfig{
+		StaticCollectionConfig: &common.StaticCollectionConfig{
+			Name: coll1,
+		},
+	}
+	transientColl := &common.CollectionConfig_StaticCollectionConfig{
+		StaticCollectionConfig: &common.StaticCollectionConfig{
+			Name: coll2,
+			Type: common.CollectionType_COL_TRANSIENT,
+		},
+	}
+
+	collConfigs := make(map[string]*common.CollectionConfigPackage)
+	collConfigs[ns1] = &common.CollectionConfigPackage{
+		Config: []*common.CollectionConfig{
+			{Payload: pvtColl},
+			{Payload: transientColl},
+		},
+	}
+	ok, err := isPvtData(collConfigs, ns1, coll1)
+	assert.NoError(t, err)
+	assert.True(t, ok)
+
+	ok, err = isPvtData(collConfigs, ns1, coll2)
+	assert.NoError(t, err)
+	assert.False(t, ok)
+
+	ok, err = isPvtData(collConfigs, ns2, coll1)
+	require.Error(t, err)
+	assert.Contains(t, err.Error(), "could not find collection configs for namespace")
+	assert.False(t, ok)
+
+	ok, err = isPvtData(collConfigs, ns1, coll3)
+	assert.Error(t, err)
+	assert.Contains(t, err.Error(), "could not find collection config for collection")
+	assert.False(t, ok)
+}
+
+type mockTransientStore struct {
+	persistedData *transientstore.TxPvtReadWriteSetWithConfigInfo
+}
+
+func (m *mockTransientStore) PersistWithConfig(txid string, blockHeight uint64, privateSimulationResultsWithConfig *transientstore.TxPvtReadWriteSetWithConfigInfo) error {
+	m.persistedData = privateSimulationResultsWithConfig
+	return nil
+}
diff --git a/extensions/gossip/dispatcher/dispatcher.go b/extensions/gossip/dispatcher/dispatcher.go
new file mode 100644
index 00000000..af822278
--- /dev/null
+++ b/extensions/gossip/dispatcher/dispatcher.go
@@ -0,0 +1,249 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dispatcher
+
+import (
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/extensions/collections/api/store"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/hyperledger/fabric/extensions/common/discovery"
+	"github.com/hyperledger/fabric/extensions/common/requestmgr"
+	supp "github.com/hyperledger/fabric/extensions/common/support"
+	kgossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	gdiscovery "github.com/hyperledger/fabric/gossip/discovery"
+	cb "github.com/hyperledger/fabric/protos/common"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/pkg/errors"
+	"go.uber.org/zap/zapcore"
+)
+
+var logger = flogging.MustGetLogger("kevlar_gossip_state")
+
+type gossipAdapter interface {
+	PeersOfChannel(gcommon.ChainID) []gdiscovery.NetworkMember
+	SelfMembershipInfo() gdiscovery.NetworkMember
+	IdentityInfo() gossipapi.PeerIdentitySet
+}
+
+type blockPublisher interface {
+	AddCCUpgradeHandler(handler kgossipapi.ChaincodeUpgradeHandler)
+}
+
+type ccRetriever interface {
+	Config(ns, coll string) (*cb.StaticCollectionConfig, error)
+	Policy(ns, coll string) (privdata.CollectionAccessPolicy, error)
+}
+
+// New returns a new Gossip message dispatcher
+func New(
+	channelID string,
+	dataStore storeapi.Store,
+	gossipAdapter gossipAdapter,
+	ledger ledger.PeerLedger,
+	blockPublisher blockPublisher) *Dispatcher {
+	return &Dispatcher{
+		ccRetriever: supp.NewCollectionConfigRetriever(channelID, ledger, blockPublisher),
+		channelID:   channelID,
+		reqMgr:      requestmgr.Get(channelID),
+		dataStore:   dataStore,
+		discovery:   discovery.New(channelID, gossipAdapter),
+	}
+}
+
+// Dispatcher is a Gossip message dispatcher
+type Dispatcher struct {
+	ccRetriever
+	channelID string
+	reqMgr    requestmgr.RequestMgr
+	dataStore storeapi.Store
+	discovery *discovery.Discovery
+}
+
+// Dispatch handles the message and returns true if the message was handled; false if the message is unrecognized
+func (s *Dispatcher) Dispatch(msg proto.ReceivedMessage) bool {
+	switch {
+	case msg.GetGossipMessage().GetCollDataReq() != nil:
+		logger.Debug("Handling collection data request message")
+		s.handleDataRequest(msg)
+		return true
+	case msg.GetGossipMessage().GetCollDataRes() != nil:
+		logger.Debug("Handling collection data response message")
+		s.handleDataResponse(msg)
+		return true
+	default:
+		logger.Debug("Not handling msg")
+		return false
+	}
+}
+
+func (s *Dispatcher) handleDataRequest(msg proto.ReceivedMessage) {
+	if logger.IsEnabledFor(zapcore.DebugLevel) {
+		logger.Debugf("[ENTER] -> handleDataRequest")
+		defer logger.Debug("[EXIT] ->  handleDataRequest")
+	}
+
+	req := msg.GetGossipMessage().GetCollDataReq()
+	if len(req.Digests) == 0 {
+		logger.Warning("Got nil digests in CollDataRequestMsg")
+		return
+	}
+
+	reqMSPID, ok := s.discovery.GetMSPID(msg.GetConnectionInfo().ID)
+	if !ok {
+		logger.Warningf("Unable to get MSP ID from PKI ID of remote endpoint [%s]", msg.GetConnectionInfo().Endpoint)
+		return
+	}
+
+	responses, err := s.getData(reqMSPID, req)
+	if err != nil {
+		logger.Warning("[%s] Error processing request for data: %s", s.channelID, err.Error())
+		return
+	}
+
+	logger.Debugf("[%s] Responding with collection data for request %d", s.channelID, req.Nonce)
+
+	msg.Respond(&gproto.GossipMessage{
+		// Copy nonce field from the request, so it will be possible to match response
+		Nonce:   msg.GetGossipMessage().Nonce,
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(s.channelID),
+		Content: &gproto.GossipMessage_CollDataRes{
+			CollDataRes: &gproto.RemoteCollDataResponse{
+				Nonce:    req.Nonce,
+				Elements: responses,
+			},
+		},
+	})
+}
+
+func (s *Dispatcher) handleDataResponse(msg proto.ReceivedMessage) {
+	if logger.IsEnabledFor(zapcore.DebugLevel) {
+		logger.Debug("[ENTER] -> handleDataResponse")
+		defer logger.Debug("[EXIT] ->  handleDataResponse")
+	}
+
+	res := msg.GetGossipMessage().GetCollDataRes()
+
+	mspID, ok := s.discovery.GetMSPID(msg.GetConnectionInfo().ID)
+	if !ok {
+		logger.Errorf("Unable to get MSP ID from PKI ID")
+		return
+	}
+
+	var elements []*requestmgr.Element
+	for _, e := range res.Elements {
+		d := e.Digest
+		logger.Debugf("[%s] Coll data response for request %d - [%s:%s:%s] received", s.channelID, res.Nonce, d.Namespace, d.Collection, d.Key)
+
+		element := &requestmgr.Element{
+			Namespace:  d.Namespace,
+			Collection: d.Collection,
+			Key:        d.Key,
+			Value:      e.Value,
+		}
+
+		if e.ExpiryTime != nil {
+			element.Expiry = time.Unix(e.ExpiryTime.Seconds, 0)
+		}
+
+		elements = append(elements, element)
+	}
+
+	s.reqMgr.Respond(
+		res.Nonce,
+		&requestmgr.Response{
+			Endpoint: msg.GetConnectionInfo().Endpoint,
+			MSPID:    mspID,
+			// FIXME: Should the message be signed?
+			//Signature:   element.Signature,
+			//Identity:    element.Identity,
+			Data: elements,
+		},
+	)
+}
+
+func (s *Dispatcher) getData(reqMSPID string, req *gproto.RemoteCollDataRequest) ([]*gproto.CollDataElement, error) {
+	var responses []*gproto.CollDataElement
+	for _, digest := range req.Digests {
+		if digest == nil {
+			return nil, errors.New("got nil digest in CollDataRequestMsg")
+		}
+
+		key := store.NewKey(digest.EndorsedAtTxID, digest.Namespace, digest.Collection, digest.Key)
+
+		e := &gproto.CollDataElement{
+			Digest: digest,
+		}
+
+		authorized, err := s.isAuthorized(reqMSPID, digest.Namespace, digest.Collection)
+		if err != nil {
+			return nil, err
+		}
+
+		if authorized {
+			logger.Debugf("[%s] Getting data for key [%s]", s.channelID, key)
+			value, err := s.getDataForKey(key)
+			if err != nil {
+				return nil, errors.Wrapf(err, "error getting data for [%s]", key)
+			}
+			if value != nil {
+				e.Value = value.Value
+				e.ExpiryTime = common.ToTimestamp(value.Expiry)
+			}
+		} else {
+			logger.Infof("[%s] Requesting MSP [%s] is not authorized to read data for key [%s]", s.channelID, reqMSPID, key)
+		}
+
+		responses = append(responses, e)
+	}
+	return responses, nil
+}
+
+func (s *Dispatcher) getDataForKey(key *storeapi.Key) (*storeapi.ExpiringValue, error) {
+	logger.Debugf("[%s] Getting config for [%s:%s]", s.channelID, key.Namespace, key.Collection)
+	config, err := s.Config(key.Namespace, key.Collection)
+	if err != nil {
+		return nil, err
+	}
+
+	switch config.Type {
+	case cb.CollectionType_COL_TRANSIENT:
+		logger.Debugf("[%s] Getting transient data for key [%s]", s.channelID, key)
+		return s.dataStore.GetTransientData(key)
+	case cb.CollectionType_COL_DCAS:
+		fallthrough
+	case cb.CollectionType_COL_OFFLEDGER:
+		logger.Debugf("[%s] Getting off-ledger data for key [%s]", s.channelID, key)
+		return s.dataStore.GetData(key)
+	default:
+		return nil, errors.Errorf("unsupported collection type: [%s]", config.Type)
+	}
+}
+
+// isAuthorized determines whether the given MSP ID is authorized to read data from the given collection
+func (s *Dispatcher) isAuthorized(mspID string, ns, coll string) (bool, error) {
+	policy, err := s.Policy(ns, coll)
+	if err != nil {
+		return false, errors.Wrapf(err, "unable to get policy for collection [%s:%s]", ns, coll)
+	}
+
+	for _, memberMSPID := range policy.MemberOrgs() {
+		if memberMSPID == mspID {
+			return true, nil
+		}
+	}
+
+	return false, nil
+}
diff --git a/extensions/gossip/dispatcher/dispatcher_test.go b/extensions/gossip/dispatcher/dispatcher_test.go
new file mode 100644
index 00000000..9be034c7
--- /dev/null
+++ b/extensions/gossip/dispatcher/dispatcher_test.go
@@ -0,0 +1,420 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package dispatcher
+
+import (
+	"context"
+	"testing"
+	"time"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/common/privdata"
+	"github.com/hyperledger/fabric/extensions/collections/api/store"
+	"github.com/hyperledger/fabric/extensions/common"
+	"github.com/hyperledger/fabric/extensions/common/requestmgr"
+	gmocks "github.com/hyperledger/fabric/extensions/gossip/mocks"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	ledgerconfig "github.com/hyperledger/fabric/extensions/roles"
+	gcommon "github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	"github.com/hyperledger/fabric/gossip/protoext"
+	gproto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+var (
+	org1MSPID      = "Org1MSP"
+	p1Org1Endpoint = "p1.org1.com"
+	p2Org1Endpoint = "p2.org1.com"
+	p3Org1Endpoint = "p3.org1.com"
+
+	org2MSPID      = "Org2MSP"
+	p1Org2Endpoint = "p1.org2.com"
+	p2Org2Endpoint = "p2.org2.com"
+	p3Org2Endpoint = "p3.org2.com"
+
+	org3MSPID      = "Org3MSP"
+	p1Org3Endpoint = "p1.org3.com"
+	p2Org3Endpoint = "p2.org3.com"
+	p3Org3Endpoint = "p3.org3.com"
+)
+
+var (
+	p1Org1PKIID = gcommon.PKIidType("pkiid_P1O1")
+	p2Org1PKIID = gcommon.PKIidType("pkiid_P2O1")
+	p3Org1PKIID = gcommon.PKIidType("pkiid_P3O1")
+
+	p1Org2PKIID = gcommon.PKIidType("pkiid_P1O2")
+	p2Org2PKIID = gcommon.PKIidType("pkiid_P2O2")
+	p3Org2PKIID = gcommon.PKIidType("pkiid_P3O2")
+
+	p1Org3PKIID = gcommon.PKIidType("pkiid_P1O3")
+	p2Org3PKIID = gcommon.PKIidType("pkiid_P2O3")
+	p3Org3PKIID = gcommon.PKIidType("pkiid_P3O3")
+
+	endorserRole  = string(ledgerconfig.EndorserRole)
+	committerRole = string(ledgerconfig.CommitterRole)
+)
+
+func TestDispatchUnhandled(t *testing.T) {
+	const channelID = "testchannel"
+
+	dispatcher := New(
+		channelID,
+		&mocks.DataStore{},
+		mocks.NewMockGossipAdapter(),
+		&mocks.Ledger{QueryExecutor: mocks.NewQueryExecutor(nil)},
+		gmocks.NewBlockPublisher(),
+	)
+
+	var response *gproto.GossipMessage
+	msg := &mockReceivedMessage{
+		message: newDataMsg(channelID),
+		respondTo: func(msg *gproto.GossipMessage) {
+			response = msg
+		},
+	}
+	assert.False(t, dispatcher.Dispatch(msg))
+	require.Nil(t, response)
+}
+
+func TestDispatchDataRequest(t *testing.T) {
+	const channelID = "testchannel"
+	const lscc = "lscc"
+	const ns1 = "ns1"
+	const ns2 = "ns2"
+	const coll1 = "coll1"
+	const coll2 = "coll2"
+
+	key1 := store.NewKey("txID1", ns1, coll1, "key1")
+	key2 := store.NewKey("txID1", ns2, coll2, "key2")
+	key3 := store.NewKey("txID1", ns1, coll2, "key3")
+	key4 := store.NewKey("txID1", ns2, coll1, "key4")
+
+	value1 := &store.ExpiringValue{Value: []byte("value1")}
+	value2 := &store.ExpiringValue{Value: []byte("value2")}
+	value3 := &store.ExpiringValue{Value: []byte("value3")}
+	value4 := &store.ExpiringValue{Value: []byte("value4")}
+
+	nsBuilder1 := mocks.NewNamespaceBuilder(ns1)
+	nsBuilder1.Collection(coll1).TransientConfig("OR ('Org1MSP.member','Org2MSP.member')", 3, 3, "1m")
+	nsBuilder1.Collection(coll2).DCASConfig("OR ('Org1MSP.member','Org2MSP.member')", 3, 3, "1m")
+
+	nsBuilder2 := mocks.NewNamespaceBuilder(ns2)
+	nsBuilder2.Collection(coll2).TransientConfig("OR ('Org1MSP.member','Org2MSP.member','Org3MSP.member')", 3, 3, "1m")
+	nsBuilder2.Collection(coll1).DCASConfig("OR ('Org1MSP.member','Org2MSP.member','Org3MSP.member')", 3, 3, "1m")
+
+	configPkgBytes1, err := proto.Marshal(nsBuilder1.BuildCollectionConfig())
+	require.NoError(t, err)
+	configPkgBytes2, err := proto.Marshal(nsBuilder2.BuildCollectionConfig())
+	require.NoError(t, err)
+
+	state := make(map[string]map[string][]byte)
+	state[lscc] = make(map[string][]byte)
+	state[lscc][privdata.BuildCollectionKVSKey(ns1)] = configPkgBytes1
+	state[lscc][privdata.BuildCollectionKVSKey(ns2)] = configPkgBytes2
+
+	gossipAdapter := mocks.NewMockGossipAdapter()
+	gossipAdapter.Self(org1MSPID, mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)).
+		Member(org1MSPID, mocks.NewMember(p2Org1Endpoint, p2Org1PKIID, committerRole)).
+		Member(org1MSPID, mocks.NewMember(p3Org1Endpoint, p3Org1PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)).
+		Member(org2MSPID, mocks.NewMember(p2Org2Endpoint, p2Org2PKIID, committerRole)).
+		Member(org2MSPID, mocks.NewMember(p3Org2Endpoint, p3Org2PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole)).
+		Member(org3MSPID, mocks.NewMember(p2Org3Endpoint, p2Org3PKIID, committerRole)).
+		Member(org3MSPID, mocks.NewMember(p3Org3Endpoint, p3Org3PKIID, endorserRole))
+
+	dispatcher := New(
+		channelID,
+		mocks.NewDataStore().TransientData(key1, value1).TransientData(key2, value2).Data(key3, value3).Data(key4, value4),
+		gossipAdapter,
+		&mocks.Ledger{QueryExecutor: mocks.NewQueryExecutor(state)},
+		gmocks.NewBlockPublisher(),
+	)
+	require.NotNil(t, dispatcher)
+
+	t.Run("Endorser -> success", func(t *testing.T) {
+		reqID1 := uint64(1000)
+
+		var response *gproto.GossipMessage
+		msg := &mockReceivedMessage{
+			message: newCollDataReqMsg(channelID, reqID1, key1, key2, key3, key4),
+			respondTo: func(msg *gproto.GossipMessage) {
+				response = msg
+			},
+			member: mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole),
+		}
+		assert.True(t, dispatcher.Dispatch(msg))
+		require.NotNil(t, response)
+		assert.Equal(t, []byte(channelID), response.Channel)
+		assert.Equal(t, gproto.GossipMessage_CHAN_ONLY, response.Tag)
+
+		res := response.GetCollDataRes()
+		require.NotNil(t, res)
+		assert.Equal(t, reqID1, res.Nonce)
+		require.Equal(t, 4, len(res.Elements))
+
+		element := res.Elements[0]
+		require.NotNil(t, element.Digest)
+		assert.Equal(t, key1.Namespace, element.Digest.Namespace)
+		assert.Equal(t, key1.Collection, element.Digest.Collection)
+		assert.Equal(t, key1.Key, element.Digest.Key)
+		assert.Equal(t, value1.Value, element.Value)
+
+		element = res.Elements[1]
+		require.NotNil(t, element.Digest)
+		assert.Equal(t, key2.Namespace, element.Digest.Namespace)
+		assert.Equal(t, key2.Collection, element.Digest.Collection)
+		assert.Equal(t, key2.Key, element.Digest.Key)
+		assert.Equal(t, value2.Value, element.Value)
+
+		element = res.Elements[2]
+		require.NotNil(t, element.Digest)
+		assert.Equal(t, key3.Namespace, element.Digest.Namespace)
+		assert.Equal(t, key3.Collection, element.Digest.Collection)
+		assert.Equal(t, key3.Key, element.Digest.Key)
+		assert.Equal(t, value3.Value, element.Value)
+
+		element = res.Elements[3]
+		require.NotNil(t, element.Digest)
+		assert.Equal(t, key4.Namespace, element.Digest.Namespace)
+		assert.Equal(t, key4.Collection, element.Digest.Collection)
+		assert.Equal(t, key4.Key, element.Digest.Key)
+		assert.Equal(t, value4.Value, element.Value)
+	})
+
+	t.Run("Non-Endorser -> no response", func(t *testing.T) {
+		f := isEndorser
+		defer func() { isEndorser = f }()
+		isEndorser = func() bool { return false }
+
+		reqID2 := uint64(1001)
+
+		var response *gproto.GossipMessage
+		msg := &mockReceivedMessage{
+			message: newCollDataReqMsg(channelID, reqID2, key1, key2),
+			respondTo: func(msg *gproto.GossipMessage) {
+				response = msg
+			},
+		}
+		assert.True(t, dispatcher.Dispatch(msg))
+		require.Nil(t, response)
+	})
+
+	t.Run("Access Denied -> nil response", func(t *testing.T) {
+		reqID2 := uint64(1001)
+
+		var response *gproto.GossipMessage
+		msg := &mockReceivedMessage{
+			message: newCollDataReqMsg(channelID, reqID2, key1, key2, key3, key4),
+			respondTo: func(msg *gproto.GossipMessage) {
+				response = msg
+			},
+			member: mocks.NewMember(p1Org3Endpoint, p1Org3PKIID, endorserRole), // An Org3 member is requesting data he doesn't have access to
+		}
+		assert.True(t, dispatcher.Dispatch(msg))
+		require.NotNil(t, response)
+		require.NotNil(t, response.GetCollDataRes())
+		require.Equal(t, 4, len(response.GetCollDataRes().Elements))
+
+		// Org3 doesn't have access to ns1:collection1
+		require.NotNil(t, response.GetCollDataRes().Elements[0])
+		assert.Nil(t, response.GetCollDataRes().Elements[0].Value)
+
+		// Org3 has access to ns2:collection2
+		require.NotNil(t, response.GetCollDataRes().Elements[1])
+		assert.NotNil(t, response.GetCollDataRes().Elements[1].Value)
+
+		// Org3 doesn't have access to ns1:collection2
+		require.NotNil(t, response.GetCollDataRes().Elements[2])
+		assert.Nil(t, response.GetCollDataRes().Elements[2].Value)
+
+		// Org3 has access to ns2:collection1
+		require.NotNil(t, response.GetCollDataRes().Elements[3])
+		assert.NotNil(t, response.GetCollDataRes().Elements[3].Value)
+	})
+}
+
+func TestDispatchDataResponse(t *testing.T) {
+	const channelID = "testchannel"
+	key1 := store.NewKey("txID1", "ns1", "coll1", "key1")
+	key2 := store.NewKey("txID1", "ns2", "coll2", "key2")
+
+	value1 := &store.ExpiringValue{Value: []byte("value1")}
+	value2 := &store.ExpiringValue{Value: []byte("value2")}
+
+	p1Org1 := mocks.NewMember(p1Org1Endpoint, p1Org1PKIID)
+	p1Org2 := mocks.NewMember(p1Org2Endpoint, p1Org2PKIID, endorserRole)
+
+	gossip := mocks.NewMockGossipAdapter().
+		Self(org1MSPID, p1Org1).
+		Member(org2MSPID, p1Org2)
+
+	dispatcher := New(
+		channelID,
+		mocks.NewDataStore().TransientData(key1, value1).TransientData(key2, value2),
+		gossip,
+		&mocks.Ledger{QueryExecutor: mocks.NewQueryExecutor(nil)},
+		gmocks.NewBlockPublisher(),
+	)
+	require.NotNil(t, dispatcher)
+
+	reqMgr := requestmgr.Get(channelID)
+	require.NotNil(t, reqMgr)
+
+	t.Run("Endorser -> success", func(t *testing.T) {
+		req := reqMgr.NewRequest()
+
+		msg := &mockReceivedMessage{
+			message: newTransientDataResMsg(channelID, req.ID(), newKeyValue(key1, value1), newKeyValue(key2, value2)),
+			member:  p1Org2,
+		}
+
+		go func() {
+			if !dispatcher.Dispatch(msg) {
+				t.Fatal("Message not handled")
+			}
+		}()
+		ctxt, _ := context.WithTimeout(context.Background(), 50*time.Millisecond)
+
+		res, err := req.GetResponse(ctxt)
+		assert.NoError(t, err)
+		require.NotNil(t, res)
+
+		require.Equal(t, 2, len(res.Data))
+
+		element := res.Data[0]
+		assert.Equal(t, key1.Namespace, element.Namespace)
+		assert.Equal(t, key1.Collection, element.Collection)
+		assert.Equal(t, key1.Key, element.Key)
+		assert.Equal(t, value1.Value, element.Value)
+
+		element = res.Data[1]
+		assert.Equal(t, key2.Namespace, element.Namespace)
+		assert.Equal(t, key2.Collection, element.Collection)
+		assert.Equal(t, key2.Key, element.Key)
+		assert.Equal(t, value2.Value, element.Value)
+	})
+
+	t.Run("Non-Endorser -> no response", func(t *testing.T) {
+		f := isEndorser
+		defer func() { isEndorser = f }()
+		isEndorser = func() bool { return false }
+
+		req := reqMgr.NewRequest()
+		ctxt, _ := context.WithTimeout(context.Background(), 50*time.Millisecond)
+
+		res, err := req.GetResponse(ctxt)
+		assert.EqualError(t, err, "context deadline exceeded")
+		assert.Nil(t, res)
+	})
+}
+
+func newCollDataReqMsg(channelID string, reqID uint64, keys ...*store.Key) *protoext.SignedGossipMessage {
+	var digests []*gproto.CollDataDigest
+	for _, key := range keys {
+		digests = append(digests, &gproto.CollDataDigest{
+			Namespace:      key.Namespace,
+			Collection:     key.Collection,
+			Key:            key.Key,
+			EndorsedAtTxID: key.EndorsedAtTxID,
+		})
+	}
+
+	msg, _ := protoext.NoopSign(&gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(channelID),
+		Content: &gproto.GossipMessage_CollDataReq{
+			CollDataReq: &gproto.RemoteCollDataRequest{
+				Nonce:   reqID,
+				Digests: digests,
+			},
+		},
+	})
+	return msg
+}
+
+type keyVal struct {
+	*store.Key
+	*store.ExpiringValue
+}
+
+func newKeyValue(key *store.Key, value *store.ExpiringValue) *keyVal {
+	return &keyVal{
+		Key:           key,
+		ExpiringValue: value,
+	}
+}
+
+func newTransientDataResMsg(channelID string, reqID uint64, keyVals ...*keyVal) *protoext.SignedGossipMessage {
+	var elements []*gproto.CollDataElement
+	for _, kv := range keyVals {
+		elements = append(elements, &gproto.CollDataElement{
+			Digest: &gproto.CollDataDigest{
+				Namespace:      kv.Namespace,
+				Collection:     kv.Collection,
+				Key:            kv.Key.Key,
+				EndorsedAtTxID: kv.EndorsedAtTxID,
+			},
+			Value:      kv.Value,
+			ExpiryTime: common.ToTimestamp(kv.Expiry),
+		})
+	}
+
+	msg, _ := protoext.NoopSign(&gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(channelID),
+		Content: &gproto.GossipMessage_CollDataRes{
+			CollDataRes: &gproto.RemoteCollDataResponse{
+				Nonce:    reqID,
+				Elements: elements,
+			},
+		},
+	})
+	return msg
+}
+
+func newDataMsg(channelID string) *protoext.SignedGossipMessage {
+	msg, _ := protoext.NoopSign(&gproto.GossipMessage{
+		Tag:     gproto.GossipMessage_CHAN_ONLY,
+		Channel: []byte(channelID),
+		Content: &gproto.GossipMessage_DataMsg{},
+	})
+	return msg
+}
+
+type mockReceivedMessage struct {
+	message   *protoext.SignedGossipMessage
+	respondTo func(msg *gproto.GossipMessage)
+	member    discovery.NetworkMember
+}
+
+func (m *mockReceivedMessage) Respond(msg *gproto.GossipMessage) {
+	if m.respondTo != nil {
+		m.respondTo(msg)
+	}
+}
+
+func (m *mockReceivedMessage) GetGossipMessage() *protoext.SignedGossipMessage {
+	return m.message
+}
+
+func (m *mockReceivedMessage) GetSourceEnvelope() *gproto.Envelope {
+	return nil
+}
+
+func (m *mockReceivedMessage) GetConnectionInfo() *protoext.ConnectionInfo {
+	return &protoext.ConnectionInfo{
+		ID:       m.member.PKIid,
+		Endpoint: m.member.Endpoint,
+	}
+}
+
+func (m *mockReceivedMessage) Ack(err error) {
+}
diff --git a/extensions/gossip/mocks/blockpublisher.go b/extensions/gossip/mocks/blockpublisher.go
new file mode 100644
index 00000000..688bcdaa
--- /dev/null
+++ b/extensions/gossip/mocks/blockpublisher.go
@@ -0,0 +1,51 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	"github.com/hyperledger/fabric/extensions/gossip/api"
+	cb "github.com/hyperledger/fabric/protos/common"
+)
+
+// BlockPublisher is a mock block publisher
+type BlockPublisher struct {
+}
+
+// NewBlockPublisher returns a new mock block publisher
+func NewBlockPublisher() *BlockPublisher {
+	return &BlockPublisher{}
+}
+
+// AddCCUpgradeHandler adds a handler for chaincode upgrades
+func (m *BlockPublisher) AddCCUpgradeHandler(handler api.ChaincodeUpgradeHandler) {
+	// Not implemented
+}
+
+// AddConfigUpdateHandler adds a handler for config updates
+func (m *BlockPublisher) AddConfigUpdateHandler(handler api.ConfigUpdateHandler) {
+	// Not implemented
+}
+
+// AddWriteHandler adds a handler for KV writes
+func (m *BlockPublisher) AddWriteHandler(handler api.WriteHandler) {
+	// Not implemented
+}
+
+// AddReadHandler adds a handler for KV reads
+func (m *BlockPublisher) AddReadHandler(handler api.ReadHandler) {
+	// Not implemented
+}
+
+// AddCCEventHandler adds a handler for chaincode events
+func (m *BlockPublisher) AddCCEventHandler(handler api.ChaincodeEventHandler) {
+	// Not implemented
+}
+
+// Publish traverses the block and invokes all applicable handlers
+func (m *BlockPublisher) Publish(block *cb.Block) {
+	// Not implemented
+}
diff --git a/extensions/mocks/mockgossipadapter.go b/extensions/mocks/mockgossipadapter.go
new file mode 100644
index 00000000..5d56bd7e
--- /dev/null
+++ b/extensions/mocks/mockgossipadapter.go
@@ -0,0 +1,94 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package mocks
+
+import (
+	gossipapi "github.com/hyperledger/fabric/gossip/api"
+	"github.com/hyperledger/fabric/gossip/comm"
+	"github.com/hyperledger/fabric/gossip/common"
+	"github.com/hyperledger/fabric/gossip/discovery"
+	gossipproto "github.com/hyperledger/fabric/protos/gossip"
+)
+
+// MessageHandler defines a function that handles a gossip message.
+type MessageHandler func(msg *gossipproto.GossipMessage)
+
+// MockGossipAdapter is the gossip adapter
+type MockGossipAdapter struct {
+	self        discovery.NetworkMember
+	members     []discovery.NetworkMember
+	identitySet gossipapi.PeerIdentitySet
+	handler     MessageHandler
+}
+
+// NewMockGossipAdapter returns the adapter
+func NewMockGossipAdapter() *MockGossipAdapter {
+	return &MockGossipAdapter{}
+}
+
+// Self discovers a network member
+func (m *MockGossipAdapter) Self(mspID string, self discovery.NetworkMember) *MockGossipAdapter {
+	m.self = self
+	m.identitySet = append(m.identitySet, gossipapi.PeerIdentityInfo{
+		PKIId:        self.PKIid,
+		Organization: []byte(mspID),
+	})
+	return m
+}
+
+// Member adds the network member
+func (m *MockGossipAdapter) Member(mspID string, member discovery.NetworkMember) *MockGossipAdapter {
+	m.members = append(m.members, member)
+	m.identitySet = append(m.identitySet, gossipapi.PeerIdentityInfo{
+		PKIId:        member.PKIid,
+		Organization: []byte(mspID),
+	})
+	return m
+}
+
+// MemberWithNoPKIID appends the member
+func (m *MockGossipAdapter) MemberWithNoPKIID(mspID string, member discovery.NetworkMember) *MockGossipAdapter {
+	m.members = append(m.members, member)
+	return m
+}
+
+// MessageHandler sets the handler
+func (m *MockGossipAdapter) MessageHandler(handler MessageHandler) *MockGossipAdapter {
+	m.handler = handler
+	return m
+}
+
+// PeersOfChannel returns the members
+func (m *MockGossipAdapter) PeersOfChannel(common.ChainID) []discovery.NetworkMember {
+	return m.members
+}
+
+// SelfMembershipInfo returns self
+func (m *MockGossipAdapter) SelfMembershipInfo() discovery.NetworkMember {
+	return m.self
+}
+
+// IdentityInfo returns the identitySet of this adapter
+func (m *MockGossipAdapter) IdentityInfo() gossipapi.PeerIdentitySet {
+	return m.identitySet
+}
+
+// Send sends a message to remote peers
+func (m *MockGossipAdapter) Send(msg *gossipproto.GossipMessage, peers ...*comm.RemotePeer) {
+	if m.handler != nil {
+		go m.handler(msg)
+	}
+}
+
+// NewMember creates a new network member
+func NewMember(endpoint string, pkiID []byte, roles ...string) discovery.NetworkMember {
+	return discovery.NetworkMember{
+		Endpoint:   endpoint,
+		PKIid:      pkiID,
+		Properties: &gossipproto.Properties{},
+	}
+}
diff --git a/extensions/pvtdatastorage/cachedpvtdatastore/store_impl.go b/extensions/pvtdatastorage/cachedpvtdatastore/store_impl.go
new file mode 100644
index 00000000..83a4e38e
--- /dev/null
+++ b/extensions/pvtdatastorage/cachedpvtdatastore/store_impl.go
@@ -0,0 +1,274 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cachedpvtdatastore
+
+import (
+	"fmt"
+
+	"github.com/pkg/errors"
+
+	"github.com/bluele/gcache"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/extensions/config"
+	"github.com/hyperledger/fabric/extensions/pvtdatastorage/common"
+)
+
+var logger = flogging.MustGetLogger("cachedpvtdatastore")
+
+type provider struct {
+}
+
+type store struct {
+	ledgerid           string
+	btlPolicy          pvtdatapolicy.BTLPolicy
+	cache              gcache.Cache
+	lastCommittedBlock uint64
+	pendingPvtData     *pendingPvtData
+	isEmpty            bool
+}
+
+type pendingPvtData struct {
+	batchPending bool
+	dataEntries  []*common.DataEntry
+}
+
+//////// Provider functions  /////////////
+//////////////////////////////////////////
+
+// NewProvider instantiates a private data storage provider backed by cache
+func NewProvider() pvtdatastorage.Provider {
+	logger.Debugf("constructing cached private data storage provider")
+	return &provider{}
+}
+
+// OpenStore returns a handle to a store
+func (p *provider) OpenStore(ledgerid string) (pvtdatastorage.Store, error) {
+	s := &store{cache: gcache.New(config.GetPvtDataCacheSize()).ARC().Build(), ledgerid: ledgerid,
+		pendingPvtData:     &pendingPvtData{batchPending: false},
+		isEmpty:            true,
+		lastCommittedBlock: 0,
+	}
+
+	//TODO set last committed block from couchdb
+
+	logger.Debugf("Pvtdata store opened. Initial state: isEmpty [%t], lastCommittedBlock [%d]",
+		s.isEmpty, s.lastCommittedBlock)
+
+	return s, nil
+}
+
+// Close closes the store
+func (p *provider) Close() {
+}
+
+//////// store functions  ////////////////
+//////////////////////////////////////////
+
+func (s *store) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
+	s.btlPolicy = btlPolicy
+}
+
+// Prepare implements the function in the interface `Store`
+func (s *store) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData, missingPvtData ledger.TxMissingPvtDataMap) error {
+	if s.pendingPvtData.batchPending {
+		return pvtdatastorage.NewErrIllegalCall(`A pending batch exists as as result of last invoke to "Prepare" call. Invoke "Commit" or "Rollback" on the pending batch before invoking "Prepare" function`)
+	}
+
+	expectedBlockNum := s.nextBlockNum()
+	if expectedBlockNum != blockNum {
+		return pvtdatastorage.NewErrIllegalCall(fmt.Sprintf("Expected block number=%d, received block number=%d", expectedBlockNum, blockNum))
+	}
+
+	storeEntries, err := common.PrepareStoreEntries(blockNum, pvtData, s.btlPolicy, missingPvtData)
+	if err != nil {
+		return err
+	}
+
+	s.pendingPvtData = &pendingPvtData{batchPending: true}
+	if len(storeEntries.DataEntries) > 0 {
+		s.pendingPvtData.dataEntries = storeEntries.DataEntries
+	}
+	logger.Debugf("Saved %d private data write sets for block [%d]", len(pvtData), blockNum)
+	return nil
+}
+
+// Commit implements the function in the interface `Store`
+func (s *store) Commit() error {
+	committingBlockNum := s.nextBlockNum()
+	logger.Debugf("Committing private data for block [%d]", committingBlockNum)
+
+	if s.pendingPvtData.dataEntries != nil {
+		err := s.cache.Set(committingBlockNum, s.pendingPvtData.dataEntries)
+		if err != nil {
+			return errors.WithMessage(err, fmt.Sprintf("writing private data to cache failed [%d]", committingBlockNum))
+		}
+	}
+
+	s.pendingPvtData = &pendingPvtData{batchPending: false}
+	s.isEmpty = false
+	s.lastCommittedBlock = committingBlockNum
+
+	logger.Debugf("Committed private data for block [%d]", committingBlockNum)
+	return nil
+}
+
+// Rollback implements the function in the interface `Store`
+func (s *store) Rollback() error {
+	s.pendingPvtData = &pendingPvtData{batchPending: false}
+	return nil
+}
+
+// CommitPvtDataOfOldBlocks implements the function in the interface `Store`
+func (s *store) CommitPvtDataOfOldBlocks(blocksPvtData map[uint64][]*ledger.TxPvtData) error {
+	return errors.New("not supported")
+}
+
+// GetLastUpdatedOldBlocksPvtData implements the function in the interface `Store`
+func (s *store) GetLastUpdatedOldBlocksPvtData() (map[uint64][]*ledger.TxPvtData, error) {
+	return nil, errors.New("not supported")
+}
+
+// ResetLastUpdatedOldBlocksList implements the function in the interface `Store`
+func (s *store) ResetLastUpdatedOldBlocksList() error {
+	return errors.New("not supported")
+}
+
+// GetPvtDataByBlockNum implements the function in the interface `Store`.
+// If the store is empty or the last committed block number is smaller then the
+// requested block number, an 'ErrOutOfRange' is thrown
+func (s *store) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	logger.Debugf("Get private data for block [%d], filter=%#v", blockNum, filter)
+	if s.isEmpty {
+		return nil, pvtdatastorage.NewErrOutOfRange("The store is empty")
+	}
+	if blockNum > s.lastCommittedBlock {
+		return nil, pvtdatastorage.NewErrOutOfRange(fmt.Sprintf("Last committed block=%d, block requested=%d", s.lastCommittedBlock, blockNum))
+	}
+
+	value, err := s.cache.Get(blockNum)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get must never return an error other than KeyNotFoundError err:%s", err))
+		}
+		return nil, nil
+	}
+
+	dataEntries := value.([]*common.DataEntry)
+
+	return s.getBlockPvtData(dataEntries, filter, blockNum)
+
+}
+
+// ProcessCollsEligibilityEnabled implements the function in the interface `Store`
+func (s *store) ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string) error {
+	return errors.New("not supported")
+}
+
+//GetMissingPvtDataInfoForMostRecentBlocks implements the function in the interface `Store`
+func (s *store) GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int) (ledger.MissingPvtDataInfo, error) {
+	return nil, errors.New("not supported")
+}
+
+// LastCommittedBlockHeight implements the function in the interface `Store`
+func (s *store) LastCommittedBlockHeight() (uint64, error) {
+	if s.isEmpty {
+		return 0, nil
+	}
+	return s.lastCommittedBlock + 1, nil
+}
+
+// HasPendingBatch implements the function in the interface `Store`
+func (s *store) HasPendingBatch() (bool, error) {
+	return s.pendingPvtData.batchPending, nil
+}
+
+// IsEmpty implements the function in the interface `Store`
+func (s *store) IsEmpty() (bool, error) {
+	return s.isEmpty, nil
+}
+
+// InitLastCommittedBlock implements the function in the interface `Store`
+func (s *store) InitLastCommittedBlock(blockNum uint64) error {
+	if !(s.isEmpty && !s.pendingPvtData.batchPending) {
+		return pvtdatastorage.NewErrIllegalCall("The private data store is not empty. InitLastCommittedBlock() function call is not allowed")
+	}
+	s.isEmpty = false
+	s.lastCommittedBlock = blockNum
+
+	logger.Debugf("InitLastCommittedBlock set to block [%d]", blockNum)
+	return nil
+}
+
+// Shutdown implements the function in the interface `Store`
+func (s *store) Shutdown() {
+	// do nothing
+}
+
+func v11RetrievePvtdata(dataEntries []*common.DataEntry, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	var blkPvtData []*ledger.TxPvtData
+	for _, dataEntry := range dataEntries {
+		value, err := common.EncodeDataValue(dataEntry.Value)
+		if err != nil {
+			return nil, err
+		}
+		pvtDatum, err := common.V11DecodeKV(common.EncodeDataKey(dataEntry.Key), value, filter)
+		if err != nil {
+			return nil, err
+		}
+		blkPvtData = append(blkPvtData, pvtDatum)
+	}
+	return blkPvtData, nil
+}
+
+func (s *store) nextBlockNum() uint64 {
+	if s.isEmpty {
+		return 0
+	}
+	return s.lastCommittedBlock + 1
+}
+
+func (s *store) getBlockPvtData(dataEntries []*common.DataEntry, filter ledger.PvtNsCollFilter, blockNum uint64) ([]*ledger.TxPvtData, error) {
+	var blockPvtdata []*ledger.TxPvtData
+	var currentTxNum uint64
+	var currentTxWsetAssember *common.TxPvtdataAssembler
+	firstItr := true
+
+	for _, dataEntry := range dataEntries {
+		dataKeyBytes := common.EncodeDataKey(dataEntry.Key)
+		if common.V11Format(dataKeyBytes) {
+			return v11RetrievePvtdata(dataEntries, filter)
+		}
+		expired, err := common.IsExpired(dataEntry.Key.NsCollBlk, s.btlPolicy, s.lastCommittedBlock)
+		if err != nil {
+			return nil, err
+		}
+		if expired || !common.PassesFilter(dataEntry.Key, filter) {
+			continue
+		}
+
+		if firstItr {
+			currentTxNum = dataEntry.Key.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+			firstItr = false
+		}
+
+		if dataEntry.Key.TxNum != currentTxNum {
+			blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+			currentTxNum = dataEntry.Key.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+		}
+		currentTxWsetAssember.Add(dataEntry.Key.Ns, dataEntry.Value)
+	}
+	if currentTxWsetAssember != nil {
+		blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+	}
+	return blockPvtdata, nil
+}
diff --git a/extensions/pvtdatastorage/cachedpvtdatastore/store_impl_test.go b/extensions/pvtdatastorage/cachedpvtdatastore/store_impl_test.go
new file mode 100644
index 00000000..1d2e84ef
--- /dev/null
+++ b/extensions/pvtdatastorage/cachedpvtdatastore/store_impl_test.go
@@ -0,0 +1,305 @@
+/*
+Copyright IBM Corp., SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cachedpvtdatastore
+
+import (
+	"bytes"
+	"fmt"
+	"strings"
+	"testing"
+
+	"github.com/bluele/gcache"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	btltestutil "github.com/hyperledger/fabric/core/ledger/pvtdatapolicy/testutil"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/extensions/pvtdatastorage/common"
+	"github.com/stretchr/testify/require"
+)
+
+func TestEmptyStore(t *testing.T) {
+	env := NewTestStoreEnv(t, "testemptystore", nil)
+	req := require.New(t)
+	store := env.TestStore
+	testEmpty(true, req, store)
+	testPendingBatch(false, req, store)
+}
+
+func TestStoreBasicCommitAndRetrieval(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 0,
+			{"ns-3", "coll-1"}: 0,
+			{"ns-4", "coll-1"}: 0,
+			{"ns-4", "coll-2"}: 0,
+		},
+	)
+
+	env := NewTestStoreEnv(t, "teststorebasiccommitandretrieval", btlPolicy)
+	req := require.New(t)
+	store := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+
+	// construct missing data for block 1
+	blk1MissingData := make(ledger.TxMissingPvtDataMap)
+
+	// eligible missing data in tx1
+	blk1MissingData.Add(1, "ns-1", "coll-1", true)
+	blk1MissingData.Add(1, "ns-1", "coll-2", true)
+	blk1MissingData.Add(1, "ns-2", "coll-1", true)
+	blk1MissingData.Add(1, "ns-2", "coll-2", true)
+	// eligible missing data in tx2
+	blk1MissingData.Add(2, "ns-3", "coll-1", true)
+	// ineligible missing data in tx4
+	blk1MissingData.Add(4, "ns-4", "coll-1", false)
+	blk1MissingData.Add(4, "ns-4", "coll-2", false)
+
+	// construct missing data for block 2
+	blk2MissingData := make(ledger.TxMissingPvtDataMap)
+	// eligible missing data in tx1
+	blk2MissingData.Add(1, "ns-1", "coll-1", true)
+	blk2MissingData.Add(1, "ns-1", "coll-2", true)
+	// eligible missing data in tx3
+	blk2MissingData.Add(3, "ns-1", "coll-1", true)
+
+	// no pvt data with block 0
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	// pvt data with block 1 - commit
+	req.NoError(store.Prepare(1, testData, blk1MissingData))
+	req.NoError(store.Commit())
+
+	// pvt data with block 2 - rollback
+	req.NoError(store.Prepare(2, testData, nil))
+	req.NoError(store.Rollback())
+
+	// pvt data retrieval for block 0 should return nil
+	var nilFilter ledger.PvtNsCollFilter
+	retrievedData, err := store.GetPvtDataByBlockNum(0, nilFilter)
+	req.NoError(err)
+	req.Nil(retrievedData)
+
+	// pvt data retrieval for block 1 should return full pvtdata
+	retrievedData, err = store.GetPvtDataByBlockNum(1, nilFilter)
+	req.NoError(err)
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, testData[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, testData[i].WriteSet))
+	}
+
+	// pvt data retrieval for block 1 with filter should return filtered pvtdata
+	filter := ledger.NewPvtNsCollFilter()
+	filter.Add("ns-1", "coll-1")
+	filter.Add("ns-2", "coll-2")
+	retrievedData, err = store.GetPvtDataByBlockNum(1, filter)
+	expectedRetrievedData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+	}
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, expectedRetrievedData[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, expectedRetrievedData[i].WriteSet))
+	}
+
+	// pvt data retrieval for block 2 should return ErrOutOfRange
+	retrievedData, err = store.GetPvtDataByBlockNum(2, nilFilter)
+	_, ok := err.(*pvtdatastorage.ErrOutOfRange)
+	req.True(ok)
+	req.Nil(retrievedData)
+
+	// pvt data with block 2 - commit
+	req.NoError(store.Prepare(2, testData, blk2MissingData))
+	req.NoError(store.Commit())
+}
+
+func TestStoreState(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "teststorestate", btlPolicy)
+	req := require.New(t)
+	store := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 0, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+	}
+	_, ok := store.Prepare(1, testData, nil).(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+
+	req.Nil(store.Prepare(0, testData, nil))
+	req.NoError(store.Commit())
+
+	req.Nil(store.Prepare(1, testData, nil))
+	_, ok = store.Prepare(2, testData, nil).(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+}
+
+func TestInitLastCommittedBlock(t *testing.T) {
+	env := NewTestStoreEnv(t, "teststorestate", nil)
+	req := require.New(t)
+	store := env.TestStore
+	existingLastBlockNum := uint64(25)
+	req.NoError(store.InitLastCommittedBlock(existingLastBlockNum))
+
+	testEmpty(false, req, store)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(existingLastBlockNum+1, req, store)
+
+	env.CloseAndReopen()
+	testEmpty(false, req, store)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(existingLastBlockNum+1, req, store)
+
+	err := store.InitLastCommittedBlock(30)
+	_, ok := err.(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+}
+
+func TestRollBack(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "testrollback", btlPolicy)
+	req := require.New(t)
+	store := env.TestStore
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	pvtdata := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 0, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+		produceSamplePvtdata(t, 5, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+	}
+	missingData := make(ledger.TxMissingPvtDataMap)
+	missingData.Add(1, "ns-1", "coll-1", true)
+	missingData.Add(5, "ns-1", "coll-1", true)
+	missingData.Add(5, "ns-2", "coll-2", false)
+
+	for i := 1; i <= 9; i++ {
+		req.NoError(store.Prepare(uint64(i), pvtdata, missingData))
+		req.NoError(store.Commit())
+	}
+
+	datakeyTx0 := &common.DataKey{
+		NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1"},
+		TxNum:     0,
+	}
+	datakeyTx5 := &common.DataKey{
+		NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1"},
+		TxNum:     5,
+	}
+	eligibleMissingdatakey := &common.MissingDataKey{
+		NsCollBlk:  common.NsCollBlk{Ns: "ns-1", Coll: "coll-1"},
+		IsEligible: true,
+	}
+
+	// test store state before preparing for block 10
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(10, req, store)
+
+	// prepare for block 10 and test store for presence of datakeys and eligibile missingdatakeys
+	req.NoError(store.Prepare(10, pvtdata, missingData))
+	testPendingBatch(true, req, store)
+	testLastCommittedBlockHeight(10, req, store)
+
+	datakeyTx0.BlkNum = 10
+	datakeyTx5.BlkNum = 10
+	req.True(testPendingDataKeyExists(t, store, datakeyTx0))
+	req.True(testPendingDataKeyExists(t, store, datakeyTx5))
+
+	// rollback last prepared block and test store for absence of datakeys and eligibile missingdatakeys
+	err := store.Rollback()
+	req.NoError(err)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(10, req, store)
+	req.False(testPendingDataKeyExists(t, store, datakeyTx0))
+	req.False(testPendingDataKeyExists(t, store, datakeyTx5))
+
+	// For previously committed blocks the datakeys and eligibile missingdatakeys should still be present
+	for i := 1; i <= 9; i++ {
+		datakeyTx0.BlkNum = uint64(i)
+		datakeyTx5.BlkNum = uint64(i)
+		eligibleMissingdatakey.BlkNum = uint64(i)
+		req.True(testDataKeyExists(t, store, datakeyTx0))
+		req.True(testDataKeyExists(t, store, datakeyTx5))
+	}
+}
+
+func testLastCommittedBlockHeight(expectedBlockHt uint64, req *require.Assertions, store pvtdatastorage.Store) {
+	blkHt, err := store.LastCommittedBlockHeight()
+	req.NoError(err)
+	req.Equal(expectedBlockHt, blkHt)
+}
+
+func testDataKeyExists(t *testing.T, s pvtdatastorage.Store, dataKey *common.DataKey) bool {
+
+	value, err := s.(*store).cache.Get(dataKey.BlkNum)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get must never return an error other than KeyNotFoundError err:%s", err))
+		}
+		return false
+	}
+
+	dataEntries := value.([]*common.DataEntry)
+	for _, value := range dataEntries {
+		if bytes.Equal(common.EncodeDataKey(value.Key), common.EncodeDataKey(dataKey)) {
+			return true
+		}
+	}
+	return false
+}
+
+func testPendingDataKeyExists(t *testing.T, s pvtdatastorage.Store, dataKey *common.DataKey) bool {
+	if s.(*store).pendingPvtData.dataEntries == nil {
+		return false
+	}
+	for _, value := range s.(*store).pendingPvtData.dataEntries {
+		if bytes.Equal(common.EncodeDataKey(value.Key), common.EncodeDataKey(dataKey)) {
+			return true
+		}
+	}
+	return false
+}
+
+func testEmpty(expectedEmpty bool, req *require.Assertions, store pvtdatastorage.Store) {
+	isEmpty, err := store.IsEmpty()
+	req.NoError(err)
+	req.Equal(expectedEmpty, isEmpty)
+}
+
+func testPendingBatch(expectedPending bool, req *require.Assertions, store pvtdatastorage.Store) {
+	hasPendingBatch, err := store.HasPendingBatch()
+	req.NoError(err)
+	req.Equal(expectedPending, hasPendingBatch)
+}
+
+func produceSamplePvtdata(t *testing.T, txNum uint64, nsColls []string) *ledger.TxPvtData {
+	builder := rwsetutil.NewRWSetBuilder()
+	for _, nsColl := range nsColls {
+		nsCollSplit := strings.Split(nsColl, ":")
+		ns := nsCollSplit[0]
+		coll := nsCollSplit[1]
+		builder.AddToPvtAndHashedWriteSet(ns, coll, fmt.Sprintf("key-%s-%s", ns, coll), []byte(fmt.Sprintf("value-%s-%s", ns, coll)))
+	}
+	simRes, err := builder.GetTxSimulationResults()
+	require.NoError(t, err)
+	return &ledger.TxPvtData{SeqInBlock: txNum, WriteSet: simRes.PvtSimulationResults}
+}
diff --git a/extensions/pvtdatastorage/cachedpvtdatastore/test_exports.go b/extensions/pvtdatastorage/cachedpvtdatastore/test_exports.go
new file mode 100644
index 00000000..30d29050
--- /dev/null
+++ b/extensions/pvtdatastorage/cachedpvtdatastore/test_exports.go
@@ -0,0 +1,46 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package cachedpvtdatastore
+
+import (
+	"testing"
+
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/stretchr/testify/require"
+)
+
+// StoreEnv provides the  store env for testing
+type StoreEnv struct {
+	t                 testing.TB
+	TestStoreProvider pvtdatastorage.Provider
+	TestStore         pvtdatastorage.Store
+	ledgerid          string
+	btlPolicy         pvtdatapolicy.BTLPolicy
+}
+
+// NewTestStoreEnv construct a StoreEnv for testing
+func NewTestStoreEnv(t *testing.T, ledgerid string, btlPolicy pvtdatapolicy.BTLPolicy) *StoreEnv {
+	req := require.New(t)
+	testStoreProvider := NewProvider()
+	testStore, err := testStoreProvider.OpenStore(ledgerid)
+	req.NoError(err)
+	testStore.Init(btlPolicy)
+	s := &StoreEnv{t, testStoreProvider, testStore, ledgerid, btlPolicy}
+	return s
+}
+
+// CloseAndReopen closes and opens the store provider
+func (env *StoreEnv) CloseAndReopen() {
+	var err error
+	env.TestStoreProvider.Close()
+	env.TestStoreProvider = NewProvider()
+	require.NoError(env.t, err)
+	env.TestStore, err = env.TestStoreProvider.OpenStore(env.ledgerid)
+	env.TestStore.Init(env.btlPolicy)
+	require.NoError(env.t, err)
+}
diff --git a/extensions/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go b/extensions/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go
new file mode 100644
index 00000000..1f143335
--- /dev/null
+++ b/extensions/pvtdatastorage/cdbpvtdatastore/couchdb_conv.go
@@ -0,0 +1,277 @@
+/*
+Copyright IBM Corp., SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"strconv"
+
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/extensions/pvtdatastorage/common"
+	"github.com/pkg/errors"
+)
+
+const (
+	idField                       = "_id"
+	revField                      = "_rev"
+	dataField                     = "data"
+	expiryField                   = "expiry"
+	expiringBlkNumsField          = "expiringBlkNums"
+	expiringBlockNumbersIndexName = "by_expiring_block_numbers"
+	expiringBlockNumbersIndexDoc  = "indexExpiringBlockNumbers"
+	blockKeyPrefix                = ""
+	lastCommittedBlockID          = "lastCommittedBlock"
+	lastCommittedBlockData        = "data"
+)
+
+type blockPvtDataResponse struct {
+	ID              string            `json:"_id"`
+	Rev             string            `json:"_rev"`
+	Data            map[string][]byte `json:"data"`
+	Expiry          map[string][]byte `json:"expiry"`
+	Deleted         bool              `json:"_deleted"`
+	ExpiringBlkNums []string          `json:"expiringBlkNums"`
+}
+
+type lastCommittedBlockResponse struct {
+	ID   string `json:"_id"`
+	Rev  string `json:"_rev"`
+	Data string `json:"data"`
+}
+
+const expiringBlockNumbersIndexDef = `
+	{
+		"index": {
+			"fields": ["` + expiringBlkNumsField + `"]
+		},
+		"name": "` + expiringBlockNumbersIndexName + `",
+		"ddoc": "` + expiringBlockNumbersIndexDoc + `",
+		"type": "json"
+	}`
+
+type jsonValue map[string]interface{}
+
+func (v jsonValue) toBytes() ([]byte, error) {
+	return json.Marshal(v)
+}
+
+func createPvtDataCouchDoc(storeEntries *common.StoreEntries, blockNumber uint64, rev string) (*couchdb.CouchDoc, error) {
+	if len(storeEntries.DataEntries) <= 0 && len(storeEntries.ExpiryEntries) <= 0 {
+		return nil, nil
+	}
+	jsonMap := make(jsonValue)
+	jsonMap[idField] = blockNumberToKey(blockNumber)
+
+	if rev != "" {
+		jsonMap[revField] = rev
+	}
+
+	dataEntriesJSON, err := dataEntriesToJSONValue(storeEntries.DataEntries)
+	if err != nil {
+		return nil, err
+	}
+	jsonMap[dataField] = dataEntriesJSON
+
+	expiryEntriesJSON, expiringBlkNums, err := expiryEntriesToJSONValue(storeEntries.ExpiryEntries)
+	if err != nil {
+		return nil, err
+	}
+	jsonMap[expiryField] = expiryEntriesJSON
+
+	jsonMap[expiringBlkNumsField] = expiringBlkNums
+
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	return &couchDoc, nil
+
+}
+
+func createLastCommittedBlockDoc(committingBlockNum uint64, rev string) (*couchdb.CouchDoc, error) {
+	jsonMap := make(jsonValue)
+	jsonMap[idField] = lastCommittedBlockID
+	if rev != "" {
+		jsonMap[revField] = rev
+	}
+	jsonMap[lastCommittedBlockData] = strconv.FormatUint(committingBlockNum, 10)
+	jsonBytes, err := jsonMap.toBytes()
+	if err != nil {
+		return nil, err
+	}
+
+	couchDoc := couchdb.CouchDoc{JSONValue: jsonBytes}
+
+	return &couchDoc, nil
+
+}
+
+func lookupLastBlock(db *couchdb.CouchDatabase) (uint64, string, error) {
+	v, _, err := db.ReadDoc(lastCommittedBlockID)
+	if err != nil {
+		return 0, "", err
+	}
+	if v != nil {
+		var lastBlockResponse lastCommittedBlockResponse
+		if err = json.Unmarshal(v.JSONValue, &lastBlockResponse); err != nil {
+			return 0, "", err
+		}
+		lastBlockNum, err := strconv.ParseInt(lastBlockResponse.Data, 10, 64)
+		if err != nil {
+			return 0, "", err
+		}
+		return uint64(lastBlockNum), lastBlockResponse.Rev, nil
+	}
+	return 0, "", nil
+}
+
+func dataEntriesToJSONValue(dataEntries []*common.DataEntry) (jsonValue, error) {
+	data := make(jsonValue)
+
+	for _, dataEntry := range dataEntries {
+		keyBytes := common.EncodeDataKey(dataEntry.Key)
+		valBytes, err := common.EncodeDataValue(dataEntry.Value)
+		if err != nil {
+			return nil, err
+		}
+
+		keyBytesHex := hex.EncodeToString(keyBytes)
+		data[keyBytesHex] = valBytes
+	}
+
+	return data, nil
+}
+
+func expiryEntriesToJSONValue(expiryEntries []*common.ExpiryEntry) (jsonValue, []string, error) {
+	data := make(jsonValue)
+	expiringBlkNums := make([]string, 0)
+
+	for _, expEntry := range expiryEntries {
+		keyBytes := common.EncodeExpiryKey(expEntry.Key)
+		valBytes, err := common.EncodeExpiryValue(expEntry.Value)
+		if err != nil {
+			return nil, nil, err
+		}
+		expiringBlkNums = append(expiringBlkNums, blockNumberToKey(expEntry.Key.ExpiringBlk))
+		keyBytesHex := hex.EncodeToString(keyBytes)
+		data[keyBytesHex] = valBytes
+	}
+
+	return data, expiringBlkNums, nil
+}
+
+func createPvtDataCouchDB(couchInstance *couchdb.CouchInstance, dbName string) (*couchdb.CouchDatabase, error) {
+	db, err := couchdb.CreateCouchDatabase(couchInstance, dbName)
+	if err != nil {
+		return nil, err
+	}
+	err = db.CreateNewIndexWithRetry(expiringBlockNumbersIndexDef, expiringBlockNumbersIndexDoc)
+	if err != nil {
+		return nil, errors.WithMessage(err, "creation of purge block number index failed")
+	}
+	return db, err
+}
+
+func getPvtDataCouchInstance(couchInstance *couchdb.CouchInstance, dbName string) (*couchdb.CouchDatabase, error) {
+	db, err := couchdb.NewCouchDatabase(couchInstance, dbName)
+	if err != nil {
+		return nil, err
+	}
+
+	dbExists, err := db.ExistsWithRetry()
+	if err != nil {
+		return nil, err
+	}
+	if !dbExists {
+		return nil, errors.Errorf("DB not found: [%s]", db.DBName)
+	}
+
+	indexExists, err := db.IndexDesignDocExistsWithRetry(expiringBlockNumbersIndexDoc)
+	if err != nil {
+		return nil, err
+	}
+	if !indexExists {
+		return nil, errors.Errorf("DB index not found: [%s]", db.DBName)
+	}
+	return db, nil
+}
+
+func retrieveBlockPvtData(db *couchdb.CouchDatabase, id string) (*blockPvtDataResponse, error) {
+	doc, _, err := db.ReadDoc(id)
+	if err != nil {
+		return nil, err
+	}
+
+	if doc == nil {
+		return nil, NewErrNotFoundInIndex()
+	}
+
+	var blockPvtData blockPvtDataResponse
+	err = json.Unmarshal(doc.JSONValue, &blockPvtData)
+	if err != nil {
+		return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+	}
+
+	return &blockPvtData, nil
+}
+
+func retrieveBlockExpiryData(db *couchdb.CouchDatabase, id string) ([]*blockPvtDataResponse, error) {
+	const queryFmt = `
+	{
+		"selector": {
+			"` + expiringBlkNumsField + `": {
+				"$elemMatch": {
+					"$lte": "%s"
+				}
+			}
+		},
+		"use_index": ["_design/` + expiringBlockNumbersIndexDoc + `", "` + expiringBlockNumbersIndexName + `"]
+	}`
+
+	results, _, err := db.QueryDocuments(fmt.Sprintf(queryFmt, id))
+	if err != nil {
+		return nil, err
+	}
+
+	if len(results) == 0 {
+		return nil, nil
+	}
+
+	var responses []*blockPvtDataResponse
+	for _, result := range results {
+		var blockPvtData blockPvtDataResponse
+		err = json.Unmarshal(result.Value, &blockPvtData)
+		if err != nil {
+			return nil, errors.Wrapf(err, "result from DB is not JSON encoded")
+		}
+		responses = append(responses, &blockPvtData)
+	}
+
+	return responses, nil
+}
+
+func blockNumberToKey(blockNum uint64) string {
+	return fmt.Sprintf("%064s", blockKeyPrefix+strconv.FormatUint(blockNum, 10))
+}
+
+// NotFoundInIndexErr is used to indicate missing entry in the index
+type NotFoundInIndexErr struct {
+}
+
+// NewErrNotFoundInIndex creates an missing entry in the index error
+func NewErrNotFoundInIndex() *NotFoundInIndexErr {
+	return &NotFoundInIndexErr{}
+}
+
+func (err *NotFoundInIndexErr) Error() string {
+	return "Entry not found in index"
+}
diff --git a/extensions/pvtdatastorage/cdbpvtdatastore/store_impl.go b/extensions/pvtdatastorage/cdbpvtdatastore/store_impl.go
new file mode 100644
index 00000000..675de226
--- /dev/null
+++ b/extensions/pvtdatastorage/cdbpvtdatastore/store_impl.go
@@ -0,0 +1,828 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"sort"
+	"strings"
+	"sync"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/extensions/pvtdatastorage/common"
+	"github.com/pkg/errors"
+	"github.com/willf/bitset"
+)
+
+var logger = flogging.MustGetLogger("cdbpvtdatastore")
+
+const (
+	pvtDataStoreName = "pvtdata"
+)
+
+type provider struct {
+	couchInstance            *couchdb.CouchInstance
+	missingKeysIndexProvider *leveldbhelper.Provider
+}
+
+type store struct {
+	ledgerid           string
+	btlPolicy          pvtdatapolicy.BTLPolicy
+	db                 *couchdb.CouchDatabase
+	lastCommittedBlock uint64
+	purgerLock         *sync.Mutex
+	pendingPvtData     *pendingPvtData
+	collElgProc        *common.CollElgProc
+	// missing keys db
+	missingKeysIndexDB *leveldbhelper.DBHandle
+	isEmpty            bool
+	// After committing the pvtdata of old blocks,
+	// the `isLastUpdatedOldBlocksSet` is set to true.
+	// Once the stateDB is updated with these pvtdata,
+	// the `isLastUpdatedOldBlocksSet` is set to false.
+	// isLastUpdatedOldBlocksSet is mainly used during the
+	// recovery process. During the peer startup, if the
+	// isLastUpdatedOldBlocksSet is set to true, the pvtdata
+	// in the stateDB needs to be updated before finishing the
+	// recovery operation.
+	isLastUpdatedOldBlocksSet bool
+}
+
+type pendingPvtData struct {
+	PvtDataDoc         *couchdb.CouchDoc `json:"pvtDataDoc"`
+	MissingDataEntries map[string]string `json:"missingDataEntries"`
+	BatchPending       bool              `json:"batchPending"`
+}
+
+// lastUpdatedOldBlocksList keeps the list of last updated blocks
+// and is stored as the value of lastUpdatedOldBlocksKey (defined in kv_encoding.go)
+type lastUpdatedOldBlocksList []uint64
+
+//////// Provider functions  /////////////
+//////////////////////////////////////////
+
+// NewProvider instantiates a private data storage provider backed by CouchDB
+func NewProvider() (pvtdatastorage.Provider, error) {
+	logger.Debugf("constructing CouchDB private data storage provider")
+	couchDBDef := couchdb.GetCouchDBDefinition()
+
+	return newProviderWithDBDef(couchDBDef)
+}
+
+func newProviderWithDBDef(couchDBDef *couchdb.CouchDBDef) (pvtdatastorage.Provider, error) {
+	couchInstance, err := couchdb.CreateCouchInstance(couchDBDef.URL, couchDBDef.Username, couchDBDef.Password,
+		couchDBDef.MaxRetries, couchDBDef.MaxRetriesOnStartup, couchDBDef.RequestTimeout, couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	if err != nil {
+		return nil, errors.WithMessage(err, "obtaining CouchDB instance failed")
+	}
+
+	dbPath := ledgerconfig.GetPvtdataStorePath()
+	missingKeysIndexProvider := leveldbhelper.NewProvider(&leveldbhelper.Conf{DBPath: dbPath})
+
+	return &provider{couchInstance, missingKeysIndexProvider}, nil
+}
+
+// OpenStore returns a handle to a store
+func (p *provider) OpenStore(ledgerid string) (pvtdatastorage.Store, error) {
+	// Create couchdb
+	pvtDataStoreDBName := couchdb.ConstructBlockchainDBName(strings.ToLower(ledgerid), pvtDataStoreName)
+	var db *couchdb.CouchDatabase
+	var err error
+
+	db, err = getPvtDataCouchInstance(p.couchInstance, pvtDataStoreDBName)
+	if err != nil {
+		if strings.Contains(err.Error(), "not found") {
+			db, err = createPvtDataCouchDB(p.couchInstance, pvtDataStoreDBName)
+			if err != nil {
+				return nil, err
+			}
+			// Create missing pvt keys index in leveldb
+			missingKeysIndexDB := p.missingKeysIndexProvider.GetDBHandle(ledgerid)
+
+			purgerLock := &sync.Mutex{}
+			s := &store{db: db, ledgerid: ledgerid,
+				collElgProc:        common.NewCollElgProc(purgerLock, missingKeysIndexDB),
+				purgerLock:         purgerLock,
+				missingKeysIndexDB: missingKeysIndexDB,
+				pendingPvtData:     &pendingPvtData{BatchPending: false},
+			}
+
+			if errInitState := s.initState(); errInitState != nil {
+				return nil, errInitState
+			}
+			s.collElgProc.LaunchCollElgProc()
+
+			logger.Debugf("Pvtdata store opened. Initial state: isEmpty [%t], lastCommittedBlock [%d]",
+				s.isEmpty, s.lastCommittedBlock)
+
+			return s, nil
+		}
+		return nil, err
+	}
+	s := &store{db: db, ledgerid: ledgerid,
+		pendingPvtData: &pendingPvtData{BatchPending: false},
+	}
+	lastCommittedBlock, _, err := lookupLastBlock(db)
+	if err != nil {
+		return nil, err
+	}
+	s.isEmpty = true
+	if lastCommittedBlock != 0 {
+		s.lastCommittedBlock = lastCommittedBlock
+		s.isEmpty = false
+	}
+	return s, nil
+
+}
+
+// Close closes the store
+func (p *provider) Close() {
+	p.missingKeysIndexProvider.Close()
+}
+
+//////// store functions  ////////////////
+//////////////////////////////////////////
+
+func (s *store) initState() error {
+	var blist lastUpdatedOldBlocksList
+	lastCommittedBlock, _, err := lookupLastBlock(s.db)
+	if err != nil {
+		return err
+	}
+	s.isEmpty = true
+	if lastCommittedBlock != 0 {
+		s.lastCommittedBlock = lastCommittedBlock
+		s.isEmpty = false
+	}
+
+	if s.pendingPvtData, err = s.hasPendingCommit(); err != nil {
+		return err
+	}
+
+	if blist, err = common.GetLastUpdatedOldBlocksList(s.missingKeysIndexDB); err != nil {
+		return err
+	}
+	if len(blist) > 0 {
+		s.isLastUpdatedOldBlocksSet = true
+	} // false if not set
+
+	return nil
+}
+
+func (s *store) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
+	s.btlPolicy = btlPolicy
+}
+
+// Prepare implements the function in the interface `Store`
+func (s *store) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData, missingPvtData ledger.TxMissingPvtDataMap) error {
+
+	if s.pendingPvtData.BatchPending {
+		return pvtdatastorage.NewErrIllegalCall(`A pending batch exists as as result of last invoke to "Prepare" call. Invoke "Commit" or "Rollback" on the pending batch before invoking "Prepare" function`)
+	}
+
+	expectedBlockNum := s.nextBlockNum()
+	if expectedBlockNum != blockNum {
+		return pvtdatastorage.NewErrIllegalCall(fmt.Sprintf("Expected block number=%d, received block number=%d", expectedBlockNum, blockNum))
+	}
+
+	storeEntries, err := common.PrepareStoreEntries(blockNum, pvtData, s.btlPolicy, missingPvtData)
+	if err != nil {
+		return err
+	}
+
+	pvtDataDoc, err := createPvtDataCouchDoc(storeEntries, blockNum, "")
+	if err != nil {
+		return err
+	}
+
+	s.pendingPvtData = &pendingPvtData{BatchPending: true}
+	if pvtDataDoc != nil || len(storeEntries.MissingDataEntries) > 0 {
+		s.pendingPvtData.MissingDataEntries, err = s.perparePendingMissingDataEntries(storeEntries.MissingDataEntries)
+		if err != nil {
+			return err
+		}
+		s.pendingPvtData.PvtDataDoc = pvtDataDoc
+		if err := s.savePendingKey(); err != nil {
+			return err
+		}
+
+	}
+	logger.Debugf("Saved %d private data write sets for block [%d]", len(pvtData), blockNum)
+	return nil
+}
+
+// Commit implements the function in the interface `Store`
+func (s *store) Commit() error {
+
+	if !s.pendingPvtData.BatchPending {
+		return pvtdatastorage.NewErrIllegalCall("No pending batch to commit")
+	}
+
+	committingBlockNum := s.nextBlockNum()
+	logger.Debugf("Committing private data for block [%d]", committingBlockNum)
+
+	var docs []*couchdb.CouchDoc
+	if s.pendingPvtData.PvtDataDoc != nil {
+		docs = append(docs, s.pendingPvtData.PvtDataDoc)
+	}
+
+	lastCommittedBlockDoc, err := s.prepareLastCommittedBlockDoc(committingBlockNum)
+	if err != nil {
+		return err
+	}
+	docs = append(docs, lastCommittedBlockDoc)
+
+	_, err = s.db.BatchUpdateDocuments(docs)
+	if err != nil {
+		return errors.WithMessage(err, fmt.Sprintf("writing private data to CouchDB failed [%d]", committingBlockNum))
+	}
+
+	batch := leveldbhelper.NewUpdateBatch()
+	if len(s.pendingPvtData.MissingDataEntries) > 0 {
+		for missingDataKey, missingDataValue := range s.pendingPvtData.MissingDataEntries {
+			batch.Put([]byte(missingDataKey), []byte(missingDataValue))
+		}
+		if err := s.missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+			return err
+		}
+	}
+
+	batch.Delete(common.PendingCommitKey)
+	if err := s.missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+
+	s.pendingPvtData = &pendingPvtData{BatchPending: false}
+	s.isEmpty = false
+	s.lastCommittedBlock = committingBlockNum
+
+	logger.Debugf("Committed private data for block [%d]", committingBlockNum)
+	s.performPurgeIfScheduled(committingBlockNum)
+	return nil
+}
+
+func (s *store) prepareLastCommittedBlockDoc(committingBlockNum uint64) (*couchdb.CouchDoc, error) {
+	_, rev, err := lookupLastBlock(s.db)
+	if err != nil {
+		return nil, err
+	}
+	lastCommittedBlockDoc, err := createLastCommittedBlockDoc(committingBlockNum, rev)
+	if err != nil {
+		return nil, err
+	}
+	return lastCommittedBlockDoc, nil
+}
+
+// Rollback implements the function in the interface `Store`
+func (s *store) Rollback() error {
+
+	if !s.pendingPvtData.BatchPending {
+		return pvtdatastorage.NewErrIllegalCall("No pending batch to rollback")
+	}
+
+	s.pendingPvtData = &pendingPvtData{BatchPending: false}
+	if err := s.missingKeysIndexDB.Delete(common.PendingCommitKey, true); err != nil {
+		return err
+	}
+	return nil
+}
+
+// CommitPvtDataOfOldBlocks commits the pvtData (i.e., previously missing data) of old blocks.
+// The parameter `blocksPvtData` refers a list of old block's pvtdata which are missing in the pvtstore.
+// Given a list of old block's pvtData, `CommitPvtDataOfOldBlocks` performs the following four
+// operations
+// (1) construct dataEntries for all pvtData
+// (2) construct update entries (i.e., dataEntries, expiryEntries, missingDataEntries, and
+//     lastUpdatedOldBlocksList) from the above created data entries
+// (3) create a db update batch from the update entries
+// (4) commit the update entries to the pvtStore
+func (s *store) CommitPvtDataOfOldBlocks(blocksPvtData map[uint64][]*ledger.TxPvtData) error {
+	if s.isLastUpdatedOldBlocksSet {
+		return pvtdatastorage.NewErrIllegalCall(`The lastUpdatedOldBlocksList is set. It means that the
+		stateDB may not be in sync with the pvtStore`)
+	}
+
+	batch := leveldbhelper.NewUpdateBatch()
+	docs := make([]*couchdb.CouchDoc, 0)
+	// create a list of blocks' pvtData which are being stored. If this list is
+	// found during the recovery, the stateDB may not be in sync with the pvtData
+	// and needs recovery. In a normal flow, once the stateDB is synced, the
+	// block list would be deleted.
+	updatedBlksListMap := make(map[uint64]bool)
+	// (1) construct dataEntries for all pvtData
+	entries := common.ConstructDataEntriesFromBlocksPvtData(blocksPvtData)
+
+	for blockNum, value := range entries {
+		// (2) construct update entries (i.e., dataEntries, expiryEntries, missingDataEntries) from the above created data entries
+		logger.Debugf("Constructing pvtdatastore entries for pvtData of [%d] old blocks", len(blocksPvtData))
+		updateEntries, err := common.ConstructUpdateEntriesFromDataEntries(value, s.btlPolicy, s.getExpiryDataOfExpiryKey, s.getBitmapOfMissingDataKey)
+		if err != nil {
+			return err
+		}
+		// (3) create a db update batch from the update entries
+		logger.Debug("Constructing update batch from pvtdatastore entries")
+		batch, err = common.ConstructUpdateBatchFromUpdateEntries(updateEntries, batch)
+		if err != nil {
+			return err
+		}
+		pvtDataDoc, err := s.preparePvtDataDoc(blockNum, updateEntries)
+		if err != nil {
+			return err
+		}
+		if pvtDataDoc != nil {
+			docs = append(docs, pvtDataDoc)
+		}
+		updatedBlksListMap[blockNum] = true
+	}
+	if err := s.addLastUpdatedOldBlocksList(batch, updatedBlksListMap); err != nil {
+		return err
+	}
+	// (4) commit the update entries to the pvtStore
+	logger.Debug("Committing the update batch to pvtdatastore")
+	if _, err := s.db.BatchUpdateDocuments(docs); err != nil {
+		return err
+	}
+	if err := s.missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+	s.isLastUpdatedOldBlocksSet = true
+
+	return nil
+}
+
+// GetLastUpdatedOldBlocksPvtData implements the function in the interface `Store`
+func (s *store) GetLastUpdatedOldBlocksPvtData() (map[uint64][]*ledger.TxPvtData, error) {
+	if !s.isLastUpdatedOldBlocksSet {
+		return nil, nil
+	}
+
+	updatedBlksList, err := common.GetLastUpdatedOldBlocksList(s.missingKeysIndexDB)
+	if err != nil {
+		return nil, err
+	}
+
+	blksPvtData := make(map[uint64][]*ledger.TxPvtData)
+	for _, blkNum := range updatedBlksList {
+		if blksPvtData[blkNum], err = s.GetPvtDataByBlockNum(blkNum, nil); err != nil {
+			return nil, err
+		}
+	}
+	return blksPvtData, nil
+}
+
+// ResetLastUpdatedOldBlocksList implements the function in the interface `Store`
+func (s *store) ResetLastUpdatedOldBlocksList() error {
+	if err := common.ResetLastUpdatedOldBlocksList(s.missingKeysIndexDB); err != nil {
+		return err
+	}
+	s.isLastUpdatedOldBlocksSet = false
+	return nil
+}
+
+// GetPvtDataByBlockNum implements the function in the interface `Store`.
+// If the store is empty or the last committed block number is smaller then the
+// requested block number, an 'ErrOutOfRange' is thrown
+func (s *store) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	logger.Debugf("Get private data for block [%d], filter=%#v", blockNum, filter)
+
+	if err := s.checkLastCommittedBlock(blockNum); err != nil {
+		return nil, err
+	}
+
+	blockPvtDataResponse, err := retrieveBlockPvtData(s.db, blockNumberToKey(blockNum))
+	if err != nil {
+		_, ok := err.(*NotFoundInIndexErr)
+		if ok {
+			return nil, nil
+		}
+		return nil, err
+	}
+
+	var sortedKeys []string
+	for key := range blockPvtDataResponse.Data {
+		sortedKeys = append(sortedKeys, key)
+	}
+	sort.Strings(sortedKeys)
+
+	return s.getBlockPvtData(blockPvtDataResponse.Data, filter, blockNum, sortedKeys)
+
+}
+
+func (s *store) checkLastCommittedBlock(blockNum uint64) error {
+	if s.isEmpty {
+		return pvtdatastorage.NewErrOutOfRange("The store is empty")
+	}
+	if blockNum > s.lastCommittedBlock {
+		return pvtdatastorage.NewErrOutOfRange(fmt.Sprintf("Last committed block=%d, block requested=%d", s.lastCommittedBlock, blockNum))
+	}
+
+	return nil
+}
+
+// ProcessCollsEligibilityEnabled implements the function in the interface `Store`
+func (s *store) ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string) error {
+	return common.ProcessCollsEligibilityEnabled(committingBlk, nsCollMap, s.collElgProc, s.missingKeysIndexDB)
+}
+
+// LastCommittedBlockHeight implements the function in the interface `Store`
+func (s *store) LastCommittedBlockHeight() (uint64, error) {
+	if s.isEmpty {
+		return 0, nil
+	}
+	return s.lastCommittedBlock + 1, nil
+}
+
+// HasPendingBatch implements the function in the interface `Store`
+func (s *store) HasPendingBatch() (bool, error) {
+	return s.pendingPvtData.BatchPending, nil
+}
+
+// IsEmpty implements the function in the interface `Store`
+func (s *store) IsEmpty() (bool, error) {
+	return s.isEmpty, nil
+}
+
+// Shutdown implements the function in the interface `Store`
+func (s *store) Shutdown() {
+	// do nothing
+}
+
+func (s *store) preparePvtDataDoc(blockNum uint64, updateEntries *common.EntriesForPvtDataOfOldBlocks) (*couchdb.CouchDoc, error) {
+	dataEntries, expiryEntries, rev, err := s.retrieveBlockPvtEntries(blockNum)
+	if err != nil {
+		return nil, err
+	}
+	pvtDataDoc, err := createPvtDataCouchDoc(s.prepareStoreEntries(updateEntries, dataEntries, expiryEntries), blockNum, rev)
+	if err != nil {
+		return nil, err
+	}
+	return pvtDataDoc, nil
+}
+
+func (s *store) retrieveBlockPvtEntries(blockNum uint64) ([]*common.DataEntry, []*common.ExpiryEntry, string, error) {
+	rev := ""
+	var dataEntries []*common.DataEntry
+	var expiryEntries []*common.ExpiryEntry
+	blockPvtDataResponse, err := retrieveBlockPvtData(s.db, blockNumberToKey(blockNum))
+	if err != nil {
+		_, ok := err.(*NotFoundInIndexErr)
+		if ok {
+			return nil, nil, "", nil
+		}
+		return nil, nil, "", err
+	}
+
+	if blockPvtDataResponse != nil {
+		rev = blockPvtDataResponse.Rev
+		for key := range blockPvtDataResponse.Data {
+			dataKeyBytes, errDecodeString := hex.DecodeString(key)
+			if errDecodeString != nil {
+				return nil, nil, "", errDecodeString
+			}
+			dataKey := common.DecodeDatakey(dataKeyBytes)
+			dataValue, err := common.DecodeDataValue(blockPvtDataResponse.Data[key])
+			if err != nil {
+				return nil, nil, "", err
+			}
+			dataEntries = append(dataEntries, &common.DataEntry{Key: dataKey, Value: dataValue})
+		}
+		for key := range blockPvtDataResponse.Expiry {
+			expiryKeyBytes, err := hex.DecodeString(key)
+			if err != nil {
+				return nil, nil, "", err
+			}
+			expiryKey := common.DecodeExpiryKey(expiryKeyBytes)
+			expiryValue, err := common.DecodeExpiryValue(blockPvtDataResponse.Expiry[key])
+			if err != nil {
+				return nil, nil, "", err
+			}
+			expiryEntries = append(expiryEntries, &common.ExpiryEntry{Key: expiryKey, Value: expiryValue})
+		}
+	}
+	return dataEntries, expiryEntries, rev, nil
+}
+
+func (s *store) addLastUpdatedOldBlocksList(batch *leveldbhelper.UpdateBatch, updatedBlksListMap map[uint64]bool) error {
+	var updatedBlksList lastUpdatedOldBlocksList
+	for blkNum := range updatedBlksListMap {
+		updatedBlksList = append(updatedBlksList, blkNum)
+	}
+
+	// better to store as sorted list
+	sort.SliceStable(updatedBlksList, func(i, j int) bool {
+		return updatedBlksList[i] < updatedBlksList[j]
+	})
+
+	buf := proto.NewBuffer(nil)
+	if err := buf.EncodeVarint(uint64(len(updatedBlksList))); err != nil {
+		return err
+	}
+	for _, blkNum := range updatedBlksList {
+		if err := buf.EncodeVarint(blkNum); err != nil {
+			return err
+		}
+	}
+
+	batch.Put(common.LastUpdatedOldBlocksKey, buf.Bytes())
+	return nil
+}
+
+func (s *store) getBlockPvtData(results map[string][]byte, filter ledger.PvtNsCollFilter, blockNum uint64, sortedKeys []string) ([]*ledger.TxPvtData, error) {
+	var blockPvtdata []*ledger.TxPvtData
+	var currentTxNum uint64
+	var currentTxWsetAssember *common.TxPvtdataAssembler
+	firstItr := true
+
+	for _, key := range sortedKeys {
+		dataKeyBytes, err := hex.DecodeString(key)
+		if err != nil {
+			return nil, err
+		}
+		if common.V11Format(dataKeyBytes) {
+			return v11RetrievePvtdata(results, filter)
+		}
+		dataValueBytes := results[key]
+		dataKey := common.DecodeDatakey(dataKeyBytes)
+		expired, err := s.checkIsExpired(dataKey, filter, s.lastCommittedBlock)
+		if err != nil {
+			return nil, err
+		}
+		if expired {
+			continue
+		}
+
+		dataValue, err := common.DecodeDataValue(dataValueBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		if firstItr {
+			currentTxNum = dataKey.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+			firstItr = false
+		}
+
+		if dataKey.TxNum != currentTxNum {
+			blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+			currentTxNum = dataKey.TxNum
+			currentTxWsetAssember = common.NewTxPvtdataAssembler(blockNum, currentTxNum)
+		}
+		currentTxWsetAssember.Add(dataKey.Ns, dataValue)
+	}
+	if currentTxWsetAssember != nil {
+		blockPvtdata = append(blockPvtdata, currentTxWsetAssember.GetTxPvtdata())
+	}
+	return blockPvtdata, nil
+}
+
+func (s *store) checkIsExpired(dataKey *common.DataKey, filter ledger.PvtNsCollFilter, lastCommittedBlock uint64) (bool, error) {
+	expired, err := common.IsExpired(dataKey.NsCollBlk, s.btlPolicy, lastCommittedBlock)
+	if err != nil {
+		return false, err
+	}
+	if expired || !common.PassesFilter(dataKey, filter) {
+		return true, nil
+	}
+	return false, nil
+}
+
+// InitLastCommittedBlock implements the function in the interface `Store`
+func (s *store) InitLastCommittedBlock(blockNum uint64) error {
+	if !(s.isEmpty && !s.pendingPvtData.BatchPending) {
+		return pvtdatastorage.NewErrIllegalCall("The private data store is not empty. InitLastCommittedBlock() function call is not allowed")
+	}
+	s.isEmpty = false
+	s.lastCommittedBlock = blockNum
+
+	_, rev, err := lookupLastBlock(s.db)
+	if err != nil {
+		return err
+	}
+	lastCommittedBlockDoc, err := createLastCommittedBlockDoc(s.lastCommittedBlock, rev)
+	if err != nil {
+		return err
+	}
+	_, err = s.db.BatchUpdateDocuments([]*couchdb.CouchDoc{lastCommittedBlockDoc})
+	if err != nil {
+		return err
+	}
+
+	logger.Debugf("InitLastCommittedBlock set to block [%d]", blockNum)
+	return nil
+}
+
+//GetMissingPvtDataInfoForMostRecentBlocks implements the function in the interface `Store`
+func (s *store) GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int) (ledger.MissingPvtDataInfo, error) {
+	return common.GetMissingPvtDataInfoForMostRecentBlocks(maxBlock, s.lastCommittedBlock, s.btlPolicy, s.missingKeysIndexDB)
+}
+
+func v11RetrievePvtdata(pvtDataResults map[string][]byte, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	var blkPvtData []*ledger.TxPvtData
+	for key, val := range pvtDataResults {
+		pvtDatum, err := common.V11DecodeKV([]byte(key), val, filter)
+		if err != nil {
+			return nil, err
+		}
+		blkPvtData = append(blkPvtData, pvtDatum)
+	}
+	return blkPvtData, nil
+}
+
+func (s *store) getExpiryDataOfExpiryKey(expiryKey *common.ExpiryKey) (*common.ExpiryData, error) {
+	var expiryEntriesMap map[string][]byte
+	var err error
+	if expiryEntriesMap, err = s.getExpiryEntriesDB(expiryKey.CommittingBlk); err != nil {
+		return nil, err
+	}
+	v := expiryEntriesMap[hex.EncodeToString(common.EncodeExpiryKey(expiryKey))]
+	if v == nil {
+		return nil, nil
+	}
+	return common.DecodeExpiryValue(v)
+}
+
+func (s *store) getExpiryEntriesDB(blockNum uint64) (map[string][]byte, error) {
+	blockPvtData, err := retrieveBlockPvtData(s.db, blockNumberToKey(blockNum))
+	if err != nil {
+		return nil, err
+	}
+	return blockPvtData.Expiry, nil
+}
+
+func (s *store) getBitmapOfMissingDataKey(missingDataKey *common.MissingDataKey) (*bitset.BitSet, error) {
+	var v []byte
+	var err error
+	if v, err = s.missingKeysIndexDB.Get(common.EncodeMissingDataKey(missingDataKey)); err != nil {
+		return nil, err
+	}
+	if v == nil {
+		return nil, nil
+	}
+	return common.DecodeMissingDataValue(v)
+}
+
+func (s *store) prepareStoreEntries(updateEntries *common.EntriesForPvtDataOfOldBlocks, dataEntries []*common.DataEntry, expiryEntries []*common.ExpiryEntry) *common.StoreEntries {
+	if dataEntries == nil {
+		dataEntries = make([]*common.DataEntry, 0)
+	}
+	if expiryEntries == nil {
+		expiryEntries = make([]*common.ExpiryEntry, 0)
+	}
+	for k, v := range updateEntries.DataEntries {
+		k := k
+		dataEntries = append(dataEntries, &common.DataEntry{Key: &k, Value: v})
+	}
+	for k, v := range updateEntries.ExpiryEntries {
+		k := k
+		expiryEntries = append(expiryEntries, &common.ExpiryEntry{Key: &k, Value: v})
+	}
+	return &common.StoreEntries{DataEntries: dataEntries, ExpiryEntries: expiryEntries}
+}
+
+func (s *store) hasPendingCommit() (*pendingPvtData, error) {
+	var v []byte
+	var err error
+	if v, err = s.missingKeysIndexDB.Get(common.PendingCommitKey); err != nil {
+		return nil, err
+	}
+	if v != nil {
+		var pPvtData pendingPvtData
+		if err := json.Unmarshal(v, &pPvtData); err != nil {
+			return nil, err
+		}
+		return &pPvtData, nil
+	}
+	return &pendingPvtData{BatchPending: false}, nil
+
+}
+
+func (s *store) savePendingKey() error {
+	bytes, err := json.Marshal(s.pendingPvtData)
+	if err != nil {
+		return err
+	}
+	if err := s.missingKeysIndexDB.Put(common.PendingCommitKey, bytes, true); err != nil {
+		return err
+	}
+	return nil
+}
+
+func (s *store) perparePendingMissingDataEntries(mssingDataEntries map[common.MissingDataKey]*bitset.BitSet) (map[string]string, error) {
+	pendingMissingDataEntries := make(map[string]string)
+	for missingDataKey, missingDataValue := range mssingDataEntries {
+		missingDataKey := missingDataKey
+		keyBytes := common.EncodeMissingDataKey(&missingDataKey)
+		valBytes, err := common.EncodeMissingDataValue(missingDataValue)
+		if err != nil {
+			return nil, err
+		}
+		pendingMissingDataEntries[string(keyBytes)] = string(valBytes)
+	}
+	return pendingMissingDataEntries, nil
+}
+
+func (s *store) nextBlockNum() uint64 {
+	if s.isEmpty {
+		return 0
+	}
+	return s.lastCommittedBlock + 1
+}
+
+func (s *store) performPurgeIfScheduled(latestCommittedBlk uint64) {
+	if latestCommittedBlk%ledgerconfig.GetPvtdataStorePurgeInterval() != 0 {
+		return
+	}
+	go func() {
+		s.purgerLock.Lock()
+		logger.Debugf("Purger started: Purging expired private data till block number [%d]", latestCommittedBlk)
+		defer s.purgerLock.Unlock()
+		err := s.purgeExpiredData(latestCommittedBlk)
+		if err != nil {
+			logger.Warningf("Could not purge data from pvtdata store:%s", err)
+		}
+		logger.Debug("Purger finished")
+	}()
+}
+
+func (s *store) purgeExpiredData(maxBlkNum uint64) error {
+	pvtData, err := retrieveBlockExpiryData(s.db, blockNumberToKey(maxBlkNum))
+	if err != nil {
+		return err
+	}
+	if len(pvtData) == 0 {
+		return nil
+	}
+
+	docs, batch, err := s.prepareExpiredData(pvtData, maxBlkNum)
+	if err != nil {
+		return err
+	}
+	if len(docs) > 0 {
+		_, err := s.db.BatchUpdateDocuments(docs)
+		if err != nil {
+			return errors.WithMessage(err, fmt.Sprintf("BatchUpdateDocuments failed for [%d] documents", len(docs)))
+		}
+	}
+	if err := s.missingKeysIndexDB.WriteBatch(batch, false); err != nil {
+		return err
+	}
+
+	logger.Infof("[%s] - Entries purged from private data storage till block number [%d]", s.ledgerid, maxBlkNum)
+	return nil
+}
+
+func (s *store) prepareExpiredData(pvtData []*blockPvtDataResponse, maxBlkNum uint64) ([]*couchdb.CouchDoc, *leveldbhelper.UpdateBatch, error) {
+	batch := leveldbhelper.NewUpdateBatch()
+	var docs []*couchdb.CouchDoc
+	for _, data := range pvtData {
+		expBlkNums := make([]string, 0)
+		for key, value := range data.Expiry {
+			expiryKeyBytes, err := hex.DecodeString(key)
+			if err != nil {
+				return nil, nil, err
+			}
+			expiryKey := common.DecodeExpiryKey(expiryKeyBytes)
+			if expiryKey.ExpiringBlk <= maxBlkNum {
+				expiryValue, err := common.DecodeExpiryValue(value)
+				if err != nil {
+					return nil, nil, err
+				}
+				dataKeys, missingDataKeys := common.DeriveKeys(&common.ExpiryEntry{Key: expiryKey, Value: expiryValue})
+				for _, dataKey := range dataKeys {
+					delete(data.Data, hex.EncodeToString(common.EncodeDataKey(dataKey)))
+				}
+				for _, missingDataKey := range missingDataKeys {
+					batch.Delete(common.EncodeMissingDataKey(missingDataKey))
+				}
+			} else {
+				expBlkNums = append(expBlkNums, blockNumberToKey(expiryKey.ExpiringBlk))
+			}
+		}
+		if len(data.Data) == 0 {
+			data.Deleted = true
+		}
+		data.ExpiringBlkNums = expBlkNums
+		jsonBytes, err := json.Marshal(data)
+		if err != nil {
+			return nil, nil, err
+		}
+		docs = append(docs, &couchdb.CouchDoc{JSONValue: jsonBytes})
+	}
+
+	return docs, batch, nil
+
+}
diff --git a/extensions/pvtdatastorage/cdbpvtdatastore/store_impl_test.go b/extensions/pvtdatastorage/cdbpvtdatastore/store_impl_test.go
new file mode 100644
index 00000000..1604aa40
--- /dev/null
+++ b/extensions/pvtdatastorage/cdbpvtdatastore/store_impl_test.go
@@ -0,0 +1,1047 @@
+/*
+Copyright IBM Corp., SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"os"
+	"strings"
+	"testing"
+	"time"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	btltestutil "github.com/hyperledger/fabric/core/ledger/pvtdatapolicy/testutil"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	ledgertestutil "github.com/hyperledger/fabric/core/ledger/testutil"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/hyperledger/fabric/extensions/pvtdatastorage/common"
+	xtestutil "github.com/hyperledger/fabric/extensions/testutil"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+// This unit tests are copied from fabric, original file from fabric is found in fabric/core/ledger/pvtdatastorage/store_impl_test.go
+// modification are made
+// 1- setup couchdb
+// 2- add TestLookupLastBlock unit test
+
+var couchDBDef *couchdb.CouchDBDef
+
+func TestMain(m *testing.M) {
+	// Read the core.yaml fle for default config.
+	ledgertestutil.SetupCoreYAMLConfig()
+	//setup extension test environment
+	_, _, destroy := xtestutil.SetupExtTestEnv()
+
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/core/ledger/pvtdatastorage")
+	// Create CouchDB definition from config parameters
+	couchDBDef = couchdb.GetCouchDBDefinition()
+
+	code := m.Run()
+	//stop couchdb
+	destroy()
+	os.Exit(code)
+}
+
+func TestStorePurge(t *testing.T) {
+	ledgerid := "teststorepurge"
+	viper.Set("ledger.pvtdataStore.purgeInterval", 2)
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 1,
+			{"ns-1", "coll-2"}: 0,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 4,
+			{"ns-3", "coll-1"}: 1,
+			{"ns-3", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, ledgerid, btlPolicy, couchDBDef)
+	defer env.Cleanup(ledgerid)
+	req := require.New(t)
+	s := env.TestStore
+
+	// no pvt data with block 0
+	req.NoError(s.Prepare(0, nil, nil))
+	req.NoError(s.Commit())
+
+	// construct missing data for block 1
+	blk1MissingData := make(ledger.TxMissingPvtDataMap)
+	// eligible missing data in tx1
+	blk1MissingData.Add(1, "ns-1", "coll-1", true)
+	blk1MissingData.Add(1, "ns-1", "coll-2", true)
+	// ineligible missing data in tx4
+	blk1MissingData.Add(4, "ns-3", "coll-1", false)
+	blk1MissingData.Add(4, "ns-3", "coll-2", false)
+
+	// write pvt data for block 1
+	testDataForBlk1 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	req.NoError(s.Prepare(1, testDataForBlk1, blk1MissingData))
+	req.NoError(s.Commit())
+
+	// write pvt data for block 2
+	req.NoError(s.Prepare(2, nil, nil))
+	req.NoError(s.Commit())
+	// data for ns-1:coll-1 and ns-2:coll-2 should exist in store
+	ns1Coll1 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1", BlkNum: 1}, TxNum: 2}
+	ns2Coll2 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-2", Coll: "coll-2", BlkNum: 1}, TxNum: 2}
+
+	// eligible missingData entries for ns-1:coll-1, ns-1:coll-2 (neverExpires) should exist in store
+	ns1Coll1elgMD := &common.MissingDataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1", BlkNum: 1}, IsEligible: true}
+	ns1Coll2elgMD := &common.MissingDataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-2", BlkNum: 1}, IsEligible: true}
+
+	// ineligible missingData entries for ns-3:col-1, ns-3:coll-2 (neverExpires) should exist in store
+	ns3Coll1inelgMD := &common.MissingDataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-3", Coll: "coll-1", BlkNum: 1}, IsEligible: false}
+	ns3Coll2inelgMD := &common.MissingDataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-3", Coll: "coll-2", BlkNum: 1}, IsEligible: false}
+
+	testWaitForPurgerRoutineToFinish(s)
+	req.True(testDataKeyExists(t, s, ns1Coll1))
+	req.True(testDataKeyExists(t, s, ns2Coll2))
+
+	req.True(testMissingDataKeyExists(t, s, ns1Coll1elgMD))
+	req.True(testMissingDataKeyExists(t, s, ns1Coll2elgMD))
+
+	req.True(testMissingDataKeyExists(t, s, ns3Coll1inelgMD))
+	req.True(testMissingDataKeyExists(t, s, ns3Coll2inelgMD))
+
+	// write pvt data for block 3
+	req.NoError(s.Prepare(3, nil, nil))
+	req.NoError(s.Commit())
+	// data for ns-1:coll-1 and ns-2:coll-2 should exist in store (because purger should not be launched at block 3)
+	testWaitForPurgerRoutineToFinish(s)
+	req.True(testDataKeyExists(t, s, ns1Coll1))
+	req.True(testDataKeyExists(t, s, ns2Coll2))
+	// eligible missingData entries for ns-1:coll-1, ns-1:coll-2 (neverExpires) should exist in store
+	req.True(testMissingDataKeyExists(t, s, ns1Coll1elgMD))
+	req.True(testMissingDataKeyExists(t, s, ns1Coll2elgMD))
+	// ineligible missingData entries for ns-3:col-1, ns-3:coll-2 (neverExpires) should exist in store
+	req.True(testMissingDataKeyExists(t, s, ns3Coll1inelgMD))
+	req.True(testMissingDataKeyExists(t, s, ns3Coll2inelgMD))
+
+	// write pvt data for block 4
+	req.NoError(s.Prepare(4, nil, nil))
+	req.NoError(s.Commit())
+	// data for ns-1:coll-1 should not exist in store (because purger should be launched at block 4)
+	// but ns-2:coll-2 should exist because it expires at block 5
+	testWaitForPurgerRoutineToFinish(s)
+	req.False(testDataKeyExists(t, s, ns1Coll1))
+	req.True(testDataKeyExists(t, s, ns2Coll2))
+	// eligible missingData entries for ns-1:coll-1 should have expired and ns-1:coll-2 (neverExpires) should exist in store
+	req.False(testMissingDataKeyExists(t, s, ns1Coll1elgMD))
+	req.True(testMissingDataKeyExists(t, s, ns1Coll2elgMD))
+	// ineligible missingData entries for ns-3:col-1 should have expired and ns-3:coll-2 (neverExpires) should exist in store
+	req.False(testMissingDataKeyExists(t, s, ns3Coll1inelgMD))
+	req.True(testMissingDataKeyExists(t, s, ns3Coll2inelgMD))
+
+	// write pvt data for block 5
+	req.NoError(s.Prepare(5, nil, nil))
+	req.NoError(s.Commit())
+	// ns-2:coll-2 should exist because though the data expires at block 5 but purger is launched every second block
+	testWaitForPurgerRoutineToFinish(s)
+	req.False(testDataKeyExists(t, s, ns1Coll1))
+	req.True(testDataKeyExists(t, s, ns2Coll2))
+
+	// write pvt data for block 6
+	req.NoError(s.Prepare(6, nil, nil))
+	req.NoError(s.Commit())
+	// ns-2:coll-2 should not exists now (because purger should be launched at block 6)
+	testWaitForPurgerRoutineToFinish(s)
+	req.False(testDataKeyExists(t, s, ns1Coll1))
+	req.False(testDataKeyExists(t, s, ns2Coll2))
+
+	// "ns-2:coll-1" should never have been purged (because, it was no btl was declared for this)
+	req.True(testDataKeyExists(t, s, &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-2", BlkNum: 1}, TxNum: 2}))
+
+}
+
+func testWaitForPurgerRoutineToFinish(s pvtdatastorage.Store) {
+	time.Sleep(1 * time.Second)
+	s.(*store).purgerLock.Lock()
+	s.(*store).purgerLock.Unlock()
+}
+
+func TestEmptyStore(t *testing.T) {
+	env := NewTestStoreEnv(t, "testemptystore", nil, couchDBDef)
+	defer env.Cleanup("testemptystore")
+	req := require.New(t)
+	store := env.TestStore
+	testEmpty(true, req, store)
+	testPendingBatch(false, req, store)
+}
+
+func TestEndorserRole(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 0,
+			{"ns-3", "coll-1"}: 0,
+			{"ns-4", "coll-1"}: 0,
+			{"ns-4", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "testendorserrole", btlPolicy, couchDBDef)
+	defer env.Cleanup("testendorserrole")
+	req := require.New(t)
+	committerStore := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	// no pvt data with block 0
+	req.NoError(committerStore.Prepare(0, nil, nil))
+	req.NoError(committerStore.Commit())
+
+	// pvt data with block 1 - commit
+	req.NoError(committerStore.Prepare(1, testData, nil))
+	req.NoError(committerStore.Commit())
+
+	// create endorser store
+	endorserStore := NewTestStoreEnv(t, "testendorserrole", btlPolicy, couchDBDef).TestStore
+
+	var nilFilter ledger.PvtNsCollFilter
+	retrievedData, err := endorserStore.GetPvtDataByBlockNum(0, nilFilter)
+	req.NoError(err)
+	req.Nil(retrievedData)
+
+	// pvt data retrieval for block 1 should return full pvtdata
+	retrievedData, err = endorserStore.GetPvtDataByBlockNum(1, nilFilter)
+	req.NoError(err)
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, testData[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, testData[i].WriteSet))
+	}
+
+}
+
+func TestStoreBasicCommitAndRetrieval(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 0,
+			{"ns-3", "coll-1"}: 0,
+			{"ns-4", "coll-1"}: 0,
+			{"ns-4", "coll-2"}: 0,
+		},
+	)
+
+	env := NewTestStoreEnv(t, "teststorebasiccommitandretrieval", btlPolicy, couchDBDef)
+	defer env.Cleanup("teststorebasiccommitandretrieval")
+	req := require.New(t)
+	store := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+
+	// construct missing data for block 1
+	blk1MissingData := make(ledger.TxMissingPvtDataMap)
+
+	// eligible missing data in tx1
+	blk1MissingData.Add(1, "ns-1", "coll-1", true)
+	blk1MissingData.Add(1, "ns-1", "coll-2", true)
+	blk1MissingData.Add(1, "ns-2", "coll-1", true)
+	blk1MissingData.Add(1, "ns-2", "coll-2", true)
+	// eligible missing data in tx2
+	blk1MissingData.Add(2, "ns-3", "coll-1", true)
+	// ineligible missing data in tx4
+	blk1MissingData.Add(4, "ns-4", "coll-1", false)
+	blk1MissingData.Add(4, "ns-4", "coll-2", false)
+
+	// construct missing data for block 2
+	blk2MissingData := make(ledger.TxMissingPvtDataMap)
+	// eligible missing data in tx1
+	blk2MissingData.Add(1, "ns-1", "coll-1", true)
+	blk2MissingData.Add(1, "ns-1", "coll-2", true)
+	// eligible missing data in tx3
+	blk2MissingData.Add(3, "ns-1", "coll-1", true)
+
+	// no pvt data with block 0
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	// pvt data with block 1 - commit
+	req.NoError(store.Prepare(1, testData, blk1MissingData))
+	req.NoError(store.Commit())
+
+	// pvt data with block 2 - rollback
+	req.NoError(store.Prepare(2, testData, nil))
+	req.NoError(store.Rollback())
+
+	// pvt data retrieval for block 0 should return nil
+	var nilFilter ledger.PvtNsCollFilter
+	retrievedData, err := store.GetPvtDataByBlockNum(0, nilFilter)
+	req.NoError(err)
+	req.Nil(retrievedData)
+
+	// pvt data retrieval for block 1 should return full pvtdata
+	retrievedData, err = store.GetPvtDataByBlockNum(1, nilFilter)
+	req.NoError(err)
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, testData[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, testData[i].WriteSet))
+	}
+
+	// pvt data retrieval for block 1 with filter should return filtered pvtdata
+	filter := ledger.NewPvtNsCollFilter()
+	filter.Add("ns-1", "coll-1")
+	filter.Add("ns-2", "coll-2")
+	retrievedData, err = store.GetPvtDataByBlockNum(1, filter)
+	expectedRetrievedData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+	}
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, expectedRetrievedData[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, expectedRetrievedData[i].WriteSet))
+	}
+
+	// pvt data retrieval for block 2 should return ErrOutOfRange
+	retrievedData, err = store.GetPvtDataByBlockNum(2, nilFilter)
+	_, ok := err.(*pvtdatastorage.ErrOutOfRange)
+	req.True(ok)
+	req.Nil(retrievedData)
+
+	// pvt data with block 2 - commit
+	req.NoError(store.Prepare(2, testData, blk2MissingData))
+	req.NoError(store.Commit())
+
+	// retrieve the stored missing entries using GetMissingPvtDataInfoForMostRecentBlocks
+	// Only the code path of eligible entries would be covered in this unit-test. For
+	// ineligible entries, the code path will be covered in FAB-11437
+
+	expectedMissingPvtDataInfo := make(ledger.MissingPvtDataInfo)
+	// missing data in block2, tx1
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-2")
+	expectedMissingPvtDataInfo.Add(2, 3, "ns-1", "coll-1")
+
+	missingPvtDataInfo, err := store.GetMissingPvtDataInfoForMostRecentBlocks(1)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	// missing data in block1, tx1
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-2")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-2", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-2", "coll-2")
+
+	// missing data in block1, tx2
+	expectedMissingPvtDataInfo.Add(1, 2, "ns-3", "coll-1")
+
+	missingPvtDataInfo, err = store.GetMissingPvtDataInfoForMostRecentBlocks(2)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	missingPvtDataInfo, err = store.GetMissingPvtDataInfoForMostRecentBlocks(10)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+}
+
+func TestCommitPvtDataOfOldBlocks(t *testing.T) {
+	viper.Set("ledger.pvtdataStore.purgeInterval", 2)
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 3,
+			{"ns-1", "coll-2"}: 1,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 1,
+			{"ns-3", "coll-1"}: 0,
+			{"ns-3", "coll-2"}: 3,
+			{"ns-4", "coll-1"}: 0,
+			{"ns-4", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "testcommitpvtdataofoldblocks", btlPolicy, couchDBDef)
+	defer env.Cleanup("testcommitpvtdataofoldblocks")
+	req := require.New(t)
+	store := env.TestStore
+
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+
+	// CONSTRUCT MISSING DATA FOR BLOCK 1
+	blk1MissingData := make(ledger.TxMissingPvtDataMap)
+
+	// eligible missing data in tx1
+	blk1MissingData.Add(1, "ns-1", "coll-1", true)
+	blk1MissingData.Add(1, "ns-1", "coll-2", true)
+	blk1MissingData.Add(1, "ns-2", "coll-1", true)
+	blk1MissingData.Add(1, "ns-2", "coll-2", true)
+	// eligible missing data in tx2
+	blk1MissingData.Add(2, "ns-1", "coll-1", true)
+	blk1MissingData.Add(2, "ns-1", "coll-2", true)
+	blk1MissingData.Add(2, "ns-3", "coll-1", true)
+	blk1MissingData.Add(2, "ns-3", "coll-2", true)
+
+	// CONSTRUCT MISSING DATA FOR BLOCK 2
+	blk2MissingData := make(ledger.TxMissingPvtDataMap)
+	// eligible missing data in tx1
+	blk2MissingData.Add(1, "ns-1", "coll-1", true)
+	blk2MissingData.Add(1, "ns-1", "coll-2", true)
+	// eligible missing data in tx3
+	blk2MissingData.Add(3, "ns-1", "coll-1", true)
+
+	// COMMIT BLOCK 0 WITH NO DATA
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	// COMMIT BLOCK 1 WITH PVTDATA AND MISSINGDATA
+	req.NoError(store.Prepare(1, testData, blk1MissingData))
+	req.NoError(store.Commit())
+
+	// COMMIT BLOCK 2 WITH PVTDATA AND MISSINGDATA
+	req.NoError(store.Prepare(2, nil, blk2MissingData))
+	req.NoError(store.Commit())
+
+	// CHECK MISSINGDATA ENTRIES ARE CORRECTLY STORED
+	expectedMissingPvtDataInfo := make(ledger.MissingPvtDataInfo)
+	// missing data in block1, tx1
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-2")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-2", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-2", "coll-2")
+
+	// missing data in block1, tx2
+	expectedMissingPvtDataInfo.Add(1, 2, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 2, "ns-1", "coll-2")
+	expectedMissingPvtDataInfo.Add(1, 2, "ns-3", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 2, "ns-3", "coll-2")
+
+	// missing data in block2, tx1
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-2")
+	// missing data in block2, tx3
+	expectedMissingPvtDataInfo.Add(2, 3, "ns-1", "coll-1")
+
+	missingPvtDataInfo, err := store.GetMissingPvtDataInfoForMostRecentBlocks(2)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	// COMMIT THE MISSINGDATA IN BLOCK 1 AND BLOCK 2
+	oldBlocksPvtData := make(map[uint64][]*ledger.TxPvtData)
+	oldBlocksPvtData[1] = []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 1, []string{"ns-1:coll-1", "ns-2:coll-1"}),
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-3:coll-1"}),
+	}
+	oldBlocksPvtData[2] = []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 3, []string{"ns-1:coll-1"}),
+	}
+
+	err = store.CommitPvtDataOfOldBlocks(oldBlocksPvtData)
+	req.NoError(err)
+
+	// ENSURE THAT THE CURRENT PVTDATA OF BLOCK 1 STILL EXIST IN THE STORE
+	ns2Coll1Blk1Tx2 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-2", Coll: "coll-1", BlkNum: 1}, TxNum: 2}
+	ns2Coll2Blk1Tx2 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-2", Coll: "coll-2", BlkNum: 1}, TxNum: 2}
+	req.True(testDataKeyExists(t, store, ns2Coll1Blk1Tx2))
+	req.True(testDataKeyExists(t, store, ns2Coll2Blk1Tx2))
+
+	// ENSURE THAT THE PREVIOUSLY MISSING PVTDATA OF BLOCK 1 & 2 EXIST IN THE STORE
+	ns1Coll1Blk1Tx1 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1", BlkNum: 1}, TxNum: 1}
+	ns2Coll1Blk1Tx1 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-2", Coll: "coll-1", BlkNum: 1}, TxNum: 1}
+	ns1Coll1Blk1Tx2 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1", BlkNum: 1}, TxNum: 2}
+	ns3Coll1Blk1Tx2 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-3", Coll: "coll-1", BlkNum: 1}, TxNum: 2}
+	ns1Coll1Blk2Tx3 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1", BlkNum: 2}, TxNum: 3}
+
+	req.True(testDataKeyExists(t, store, ns1Coll1Blk1Tx1))
+	req.True(testDataKeyExists(t, store, ns2Coll1Blk1Tx1))
+	req.True(testDataKeyExists(t, store, ns1Coll1Blk1Tx2))
+	req.True(testDataKeyExists(t, store, ns3Coll1Blk1Tx2))
+	req.True(testDataKeyExists(t, store, ns1Coll1Blk2Tx3))
+
+	// pvt data retrieval for block 2 should return the just committed pvtdata
+	var nilFilter ledger.PvtNsCollFilter
+	retrievedData, err := store.GetPvtDataByBlockNum(2, nilFilter)
+	req.NoError(err)
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, oldBlocksPvtData[2][i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, oldBlocksPvtData[2][i].WriteSet))
+	}
+
+	expectedMissingPvtDataInfo = make(ledger.MissingPvtDataInfo)
+	// missing data in block1, tx1
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-2")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-2", "coll-2")
+
+	// missing data in block1, tx2
+	expectedMissingPvtDataInfo.Add(1, 2, "ns-1", "coll-2")
+	expectedMissingPvtDataInfo.Add(1, 2, "ns-3", "coll-2")
+
+	// missing data in block2, tx1
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-2")
+
+	missingPvtDataInfo, err = store.GetMissingPvtDataInfoForMostRecentBlocks(2)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	// blksPvtData returns all the pvt data for a block for which the any pvtdata has been submitted
+	// using CommitPvtDataOfOldBlocks
+	blksPvtData, err := store.GetLastUpdatedOldBlocksPvtData()
+	req.NoError(err)
+
+	expectedLastupdatedPvtdata := make(map[uint64][]*ledger.TxPvtData)
+	expectedLastupdatedPvtdata[1] = []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 1, []string{"ns-1:coll-1", "ns-2:coll-1"}),
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-2:coll-1", "ns-2:coll-2", "ns-3:coll-1"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	expectedLastupdatedPvtdata[2] = []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 3, []string{"ns-1:coll-1"}),
+	}
+
+	req.Equal(expectedLastupdatedPvtdata, blksPvtData)
+
+	err = store.ResetLastUpdatedOldBlocksList()
+	req.NoError(err)
+
+	blksPvtData, err = store.GetLastUpdatedOldBlocksPvtData()
+	req.NoError(err)
+	req.Nil(blksPvtData)
+
+	// COMMIT BLOCK 3 WITH NO PVTDATA
+	req.NoError(store.Prepare(3, nil, nil))
+	req.NoError(store.Commit())
+
+	// IN BLOCK 1, NS-1:COLL-2 AND NS-2:COLL-2 SHOULD HAVE EXPIRED BUT NOT PURGED
+	// HENCE, THE FOLLOWING COMMIT SHOULD CREATE ENTRIES IN THE STORE
+	oldBlocksPvtData = make(map[uint64][]*ledger.TxPvtData)
+	oldBlocksPvtData[1] = []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 1, []string{"ns-1:coll-2"}), // though expired, it
+		// would get committed to the store as it is not purged yet
+		produceSamplePvtdata(t, 2, []string{"ns-3:coll-2"}), // never expires
+	}
+
+	err = store.CommitPvtDataOfOldBlocks(oldBlocksPvtData)
+	req.NoError(err)
+
+	ns1Coll2Blk1Tx1 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-2", BlkNum: 1}, TxNum: 1}
+	ns2Coll2Blk1Tx1 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-2", Coll: "coll-2", BlkNum: 1}, TxNum: 1}
+	ns1Coll2Blk1Tx2 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-2", BlkNum: 1}, TxNum: 2}
+	ns3Coll2Blk1Tx2 := &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-3", Coll: "coll-2", BlkNum: 1}, TxNum: 2}
+
+	// though the pvtdata are expired but not purged yet, we do
+	// commit the data and hence the entries would exist in the
+	// store
+	req.True(testDataKeyExists(t, store, ns1Coll2Blk1Tx1))  // expired but committed
+	req.False(testDataKeyExists(t, store, ns2Coll2Blk1Tx1)) // expired but still missing
+	req.False(testDataKeyExists(t, store, ns1Coll2Blk1Tx2)) // expired still missing
+	req.True(testDataKeyExists(t, store, ns3Coll2Blk1Tx2))  // never expires
+
+	err = store.ResetLastUpdatedOldBlocksList()
+	req.NoError(err)
+
+	// COMMIT BLOCK 4 WITH NO PVTDATA
+	req.NoError(store.Prepare(4, nil, nil))
+	req.NoError(store.Commit())
+
+	testWaitForPurgerRoutineToFinish(store)
+
+	// IN BLOCK 1, NS-1:COLL-2 AND NS-2:COLL-2 SHOULD HAVE EXPIRED BUT NOT PURGED
+	// HENCE, THE FOLLOWING COMMIT SHOULD NOT CREATE ENTRIES IN THE STORE
+	oldBlocksPvtData = make(map[uint64][]*ledger.TxPvtData)
+	oldBlocksPvtData[1] = []*ledger.TxPvtData{
+		// both data are expired and purged. hence, it won't be
+		// committed to the store
+		produceSamplePvtdata(t, 1, []string{"ns-2:coll-2"}),
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-2"}),
+	}
+
+	err = store.CommitPvtDataOfOldBlocks(oldBlocksPvtData)
+	req.NoError(err)
+
+	ns1Coll2Blk1Tx1 = &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-2", BlkNum: 1}, TxNum: 1}
+	ns2Coll2Blk1Tx1 = &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-2", Coll: "coll-2", BlkNum: 1}, TxNum: 1}
+	ns1Coll2Blk1Tx2 = &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-2", BlkNum: 1}, TxNum: 2}
+	ns3Coll2Blk1Tx2 = &common.DataKey{NsCollBlk: common.NsCollBlk{Ns: "ns-3", Coll: "coll-2", BlkNum: 1}, TxNum: 2}
+
+	req.False(testDataKeyExists(t, store, ns1Coll2Blk1Tx1)) // purged
+	req.False(testDataKeyExists(t, store, ns2Coll2Blk1Tx1)) // purged
+	req.False(testDataKeyExists(t, store, ns1Coll2Blk1Tx2)) // purged
+	req.True(testDataKeyExists(t, store, ns3Coll2Blk1Tx2))  // never expires
+}
+
+func TestExpiryDataNotIncluded(t *testing.T) {
+	ledgerid := "testexpirydatanotincluded"
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 1,
+			{"ns-1", "coll-2"}: 0,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 2,
+			{"ns-3", "coll-1"}: 1,
+			{"ns-3", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, ledgerid, btlPolicy, couchDBDef)
+	defer env.Cleanup(ledgerid)
+	req := require.New(t)
+	store := env.TestStore
+
+	// construct missing data for block 1
+	blk1MissingData := make(ledger.TxMissingPvtDataMap)
+	// eligible missing data in tx1
+	blk1MissingData.Add(1, "ns-1", "coll-1", true)
+	blk1MissingData.Add(1, "ns-1", "coll-2", true)
+	// ineligible missing data in tx4
+	blk1MissingData.Add(4, "ns-3", "coll-1", false)
+	blk1MissingData.Add(4, "ns-3", "coll-2", false)
+
+	// construct missing data for block 2
+	blk2MissingData := make(ledger.TxMissingPvtDataMap)
+	// eligible missing data in tx1
+	blk2MissingData.Add(1, "ns-1", "coll-1", true)
+	blk2MissingData.Add(1, "ns-1", "coll-2", true)
+
+	// no pvt data with block 0
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	// write pvt data for block 1
+	testDataForBlk1 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	req.NoError(store.Prepare(1, testDataForBlk1, blk1MissingData))
+	req.NoError(store.Commit())
+
+	// write pvt data for block 2
+	testDataForBlk2 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 3, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 5, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	req.NoError(store.Prepare(2, testDataForBlk2, blk2MissingData))
+	req.NoError(store.Commit())
+
+	retrievedData, _ := store.GetPvtDataByBlockNum(1, nil)
+	// block 1 data should still be not expired
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, testDataForBlk1[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, testDataForBlk1[i].WriteSet))
+	}
+
+	// none of the missing data entries would have expired
+	expectedMissingPvtDataInfo := make(ledger.MissingPvtDataInfo)
+	// missing data in block2, tx1
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-2")
+
+	// missing data in block1, tx1
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-2")
+
+	missingPvtDataInfo, err := store.GetMissingPvtDataInfoForMostRecentBlocks(10)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	// Commit block 3 with no pvtdata
+	req.NoError(store.Prepare(3, nil, nil))
+	req.NoError(store.Commit())
+
+	// After committing block 3, the data for "ns-1:coll1" of block 1 should have expired and should not be returned by the store
+	expectedPvtdataFromBlock1 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	retrievedData, _ = store.GetPvtDataByBlockNum(1, nil)
+	req.Equal(expectedPvtdataFromBlock1, retrievedData)
+
+	// After committing block 3, the missing data of "ns1-coll1" in block1-tx1 should have expired
+	expectedMissingPvtDataInfo = make(ledger.MissingPvtDataInfo)
+	// missing data in block2, tx1
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-2")
+	// missing data in block1, tx1
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-2")
+
+	missingPvtDataInfo, err = store.GetMissingPvtDataInfoForMostRecentBlocks(10)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	// Commit block 4 with no pvtdata
+	req.NoError(store.Prepare(4, nil, nil))
+	req.NoError(store.Commit())
+
+	// After committing block 4, the data for "ns-2:coll2" of block 1 should also have expired and should not be returned by the store
+	expectedPvtdataFromBlock1 = []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-2", "ns-2:coll-1"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-2", "ns-2:coll-1"}),
+	}
+	retrievedData, _ = store.GetPvtDataByBlockNum(1, nil)
+	req.Equal(expectedPvtdataFromBlock1, retrievedData)
+
+	// Now, for block 2, "ns-1:coll1" should also have expired
+	expectedPvtdataFromBlock2 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 3, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 5, []string{"ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+	retrievedData, _ = store.GetPvtDataByBlockNum(2, nil)
+	req.Equal(expectedPvtdataFromBlock2, retrievedData)
+
+	// After committing block 4, the missing data of "ns1-coll1" in block2-tx1 should have expired
+	expectedMissingPvtDataInfo = make(ledger.MissingPvtDataInfo)
+	// missing data in block2, tx1
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-2")
+
+	// missing data in block1, tx1
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-2")
+
+	missingPvtDataInfo, err = store.GetMissingPvtDataInfoForMostRecentBlocks(10)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+}
+
+func TestLookupLastBlock(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "teststorestate", btlPolicy, couchDBDef)
+	defer env.Cleanup("teststorestate")
+	req := require.New(t)
+	s := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 0, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+	}
+	checkLastCommittedBlock(t, s, uint64(0))
+
+	req.Nil(s.Prepare(0, nil, nil))
+	req.NoError(s.Commit())
+	checkLastCommittedBlock(t, s, uint64(0))
+
+	req.Nil(s.Prepare(1, testData, nil))
+	req.NoError(s.Commit())
+	checkLastCommittedBlock(t, s, uint64(1))
+
+	req.Nil(s.Prepare(2, nil, nil))
+	req.NoError(s.Commit())
+	checkLastCommittedBlock(t, s, uint64(2))
+
+	req.Nil(s.Prepare(3, testData, nil))
+	req.NoError(s.Commit())
+	checkLastCommittedBlock(t, s, uint64(3))
+
+	// Delete block num 2
+	req.NoError(s.(*store).db.DeleteDoc(blockNumberToKey(2), ""))
+	checkLastCommittedBlock(t, s, uint64(3))
+
+}
+
+func checkLastCommittedBlock(t *testing.T, s pvtdatastorage.Store, expectedLastCommittedBlock uint64) {
+	lastCommitBlock, _, err := lookupLastBlock(s.(*store).db)
+	require.NoError(t, err)
+	require.Equal(t, expectedLastCommittedBlock, lastCommitBlock)
+}
+
+func TestStoreState(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "teststorestate", btlPolicy, couchDBDef)
+	defer env.Cleanup("teststorestate")
+	req := require.New(t)
+	store := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 0, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+	}
+	_, ok := store.Prepare(1, testData, nil).(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+
+	req.Nil(store.Prepare(0, testData, nil))
+	req.NoError(store.Commit())
+
+	req.Nil(store.Prepare(1, testData, nil))
+	_, ok = store.Prepare(2, testData, nil).(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+}
+
+func TestInitLastCommittedBlock(t *testing.T) {
+	env := NewTestStoreEnv(t, "teststorestate", nil, couchDBDef)
+	defer env.Cleanup("teststorestate")
+	req := require.New(t)
+	store := env.TestStore
+	existingLastBlockNum := uint64(25)
+	req.NoError(store.InitLastCommittedBlock(existingLastBlockNum))
+
+	testEmpty(false, req, store)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(existingLastBlockNum+1, req, store)
+
+	env.CloseAndReopen()
+	testEmpty(false, req, store)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(existingLastBlockNum+1, req, store)
+
+	err := store.InitLastCommittedBlock(30)
+	_, ok := err.(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+}
+
+func TestCollElgEnabled(t *testing.T) {
+	testCollElgEnabled(t)
+	defaultValBatchSize := ledgerconfig.GetPvtdataStoreCollElgProcMaxDbBatchSize()
+	defaultValInterval := ledgerconfig.GetPvtdataStoreCollElgProcDbBatchesInterval()
+	defer func() {
+		viper.Set("ledger.pvtdataStore.collElgProcMaxDbBatchSize", defaultValBatchSize)
+		viper.Set("ledger.pvtdataStore.collElgProcMaxDbBatchSize", defaultValInterval)
+	}()
+	viper.Set("ledger.pvtdataStore.collElgProcMaxDbBatchSize", 1)
+	viper.Set("ledger.pvtdataStore.collElgProcDbBatchesInterval", 1)
+	testCollElgEnabled(t)
+}
+
+func testCollElgEnabled(t *testing.T) {
+	ledgerid := "testcollelgenabled"
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, ledgerid, btlPolicy, couchDBDef)
+	defer env.Cleanup(ledgerid)
+	req := require.New(t)
+	store := env.TestStore
+
+	// Initial state: eligible for {ns-1:coll-1 and ns-2:coll-1 }
+
+	// no pvt data with block 0
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	// construct and commit block 1
+	blk1MissingData := make(ledger.TxMissingPvtDataMap)
+	blk1MissingData.Add(1, "ns-1", "coll-1", true)
+	blk1MissingData.Add(1, "ns-2", "coll-1", true)
+	blk1MissingData.Add(4, "ns-1", "coll-2", false)
+	blk1MissingData.Add(4, "ns-2", "coll-2", false)
+	testDataForBlk1 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1"}),
+	}
+	req.NoError(store.Prepare(1, testDataForBlk1, blk1MissingData))
+	req.NoError(store.Commit())
+
+	// construct and commit block 2
+	blk2MissingData := make(ledger.TxMissingPvtDataMap)
+	// ineligible missing data in tx1
+	blk2MissingData.Add(1, "ns-1", "coll-2", false)
+	blk2MissingData.Add(1, "ns-2", "coll-2", false)
+	testDataForBlk2 := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 3, []string{"ns-1:coll-1"}),
+	}
+	req.NoError(store.Prepare(2, testDataForBlk2, blk2MissingData))
+	req.NoError(store.Commit())
+
+	// Retrieve and verify missing data reported
+	// Expected missing data should be only blk1-tx1 (because, the other missing data is marked as ineliigible)
+	expectedMissingPvtDataInfo := make(ledger.MissingPvtDataInfo)
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-1", "coll-1")
+	expectedMissingPvtDataInfo.Add(1, 1, "ns-2", "coll-1")
+	missingPvtDataInfo, err := store.GetMissingPvtDataInfoForMostRecentBlocks(10)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	// Enable eligibility for {ns-1:coll2}
+	err = store.ProcessCollsEligibilityEnabled(
+		5,
+		map[string][]string{
+			"ns-1": {"coll-2"},
+		},
+	)
+	req.NoError(err)
+	testutilWaitForCollElgProcToFinish(store)
+
+	// Retrieve and verify missing data reported
+	// Expected missing data should include newly eiligible collections
+	expectedMissingPvtDataInfo.Add(1, 4, "ns-1", "coll-2")
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-1", "coll-2")
+	missingPvtDataInfo, err = store.GetMissingPvtDataInfoForMostRecentBlocks(10)
+	req.NoError(err)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+
+	// Enable eligibility for {ns-2:coll2}
+	err = store.ProcessCollsEligibilityEnabled(6,
+		map[string][]string{
+			"ns-2": {"coll-2"},
+		},
+	)
+	req.NoError(err)
+	testutilWaitForCollElgProcToFinish(store)
+
+	// Retrieve and verify missing data reported
+	// Expected missing data should include newly eiligible collections
+	expectedMissingPvtDataInfo.Add(1, 4, "ns-2", "coll-2")
+	expectedMissingPvtDataInfo.Add(2, 1, "ns-2", "coll-2")
+	missingPvtDataInfo, err = store.GetMissingPvtDataInfoForMostRecentBlocks(10)
+	req.Equal(expectedMissingPvtDataInfo, missingPvtDataInfo)
+}
+
+func TestRollBack(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "testrollback", btlPolicy, couchDBDef)
+	defer env.Cleanup("testrollback")
+	req := require.New(t)
+	store := env.TestStore
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	pvtdata := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 0, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+		produceSamplePvtdata(t, 5, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+	}
+	missingData := make(ledger.TxMissingPvtDataMap)
+	missingData.Add(1, "ns-1", "coll-1", true)
+	missingData.Add(5, "ns-1", "coll-1", true)
+	missingData.Add(5, "ns-2", "coll-2", false)
+
+	for i := 1; i <= 9; i++ {
+		req.NoError(store.Prepare(uint64(i), pvtdata, missingData))
+		req.NoError(store.Commit())
+	}
+
+	datakeyTx0 := &common.DataKey{
+		NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1"},
+		TxNum:     0,
+	}
+	datakeyTx5 := &common.DataKey{
+		NsCollBlk: common.NsCollBlk{Ns: "ns-1", Coll: "coll-1"},
+		TxNum:     5,
+	}
+	eligibleMissingdatakey := &common.MissingDataKey{
+		NsCollBlk:  common.NsCollBlk{Ns: "ns-1", Coll: "coll-1"},
+		IsEligible: true,
+	}
+
+	// test store state before preparing for block 10
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(10, req, store)
+
+	// prepare for block 10 and test store for presence of datakeys and eligibile missingdatakeys
+	req.NoError(store.Prepare(10, pvtdata, missingData))
+	testPendingBatch(true, req, store)
+	testLastCommittedBlockHeight(10, req, store)
+
+	datakeyTx0.BlkNum = 10
+	datakeyTx5.BlkNum = 10
+	eligibleMissingdatakey.BlkNum = 10
+	req.True(testPendingDataKeyExists(t, store, datakeyTx0))
+	req.True(testPendingDataKeyExists(t, store, datakeyTx5))
+	req.True(testPendingMissingDataKeyExists(t, store, eligibleMissingdatakey))
+
+	// rollback last prepared block and test store for absence of datakeys and eligibile missingdatakeys
+	err := store.Rollback()
+	req.NoError(err)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(10, req, store)
+	req.False(testPendingDataKeyExists(t, store, datakeyTx0))
+	req.False(testPendingDataKeyExists(t, store, datakeyTx5))
+	req.False(testPendingMissingDataKeyExists(t, store, eligibleMissingdatakey))
+
+	// For previously committed blocks the datakeys and eligibile missingdatakeys should still be present
+	for i := 1; i <= 9; i++ {
+		datakeyTx0.BlkNum = uint64(i)
+		datakeyTx5.BlkNum = uint64(i)
+		eligibleMissingdatakey.BlkNum = uint64(i)
+		req.True(testDataKeyExists(t, store, datakeyTx0))
+		req.True(testDataKeyExists(t, store, datakeyTx5))
+		req.True(testMissingDataKeyExists(t, store, eligibleMissingdatakey))
+	}
+}
+
+func testMissingDataKeyExists(t *testing.T, s pvtdatastorage.Store, missingDataKey *common.MissingDataKey) bool {
+	dataKeyBytes := common.EncodeMissingDataKey(missingDataKey)
+	val, err := s.(*store).missingKeysIndexDB.Get(dataKeyBytes)
+	require.NoError(t, err)
+	return len(val) != 0
+}
+
+func testLastCommittedBlockHeight(expectedBlockHt uint64, req *require.Assertions, store pvtdatastorage.Store) {
+	blkHt, err := store.LastCommittedBlockHeight()
+	req.NoError(err)
+	req.Equal(expectedBlockHt, blkHt)
+}
+
+func testDataKeyExists(t *testing.T, s pvtdatastorage.Store, dataKey *common.DataKey) bool {
+	r, err := retrieveBlockPvtData(s.(*store).db, blockNumberToKey(dataKey.BlkNum))
+	require.NoError(t, err)
+	dataKeyBytes := common.EncodeDataKey(dataKey)
+	_, exists := r.Data[hex.EncodeToString(dataKeyBytes)]
+	return exists
+}
+
+func testPendingDataKeyExists(t *testing.T, s pvtdatastorage.Store, dataKey *common.DataKey) bool {
+	var blockPvtData blockPvtDataResponse
+	if s.(*store).pendingPvtData.PvtDataDoc == nil {
+		return false
+	}
+	err := json.Unmarshal(s.(*store).pendingPvtData.PvtDataDoc.JSONValue, &blockPvtData)
+	require.NoError(t, err)
+	dataKeyBytes := common.EncodeDataKey(dataKey)
+	_, exists := blockPvtData.Data[hex.EncodeToString(dataKeyBytes)]
+	return exists
+}
+
+func testPendingMissingDataKeyExists(t *testing.T, s pvtdatastorage.Store, missingDataKey *common.MissingDataKey) bool {
+	keyBytes := common.EncodeMissingDataKey(missingDataKey)
+	_, exists := s.(*store).pendingPvtData.MissingDataEntries[string(keyBytes)]
+	return exists
+}
+
+func testEmpty(expectedEmpty bool, req *require.Assertions, store pvtdatastorage.Store) {
+	isEmpty, err := store.IsEmpty()
+	req.NoError(err)
+	req.Equal(expectedEmpty, isEmpty)
+}
+
+func testPendingBatch(expectedPending bool, req *require.Assertions, store pvtdatastorage.Store) {
+	hasPendingBatch, err := store.HasPendingBatch()
+	req.NoError(err)
+	req.Equal(expectedPending, hasPendingBatch)
+}
+
+func produceSamplePvtdata(t *testing.T, txNum uint64, nsColls []string) *ledger.TxPvtData {
+	builder := rwsetutil.NewRWSetBuilder()
+	for _, nsColl := range nsColls {
+		nsCollSplit := strings.Split(nsColl, ":")
+		ns := nsCollSplit[0]
+		coll := nsCollSplit[1]
+		builder.AddToPvtAndHashedWriteSet(ns, coll, fmt.Sprintf("key-%s-%s", ns, coll), []byte(fmt.Sprintf("value-%s-%s", ns, coll)))
+	}
+	simRes, err := builder.GetTxSimulationResults()
+	require.NoError(t, err)
+	return &ledger.TxPvtData{SeqInBlock: txNum, WriteSet: simRes.PvtSimulationResults}
+}
+
+func testutilWaitForCollElgProcToFinish(s pvtdatastorage.Store) {
+	s.(*store).collElgProc.WaitForDone()
+}
diff --git a/extensions/pvtdatastorage/cdbpvtdatastore/test_exports.go b/extensions/pvtdatastorage/cdbpvtdatastore/test_exports.go
new file mode 100644
index 00000000..8cc131d2
--- /dev/null
+++ b/extensions/pvtdatastorage/cdbpvtdatastore/test_exports.go
@@ -0,0 +1,77 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"os"
+	"testing"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/stretchr/testify/require"
+)
+
+// StoreEnv provides the  store env for testing
+type StoreEnv struct {
+	t                 testing.TB
+	TestStoreProvider pvtdatastorage.Provider
+	TestStore         pvtdatastorage.Store
+	ledgerid          string
+	btlPolicy         pvtdatapolicy.BTLPolicy
+	couchDBDef        *couchdb.CouchDBDef
+}
+
+// NewTestStoreEnv construct a StoreEnv for testing
+func NewTestStoreEnv(t *testing.T, ledgerid string, btlPolicy pvtdatapolicy.BTLPolicy, couchDBDef *couchdb.CouchDBDef) *StoreEnv {
+	removeStorePath()
+	req := require.New(t)
+	testStoreProvider, err := NewProvider()
+	req.NoError(err)
+	testStore, err := testStoreProvider.OpenStore(ledgerid)
+	req.NoError(err)
+	testStore.Init(btlPolicy)
+	s := &StoreEnv{t, testStoreProvider, testStore, ledgerid, btlPolicy, couchDBDef}
+	return s
+}
+
+// CloseAndReopen closes and opens the store provider
+func (env *StoreEnv) CloseAndReopen() {
+	var err error
+	env.TestStoreProvider.Close()
+	env.TestStoreProvider, err = NewProvider()
+	require.NoError(env.t, err)
+	env.TestStore, err = env.TestStoreProvider.OpenStore(env.ledgerid)
+	env.TestStore.Init(env.btlPolicy)
+	require.NoError(env.t, err)
+}
+
+//Cleanup env test
+func (env *StoreEnv) Cleanup(ledgerid string) {
+	//create a new connection
+	couchInstance, err := couchdb.CreateCouchInstance(env.couchDBDef.URL, env.couchDBDef.Username, env.couchDBDef.Password,
+		env.couchDBDef.MaxRetries, env.couchDBDef.MaxRetriesOnStartup, env.couchDBDef.RequestTimeout, env.couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	if err != nil {
+		panic(err.Error())
+	}
+	pvtDataStoreDBName := couchdb.ConstructBlockchainDBName(ledgerid, pvtDataStoreName)
+	db := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: pvtDataStoreDBName}
+	//drop the test database
+	if _, err := db.DropDatabase(); err != nil {
+		panic(err.Error())
+	}
+	removeStorePath()
+}
+
+func removeStorePath() {
+	dbPath := ledgerconfig.GetPvtdataStorePath()
+	if err := os.RemoveAll(dbPath); err != nil {
+		panic(err.Error())
+	}
+}
diff --git a/extensions/pvtdatastorage/common/collelgproc.go b/extensions/pvtdatastorage/common/collelgproc.go
new file mode 100644
index 00000000..d8ef4396
--- /dev/null
+++ b/extensions/pvtdatastorage/common/collelgproc.go
@@ -0,0 +1,139 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"sync"
+	"time"
+
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+)
+
+// TODO add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/store_imp.go
+// TODO below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+var logger = flogging.MustGetLogger("collelgproc")
+
+type CollElgProc struct {
+	notification, procComplete chan bool
+	purgerLock                 *sync.Mutex
+	db                         *leveldbhelper.DBHandle
+}
+
+func NewCollElgProc(purgerLock *sync.Mutex, missingKeysIndexDB *leveldbhelper.DBHandle) *CollElgProc {
+
+	return &CollElgProc{
+		notification: make(chan bool, 1),
+		procComplete: make(chan bool, 1),
+		purgerLock:   purgerLock,
+		db:           missingKeysIndexDB,
+	}
+}
+
+func (c *CollElgProc) notify() {
+	select {
+	case c.notification <- true:
+		logger.Debugf("Signaled to collection eligibility processing routine")
+	default: //noop
+		logger.Debugf("Previous signal still pending. Skipping new signal")
+	}
+}
+
+func (c *CollElgProc) waitForNotification() {
+	<-c.notification
+}
+
+func (c *CollElgProc) done() {
+	select {
+	case c.procComplete <- true:
+	default:
+	}
+}
+
+func (c *CollElgProc) WaitForDone() {
+	<-c.procComplete
+}
+
+func (c *CollElgProc) LaunchCollElgProc() {
+	maxBatchSize := ledgerconfig.GetPvtdataStoreCollElgProcMaxDbBatchSize()
+	batchesInterval := ledgerconfig.GetPvtdataStoreCollElgProcDbBatchesInterval()
+	go func() {
+		c.processCollElgEvents(maxBatchSize, batchesInterval) // process collection eligibility events when store is opened - in case there is an unprocessed events from previous run
+		for {
+			logger.Debugf("Waiting for collection eligibility event")
+			c.waitForNotification()
+			c.processCollElgEvents(maxBatchSize, batchesInterval)
+			c.done()
+		}
+	}()
+}
+
+func (c *CollElgProc) processCollElgEvents(maxBatchSize, batchesInterval int) {
+	logger.Debugf("Starting to process collection eligibility events")
+	c.purgerLock.Lock()
+	defer c.purgerLock.Unlock()
+	collElgStartKey, collElgEndKey := createRangeScanKeysForCollElg()
+	eventItr := c.db.GetIterator(collElgStartKey, collElgEndKey)
+	defer eventItr.Release()
+	batch := leveldbhelper.NewUpdateBatch()
+	totalEntriesConverted := 0
+
+	for eventItr.Next() {
+		collElgKey, collElgVal := eventItr.Key(), eventItr.Value()
+		blkNum := decodeCollElgKey(collElgKey)
+		CollElgInfo, err := decodeCollElgVal(collElgVal)
+		logger.Debugf("Processing collection eligibility event [blkNum=%d], CollElgInfo=%s", blkNum, CollElgInfo)
+		if err != nil {
+			logger.Errorf("This error is not expected %s", err)
+			continue
+		}
+		for ns, colls := range CollElgInfo.NsCollMap {
+			var coll string
+			for _, coll = range colls.Entries {
+				logger.Infof("Converting missing data entries from ineligible to eligible for [ns=%s, coll=%s]", ns, coll)
+				startKey, endKey := createRangeScanKeysForIneligibleMissingData(blkNum, ns, coll)
+				collItr := c.db.GetIterator(startKey, endKey)
+				collEntriesConverted := 0
+
+				for collItr.Next() { // each entry
+					originalKey, originalVal := collItr.Key(), collItr.Value()
+					modifiedKey := decodeMissingDataKey(originalKey)
+					modifiedKey.IsEligible = true
+					batch.Delete(originalKey)
+					copyVal := make([]byte, len(originalVal))
+					copy(copyVal, originalVal)
+					batch.Put(EncodeMissingDataKey(modifiedKey), copyVal)
+					collEntriesConverted++
+					if batch.Len() > maxBatchSize {
+						if err := c.db.WriteBatch(batch, true); err != nil {
+							logger.Error(err.Error())
+						}
+						batch = leveldbhelper.NewUpdateBatch()
+						sleepTime := time.Duration(batchesInterval)
+						logger.Infof("Going to sleep for %d milliseconds between batches. Entries for [ns=%s, coll=%s] converted so far = %d",
+							sleepTime, ns, coll, collEntriesConverted)
+						c.purgerLock.Unlock()
+						time.Sleep(sleepTime * time.Millisecond)
+						c.purgerLock.Lock()
+					}
+				} // entry loop
+
+				collItr.Release()
+				logger.Infof("Converted all [%d] entries for [ns=%s, coll=%s]", collEntriesConverted, ns, coll)
+				totalEntriesConverted += collEntriesConverted
+			} // coll loop
+		} // ns loop
+		batch.Delete(collElgKey) // delete the collection eligibility event key as well
+	} // event loop
+
+	if err := c.db.WriteBatch(batch, true); err != nil {
+		logger.Error(err.Error())
+	}
+	logger.Debugf("Converted [%d] inelligible mising data entries to elligible", totalEntriesConverted)
+}
diff --git a/extensions/pvtdatastorage/common/helper.go b/extensions/pvtdatastorage/common/helper.go
new file mode 100644
index 00000000..e747472a
--- /dev/null
+++ b/extensions/pvtdatastorage/common/helper.go
@@ -0,0 +1,235 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"math"
+
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/willf/bitset"
+)
+
+// TODO add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/helper.go
+// TODO below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+func PrepareStoreEntries(blockNum uint64, pvtData []*ledger.TxPvtData, btlPolicy pvtdatapolicy.BTLPolicy,
+	missingPvtData ledger.TxMissingPvtDataMap) (*StoreEntries, error) {
+	dataEntries := prepareDataEntries(blockNum, pvtData)
+
+	missingDataEntries := prepareMissingDataEntries(blockNum, missingPvtData)
+
+	expiryEntries, err := prepareExpiryEntries(blockNum, dataEntries, missingDataEntries, btlPolicy)
+	if err != nil {
+		return nil, err
+	}
+
+	return &StoreEntries{
+		DataEntries:        dataEntries,
+		ExpiryEntries:      expiryEntries,
+		MissingDataEntries: missingDataEntries}, nil
+}
+
+func prepareDataEntries(blockNum uint64, pvtData []*ledger.TxPvtData) []*DataEntry {
+	var dataEntries []*DataEntry
+	for _, txPvtdata := range pvtData {
+		for _, nsPvtdata := range txPvtdata.WriteSet.NsPvtRwset {
+			for _, collPvtdata := range nsPvtdata.CollectionPvtRwset {
+				txnum := txPvtdata.SeqInBlock
+				ns := nsPvtdata.Namespace
+				coll := collPvtdata.CollectionName
+				dataKey := &DataKey{NsCollBlk: NsCollBlk{Ns: ns, Coll: coll, BlkNum: blockNum}, TxNum: txnum}
+				dataEntries = append(dataEntries, &DataEntry{Key: dataKey, Value: collPvtdata})
+			}
+		}
+	}
+	return dataEntries
+}
+
+func prepareMissingDataEntries(committingBlk uint64, missingPvtData ledger.TxMissingPvtDataMap) map[MissingDataKey]*bitset.BitSet {
+	missingDataEntries := make(map[MissingDataKey]*bitset.BitSet)
+
+	for txNum, missingData := range missingPvtData {
+		for _, nsColl := range missingData {
+			key := MissingDataKey{NsCollBlk: NsCollBlk{Ns: nsColl.Namespace, Coll: nsColl.Collection, BlkNum: committingBlk},
+				IsEligible: nsColl.IsEligible}
+
+			if _, ok := missingDataEntries[key]; !ok {
+				missingDataEntries[key] = &bitset.BitSet{}
+			}
+			bitmap := missingDataEntries[key]
+
+			bitmap.Set(uint(txNum))
+		}
+	}
+
+	return missingDataEntries
+}
+
+// prepareExpiryEntries returns expiry entries for both private data which is present in the committingBlk
+// and missing private.
+func prepareExpiryEntries(committingBlk uint64, dataEntries []*DataEntry, missingDataEntries map[MissingDataKey]*bitset.BitSet,
+	btlPolicy pvtdatapolicy.BTLPolicy) ([]*ExpiryEntry, error) {
+
+	var expiryEntries []*ExpiryEntry
+	mapByExpiringBlk := make(map[uint64]*ExpiryData)
+
+	// 1. prepare expiryData for non-missing data
+	for _, dataEntry := range dataEntries {
+		prepareExpiryEntriesForPresentData(mapByExpiringBlk, dataEntry.Key, btlPolicy)
+
+	}
+
+	// 2. prepare expiryData for missing data
+	for missingDataKey := range missingDataEntries {
+		prepareExpiryEntriesForMissingData(mapByExpiringBlk, &missingDataKey, btlPolicy)
+
+	}
+
+	for expiryBlk, expiryData := range mapByExpiringBlk {
+		expiryKey := &ExpiryKey{ExpiringBlk: expiryBlk, CommittingBlk: committingBlk}
+		expiryEntries = append(expiryEntries, &ExpiryEntry{Key: expiryKey, Value: expiryData})
+	}
+
+	return expiryEntries, nil
+}
+
+// prepareExpiryDataForPresentData creates expiryData for non-missing pvt data
+func prepareExpiryEntriesForPresentData(mapByExpiringBlk map[uint64]*ExpiryData, dataKey *DataKey, btlPolicy pvtdatapolicy.BTLPolicy) error {
+	expiringBlk, err := btlPolicy.GetExpiringBlock(dataKey.Ns, dataKey.Coll, dataKey.BlkNum)
+	if err != nil {
+		return err
+	}
+	if neverExpires(expiringBlk) {
+		return nil
+	}
+
+	expiryData := getOrCreateExpiryData(mapByExpiringBlk, expiringBlk)
+
+	expiryData.AddPresentData(dataKey.Ns, dataKey.Coll, dataKey.TxNum)
+	return nil
+}
+
+// prepareExpiryDataForMissingData creates expiryData for missing pvt data
+func prepareExpiryEntriesForMissingData(mapByExpiringBlk map[uint64]*ExpiryData, missingKey *MissingDataKey, btlPolicy pvtdatapolicy.BTLPolicy) error {
+	expiringBlk, err := btlPolicy.GetExpiringBlock(missingKey.Ns, missingKey.Coll, missingKey.BlkNum)
+	if err != nil {
+		return err
+	}
+	if neverExpires(expiringBlk) {
+		return nil
+	}
+
+	expiryData := getOrCreateExpiryData(mapByExpiringBlk, expiringBlk)
+
+	expiryData.AddMissingData(missingKey.Ns, missingKey.Coll)
+	return nil
+}
+
+func getOrCreateExpiryData(mapByExpiringBlk map[uint64]*ExpiryData, expiringBlk uint64) *ExpiryData {
+	expiryData, ok := mapByExpiringBlk[expiringBlk]
+	if !ok {
+		expiryData = NewExpiryData()
+		mapByExpiringBlk[expiringBlk] = expiryData
+	}
+	return expiryData
+}
+
+func PassesFilter(dataKey *DataKey, filter ledger.PvtNsCollFilter) bool {
+	return filter == nil || filter.Has(dataKey.Ns, dataKey.Coll)
+}
+
+func IsExpired(key NsCollBlk, btl pvtdatapolicy.BTLPolicy, latestBlkNum uint64) (bool, error) {
+	expiringBlk, err := btl.GetExpiringBlock(key.Ns, key.Coll, key.BlkNum)
+	if err != nil {
+		return false, err
+	}
+
+	return latestBlkNum >= expiringBlk, nil
+}
+
+// DeriveKeys constructs dataKeys and missingDataKey from an expiryEntry
+func DeriveKeys(expiryEntry *ExpiryEntry) (dataKeys []*DataKey, missingDataKeys []*MissingDataKey) {
+	for ns, colls := range expiryEntry.Value.Map {
+		// 1. constructs dataKeys of expired existing pvt data
+		for coll, txNums := range colls.Map {
+			for _, txNum := range txNums.List {
+				dataKeys = append(dataKeys,
+					&DataKey{NsCollBlk{ns, coll, expiryEntry.Key.CommittingBlk}, txNum})
+			}
+		}
+		// 2. constructs missingDataKeys of expired missing pvt data
+		for coll := range colls.MissingDataMap {
+			// one key for eligible entries and another for ieligible entries
+			missingDataKeys = append(missingDataKeys,
+				&MissingDataKey{NsCollBlk{ns, coll, expiryEntry.Key.CommittingBlk}, true})
+			missingDataKeys = append(missingDataKeys,
+				&MissingDataKey{NsCollBlk{ns, coll, expiryEntry.Key.CommittingBlk}, false})
+
+		}
+	}
+	return
+}
+
+func newCollElgInfo(nsCollMap map[string][]string) *pvtdatastorage.CollElgInfo {
+	m := &pvtdatastorage.CollElgInfo{NsCollMap: map[string]*pvtdatastorage.CollNames{}}
+	for ns, colls := range nsCollMap {
+		collNames, ok := m.NsCollMap[ns]
+		if !ok {
+			collNames = &pvtdatastorage.CollNames{}
+			m.NsCollMap[ns] = collNames
+		}
+		collNames.Entries = colls
+	}
+	return m
+}
+
+type TxPvtdataAssembler struct {
+	blockNum, txNum uint64
+	txWset          *rwset.TxPvtReadWriteSet
+	currentNsWSet   *rwset.NsPvtReadWriteSet
+	firstCall       bool
+}
+
+func NewTxPvtdataAssembler(blockNum, txNum uint64) *TxPvtdataAssembler {
+	return &TxPvtdataAssembler{blockNum, txNum, &rwset.TxPvtReadWriteSet{}, nil, true}
+}
+
+func (a *TxPvtdataAssembler) Add(ns string, collPvtWset *rwset.CollectionPvtReadWriteSet) {
+	// start a NsWset
+	if a.firstCall {
+		a.currentNsWSet = &rwset.NsPvtReadWriteSet{Namespace: ns}
+		a.firstCall = false
+	}
+
+	// if a new ns started, add the existing NsWset to TxWset and start a new one
+	if a.currentNsWSet.Namespace != ns {
+		a.txWset.NsPvtRwset = append(a.txWset.NsPvtRwset, a.currentNsWSet)
+		a.currentNsWSet = &rwset.NsPvtReadWriteSet{Namespace: ns}
+	}
+	// add the collWset to the current NsWset
+	a.currentNsWSet.CollectionPvtRwset = append(a.currentNsWSet.CollectionPvtRwset, collPvtWset)
+}
+
+func (a *TxPvtdataAssembler) done() {
+	if a.currentNsWSet != nil {
+		a.txWset.NsPvtRwset = append(a.txWset.NsPvtRwset, a.currentNsWSet)
+	}
+	a.currentNsWSet = nil
+}
+
+func (a *TxPvtdataAssembler) GetTxPvtdata() *ledger.TxPvtData {
+	a.done()
+	return &ledger.TxPvtData{SeqInBlock: a.txNum, WriteSet: a.txWset}
+}
+
+func neverExpires(expiringBlkNum uint64) bool {
+	return expiringBlkNum == math.MaxUint64
+}
diff --git a/extensions/pvtdatastorage/common/kv_encoding.go b/extensions/pvtdatastorage/common/kv_encoding.go
new file mode 100644
index 00000000..68e8857a
--- /dev/null
+++ b/extensions/pvtdatastorage/common/kv_encoding.go
@@ -0,0 +1,192 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"bytes"
+	"math"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/pkg/errors"
+	"github.com/willf/bitset"
+)
+
+// TODO add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/kv_encoding.go
+// TODO below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+var (
+	PendingCommitKey               = []byte{0}
+	pvtDataKeyPrefix               = []byte{2}
+	expiryKeyPrefix                = []byte{3}
+	eligibleMissingDataKeyPrefix   = []byte{4}
+	ineligibleMissingDataKeyPrefix = []byte{5}
+	collElgKeyPrefix               = []byte{6}
+	LastUpdatedOldBlocksKey        = []byte{7}
+	lastCommittedBlockKey          = []byte{8}
+	nilByte                        = byte(0)
+)
+
+func EncodeDataKey(key *DataKey) []byte {
+	dataKeyBytes := append(pvtDataKeyPrefix, version.NewHeight(key.BlkNum, key.TxNum).ToBytes()...)
+	dataKeyBytes = append(dataKeyBytes, []byte(key.Ns)...)
+	dataKeyBytes = append(dataKeyBytes, nilByte)
+	return append(dataKeyBytes, []byte(key.Coll)...)
+}
+
+func EncodeDataValue(collData *rwset.CollectionPvtReadWriteSet) ([]byte, error) {
+	return proto.Marshal(collData)
+}
+
+func EncodeExpiryKey(expiryKey *ExpiryKey) []byte {
+	// reusing version encoding scheme here
+	return append(expiryKeyPrefix, version.NewHeight(expiryKey.ExpiringBlk, expiryKey.CommittingBlk).ToBytes()...)
+}
+
+func DecodeExpiryKey(expiryKeyBytes []byte) *ExpiryKey {
+	height, _ := version.NewHeightFromBytes(expiryKeyBytes[1:])
+	return &ExpiryKey{ExpiringBlk: height.BlockNum, CommittingBlk: height.TxNum}
+}
+
+func EncodeExpiryValue(expiryData *ExpiryData) ([]byte, error) {
+	return proto.Marshal(expiryData)
+}
+
+func DecodeExpiryValue(expiryValueBytes []byte) (*ExpiryData, error) {
+	expiryData := &ExpiryData{}
+	err := proto.Unmarshal(expiryValueBytes, expiryData)
+	return expiryData, err
+}
+
+func DecodeDatakey(datakeyBytes []byte) *DataKey {
+	v, n := version.NewHeightFromBytes(datakeyBytes[1:])
+	blkNum := v.BlockNum
+	tranNum := v.TxNum
+	remainingBytes := datakeyBytes[n+1:]
+	nilByteIndex := bytes.IndexByte(remainingBytes, nilByte)
+	ns := string(remainingBytes[:nilByteIndex])
+	coll := string(remainingBytes[nilByteIndex+1:])
+	return &DataKey{NsCollBlk: NsCollBlk{Ns: ns, Coll: coll, BlkNum: blkNum}, TxNum: tranNum}
+}
+
+func DecodeDataValue(datavalueBytes []byte) (*rwset.CollectionPvtReadWriteSet, error) {
+	collPvtdata := &rwset.CollectionPvtReadWriteSet{}
+	err := proto.Unmarshal(datavalueBytes, collPvtdata)
+	return collPvtdata, err
+}
+
+func EncodeMissingDataKey(key *MissingDataKey) []byte {
+	if key.IsEligible {
+		keyBytes := append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(key.BlkNum)...)
+		keyBytes = append(keyBytes, []byte(key.Ns)...)
+		keyBytes = append(keyBytes, nilByte)
+		return append(keyBytes, []byte(key.Coll)...)
+	}
+
+	keyBytes := append(ineligibleMissingDataKeyPrefix, []byte(key.Ns)...)
+	keyBytes = append(keyBytes, nilByte)
+	keyBytes = append(keyBytes, []byte(key.Coll)...)
+	keyBytes = append(keyBytes, nilByte)
+	return append(keyBytes, []byte(util.EncodeReverseOrderVarUint64(key.BlkNum))...)
+}
+
+func decodeMissingDataKey(keyBytes []byte) *MissingDataKey {
+	key := &MissingDataKey{NsCollBlk: NsCollBlk{}}
+	if keyBytes[0] == eligibleMissingDataKeyPrefix[0] {
+		blkNum, numBytesConsumed := util.DecodeReverseOrderVarUint64(keyBytes[1:])
+
+		splittedKey := bytes.Split(keyBytes[numBytesConsumed+1:], []byte{nilByte})
+		key.Ns = string(splittedKey[0])
+		key.Coll = string(splittedKey[1])
+		key.BlkNum = blkNum
+		key.IsEligible = true
+		return key
+	}
+
+	splittedKey := bytes.SplitN(keyBytes[1:], []byte{nilByte}, 3) //encoded bytes for blknum may contain empty bytes
+	key.Ns = string(splittedKey[0])
+	key.Coll = string(splittedKey[1])
+	key.BlkNum, _ = util.DecodeReverseOrderVarUint64(splittedKey[2])
+	key.IsEligible = false
+	return key
+}
+
+func EncodeMissingDataValue(bitmap *bitset.BitSet) ([]byte, error) {
+	return bitmap.MarshalBinary()
+}
+
+func DecodeMissingDataValue(bitmapBytes []byte) (*bitset.BitSet, error) {
+	bitmap := &bitset.BitSet{}
+	if err := bitmap.UnmarshalBinary(bitmapBytes); err != nil {
+		return nil, err
+	}
+	return bitmap, nil
+}
+
+func encodeCollElgKey(blkNum uint64) []byte {
+	return append(collElgKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum)...)
+}
+
+func decodeCollElgKey(b []byte) uint64 {
+	blkNum, _ := util.DecodeReverseOrderVarUint64(b[1:])
+	return blkNum
+}
+
+func encodeCollElgVal(m *pvtdatastorage.CollElgInfo) ([]byte, error) {
+	return proto.Marshal(m)
+}
+
+func decodeCollElgVal(b []byte) (*pvtdatastorage.CollElgInfo, error) {
+	m := &pvtdatastorage.CollElgInfo{}
+	if err := proto.Unmarshal(b, m); err != nil {
+		return nil, errors.WithStack(err)
+	}
+	return m, nil
+}
+
+func createRangeScanKeysForIneligibleMissingData(maxBlkNum uint64, ns, coll string) (startKey, endKey []byte) {
+	startKey = EncodeMissingDataKey(
+		&MissingDataKey{
+			NsCollBlk:  NsCollBlk{Ns: ns, Coll: coll, BlkNum: maxBlkNum},
+			IsEligible: false,
+		},
+	)
+	endKey = EncodeMissingDataKey(
+		&MissingDataKey{
+			NsCollBlk:  NsCollBlk{Ns: ns, Coll: coll, BlkNum: 0},
+			IsEligible: false,
+		},
+	)
+	return
+}
+
+func createRangeScanKeysForEligibleMissingDataEntries(blkNum uint64) (startKey, endKey []byte) {
+	startKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum)...)
+	endKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(0)...)
+
+	return startKey, endKey
+}
+
+func createRangeScanKeysForCollElg() (startKey, endKey []byte) {
+	return encodeCollElgKey(math.MaxUint64),
+		encodeCollElgKey(0)
+}
+
+func datakeyRange(blockNum uint64) (startKey, endKey []byte) {
+	startKey = append(pvtDataKeyPrefix, version.NewHeight(blockNum, 0).ToBytes()...)
+	endKey = append(pvtDataKeyPrefix, version.NewHeight(blockNum, math.MaxUint64).ToBytes()...)
+	return
+}
+
+func eligibleMissingdatakeyRange(blkNum uint64) (startKey, endKey []byte) {
+	startKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum)...)
+	endKey = append(eligibleMissingDataKeyPrefix, util.EncodeReverseOrderVarUint64(blkNum-1)...)
+	return
+}
diff --git a/extensions/pvtdatastorage/common/store.go b/extensions/pvtdatastorage/common/store.go
new file mode 100644
index 00000000..f44a6d13
--- /dev/null
+++ b/extensions/pvtdatastorage/common/store.go
@@ -0,0 +1,404 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"sync/atomic"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/ledger/util/leveldbhelper"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/willf/bitset"
+)
+
+// TODO add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/store_imp.go
+// TODO below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+type DataEntry struct {
+	Key   *DataKey
+	Value *rwset.CollectionPvtReadWriteSet
+}
+
+type ExpiryEntry struct {
+	Key   *ExpiryKey
+	Value *ExpiryData
+}
+
+type ExpiryKey struct {
+	ExpiringBlk   uint64
+	CommittingBlk uint64
+}
+
+type NsCollBlk struct {
+	Ns, Coll string
+	BlkNum   uint64
+}
+
+type DataKey struct {
+	NsCollBlk
+	TxNum uint64
+}
+
+type MissingDataKey struct {
+	NsCollBlk
+	IsEligible bool
+}
+
+type StoreEntries struct {
+	DataEntries        []*DataEntry
+	ExpiryEntries      []*ExpiryEntry
+	MissingDataEntries map[MissingDataKey]*bitset.BitSet
+}
+
+type EntriesForPvtDataOfOldBlocks struct {
+	// for each <ns, coll, blkNum, txNum>, store the dataEntry, i.e., pvtData
+	DataEntries map[DataKey]*rwset.CollectionPvtReadWriteSet
+	// store the retrieved (& updated) expiryData in expiryEntries
+	ExpiryEntries map[ExpiryKey]*ExpiryData
+	// for each <ns, coll, blkNum>, store the retrieved (& updated) bitmap in the missingDataEntries
+	MissingDataEntries map[NsCollBlk]*bitset.BitSet
+}
+
+func (updateEntries *EntriesForPvtDataOfOldBlocks) AddDataEntry(dataEntry *DataEntry) {
+	dataKey := DataKey{NsCollBlk: dataEntry.Key.NsCollBlk, TxNum: dataEntry.Key.TxNum}
+	updateEntries.DataEntries[dataKey] = dataEntry.Value
+}
+
+func (updateEntries *EntriesForPvtDataOfOldBlocks) UpdateAndAddExpiryEntry(expiryEntry *ExpiryEntry, dataKey *DataKey) {
+	txNum := dataKey.TxNum
+	nsCollBlk := dataKey.NsCollBlk
+	// update
+	expiryEntry.Value.AddPresentData(nsCollBlk.Ns, nsCollBlk.Coll, txNum)
+	// we cannot delete entries from MissingDataMap as
+	// we keep only one entry per missing <ns-col>
+	// irrespective of the number of txNum.
+
+	// add
+	expiryKey := ExpiryKey{expiryEntry.Key.ExpiringBlk, expiryEntry.Key.CommittingBlk}
+	updateEntries.ExpiryEntries[expiryKey] = expiryEntry.Value
+}
+
+func (updateEntries *EntriesForPvtDataOfOldBlocks) UpdateAndAddMissingDataEntry(missingData *bitset.BitSet, dataKey *DataKey) {
+
+	txNum := dataKey.TxNum
+	nsCollBlk := dataKey.NsCollBlk
+	// update
+	missingData.Clear(uint(txNum))
+	// add
+	updateEntries.MissingDataEntries[nsCollBlk] = missingData
+}
+
+type ExpiryData pvtdatastorage.ExpiryData
+
+func NewExpiryData() *ExpiryData {
+	return &ExpiryData{Map: make(map[string]*pvtdatastorage.Collections)}
+}
+
+func (e *ExpiryData) getOrCreateCollections(ns string) *pvtdatastorage.Collections {
+	collections, ok := e.Map[ns]
+	if !ok {
+		collections = &pvtdatastorage.Collections{
+			Map:            make(map[string]*pvtdatastorage.TxNums),
+			MissingDataMap: make(map[string]bool)}
+		e.Map[ns] = collections
+	} else {
+		// due to protobuf encoding/decoding, the previously
+		// initialized map could be a nil now due to 0 length.
+		// Hence, we need to reinitialize the map.
+		if collections.Map == nil {
+			collections.Map = make(map[string]*pvtdatastorage.TxNums)
+		}
+		if collections.MissingDataMap == nil {
+			collections.MissingDataMap = make(map[string]bool)
+		}
+	}
+	return collections
+}
+
+func (e *ExpiryData) AddPresentData(ns, coll string, txNum uint64) {
+	collections := e.getOrCreateCollections(ns)
+
+	txNums, ok := collections.Map[coll]
+	if !ok {
+		txNums = &pvtdatastorage.TxNums{}
+		collections.Map[coll] = txNums
+	}
+	txNums.List = append(txNums.List, txNum)
+}
+
+func (e *ExpiryData) AddMissingData(ns, coll string) {
+	collections := e.getOrCreateCollections(ns)
+	collections.MissingDataMap[coll] = true
+}
+
+func (e *ExpiryData) Reset() {
+	*e = ExpiryData{}
+}
+func (e *ExpiryData) String() string {
+	return proto.CompactTextString(e)
+}
+
+func (*ExpiryData) ProtoMessage() {
+}
+
+func ConstructDataEntriesFromBlocksPvtData(blocksPvtData map[uint64][]*ledger.TxPvtData) map[uint64][]*DataEntry {
+	// construct dataEntries for all pvtData
+	dataEntries := make(map[uint64][]*DataEntry)
+	for blkNum, pvtData := range blocksPvtData {
+		// prepare the dataEntries for the pvtData
+		dataEntries[blkNum] = prepareDataEntries(blkNum, pvtData)
+	}
+	return dataEntries
+}
+
+func ConstructUpdateEntriesFromDataEntries(dataEntries []*DataEntry, btlPolicy pvtdatapolicy.BTLPolicy,
+	getExpiryDataOfExpiryKey func(*ExpiryKey) (*ExpiryData, error), getBitmapOfMissingDataKey func(*MissingDataKey) (*bitset.BitSet, error)) (*EntriesForPvtDataOfOldBlocks, error) {
+	updateEntries := &EntriesForPvtDataOfOldBlocks{
+		DataEntries:        make(map[DataKey]*rwset.CollectionPvtReadWriteSet),
+		ExpiryEntries:      make(map[ExpiryKey]*ExpiryData),
+		MissingDataEntries: make(map[NsCollBlk]*bitset.BitSet)}
+
+	// for each data entry, first, get the expiryData and missingData from the pvtStore.
+	// Second, update the expiryData and missingData as per the data entry. Finally, add
+	// the data entry along with the updated expiryData and missingData to the update entries
+	for _, dataEntry := range dataEntries {
+		// get the expiryBlk number to construct the expiryKey
+		expiryKey, err := constructExpiryKeyFromDataEntry(dataEntry, btlPolicy)
+		if err != nil {
+			return nil, err
+		}
+
+		// get the existing expiryData ntry
+		var expiryData *ExpiryData
+		if !neverExpires(expiryKey.ExpiringBlk) {
+			if expiryData, err = getExpiryDataFromUpdateEntriesOrStore(updateEntries, expiryKey, getExpiryDataOfExpiryKey); err != nil {
+				return nil, err
+			}
+			if expiryData == nil {
+				// data entry is already expired
+				// and purged (a rare scenario)
+				continue
+			}
+		}
+
+		// get the existing missingData entry
+		var missingData *bitset.BitSet
+		nsCollBlk := dataEntry.Key.NsCollBlk
+		if missingData, err = getMissingDataFromUpdateEntriesOrStore(updateEntries, nsCollBlk, getBitmapOfMissingDataKey); err != nil {
+			return nil, err
+		}
+		if missingData == nil {
+			// data entry is already expired
+			// and purged (a rare scenario)
+			continue
+		}
+
+		updateEntries.AddDataEntry(dataEntry)
+		if expiryData != nil { // would be nill for the never expiring entry
+			expiryEntry := &ExpiryEntry{Key: &expiryKey, Value: expiryData}
+			updateEntries.UpdateAndAddExpiryEntry(expiryEntry, dataEntry.Key)
+		}
+		updateEntries.UpdateAndAddMissingDataEntry(missingData, dataEntry.Key)
+	}
+	return updateEntries, nil
+}
+
+func ConstructUpdateBatchFromUpdateEntries(updateEntries *EntriesForPvtDataOfOldBlocks, batch *leveldbhelper.UpdateBatch) (*leveldbhelper.UpdateBatch, error) {
+	// add the following four types of entries to the update batch: (1) updated missing data entries
+
+	// (1) add updated missingData to the batch
+	if err := addUpdatedMissingDataEntriesToUpdateBatch(batch, updateEntries); err != nil {
+		return nil, err
+	}
+
+	return batch, nil
+}
+
+func constructExpiryKeyFromDataEntry(dataEntry *DataEntry, btlPolicy pvtdatapolicy.BTLPolicy) (ExpiryKey, error) {
+	// get the expiryBlk number to construct the expiryKey
+	nsCollBlk := dataEntry.Key.NsCollBlk
+	expiringBlk, err := btlPolicy.GetExpiringBlock(nsCollBlk.Ns, nsCollBlk.Coll, nsCollBlk.BlkNum)
+	if err != nil {
+		return ExpiryKey{}, err
+	}
+	return ExpiryKey{ExpiringBlk: expiringBlk, CommittingBlk: nsCollBlk.BlkNum}, nil
+}
+
+func getExpiryDataFromUpdateEntriesOrStore(updateEntries *EntriesForPvtDataOfOldBlocks, expiryKey ExpiryKey, getExpiryDataOfExpiryKey func(*ExpiryKey) (*ExpiryData, error)) (*ExpiryData, error) {
+	expiryData, ok := updateEntries.ExpiryEntries[expiryKey]
+	if !ok {
+		var err error
+		expiryData, err = getExpiryDataOfExpiryKey(&expiryKey)
+		if err != nil {
+			return nil, err
+		}
+	}
+	return expiryData, nil
+}
+
+func getMissingDataFromUpdateEntriesOrStore(updateEntries *EntriesForPvtDataOfOldBlocks, nsCollBlk NsCollBlk, getBitmapOfMissingDataKey func(*MissingDataKey) (*bitset.BitSet, error)) (*bitset.BitSet, error) {
+	missingData, ok := updateEntries.MissingDataEntries[nsCollBlk]
+	if !ok {
+		var err error
+		missingDataKey := &MissingDataKey{NsCollBlk: nsCollBlk, IsEligible: true}
+		missingData, err = getBitmapOfMissingDataKey(missingDataKey)
+		if err != nil {
+			return nil, err
+		}
+	}
+	return missingData, nil
+}
+
+func addUpdatedMissingDataEntriesToUpdateBatch(batch *leveldbhelper.UpdateBatch, entries *EntriesForPvtDataOfOldBlocks) error {
+	var keyBytes, valBytes []byte
+	var err error
+	for nsCollBlk, missingData := range entries.MissingDataEntries {
+		keyBytes = EncodeMissingDataKey(&MissingDataKey{nsCollBlk, true})
+		// if the missingData is empty, we need to delete the missingDataKey
+		if missingData.None() {
+			batch.Delete(keyBytes)
+			continue
+		}
+		if valBytes, err = EncodeMissingDataValue(missingData); err != nil {
+			return err
+		}
+		batch.Put(keyBytes, valBytes)
+	}
+	return nil
+}
+
+func GetLastUpdatedOldBlocksList(missingKeysIndexDB *leveldbhelper.DBHandle) ([]uint64, error) {
+	var v []byte
+	var err error
+	if v, err = missingKeysIndexDB.Get(LastUpdatedOldBlocksKey); err != nil {
+		return nil, err
+	}
+	if v == nil {
+		return nil, nil
+	}
+
+	var updatedBlksList []uint64
+	buf := proto.NewBuffer(v)
+	numBlks, err := buf.DecodeVarint()
+	if err != nil {
+		return nil, err
+	}
+	for i := 0; i < int(numBlks); i++ {
+		blkNum, err := buf.DecodeVarint()
+		if err != nil {
+			return nil, err
+		}
+		updatedBlksList = append(updatedBlksList, blkNum)
+	}
+	return updatedBlksList, nil
+}
+
+func ResetLastUpdatedOldBlocksList(missingKeysIndexDB *leveldbhelper.DBHandle) error {
+	batch := leveldbhelper.NewUpdateBatch()
+	batch.Delete(LastUpdatedOldBlocksKey)
+	if err := missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+	return nil
+}
+
+// GetMissingPvtDataInfoForMostRecentBlocks
+func GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int, lastCommittedBlk uint64, btlPolicy pvtdatapolicy.BTLPolicy, missingKeysIndexDB *leveldbhelper.DBHandle) (ledger.MissingPvtDataInfo, error) {
+	// we assume that this function would be called by the gossip only after processing the
+	// last retrieved missing pvtdata info and committing the same.
+	if maxBlock < 1 {
+		return nil, nil
+	}
+
+	missingPvtDataInfo := make(ledger.MissingPvtDataInfo)
+	numberOfBlockProcessed := 0
+	lastProcessedBlock := uint64(0)
+	isMaxBlockLimitReached := false
+	// as we are not acquiring a read lock, new blocks can get committed while we
+	// construct the MissingPvtDataInfo. As a result, lastCommittedBlock can get
+	// changed. To ensure consistency, we atomically load the lastCommittedBlock value
+	lastCommittedBlock := atomic.LoadUint64(&lastCommittedBlk)
+
+	startKey, endKey := createRangeScanKeysForEligibleMissingDataEntries(lastCommittedBlock)
+	dbItr := missingKeysIndexDB.GetIterator(startKey, endKey)
+	defer dbItr.Release()
+
+	for dbItr.Next() {
+		missingDataKeyBytes := dbItr.Key()
+		missingDataKey := decodeMissingDataKey(missingDataKeyBytes)
+
+		if isMaxBlockLimitReached && (missingDataKey.BlkNum != lastProcessedBlock) {
+			// esnures that exactly maxBlock number
+			// of blocks' entries are processed
+			break
+		}
+
+		// check whether the entry is expired. If so, move to the next item.
+		// As we may use the old lastCommittedBlock value, there is a possibility that
+		// this missing data is actually expired but we may get the stale information.
+		// Though it may leads to extra work of pulling the expired data, it will not
+		// affect the correctness. Further, as we try to fetch the most recent missing
+		// data (less possibility of expiring now), such scenario would be rare. In the
+		// best case, we can load the latest lastCommittedBlock value here atomically to
+		// make this scenario very rare.
+		lastCommittedBlock = atomic.LoadUint64(&lastCommittedBlk)
+		expired, err := IsExpired(missingDataKey.NsCollBlk, btlPolicy, lastCommittedBlock)
+		if err != nil {
+			return nil, err
+		}
+		if expired {
+			continue
+		}
+
+		// check for an existing entry for the blkNum in the MissingPvtDataInfo.
+		// If no such entry exists, create one. Also, keep track of the number of
+		// processed block due to maxBlock limit.
+		if _, ok := missingPvtDataInfo[missingDataKey.BlkNum]; !ok {
+			numberOfBlockProcessed++
+			if numberOfBlockProcessed == maxBlock {
+				isMaxBlockLimitReached = true
+				// as there can be more than one entry for this block,
+				// we cannot `break` here
+				lastProcessedBlock = missingDataKey.BlkNum
+			}
+		}
+
+		valueBytes := dbItr.Value()
+		bitmap, err := DecodeMissingDataValue(valueBytes)
+		if err != nil {
+			return nil, err
+		}
+
+		// for each transaction which misses private data, make an entry in missingBlockPvtDataInfo
+		for index, isSet := bitmap.NextSet(0); isSet; index, isSet = bitmap.NextSet(index + 1) {
+			txNum := uint64(index)
+			missingPvtDataInfo.Add(missingDataKey.BlkNum, txNum, missingDataKey.Ns, missingDataKey.Coll)
+		}
+	}
+
+	return missingPvtDataInfo, nil
+}
+
+// ProcessCollsEligibilityEnabled
+func ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string, collElgProcSync *CollElgProc, missingKeysIndexDB *leveldbhelper.DBHandle) error {
+	key := encodeCollElgKey(committingBlk)
+	m := newCollElgInfo(nsCollMap)
+	val, err := encodeCollElgVal(m)
+	if err != nil {
+		return err
+	}
+	batch := leveldbhelper.NewUpdateBatch()
+	batch.Put(key, val)
+	if err = missingKeysIndexDB.WriteBatch(batch, true); err != nil {
+		return err
+	}
+	collElgProcSync.notify()
+	return nil
+}
diff --git a/extensions/pvtdatastorage/common/v11.go b/extensions/pvtdatastorage/common/v11.go
new file mode 100644
index 00000000..6360dee0
--- /dev/null
+++ b/extensions/pvtdatastorage/common/v11.go
@@ -0,0 +1,78 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/version"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+)
+
+// TODO add pinning script to include copied code into this file, original file from fabric is found in fabric/core/ledger/pvtdatastorage/v11.go
+// TODO below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+type blkTranNumKey []byte
+
+func V11Format(datakeyBytes []byte) bool {
+	_, n := version.NewHeightFromBytes(datakeyBytes[1:])
+	remainingBytes := datakeyBytes[n+1:]
+	return len(remainingBytes) == 0
+}
+
+func v11DecodePK(key blkTranNumKey) (blockNum uint64, tranNum uint64) {
+	height, _ := version.NewHeightFromBytes(key[1:])
+	return height.BlockNum, height.TxNum
+}
+
+func v11DecodePvtRwSet(encodedBytes []byte) (*rwset.TxPvtReadWriteSet, error) {
+	writeset := &rwset.TxPvtReadWriteSet{}
+	return writeset, proto.Unmarshal(encodedBytes, writeset)
+}
+
+func V11DecodeKV(k, v []byte, filter ledger.PvtNsCollFilter) (*ledger.TxPvtData, error) {
+	_, tNum := v11DecodePK(k)
+	var pvtWSet *rwset.TxPvtReadWriteSet
+	var err error
+	if pvtWSet, err = v11DecodePvtRwSet(v); err != nil {
+		return nil, err
+	}
+	filteredWSet := v11TrimPvtWSet(pvtWSet, filter)
+	return &ledger.TxPvtData{SeqInBlock: tNum, WriteSet: filteredWSet}, nil
+}
+
+func v11TrimPvtWSet(pvtWSet *rwset.TxPvtReadWriteSet, filter ledger.PvtNsCollFilter) *rwset.TxPvtReadWriteSet {
+	if filter == nil {
+		return pvtWSet
+	}
+
+	var filteredNsRwSet []*rwset.NsPvtReadWriteSet
+	for _, ns := range pvtWSet.NsPvtRwset {
+		var filteredCollRwSet []*rwset.CollectionPvtReadWriteSet
+		for _, coll := range ns.CollectionPvtRwset {
+			if filter.Has(ns.Namespace, coll.CollectionName) {
+				filteredCollRwSet = append(filteredCollRwSet, coll)
+			}
+		}
+		if filteredCollRwSet != nil {
+			filteredNsRwSet = append(filteredNsRwSet,
+				&rwset.NsPvtReadWriteSet{
+					Namespace:          ns.Namespace,
+					CollectionPvtRwset: filteredCollRwSet,
+				},
+			)
+		}
+	}
+	var filteredTxPvtRwSet *rwset.TxPvtReadWriteSet
+	if filteredNsRwSet != nil {
+		filteredTxPvtRwSet = &rwset.TxPvtReadWriteSet{
+			DataModel:  pvtWSet.GetDataModel(),
+			NsPvtRwset: filteredNsRwSet,
+		}
+	}
+	return filteredTxPvtRwSet
+}
diff --git a/extensions/pvtdatastorage/store_impl.go b/extensions/pvtdatastorage/store_impl.go
new file mode 100644
index 00000000..191bb9e8
--- /dev/null
+++ b/extensions/pvtdatastorage/store_impl.go
@@ -0,0 +1,205 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/extensions/pvtdatastorage/cachedpvtdatastore"
+	cdbpvtdatastore "github.com/hyperledger/fabric/extensions/pvtdatastorage/cdbpvtdatastore"
+	"github.com/pkg/errors"
+)
+
+//////// Provider functions  /////////////
+//////////////////////////////////////////
+
+// PvtDataProvider encapsulates the storage and cache providers in addition to the missing data index provider
+type PvtDataProvider struct {
+	storageProvider pvtdatastorage.Provider
+	cacheProvider   pvtdatastorage.Provider
+}
+
+// NewProvider creates a new PvtDataStoreProvider that combines a cache provider and a backing storage provider
+func NewProvider() *PvtDataProvider {
+	// create couchdb pvt date store provider
+	storageProvider, err := cdbpvtdatastore.NewProvider()
+	if err != nil {
+		panic(err)
+	}
+	// create cache pvt date store provider
+	cacheProvider := cachedpvtdatastore.NewProvider()
+
+	p := PvtDataProvider{
+		storageProvider: storageProvider,
+		cacheProvider:   cacheProvider,
+	}
+	return &p
+}
+
+// OpenStore creates a pvt data store instance for the given ledger ID
+func (c *PvtDataProvider) OpenStore(ledgerID string) (pvtdatastorage.Store, error) {
+	pvtDataStore, err := c.storageProvider.OpenStore(ledgerID)
+	if err != nil {
+		return nil, err
+	}
+	cachePvtDataStore, err := c.cacheProvider.OpenStore(ledgerID)
+	if err != nil {
+		return nil, err
+	}
+
+	return newPvtDataStore(pvtDataStore, cachePvtDataStore)
+}
+
+// Close cleans up the Provider
+func (c *PvtDataProvider) Close() {
+	c.storageProvider.Close()
+	c.cacheProvider.Close()
+
+}
+
+type pvtDataStore struct {
+	pvtDataDBStore    pvtdatastorage.Store
+	cachePvtDataStore pvtdatastorage.Store
+}
+
+func newPvtDataStore(pvtDataDBStore pvtdatastorage.Store, cachePvtDataStore pvtdatastorage.Store) (*pvtDataStore, error) {
+	isEmpty, err := pvtDataDBStore.IsEmpty()
+	if err != nil {
+		return nil, err
+	}
+	// InitLastCommittedBlock for cache if pvtdata storage not empty
+	if !isEmpty {
+		lastCommittedBlockHeight, err := pvtDataDBStore.LastCommittedBlockHeight()
+		if err != nil {
+			return nil, err
+		}
+		err = cachePvtDataStore.InitLastCommittedBlock(lastCommittedBlockHeight - 1)
+		if err != nil {
+			return nil, err
+		}
+	}
+	c := pvtDataStore{
+		pvtDataDBStore:    pvtDataDBStore,
+		cachePvtDataStore: cachePvtDataStore,
+	}
+	return &c, nil
+}
+
+//////// store functions  ////////////////
+//////////////////////////////////////////
+func (c *pvtDataStore) Init(btlPolicy pvtdatapolicy.BTLPolicy) {
+	c.cachePvtDataStore.Init(btlPolicy)
+	c.pvtDataDBStore.Init(btlPolicy)
+}
+
+// Prepare pvt data in cache and send pvt data to background prepare/commit go routine
+func (c *pvtDataStore) Prepare(blockNum uint64, pvtData []*ledger.TxPvtData, pvtMissingDataMap ledger.TxMissingPvtDataMap) error {
+	// Prepare data in cache
+	err := c.cachePvtDataStore.Prepare(blockNum, pvtData, pvtMissingDataMap)
+	if err != nil {
+		return err
+	}
+	// Prepare data in storage
+	return c.pvtDataDBStore.Prepare(blockNum, pvtData, pvtMissingDataMap)
+}
+
+// Commit pvt data in cache and call background pvtDataWriter go routine to commit data
+func (c *pvtDataStore) Commit() error {
+	// Commit data in cache
+	err := c.cachePvtDataStore.Commit()
+	if err != nil {
+		return err
+	}
+	// Commit data in storage
+	return c.pvtDataDBStore.Commit()
+}
+
+//InitLastCommittedBlock initialize last committed block
+func (c *pvtDataStore) InitLastCommittedBlock(blockNum uint64) error {
+	// InitLastCommittedBlock data in cache
+	err := c.cachePvtDataStore.InitLastCommittedBlock(blockNum)
+	if err != nil {
+		return err
+	}
+	// InitLastCommittedBlock data in storage
+	return c.pvtDataDBStore.InitLastCommittedBlock(blockNum)
+}
+
+//GetPvtDataByBlockNum implements the function in the interface `Store`
+func (c *pvtDataStore) GetPvtDataByBlockNum(blockNum uint64, filter ledger.PvtNsCollFilter) ([]*ledger.TxPvtData, error) {
+	result, err := c.cachePvtDataStore.GetPvtDataByBlockNum(blockNum, filter)
+	if err != nil {
+		return nil, err
+	}
+	if len(result) > 0 {
+		return result, nil
+	}
+
+	// data is not in cache will try to get it from storage
+	return c.pvtDataDBStore.GetPvtDataByBlockNum(blockNum, filter)
+}
+
+//HasPendingBatch implements the function in the interface `Store`
+func (c *pvtDataStore) HasPendingBatch() (bool, error) {
+	return c.pvtDataDBStore.HasPendingBatch()
+}
+
+//LastCommittedBlockHeight implements the function in the interface `Store`
+func (c *pvtDataStore) LastCommittedBlockHeight() (uint64, error) {
+	return c.pvtDataDBStore.LastCommittedBlockHeight()
+}
+
+//IsEmpty implements the function in the interface `Store`
+func (c *pvtDataStore) IsEmpty() (bool, error) {
+	return c.pvtDataDBStore.IsEmpty()
+}
+
+// Rollback pvt data in cache and call background pvtDataWriter go routine to rollback data
+func (c *pvtDataStore) Rollback() error {
+	// Rollback data in cache
+	err := c.cachePvtDataStore.Rollback()
+	if err != nil {
+		return err
+	}
+	// Rollback data in storage
+	return c.pvtDataDBStore.Rollback()
+}
+
+//Shutdown implements the function in the interface `Store`
+func (c *pvtDataStore) Shutdown() {
+	c.cachePvtDataStore.Shutdown()
+	c.pvtDataDBStore.Shutdown()
+}
+
+//GetMissingPvtDataInfoForMostRecentBlocks implements the function in the interface `Store`
+func (c *pvtDataStore) GetMissingPvtDataInfoForMostRecentBlocks(maxBlock int) (ledger.MissingPvtDataInfo, error) {
+	return c.pvtDataDBStore.GetMissingPvtDataInfoForMostRecentBlocks(maxBlock)
+}
+
+//ProcessCollsEligibilityEnabled implements the function in the interface `Store`
+func (c *pvtDataStore) ProcessCollsEligibilityEnabled(committingBlk uint64, nsCollMap map[string][]string) error {
+	return c.pvtDataDBStore.ProcessCollsEligibilityEnabled(committingBlk, nsCollMap)
+}
+
+//CommitPvtDataOfOldBlocks implements the function in the interface `Store`
+func (c *pvtDataStore) CommitPvtDataOfOldBlocks(blocksPvtData map[uint64][]*ledger.TxPvtData) error {
+	err := c.pvtDataDBStore.CommitPvtDataOfOldBlocks(blocksPvtData)
+	if err != nil {
+		return errors.WithMessage(err, "CommitPvtDataOfOldBlocks in store failed")
+	}
+	return nil
+}
+
+//GetLastUpdatedOldBlocksPvtData implements the function in the interface `Store`
+func (c *pvtDataStore) GetLastUpdatedOldBlocksPvtData() (map[uint64][]*ledger.TxPvtData, error) {
+	return c.pvtDataDBStore.GetLastUpdatedOldBlocksPvtData()
+}
+
+//ResetLastUpdatedOldBlocksList implements the function in the interface `Store`
+func (c *pvtDataStore) ResetLastUpdatedOldBlocksList() error {
+	return c.pvtDataDBStore.ResetLastUpdatedOldBlocksList()
+}
diff --git a/extensions/pvtdatastorage/store_impl_test.go b/extensions/pvtdatastorage/store_impl_test.go
new file mode 100644
index 00000000..36f5f2ac
--- /dev/null
+++ b/extensions/pvtdatastorage/store_impl_test.go
@@ -0,0 +1,220 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"fmt"
+	"os"
+	"strings"
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
+	btltestutil "github.com/hyperledger/fabric/core/ledger/pvtdatapolicy/testutil"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	ledgertestutil "github.com/hyperledger/fabric/core/ledger/testutil"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	xtestutil "github.com/hyperledger/fabric/extensions/testutil"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/require"
+)
+
+var couchDBDef *couchdb.CouchDBDef
+
+func TestMain(m *testing.M) {
+	// Read the core.yaml fle for default config.
+	ledgertestutil.SetupCoreYAMLConfig()
+	//setup extension test environment
+	_, _, destroy := xtestutil.SetupExtTestEnv()
+
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/core/ledger/pvtdatastore")
+
+	// Create CouchDB definition from config parameters
+	couchDBDef = couchdb.GetCouchDBDefinition()
+
+	code := m.Run()
+	destroy()
+	os.Exit(code)
+}
+
+func TestEmptyStore(t *testing.T) {
+	env := NewTestStoreEnv(t, "testempty", nil, couchDBDef)
+	req := require.New(t)
+	store := env.TestStore
+	testEmpty(true, req, store)
+	testPendingBatch(false, req, store)
+}
+
+func TestStoreBasicCommitAndRetrieval(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+			{"ns-2", "coll-1"}: 0,
+			{"ns-2", "coll-2"}: 0,
+			{"ns-3", "coll-1"}: 0,
+			{"ns-4", "coll-1"}: 0,
+			{"ns-4", "coll-2"}: 0,
+		},
+	)
+
+	env := NewTestStoreEnv(t, "testbasiccommitandretrieval", btlPolicy, couchDBDef)
+	req := require.New(t)
+	store := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-1:coll-2", "ns-2:coll-1", "ns-2:coll-2"}),
+	}
+
+	// construct missing data for block 1
+	blk1MissingData := make(ledger.TxMissingPvtDataMap)
+
+	// eligible missing data in tx1
+	blk1MissingData.Add(1, "ns-1", "coll-1", true)
+	blk1MissingData.Add(1, "ns-1", "coll-2", true)
+	blk1MissingData.Add(1, "ns-2", "coll-1", true)
+	blk1MissingData.Add(1, "ns-2", "coll-2", true)
+	// eligible missing data in tx2
+	blk1MissingData.Add(2, "ns-3", "coll-1", true)
+	// ineligible missing data in tx4
+	blk1MissingData.Add(4, "ns-4", "coll-1", false)
+	blk1MissingData.Add(4, "ns-4", "coll-2", false)
+
+	// construct missing data for block 2
+	blk2MissingData := make(ledger.TxMissingPvtDataMap)
+	// eligible missing data in tx1
+	blk2MissingData.Add(1, "ns-1", "coll-1", true)
+	blk2MissingData.Add(1, "ns-1", "coll-2", true)
+	// eligible missing data in tx3
+	blk2MissingData.Add(3, "ns-1", "coll-1", true)
+
+	// no pvt data with block 0
+	req.NoError(store.Prepare(0, nil, nil))
+	req.NoError(store.Commit())
+
+	// pvt data with block 1 - commit
+	req.NoError(store.Prepare(1, testData, blk1MissingData))
+	req.NoError(store.Commit())
+
+	// pvt data with block 2 - rollback
+	req.NoError(store.Prepare(2, testData, nil))
+	req.NoError(store.Rollback())
+
+	// pvt data retrieval for block 0 should return nil
+	var nilFilter ledger.PvtNsCollFilter
+	retrievedData, err := store.GetPvtDataByBlockNum(0, nilFilter)
+	req.NoError(err)
+	req.Nil(retrievedData)
+
+	// pvt data retrieval for block 1 should return full pvtdata
+	retrievedData, err = store.GetPvtDataByBlockNum(1, nilFilter)
+	req.NoError(err)
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, testData[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, testData[i].WriteSet))
+	}
+
+	// pvt data retrieval for block 1 with filter should return filtered pvtdata
+	filter := ledger.NewPvtNsCollFilter()
+	filter.Add("ns-1", "coll-1")
+	filter.Add("ns-2", "coll-2")
+	retrievedData, err = store.GetPvtDataByBlockNum(1, filter)
+	expectedRetrievedData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 2, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+		produceSamplePvtdata(t, 4, []string{"ns-1:coll-1", "ns-2:coll-2"}),
+	}
+	for i, data := range retrievedData {
+		req.Equal(data.SeqInBlock, expectedRetrievedData[i].SeqInBlock)
+		req.True(proto.Equal(data.WriteSet, expectedRetrievedData[i].WriteSet))
+	}
+
+	// pvt data retrieval for block 2 should return ErrOutOfRange
+	retrievedData, err = store.GetPvtDataByBlockNum(2, nilFilter)
+	_, ok := err.(*pvtdatastorage.ErrOutOfRange)
+	req.True(ok)
+	req.Nil(retrievedData)
+
+	// pvt data with block 2 - commit
+	req.NoError(store.Prepare(2, testData, blk2MissingData))
+	req.NoError(store.Commit())
+}
+
+func TestStoreState(t *testing.T) {
+	btlPolicy := btltestutil.SampleBTLPolicy(
+		map[[2]string]uint64{
+			{"ns-1", "coll-1"}: 0,
+			{"ns-1", "coll-2"}: 0,
+		},
+	)
+	env := NewTestStoreEnv(t, "teststate", btlPolicy, couchDBDef)
+	req := require.New(t)
+	store := env.TestStore
+	testData := []*ledger.TxPvtData{
+		produceSamplePvtdata(t, 0, []string{"ns-1:coll-1", "ns-1:coll-2"}),
+	}
+	_, ok := store.Prepare(1, testData, nil).(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+
+	req.Nil(store.Prepare(0, testData, nil))
+	req.NoError(store.Commit())
+
+	req.Nil(store.Prepare(1, testData, nil))
+	_, ok = store.Prepare(2, testData, nil).(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+}
+
+func TestInitLastCommittedBlock(t *testing.T) {
+	env := NewTestStoreEnv(t, "testinitlastcommittedblock", nil, couchDBDef)
+	req := require.New(t)
+	store := env.TestStore
+	existingLastBlockNum := uint64(25)
+	req.NoError(store.InitLastCommittedBlock(existingLastBlockNum))
+
+	testEmpty(false, req, store)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(existingLastBlockNum+1, req, store)
+
+	env.CloseAndReopen()
+	testEmpty(false, req, store)
+	testPendingBatch(false, req, store)
+	testLastCommittedBlockHeight(existingLastBlockNum+1, req, store)
+
+	err := store.InitLastCommittedBlock(30)
+	_, ok := err.(*pvtdatastorage.ErrIllegalCall)
+	req.True(ok)
+}
+
+func testLastCommittedBlockHeight(expectedBlockHt uint64, req *require.Assertions, store pvtdatastorage.Store) {
+	blkHt, err := store.LastCommittedBlockHeight()
+	req.NoError(err)
+	req.Equal(expectedBlockHt, blkHt)
+}
+
+func testEmpty(expectedEmpty bool, req *require.Assertions, store pvtdatastorage.Store) {
+	isEmpty, err := store.IsEmpty()
+	req.NoError(err)
+	req.Equal(expectedEmpty, isEmpty)
+}
+
+func testPendingBatch(expectedPending bool, req *require.Assertions, store pvtdatastorage.Store) {
+	hasPendingBatch, err := store.HasPendingBatch()
+	req.NoError(err)
+	req.Equal(expectedPending, hasPendingBatch)
+}
+
+func produceSamplePvtdata(t *testing.T, txNum uint64, nsColls []string) *ledger.TxPvtData {
+	builder := rwsetutil.NewRWSetBuilder()
+	for _, nsColl := range nsColls {
+		nsCollSplit := strings.Split(nsColl, ":")
+		ns := nsCollSplit[0]
+		coll := nsCollSplit[1]
+		builder.AddToPvtAndHashedWriteSet(ns, coll, fmt.Sprintf("key-%s-%s", ns, coll), []byte(fmt.Sprintf("value-%s-%s", ns, coll)))
+	}
+	simRes, err := builder.GetTxSimulationResults()
+	require.NoError(t, err)
+	return &ledger.TxPvtData{SeqInBlock: txNum, WriteSet: simRes.PvtSimulationResults}
+}
diff --git a/extensions/pvtdatastorage/test_exports.go b/extensions/pvtdatastorage/test_exports.go
new file mode 100644
index 00000000..b6778c5e
--- /dev/null
+++ b/extensions/pvtdatastorage/test_exports.go
@@ -0,0 +1,77 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package pvtdatastorage
+
+import (
+	"os"
+	"testing"
+
+	"github.com/hyperledger/fabric/common/metrics/disabled"
+	"github.com/hyperledger/fabric/core/ledger/ledgerconfig"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatapolicy"
+	"github.com/hyperledger/fabric/core/ledger/pvtdatastorage"
+	"github.com/hyperledger/fabric/core/ledger/util/couchdb"
+	"github.com/stretchr/testify/require"
+)
+
+// StoreEnv provides the  store env for testing
+type StoreEnv struct {
+	t                 testing.TB
+	TestStoreProvider pvtdatastorage.Provider
+	TestStore         pvtdatastorage.Store
+	ledgerid          string
+	btlPolicy         pvtdatapolicy.BTLPolicy
+	couchDBDef        *couchdb.CouchDBDef
+}
+
+// NewTestStoreEnv construct a StoreEnv for testing
+func NewTestStoreEnv(t *testing.T, ledgerid string, btlPolicy pvtdatapolicy.BTLPolicy, couchDBDef *couchdb.CouchDBDef) *StoreEnv {
+	removeStorePath()
+	req := require.New(t)
+	testStoreProvider := NewProvider()
+	testStore, err := testStoreProvider.OpenStore(ledgerid)
+	req.NoError(err)
+	testStore.Init(btlPolicy)
+	s := &StoreEnv{t, testStoreProvider, testStore, ledgerid, btlPolicy, couchDBDef}
+	return s
+}
+
+// CloseAndReopen closes and opens the store provider
+func (env *StoreEnv) CloseAndReopen() {
+	var err error
+	env.TestStoreProvider.Close()
+	env.TestStoreProvider = NewProvider()
+	env.TestStore, err = env.TestStoreProvider.OpenStore(env.ledgerid)
+	env.TestStore.Init(env.btlPolicy)
+	require.NoError(env.t, err)
+}
+
+//Cleanup env test
+func (env *StoreEnv) Cleanup(ledgerid string) {
+	//create a new connection
+	couchInstance, err := couchdb.CreateCouchInstance(env.couchDBDef.URL, env.couchDBDef.Username, env.couchDBDef.Password,
+		env.couchDBDef.MaxRetries, env.couchDBDef.MaxRetriesOnStartup, env.couchDBDef.RequestTimeout, env.couchDBDef.CreateGlobalChangesDB, &disabled.Provider{})
+	if err != nil {
+		panic(err.Error())
+	}
+	pvtDataStoreDBName := couchdb.ConstructBlockchainDBName(ledgerid, "pvtdata")
+	db := couchdb.CouchDatabase{CouchInstance: couchInstance, DBName: pvtDataStoreDBName}
+	//drop the test database
+	if _, err := db.DropDatabase(); err != nil {
+		panic(err.Error())
+	}
+	env.TestStore.Shutdown()
+
+	removeStorePath()
+}
+
+func removeStorePath() {
+	dbPath := ledgerconfig.GetPvtdataStorePath()
+	if err := os.RemoveAll(dbPath); err != nil {
+		panic(err.Error())
+	}
+}
diff --git a/extensions/testutil/ext_test_env.go b/extensions/testutil/ext_test_env.go
new file mode 100644
index 00000000..5bbb7785
--- /dev/null
+++ b/extensions/testutil/ext_test_env.go
@@ -0,0 +1,17 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package testutil
+
+//SetupExtTestEnv creates new extension test environment,
+// it creates couchdb instance for test, returns couchdbd address, cleanup and destroy function handle.
+func SetupExtTestEnv() (addr string, cleanup func(string), stop func()) {
+	return "", func(string) {
+			//do nothing
+		}, func() {
+			//do nothing
+		}
+}
diff --git a/extensions/transientstore/common/common_store_helper.go b/extensions/transientstore/common/common_store_helper.go
new file mode 100644
index 00000000..61e2716e
--- /dev/null
+++ b/extensions/transientstore/common/common_store_helper.go
@@ -0,0 +1,168 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package common
+
+import (
+	"bytes"
+	"errors"
+	"path/filepath"
+
+	"github.com/hyperledger/fabric/common/ledger/util"
+	"github.com/hyperledger/fabric/core/config"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+)
+
+// TODO add pinning script to include copied code into this file, original file from fabric is found in fabric/core/transientstore/store_helper.go
+// TODO below functions are originally unexported, the pinning script must capitalize these functions to export them
+
+var (
+	prwsetPrefix             = []byte("P")[0] // key prefix for storing private write set in transient store.
+	purgeIndexByHeightPrefix = []byte("H")[0] // key prefix for storing index on private write set using received at block height.
+	//purgeIndexByTxidPrefix   = []byte("T")[0] // key prefix for storing index on private write set using txid
+	compositeKeySep = byte(0x00)
+)
+
+// CreateCompositeKeyForPvtRWSet creates a key for storing private write set
+// in the transient store. The structure of the key is <prwsetPrefix>~txid~uuid~blockHeight.
+// TODO add pinning script to expose this function
+func CreateCompositeKeyForPvtRWSet(txid string, uuid string, blockHeight uint64) []byte {
+	var compositeKey []byte
+	compositeKey = append(compositeKey, prwsetPrefix)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, createCompositeKeyWithoutPrefixForTxid(txid, uuid, blockHeight)...)
+
+	return compositeKey
+}
+
+// createCompositeKeyWithoutPrefixForTxid creates a composite key of structure txid~uuid~blockHeight.
+func createCompositeKeyWithoutPrefixForTxid(txid string, uuid string, blockHeight uint64) []byte {
+	var compositeKey []byte
+	compositeKey = append(compositeKey, []byte(txid)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, []byte(uuid)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, util.EncodeOrderPreservingVarUint64(blockHeight)...)
+
+	return compositeKey
+}
+
+// CreateCompositeKeyForPurgeIndexByHeight creates a key to index private write set based on
+// received at block height such that purge based on block height can be achieved. The structure
+// of the key is <purgeIndexByHeightPrefix>~blockHeight~txid~uuid.
+// TODO add pinning script to expose this function
+func CreateCompositeKeyForPurgeIndexByHeight(blockHeight uint64, txid string, uuid string) []byte {
+	var compositeKey []byte
+	compositeKey = append(compositeKey, purgeIndexByHeightPrefix)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, util.EncodeOrderPreservingVarUint64(blockHeight)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, []byte(txid)...)
+	compositeKey = append(compositeKey, compositeKeySep)
+	compositeKey = append(compositeKey, []byte(uuid)...)
+
+	return compositeKey
+}
+
+// SplitCompositeKeyOfPvtRWSet splits the compositeKey (<prwsetPrefix>~txid~uuid~blockHeight)
+// into uuid and blockHeight.
+// TODO add pinning script to expose this function
+func SplitCompositeKeyOfPvtRWSet(compositeKey []byte) (uuid string, blockHeight uint64) {
+	return splitCompositeKeyWithoutPrefixForTxid(compositeKey[2:])
+}
+
+// SplitCompositeKeyOfPurgeIndexByHeight splits the compositeKey (<purgeIndexByHeightPrefix>~blockHeight~txid~uuid)
+// into txid, uuid and blockHeight.
+// TODO add pinning script to expose this function
+func SplitCompositeKeyOfPurgeIndexByHeight(compositeKey []byte) (txid string, uuid string, blockHeight uint64) {
+	var n int
+	blockHeight, n = util.DecodeOrderPreservingVarUint64(compositeKey[2:])
+	splits := bytes.Split(compositeKey[n+3:], []byte{compositeKeySep})
+	txid = string(splits[0])
+	uuid = string(splits[1])
+	return
+}
+
+// splitCompositeKeyWithoutPrefixForTxid splits the composite key txid~uuid~blockHeight into
+// uuid and blockHeight
+func splitCompositeKeyWithoutPrefixForTxid(compositeKey []byte) (uuid string, blockHeight uint64) {
+	// skip txid as all functions which requires split of composite key already has it
+	firstSepIndex := bytes.IndexByte(compositeKey, compositeKeySep)
+	secondSepIndex := firstSepIndex + bytes.IndexByte(compositeKey[firstSepIndex+1:], compositeKeySep) + 1
+	uuid = string(compositeKey[firstSepIndex+1 : secondSepIndex])
+	blockHeight, _ = util.DecodeOrderPreservingVarUint64(compositeKey[secondSepIndex+1:])
+	return
+}
+
+// GetTransientStorePath returns the filesystem path for temporarily storing the private rwset
+// TODO add pinning script to keep this function
+func GetTransientStorePath() string {
+	sysPath := config.GetPath("peer.fileSystemPath")
+	return filepath.Join(sysPath, "transientStore")
+}
+
+// TrimPvtWSet returns a `TxPvtReadWriteSet` that retains only list of 'ns/collections' supplied in the filter
+// A nil filter does not filter any results and returns the original `pvtWSet` as is
+// TODO add pinning script to expose this function
+func TrimPvtWSet(pvtWSet *rwset.TxPvtReadWriteSet, filter ledger.PvtNsCollFilter) *rwset.TxPvtReadWriteSet {
+	if filter == nil {
+		return pvtWSet
+	}
+
+	var filteredNsRwSet []*rwset.NsPvtReadWriteSet
+	for _, ns := range pvtWSet.NsPvtRwset {
+		var filteredCollRwSet []*rwset.CollectionPvtReadWriteSet
+		for _, coll := range ns.CollectionPvtRwset {
+			if filter.Has(ns.Namespace, coll.CollectionName) {
+				filteredCollRwSet = append(filteredCollRwSet, coll)
+			}
+		}
+		if filteredCollRwSet != nil {
+			filteredNsRwSet = append(filteredNsRwSet,
+				&rwset.NsPvtReadWriteSet{
+					Namespace:          ns.Namespace,
+					CollectionPvtRwset: filteredCollRwSet,
+				},
+			)
+		}
+	}
+	var filteredTxPvtRwSet *rwset.TxPvtReadWriteSet
+	if filteredNsRwSet != nil {
+		filteredTxPvtRwSet = &rwset.TxPvtReadWriteSet{
+			DataModel:  pvtWSet.GetDataModel(),
+			NsPvtRwset: filteredNsRwSet,
+		}
+	}
+	return filteredTxPvtRwSet
+}
+
+// TrimPvtCollectionConfigs returns a map of `CollectionConfigPackage` with configs retained only for config types 'staticCollectionConfig' supplied in the filter
+// A nil filter does not set Config to any collectionConfigPackage any returns a map with empty configs for each `configs` element
+// TODO add pinning script to expose this function and add below comment
+func TrimPvtCollectionConfigs(configs map[string]*common.CollectionConfigPackage,
+	filter ledger.PvtNsCollFilter) (map[string]*common.CollectionConfigPackage, error) {
+	if filter == nil {
+		return configs, nil
+	}
+	result := make(map[string]*common.CollectionConfigPackage)
+
+	for ns, pkg := range configs {
+		result[ns] = &common.CollectionConfigPackage{}
+		for _, colConf := range pkg.GetConfig() {
+			switch cconf := colConf.Payload.(type) {
+			case *common.CollectionConfig_StaticCollectionConfig:
+				if filter.Has(ns, cconf.StaticCollectionConfig.Name) {
+					result[ns].Config = append(result[ns].Config, colConf)
+				}
+			default:
+				return nil, errors.New("unexpected collection type")
+			}
+		}
+	}
+	return result, nil
+}
diff --git a/extensions/transientstore/store.go b/extensions/transientstore/store.go
new file mode 100644
index 00000000..aa0aa8aa
--- /dev/null
+++ b/extensions/transientstore/store.go
@@ -0,0 +1,471 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"encoding/base64"
+	"encoding/hex"
+	"fmt"
+	"sort"
+
+	"github.com/bluele/gcache"
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/util"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/transientstore"
+	"github.com/hyperledger/fabric/extensions/transientstore/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	pb "github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/pkg/errors"
+)
+
+var nilByte = byte('\x00')
+
+// ErrStoreEmpty is used to indicate that there are no entries in transient store
+var ErrStoreEmpty = errors.New("Transient store is empty")
+
+// Store manages the storage of private write sets for a ledgerId.
+// Ideally, a ledger can remove the data from this storage when it is committed to
+// the permanent storage or the pruning of some data items is enforced by the policy
+// the internal storage mechanism used in this specific store is gcache of type 'simple'
+// cache to allow 'unlimited' growth of data and avoid eviction due to size. The assumption
+// is the ledger will purge data from this storage once transactions are committed or deemed invalid.
+
+type store struct {
+	// cache contains a key of txid and a value represented as a map of composite key/TxRWSet values (real data store)
+	cache gcache.Cache
+	// blockHeightCache contains a key of blockHeight and value represented as a slice of txids
+	blockHeightCache gcache.Cache
+	// txidCache contains a key of txid and value represented as a slice of blockHeights
+	txidCache gcache.Cache
+}
+
+func newStore() *store {
+	s := &store{}
+	s.cache = gcache.New(0).LoaderFunc(loadPvtRWSetMap).Build()
+	s.blockHeightCache = gcache.New(0).LoaderFunc(loadBlockHeight).Build()
+	s.txidCache = gcache.New(0).LoaderFunc(loadTxid).Build()
+	return s
+}
+
+// Persist stores the private write set of a transaction in the transient store
+// based on txid and the block height the private data was received at
+func (s *store) Persist(txid string, blockHeight uint64, privateSimulationResults *rwset.TxPvtReadWriteSet) error {
+	logger.Debugf("Persisting private data to transient store for txid [%s] at block height [%d]", txid, blockHeight)
+
+	uuid := util.GenerateUUID()
+	compositeKeyPvtRWSet := common.CreateCompositeKeyForPvtRWSet(txid, uuid, blockHeight)
+	privateSimulationResultsBytes, err := proto.Marshal(privateSimulationResults)
+	if err != nil {
+		return err
+	}
+
+	s.setTxPvtWRSetToCache(txid, compositeKeyPvtRWSet, privateSimulationResultsBytes)
+
+	s.setTxidToBlockHeightCache(txid, blockHeight)
+
+	s.updateTxidCache(txid, blockHeight)
+	return nil
+}
+
+// PersistWithConfig stores the private write set of a transaction along with the collection config
+// in the transient store based on txid and the block height the private data was received at
+func (s *store) PersistWithConfig(txid string, blockHeight uint64, privateSimulationResultsWithConfig *pb.TxPvtReadWriteSetWithConfigInfo) error {
+	if privateSimulationResultsWithConfig != nil {
+		logger.Debugf("Persisting private data to transient store for txid [%s] at block height [%d] with [%d] config(s)", txid, blockHeight, len(privateSimulationResultsWithConfig.CollectionConfigs))
+	} else {
+		logger.Debugf("Persisting private data to transient store for txid [%s] at block height [%d] with nil config", txid, blockHeight)
+	}
+
+	uuid := util.GenerateUUID()
+	compositeKeyPvtRWSet := common.CreateCompositeKeyForPvtRWSet(txid, uuid, blockHeight)
+	privateSimulationResultsWithConfigBytes, err := proto.Marshal(privateSimulationResultsWithConfig)
+	if err != nil {
+		return err
+	}
+
+	// emulating original Fabric's new proto (post v1.2) by appending nilByte
+	// TODO remove this when Fabric stops appending nilByte
+	privateSimulationResultsWithConfigBytes = append([]byte{nilByte}, privateSimulationResultsWithConfigBytes...)
+
+	s.setTxPvtWRSetToCache(txid, compositeKeyPvtRWSet, privateSimulationResultsWithConfigBytes)
+
+	s.setTxidToBlockHeightCache(txid, blockHeight)
+
+	s.updateTxidCache(txid, blockHeight)
+	return nil
+}
+
+func (s *store) updateTxidCache(txid string, blockHeight uint64) {
+	value, err := s.txidCache.Get(txid)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+	}
+	uintVal := value.(*blockHeightsSlice)
+	if found, _ := uintVal.findBlockHeightEntryInSlice(blockHeight); !found {
+		uintVal.add(blockHeight)
+	}
+
+	err = s.txidCache.Set(txid, uintVal)
+	if err != nil {
+		panic(fmt.Sprintf("Storing blockheight '%d' for txid key '%s' in transientstore cache must never fail, err:%s", blockHeight, txid, err))
+	}
+}
+
+func (s *store) getTxPvtRWSetFromCache(txid string) *pvtRWSetMap {
+	value, err := s.cache.Get(txid)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+	}
+
+	return value.(*pvtRWSetMap)
+}
+
+func (s *store) getTxidsFromBlockHeightCache(blockHeight uint64) *txidsSlice {
+	blockHeightValue, err := s.blockHeightCache.Get(blockHeight)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+	}
+	return blockHeightValue.(*txidsSlice)
+}
+
+func (s *store) setTxPvtWRSetToCache(txid string, compositeKeyPvtRWSet, privSimulationResults []byte) {
+	txPvtRWSetMap := s.getTxPvtRWSetFromCache(txid)
+	k := hex.EncodeToString(compositeKeyPvtRWSet)
+	v := base64.StdEncoding.EncodeToString(privSimulationResults)
+	txPvtRWSetMap.set(k, v)
+
+	err := s.cache.Set(txid, txPvtRWSetMap)
+	if err != nil {
+		panic(fmt.Sprintf("Set to cache must never return an error, got error:%s", err))
+	}
+}
+
+func (s *store) setTxidToBlockHeightCache(txid string, blockHeight uint64) {
+	blockHeightTxids := s.getTxidsFromBlockHeightCache(blockHeight)
+	found, _ := blockHeightTxids.findTxidEntryInSlice(txid)
+	if !found {
+		blockHeightTxids.add(txid)
+		err := s.blockHeightCache.Set(blockHeight, blockHeightTxids)
+		if err != nil {
+			panic(fmt.Sprintf("Set to cache must never return an error, got error:%s", err))
+		}
+	}
+}
+
+// GetTxPvtRWSetByTxid returns an iterator due to the fact that the txid may have multiple private
+// write sets persisted from different endorsers (via Gossip)
+func (s *store) GetTxPvtRWSetByTxid(txid string, filter ledger.PvtNsCollFilter) (transientstore.RWSetScanner, error) {
+	logger.Debugf("Calling GetTxPvtRWSetByTxid on transient store for txid [%s]", txid)
+	var results []keyValue
+
+	val, err := s.cache.Get(txid)
+	if err != nil {
+		if err != gcache.KeyNotFoundError {
+			panic(fmt.Sprintf("Get from cache must never return an error other than KeyNotFoundError err:%s", err))
+		}
+		// return empty results
+		return &RwsetScanner{filter: filter, results: []keyValue{}}, nil
+	}
+
+	pvtRWsm := val.(*pvtRWSetMap)
+	pvtRWsm.mu.RLock()
+	defer pvtRWsm.mu.RUnlock()
+	for key, value := range pvtRWsm.m {
+		results = append(results, keyValue{key: key, value: value})
+	}
+
+	return &RwsetScanner{filter: filter, results: results}, nil
+}
+
+// GetMinTransientBlkHt returns the lowest block height remaining in transient store
+func (s *store) GetMinTransientBlkHt() (uint64, error) {
+	var minTransientBlkHt uint64
+	val := s.blockHeightCache.GetALL()
+
+	for key := range val {
+		k := key.(uint64)
+		if minTransientBlkHt == 0 || k < minTransientBlkHt {
+			minTransientBlkHt = k
+		}
+	}
+	logger.Debugf("Called GetMinTransientBlkHt on transient store, min block height is: %d", minTransientBlkHt)
+	if minTransientBlkHt == 0 { // mimic Fabric's transientstore with leveldb -> return an error
+		return 0, ErrStoreEmpty
+	}
+	return minTransientBlkHt, nil
+}
+
+// PurgeByTxids removes private write sets of a given set of transactions from the
+// transient store
+func (s *store) PurgeByTxids(txids []string) error {
+	logger.Debugf("Calling PurgeByTxids on transient store for txids [%v]", txids)
+	s.purgeTxPvtRWSetCacheByTxids(txids)
+	return s.purgeBlockHeightCacheByTxids(txids)
+}
+
+// Shutdown noop for in memory storage
+func (s *store) Shutdown() {
+
+}
+
+func (s *store) purgeTxPvtRWSetCacheByTxids(txids []string) {
+	for _, txID := range txids {
+		s.cache.Remove(txID)
+	}
+}
+
+func (s *store) purgeTxRWSetCacheByBlockHeight(txids []string, maxBlockNumToRetain uint64) error {
+	for _, txID := range txids {
+		txMap := s.getTxPvtRWSetFromCache(txID)
+		for _, txK := range txMap.keys() {
+			hexKey, err := hex.DecodeString(txK)
+			if err != nil {
+				return err
+			}
+			_, blkHeight := common.SplitCompositeKeyOfPvtRWSet(hexKey)
+			if blkHeight < maxBlockNumToRetain {
+				txMap.delete(txK)
+			}
+		}
+		if txMap.length() == 0 {
+			s.cache.Remove(txID)
+		}
+
+	}
+	return nil
+}
+
+func (s *store) purgeBlockHeightCacheByTxids(txids []string) error {
+	sort.Strings(txids)
+	var blkHeightTxids *txidsSlice
+	var blkHeightKeys []uint64
+	// step 1 fetch block heights for txids
+	blkHeightKeys, err := s.getBlockHeightKeysFromTxidCache(txids)
+	if err != nil {
+		return err
+	}
+	// step 2 remove txids from blockHeightCache
+	for _, blkHgtKey := range blkHeightKeys {
+		value, err := s.blockHeightCache.Get(blkHgtKey)
+		if err != nil {
+			if err == gcache.KeyNotFoundError {
+				continue
+			}
+			return err
+		}
+		blkHeightTxids = value.(*txidsSlice)
+		for _, txID := range txids {
+			for { // ensure to remove duplicates
+				if isFound, i := blkHeightTxids.findTxidEntryInSlice(txID); isFound {
+					blkHeightTxids.removeTxidEntryAtIndex(i)
+				} else {
+					break
+				}
+			}
+		}
+
+		if blkHeightTxids.length() == 0 {
+			s.blockHeightCache.Remove(blkHgtKey)
+		} else {
+			err := s.blockHeightCache.Set(blkHgtKey, blkHeightTxids)
+			if err != nil {
+				return err
+			}
+		}
+	}
+
+	// step 3 remove blockHeights from txidCache
+	return s.purgeTxidsCacheByBlockHeight(txids, blkHeightKeys)
+}
+
+func (s *store) getBlockHeightKeysFromTxidCache(txids []string) ([]uint64, error) {
+	var blkHeightKeys []uint64
+	for _, t := range txids {
+		blkHgts, err := s.txidCache.Get(t)
+		if err != nil {
+			if err == gcache.KeyNotFoundError {
+				continue
+			}
+			return nil, err
+		}
+		if blkHgts.(*blockHeightsSlice).length() > 0 {
+			blkHeightKeys = append(blkHeightKeys, blkHgts.(*blockHeightsSlice).getBlockHeights()...)
+		}
+	}
+	blkHeightKeys = sliceUniqueUint64(blkHeightKeys)
+	return blkHeightKeys, nil
+}
+
+// PurgeByHeight will remove all ReadWriteSets with block height below maxBlockNumToRetain
+func (s *store) PurgeByHeight(maxBlockNumToRetain uint64) error {
+	logger.Debugf("Calling PurgeByHeight on transient store for maxBlockNumToRetain [%d]", maxBlockNumToRetain)
+	txIDs := make([]string, 0)
+	blkHgts := make([]uint64, 0)
+	for key, value := range s.blockHeightCache.GetALL() {
+		k := key.(uint64)
+		if k < maxBlockNumToRetain {
+			txIDs = append(txIDs, value.(*txidsSlice).getTxids()...)
+			blkHgts = append(blkHgts, k)
+			s.blockHeightCache.Remove(k)
+		}
+	}
+	txIDs = sliceUniqueString(txIDs)
+	blkHgts = sliceUniqueUint64(blkHgts)
+	err := s.purgeTxRWSetCacheByBlockHeight(txIDs, maxBlockNumToRetain)
+	if err != nil {
+		return err
+	}
+	return s.purgeTxidsCacheByBlockHeight(txIDs, blkHgts)
+
+}
+
+func (s *store) purgeTxidsCacheByBlockHeight(txids []string, blockHeights []uint64) error {
+	for _, txid := range txids {
+		blkHgt, err := s.txidCache.Get(txid)
+		if err != nil {
+			if err == gcache.KeyNotFoundError {
+				continue
+			}
+			return err
+		}
+		blkHgtSliceByTxid := blkHgt.(*blockHeightsSlice)
+
+		for _, b := range blockHeights {
+			if isFound, i := blkHgtSliceByTxid.findBlockHeightEntryInSlice(b); isFound {
+				blkHgtSliceByTxid.removeBlockHeightEntryAtIndex(i)
+			}
+		}
+
+		if blkHgtSliceByTxid.length() == 0 {
+			s.txidCache.Remove(txid)
+		}
+	}
+	return nil
+}
+
+type keyValue struct {
+	key   string
+	value string
+}
+
+// RwsetScanner provides an iterator for EndorserPvtSimulationResults from transientstore
+type RwsetScanner struct {
+	filter  ledger.PvtNsCollFilter
+	results []keyValue
+	next    int
+}
+
+// Next moves the iterator to the next key/value pair.
+// It returns whether the iterator is exhausted.
+// TODO: Once the related gossip changes are made as per FAB-5096, remove this function
+func (scanner *RwsetScanner) Next() (*transientstore.EndorserPvtSimulationResults, error) {
+	kv, ok := scanner.nextKV()
+	if !ok {
+		return nil, nil
+	}
+
+	keyBytes, err := hex.DecodeString(kv.key)
+	if err != nil {
+		return nil, err
+	}
+	_, blockHeight := common.SplitCompositeKeyOfPvtRWSet(keyBytes)
+	logger.Debugf("scanner next blockHeight %d", blockHeight)
+	txPvtRWSet := &rwset.TxPvtReadWriteSet{}
+	valueBytes, err := base64.StdEncoding.DecodeString(kv.value)
+	if err != nil {
+		return nil, errors.Wrapf(err, "error from DecodeString for transientDataField")
+	}
+
+	if err := proto.Unmarshal(valueBytes, txPvtRWSet); err != nil {
+		return nil, err
+	}
+	filteredTxPvtRWSet := common.TrimPvtWSet(txPvtRWSet, scanner.filter)
+	logger.Debugf("scanner next filteredTxPvtRWSet %v", filteredTxPvtRWSet)
+
+	return &transientstore.EndorserPvtSimulationResults{
+		ReceivedAtBlockHeight: blockHeight,
+		PvtSimulationResults:  filteredTxPvtRWSet,
+	}, nil
+
+}
+
+// NextWithConfig moves the iterator to the next key/value pair with configs.
+// It returns whether the iterator is exhausted.
+// TODO: Once the related gossip changes are made as per FAB-5096, rename this function to Next
+func (scanner *RwsetScanner) NextWithConfig() (*transientstore.EndorserPvtSimulationResultsWithConfig, error) {
+	kv, ok := scanner.nextKV()
+	if !ok {
+		return nil, nil
+	}
+
+	keyBytes, err := hex.DecodeString(kv.key)
+	if err != nil {
+		return nil, err
+	}
+	_, blockHeight := common.SplitCompositeKeyOfPvtRWSet(keyBytes)
+	logger.Debugf("scanner NextWithConfig blockHeight %d", blockHeight)
+
+	valueBytes, err := base64.StdEncoding.DecodeString(kv.value)
+	if err != nil {
+		return nil, errors.Wrapf(err, "error from DecodeString for transientDataField")
+	}
+
+	txPvtRWSet := &rwset.TxPvtReadWriteSet{}
+	var filteredTxPvtRWSet *rwset.TxPvtReadWriteSet
+	txPvtRWSetWithConfig := &pb.TxPvtReadWriteSetWithConfigInfo{}
+
+	if valueBytes[0] == nilByte {
+		// new proto, i.e., TxPvtReadWriteSetWithConfigInfo
+		if er := proto.Unmarshal(valueBytes[1:], txPvtRWSetWithConfig); er != nil {
+			return nil, er
+		}
+
+		logger.Debugf("scanner NextWithConfig txPvtRWSetWithConfig %v", txPvtRWSetWithConfig)
+
+		filteredTxPvtRWSet = common.TrimPvtWSet(txPvtRWSetWithConfig.GetPvtRwset(), scanner.filter)
+		logger.Debugf("scanner NextWithConfig filteredTxPvtRWSet %v", filteredTxPvtRWSet)
+		configs, err := common.TrimPvtCollectionConfigs(txPvtRWSetWithConfig.CollectionConfigs, scanner.filter)
+		if err != nil {
+			return nil, err
+		}
+		logger.Debugf("scanner NextWithConfig configs %v", configs)
+		txPvtRWSetWithConfig.CollectionConfigs = configs
+	} else {
+		// old proto, i.e., TxPvtReadWriteSet
+		if e := proto.Unmarshal(valueBytes, txPvtRWSet); e != nil {
+			return nil, e
+		}
+		filteredTxPvtRWSet = common.TrimPvtWSet(txPvtRWSet, scanner.filter)
+	}
+
+	txPvtRWSetWithConfig.PvtRwset = filteredTxPvtRWSet
+
+	return &transientstore.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          blockHeight,
+		PvtSimulationResultsWithConfig: txPvtRWSetWithConfig,
+	}, nil
+}
+
+func (scanner *RwsetScanner) nextKV() (keyValue, bool) {
+	i := scanner.next
+	if i >= len(scanner.results) {
+		return keyValue{}, false
+	}
+	scanner.next++
+	return scanner.results[i], true
+}
+
+// Close releases resource held by the iterator
+func (scanner *RwsetScanner) Close() {
+}
diff --git a/extensions/transientstore/store_helper.go b/extensions/transientstore/store_helper.go
new file mode 100644
index 00000000..64acad93
--- /dev/null
+++ b/extensions/transientstore/store_helper.go
@@ -0,0 +1,190 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"sort"
+	"sync"
+)
+
+// loadPvtRWSetMap is a loader function of store.cache
+func loadPvtRWSetMap(key interface{}) (interface{}, error) {
+	return &pvtRWSetMap{m: map[string]string{}}, nil
+}
+
+// pvtRWSetMap represents a cached element in store.cache
+type pvtRWSetMap struct {
+	mu sync.RWMutex
+	m  map[string]string
+}
+
+//func (p *pvtRWSetMap) get(k string) string {
+//	p.mu.RLock()
+//	defer p.mu.RUnlock()
+//
+//	return p.m[k]
+//}
+
+func (p *pvtRWSetMap) set(k string, v string) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m[k] = v
+}
+
+func (p *pvtRWSetMap) length() int {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return len(p.m)
+}
+
+func (p *pvtRWSetMap) delete(k string) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	delete(p.m, k)
+}
+
+func (p *pvtRWSetMap) keys() []string {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	keys := make([]string, 0, len(p.m))
+	for k := range p.m {
+		keys = append(keys, k)
+	}
+	return keys
+}
+
+// loadBlockHeight is a loader function of store.blockHeightCache
+func loadBlockHeight(key interface{}) (interface{}, error) {
+	return &txidsSlice{m: []string{}}, nil
+}
+
+// txidsSlice represents a cached element in store.blockHeightCache
+type txidsSlice struct {
+	mu sync.RWMutex
+	m  []string
+}
+
+func (p *txidsSlice) add(v string) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m = append(p.m, v)
+	sort.Strings(p.m) // ensures txids are sorted to help sort.search call in findTxidEntryInSlice below
+}
+
+// findTxidEntryInSlice will search for a str in the txidsSlice
+func (p *txidsSlice) findTxidEntryInSlice(txid string) (bool, int) {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+	i := sort.Search(len(p.m), func(i int) bool { return p.m[i] >= txid }) //  equivalent to sort.SearchStrings()
+	return i < len(p.m) && p.m[i] == txid, i
+}
+
+// removeTxidEntryAtIndex removes an entry at index in the txidsSlice
+func (p *txidsSlice) removeTxidEntryAtIndex(index int) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+	p.m = append(p.m[:index], p.m[index+1:]...)
+}
+
+func (p *txidsSlice) length() int {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return len(p.m)
+}
+
+func (p *txidsSlice) getTxids() []string {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return p.m
+}
+
+// loadTxid is a loader function of store.txidCache
+func loadTxid(key interface{}) (interface{}, error) {
+	return &blockHeightsSlice{m: []uint64{}}, nil
+}
+
+// blockHeightsSlice represents a cached element in store.txidCache
+type blockHeightsSlice struct {
+	mu sync.RWMutex
+	m  []uint64
+}
+
+func (p *blockHeightsSlice) add(v uint64) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m = append(p.m, v)
+	sort.Slice(p.m, func(i, j int) bool { return p.m[i] < p.m[j] }) // ensures blockHeights are sorted to help sort.search call in findBlockHeightEntryInSlice below
+}
+
+// findBlockHeightEntryInSlice will search for a u in a slice uint64Slice (of type uint64)
+func (p *blockHeightsSlice) findBlockHeightEntryInSlice(blockHeight uint64) (bool, int) {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	i := sort.Search(len(p.m), func(i int) bool { return p.m[i] >= blockHeight })
+	return i < len(p.m) && p.m[i] == blockHeight, i
+}
+
+// removeBlockHeightEntryAtIndex removes an entry at index in the slice s or type uint64
+func (p *blockHeightsSlice) removeBlockHeightEntryAtIndex(index int) {
+	p.mu.Lock()
+	defer p.mu.Unlock()
+
+	p.m = append(p.m[:index], p.m[index+1:]...)
+}
+
+func (p *blockHeightsSlice) length() int {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return len(p.m)
+}
+
+func (p *blockHeightsSlice) getBlockHeights() []uint64 {
+	p.mu.RLock()
+	defer p.mu.RUnlock()
+
+	return p.m
+}
+
+// sliceUniqueString will strip out any duplicate entries from a slice s (of type string)
+func sliceUniqueString(s []string) []string {
+	seen := make(map[string]struct{}, len(s))
+	j := 0
+	for _, v := range s {
+		if _, ok := seen[v]; ok {
+			continue
+		}
+		seen[v] = struct{}{}
+		s[j] = v
+		j++
+	}
+	return s[:j]
+}
+
+// sliceUniqueUint64 will strip out any duplicate entries from a slice s (of type uint64)
+func sliceUniqueUint64(s []uint64) []uint64 {
+	seen := make(map[uint64]struct{}, len(s))
+	j := 0
+	for _, v := range s {
+		if _, ok := seen[v]; ok {
+			continue
+		}
+		seen[v] = struct{}{}
+		s[j] = v
+		j++
+	}
+	return s[:j]
+}
diff --git a/extensions/transientstore/store_test.go b/extensions/transientstore/store_test.go
new file mode 100644
index 00000000..252a464c
--- /dev/null
+++ b/extensions/transientstore/store_test.go
@@ -0,0 +1,648 @@
+/*
+Copyright IBM Corp. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"fmt"
+	"os"
+	"sort"
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/common/cauthdsl"
+	"github.com/hyperledger/fabric/core/ledger"
+	"github.com/hyperledger/fabric/core/ledger/util"
+	origts "github.com/hyperledger/fabric/core/transientstore"
+	cm "github.com/hyperledger/fabric/extensions/transientstore/common"
+	"github.com/hyperledger/fabric/protos/common"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+	"github.com/hyperledger/fabric/protos/transientstore"
+	"github.com/spf13/viper"
+	"github.com/stretchr/testify/assert"
+)
+
+// test file fully copied from: github.com/hyperledger/fabric/core/transientstore/store_helper.go
+// With the following changes:
+// 1 - add the following import package to reference Fabric's original structs/interfaces:
+// 		origts "github.com/hyperledger/fabric/core/transientstore"
+// 2 - add the following import package to reference copied structs from Fabric:
+// 		cm "github.com/hyperledger/fabric/extensions/transientstore/common"
+//
+// 3 - Since the store implementation replaced leveldb with in-memory cache, this test also replaced the following:
+// 		leveldb setup (env := NewTestStoreEnv(t)) with a simple call to NewStoreProvider() and OpenStore()
+//
+
+func TestMain(m *testing.M) {
+	viper.Set("peer.fileSystemPath", "/tmp/fabric/core/transientdata")
+	os.Exit(m.Run())
+}
+
+func TestPurgeIndexKeyCodingEncoding(t *testing.T) {
+	assert := assert.New(t)
+	blkHts := []uint64{0, 10, 20000}
+	txids := []string{"txid", ""}
+	uuids := []string{"uuid", ""}
+	for _, blkHt := range blkHts {
+		for _, txid := range txids {
+			for _, uuid := range uuids {
+				testCase := fmt.Sprintf("blkHt=%d,txid=%s,uuid=%s", blkHt, txid, uuid)
+				t.Run(testCase, func(t *testing.T) {
+					t.Logf("Running test case [%s]", testCase)
+					purgeIndexKey := cm.CreateCompositeKeyForPurgeIndexByHeight(blkHt, txid, uuid)
+					txid1, uuid1, blkHt1 := cm.SplitCompositeKeyOfPurgeIndexByHeight(purgeIndexKey)
+					assert.Equal(txid, txid1)
+					assert.Equal(uuid, uuid1)
+					assert.Equal(blkHt, blkHt1)
+				})
+			}
+		}
+	}
+}
+
+func TestRWSetKeyCodingEncoding(t *testing.T) {
+	assert := assert.New(t)
+	blkHts := []uint64{0, 10, 20000}
+	txids := []string{"txid", ""}
+	uuids := []string{"uuid", ""}
+	for _, blkHt := range blkHts {
+		for _, txid := range txids {
+			for _, uuid := range uuids {
+				testCase := fmt.Sprintf("blkHt=%d,txid=%s,uuid=%s", blkHt, txid, uuid)
+				t.Run(testCase, func(t *testing.T) {
+					t.Logf("Running test case [%s]", testCase)
+					rwsetKey := cm.CreateCompositeKeyForPvtRWSet(txid, uuid, blkHt)
+					uuid1, blkHt1 := cm.SplitCompositeKeyOfPvtRWSet(rwsetKey)
+					assert.Equal(uuid, uuid1)
+					assert.Equal(blkHt, blkHt1)
+				})
+			}
+		}
+	}
+}
+
+func TestTransientStorePersistAndRetrieve(t *testing.T) {
+	assert := assert.New(t)
+	var err error
+	testStoreProvider := NewStoreProvider()
+	testStore, err := testStoreProvider.OpenStore("TestStore")
+	assert.NoError(err)
+
+	txid := "txid-1"
+	samplePvtRWSetWithConfig := samplePvtDataWithConfigInfo(t)
+
+	// Create private simulation results for txid-1
+	var endorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+
+	// Results produced by endorser 1
+	endorser0SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          10,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser0SimulationResults)
+
+	// Results produced by endorser 2
+	endorser1SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          10,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser1SimulationResults)
+
+	// Persist simulation results into  store
+	for i := 0; i < len(endorsersResults); i++ {
+		err = testStore.PersistWithConfig(txid, endorsersResults[i].ReceivedAtBlockHeight,
+			endorsersResults[i].PvtSimulationResultsWithConfig)
+		assert.NoError(err)
+	}
+
+	// Retrieve simulation results of txid-1 from  store
+	iter, err := testStore.GetTxPvtRWSetByTxid(txid, nil)
+	assert.NoError(err)
+
+	var actualEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+	for {
+		result, err := iter.NextWithConfig()
+		assert.NoError(err)
+		if result == nil {
+			break
+		}
+		actualEndorsersResults = append(actualEndorsersResults, result)
+	}
+	iter.Close()
+	sortResults(endorsersResults)
+	sortResults(actualEndorsersResults)
+	assert.Equal(endorsersResults, actualEndorsersResults)
+}
+
+func TestTransientStorePersistAndRetrieveBothOldAndNewProto(t *testing.T) {
+	assert := assert.New(t)
+	var err error
+
+	testStoreProvider := NewStoreProvider()
+	testStore, err := testStoreProvider.OpenStore("TestStore")
+	assert.NoError(err)
+
+	txid := "txid-1"
+	var receivedAtBlockHeight uint64 = 10
+
+	// Create and persist private simulation results with old proto for txid-1
+	samplePvtRWSet := samplePvtData(t)
+	err = testStore.Persist(txid, receivedAtBlockHeight, samplePvtRWSet)
+	assert.NoError(err)
+
+	// Create and persist private simulation results with new proto for txid-1
+	samplePvtRWSetWithConfig := samplePvtDataWithConfigInfo(t)
+	err = testStore.PersistWithConfig(txid, receivedAtBlockHeight, samplePvtRWSetWithConfig)
+	assert.NoError(err)
+
+	// Construct the expected results
+	var expectedEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+
+	pvtRWSetWithConfigInfo := &transientstore.TxPvtReadWriteSetWithConfigInfo{
+		PvtRwset: samplePvtRWSet,
+	}
+
+	endorser0SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          receivedAtBlockHeight,
+		PvtSimulationResultsWithConfig: pvtRWSetWithConfigInfo,
+	}
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser0SimulationResults)
+
+	endorser1SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          receivedAtBlockHeight,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser1SimulationResults)
+
+	// Retrieve simulation results of txid-1 from  store
+	iter, err := testStore.GetTxPvtRWSetByTxid(txid, nil)
+	assert.NoError(err)
+
+	var actualEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+	for {
+		result, err := iter.NextWithConfig()
+		assert.NoError(err)
+		if result == nil {
+			break
+		}
+		actualEndorsersResults = append(actualEndorsersResults, result)
+	}
+	iter.Close()
+	sortResults(expectedEndorsersResults)
+	sortResults(actualEndorsersResults)
+	assert.Equal(expectedEndorsersResults, actualEndorsersResults)
+}
+
+func TestTransientStorePurgeByTxids(t *testing.T) {
+	assert := assert.New(t)
+	var err error
+
+	testStoreProvider := NewStoreProvider()
+	testStore, err := testStoreProvider.OpenStore("TestStore")
+	assert.NoError(err)
+
+	var txids []string
+	var endorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+
+	samplePvtRWSetWithConfig := samplePvtDataWithConfigInfo(t)
+
+	// Create two private write set entry for txid-1
+	txids = append(txids, "txid-1")
+	endorser0SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          10,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser0SimulationResults)
+
+	txids = append(txids, "txid-1")
+	endorser1SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          11,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser1SimulationResults)
+
+	// Create one private write set entry for txid-2
+	txids = append(txids, "txid-2")
+	endorser2SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          11,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser2SimulationResults)
+
+	// Create three private write set entry for txid-3
+	txids = append(txids, "txid-3")
+	endorser3SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser3SimulationResults)
+
+	txids = append(txids, "txid-3")
+	endorser4SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser4SimulationResults)
+
+	txids = append(txids, "txid-3")
+	endorser5SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          13,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser5SimulationResults)
+
+	for i := 0; i < len(txids); i++ {
+		err = testStore.PersistWithConfig(txids[i], endorsersResults[i].ReceivedAtBlockHeight,
+			endorsersResults[i].PvtSimulationResultsWithConfig)
+		assert.NoError(err)
+	}
+
+	// Retrieve simulation results of txid-2 from  store
+	iter, err := testStore.GetTxPvtRWSetByTxid("txid-2", nil)
+	assert.NoError(err)
+
+	// Expected results for txid-2
+	var expectedEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser2SimulationResults)
+
+	// Check whether actual results and expected results are same
+	var actualEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+	for true {
+		result, err := iter.NextWithConfig()
+		assert.NoError(err)
+		if result == nil {
+			break
+		}
+		actualEndorsersResults = append(actualEndorsersResults, result)
+	}
+	iter.Close()
+
+	// Note that the ordering of actualRes and expectedRes is dependent on the uuid. Hence, we are sorting
+	// expectedRes and actualRes.
+	sortResults(expectedEndorsersResults)
+	sortResults(actualEndorsersResults)
+
+	assert.Equal(len(expectedEndorsersResults), len(actualEndorsersResults))
+	for i, expected := range expectedEndorsersResults {
+		assert.Equal(expected.ReceivedAtBlockHeight, actualEndorsersResults[i].ReceivedAtBlockHeight)
+		assert.True(proto.Equal(expected.PvtSimulationResultsWithConfig, actualEndorsersResults[i].PvtSimulationResultsWithConfig))
+	}
+
+	// Remove all private write set of txid-2 and txid-3
+	toRemoveTxids := []string{"txid-2", "txid-3"}
+	err = testStore.PurgeByTxids(toRemoveTxids)
+	assert.NoError(err)
+
+	for _, txid := range toRemoveTxids {
+
+		// Check whether private write sets of txid-2 are removed
+		var expectedEndorsersResults *origts.EndorserPvtSimulationResultsWithConfig
+		expectedEndorsersResults = nil
+		iter, err = testStore.GetTxPvtRWSetByTxid(txid, nil)
+		assert.NoError(err)
+		// Should return nil, nil
+		result, err := iter.NextWithConfig()
+		assert.NoError(err)
+		assert.Equal(expectedEndorsersResults, result)
+	}
+
+	// Retrieve simulation results of txid-1 from store
+	iter, err = testStore.GetTxPvtRWSetByTxid("txid-1", nil)
+	assert.NoError(err)
+
+	// Expected results for txid-1
+	expectedEndorsersResults = nil
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser0SimulationResults)
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser1SimulationResults)
+
+	// Check whether actual results and expected results are same
+	actualEndorsersResults = nil
+	for true {
+		result, err := iter.NextWithConfig()
+		assert.NoError(err)
+		if result == nil {
+			break
+		}
+		actualEndorsersResults = append(actualEndorsersResults, result)
+	}
+	iter.Close()
+
+	// Note that the ordering of actualRes and expectedRes is dependent on the uuid. Hence, we are sorting
+	// expectedRes and actualRes.
+	sortResults(expectedEndorsersResults)
+	sortResults(actualEndorsersResults)
+
+	assert.Equal(len(expectedEndorsersResults), len(actualEndorsersResults))
+	for i, expected := range expectedEndorsersResults {
+		assert.Equal(expected.ReceivedAtBlockHeight, actualEndorsersResults[i].ReceivedAtBlockHeight)
+		assert.True(proto.Equal(expected.PvtSimulationResultsWithConfig, actualEndorsersResults[i].PvtSimulationResultsWithConfig))
+	}
+
+	toRemoveTxids = []string{"txid-1"}
+	err = testStore.PurgeByTxids(toRemoveTxids)
+	assert.NoError(err)
+
+	for _, txid := range toRemoveTxids {
+
+		// Check whether private write sets of txid-1 are removed
+		var expectedEndorsersResults *origts.EndorserPvtSimulationResultsWithConfig
+		expectedEndorsersResults = nil
+		iter, err = testStore.GetTxPvtRWSetByTxid(txid, nil)
+		assert.NoError(err)
+		// Should return nil, nil
+		result, err := iter.NextWithConfig()
+		assert.NoError(err)
+		assert.Equal(expectedEndorsersResults, result)
+	}
+
+	// There should be no entries in the  store
+	_, err = testStore.GetMinTransientBlkHt()
+	assert.Equal(err, ErrStoreEmpty)
+}
+
+func TestTransientStorePurgeByHeight(t *testing.T) {
+	var err error
+	assert := assert.New(t)
+
+	testStoreProvider := NewStoreProvider()
+	testStore, err := testStoreProvider.OpenStore("TestStore")
+	assert.NoError(err)
+
+	txid := "txid-1"
+	samplePvtRWSetWithConfig := samplePvtDataWithConfigInfo(t)
+
+	// Create private simulation results for txid-1
+	var endorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+
+	// Results produced by endorser 1
+	endorser0SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          10,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser0SimulationResults)
+
+	// Results produced by endorser 2
+	endorser1SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          11,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser1SimulationResults)
+
+	// Results produced by endorser 3
+	endorser2SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser2SimulationResults)
+
+	// Results produced by endorser 3
+	endorser3SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser3SimulationResults)
+
+	// Results produced by endorser 3
+	endorser4SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          13,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	endorsersResults = append(endorsersResults, endorser4SimulationResults)
+
+	// Persist simulation results into  store
+	for i := 0; i < 5; i++ {
+		err = testStore.PersistWithConfig(txid, endorsersResults[i].ReceivedAtBlockHeight,
+			endorsersResults[i].PvtSimulationResultsWithConfig)
+		assert.NoError(err)
+	}
+
+	// Retain results generate at block height greater than or equal to 12
+	minTransientBlkHtToRetain := uint64(12)
+	err = testStore.PurgeByHeight(minTransientBlkHtToRetain)
+	assert.NoError(err)
+
+	// Retrieve simulation results of txid-1 from  store
+	iter, err := testStore.GetTxPvtRWSetByTxid(txid, nil)
+	assert.NoError(err)
+
+	// Expected results for txid-1
+	var expectedEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser2SimulationResults) //endorsed at height 12
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser3SimulationResults) //endorsed at height 12
+	expectedEndorsersResults = append(expectedEndorsersResults, endorser4SimulationResults) //endorsed at height 13
+
+	// Check whether actual results and expected results are same
+	var actualEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+	for true {
+		result, err := iter.NextWithConfig()
+		assert.NoError(err)
+		if result == nil {
+			break
+		}
+		actualEndorsersResults = append(actualEndorsersResults, result)
+	}
+	iter.Close()
+
+	// Note that the ordering of actualRes and expectedRes is dependent on the uuid. Hence, we are sorting
+	// expectedRes and actualRes.
+	sortResults(expectedEndorsersResults)
+	sortResults(actualEndorsersResults)
+
+	assert.Equal(len(expectedEndorsersResults), len(actualEndorsersResults))
+	for i, expected := range expectedEndorsersResults {
+		assert.Equal(expected.ReceivedAtBlockHeight, actualEndorsersResults[i].ReceivedAtBlockHeight)
+		assert.True(proto.Equal(expected.PvtSimulationResultsWithConfig, actualEndorsersResults[i].PvtSimulationResultsWithConfig))
+	}
+
+	// Get the minimum block height remaining in transient store
+	var actualMinTransientBlkHt uint64
+	actualMinTransientBlkHt, err = testStore.GetMinTransientBlkHt()
+	assert.NoError(err)
+	assert.Equal(minTransientBlkHtToRetain, actualMinTransientBlkHt)
+
+	// Retain results at block height greater than or equal to 15
+	minTransientBlkHtToRetain = uint64(15)
+	err = testStore.PurgeByHeight(minTransientBlkHtToRetain)
+	assert.NoError(err)
+
+	// There should be no entries in the  store
+	actualMinTransientBlkHt, err = testStore.GetMinTransientBlkHt()
+	assert.Equal(err, ErrStoreEmpty)
+
+	// Retain results at block height greater than or equal to 15
+	minTransientBlkHtToRetain = uint64(15)
+	err = testStore.PurgeByHeight(minTransientBlkHtToRetain)
+	// Should not return any error
+	assert.NoError(err)
+}
+
+func TestTransientStoreRetrievalWithFilter(t *testing.T) {
+	var err error
+
+	testStoreProvider := NewStoreProvider()
+	store, err := testStoreProvider.OpenStore("TestStore")
+	assert.NoError(t, err)
+
+	samplePvtSimResWithConfig := samplePvtDataWithConfigInfo(t)
+
+	testTxid := "testTxid"
+	numEntries := 5
+	for i := 0; i < numEntries; i++ {
+		store.PersistWithConfig(testTxid, uint64(i), samplePvtSimResWithConfig)
+	}
+
+	filter := ledger.NewPvtNsCollFilter()
+	filter.Add("ns-1", "coll-1")
+	filter.Add("ns-2", "coll-2")
+
+	itr, err := store.GetTxPvtRWSetByTxid(testTxid, filter)
+	assert.NoError(t, err)
+
+	var actualRes []*origts.EndorserPvtSimulationResultsWithConfig
+	for {
+		res, err := itr.NextWithConfig()
+		if res == nil || err != nil {
+			assert.NoError(t, err)
+			break
+		}
+		actualRes = append(actualRes, res)
+	}
+
+	// prepare the trimmed pvtrwset manually - retain only "ns-1/coll-1" and "ns-2/coll-2"
+	expectedSimulationRes := samplePvtSimResWithConfig
+	expectedSimulationRes.GetPvtRwset().NsPvtRwset[0].CollectionPvtRwset = expectedSimulationRes.GetPvtRwset().NsPvtRwset[0].CollectionPvtRwset[0:1]
+	expectedSimulationRes.GetPvtRwset().NsPvtRwset[1].CollectionPvtRwset = expectedSimulationRes.GetPvtRwset().NsPvtRwset[1].CollectionPvtRwset[1:]
+	expectedSimulationRes.CollectionConfigs, err = cm.TrimPvtCollectionConfigs(expectedSimulationRes.CollectionConfigs, filter)
+	assert.NoError(t, err)
+	for ns, colName := range map[string]string{"ns-1": "coll-1", "ns-2": "coll-2"} {
+		config := expectedSimulationRes.CollectionConfigs[ns]
+		assert.NotNil(t, config)
+		ns1Config := config.Config
+		assert.Equal(t, len(ns1Config), 1)
+		ns1ColConfig := ns1Config[0].GetStaticCollectionConfig()
+		assert.NotNil(t, ns1ColConfig.Name, colName)
+	}
+
+	var expectedRes []*origts.EndorserPvtSimulationResultsWithConfig
+	for i := 0; i < numEntries; i++ {
+		expectedRes = append(expectedRes, &origts.EndorserPvtSimulationResultsWithConfig{uint64(i), expectedSimulationRes})
+	}
+
+	// Note that the ordering of actualRes and expectedRes is dependent on the uuid. Hence, we are sorting
+	// expectedRes and actualRes.
+	sortResults(expectedRes)
+	sortResults(actualRes)
+	assert.Equal(t, len(expectedRes), len(actualRes))
+	for i, expected := range expectedRes {
+		assert.Equal(t, expected.ReceivedAtBlockHeight, actualRes[i].ReceivedAtBlockHeight)
+		assert.True(t, proto.Equal(expected.PvtSimulationResultsWithConfig, actualRes[i].PvtSimulationResultsWithConfig))
+	}
+
+}
+
+func sortResults(res []*origts.EndorserPvtSimulationResultsWithConfig) {
+	// Results are sorted by ascending order of received at block height. When the block
+	// heights are same, we sort by comparing the hash of private write set.
+	var sortCondition = func(i, j int) bool {
+		if res[i].ReceivedAtBlockHeight == res[j].ReceivedAtBlockHeight {
+			res_i, _ := proto.Marshal(res[i].PvtSimulationResultsWithConfig)
+			res_j, _ := proto.Marshal(res[j].PvtSimulationResultsWithConfig)
+			// if hashes are same, any order would work.
+			return string(util.ComputeHash(res_i)) < string(util.ComputeHash(res_j))
+		}
+		return res[i].ReceivedAtBlockHeight < res[j].ReceivedAtBlockHeight
+	}
+	sort.SliceStable(res, sortCondition)
+}
+
+func samplePvtData(t *testing.T) *rwset.TxPvtReadWriteSet {
+	pvtWriteSet := &rwset.TxPvtReadWriteSet{DataModel: rwset.TxReadWriteSet_KV}
+	pvtWriteSet.NsPvtRwset = []*rwset.NsPvtReadWriteSet{
+		{
+			Namespace: "ns-1",
+			CollectionPvtRwset: []*rwset.CollectionPvtReadWriteSet{
+				{
+					CollectionName: "coll-1",
+					Rwset:          []byte("RandomBytes-PvtRWSet-ns1-coll1"),
+				},
+				{
+					CollectionName: "coll-2",
+					Rwset:          []byte("RandomBytes-PvtRWSet-ns1-coll2"),
+				},
+			},
+		},
+
+		{
+			Namespace: "ns-2",
+			CollectionPvtRwset: []*rwset.CollectionPvtReadWriteSet{
+				{
+					CollectionName: "coll-1",
+					Rwset:          []byte("RandomBytes-PvtRWSet-ns2-coll1"),
+				},
+				{
+					CollectionName: "coll-2",
+					Rwset:          []byte("RandomBytes-PvtRWSet-ns2-coll2"),
+				},
+			},
+		},
+	}
+	return pvtWriteSet
+}
+
+func samplePvtDataWithConfigInfo(t *testing.T) *transientstore.TxPvtReadWriteSetWithConfigInfo {
+	pvtWriteSet := samplePvtData(t)
+	pvtRWSetWithConfigInfo := &transientstore.TxPvtReadWriteSetWithConfigInfo{
+		PvtRwset: pvtWriteSet,
+		CollectionConfigs: map[string]*common.CollectionConfigPackage{
+			"ns-1": {
+				Config: []*common.CollectionConfig{
+					sampleCollectionConfigPackage("coll-1"),
+					sampleCollectionConfigPackage("coll-2"),
+				},
+			},
+			"ns-2": {
+				Config: []*common.CollectionConfig{
+					sampleCollectionConfigPackage("coll-1"),
+					sampleCollectionConfigPackage("coll-2"),
+				},
+			},
+		},
+	}
+	return pvtRWSetWithConfigInfo
+}
+
+func createCollectionConfig(collectionName string, signaturePolicyEnvelope *common.SignaturePolicyEnvelope,
+	requiredPeerCount int32, maximumPeerCount int32,
+) *common.CollectionConfig {
+	signaturePolicy := &common.CollectionPolicyConfig_SignaturePolicy{
+		SignaturePolicy: signaturePolicyEnvelope,
+	}
+	accessPolicy := &common.CollectionPolicyConfig{
+		Payload: signaturePolicy,
+	}
+
+	return &common.CollectionConfig{
+		Payload: &common.CollectionConfig_StaticCollectionConfig{
+			StaticCollectionConfig: &common.StaticCollectionConfig{
+				Name:              collectionName,
+				MemberOrgsPolicy:  accessPolicy,
+				RequiredPeerCount: requiredPeerCount,
+				MaximumPeerCount:  maximumPeerCount,
+			},
+		},
+	}
+}
+
+func sampleCollectionConfigPackage(colName string) *common.CollectionConfig {
+	var signers = [][]byte{[]byte("signer0"), []byte("signer1")}
+	policyEnvelope := cauthdsl.Envelope(cauthdsl.Or(cauthdsl.SignedBy(0), cauthdsl.SignedBy(1)), signers)
+
+	var requiredPeerCount, maximumPeerCount int32
+	requiredPeerCount = 1
+	maximumPeerCount = 2
+
+	return createCollectionConfig(colName, policyEnvelope, requiredPeerCount, maximumPeerCount)
+}
diff --git a/extensions/transientstore/storeprovider.go b/extensions/transientstore/storeprovider.go
new file mode 100644
index 00000000..098dd6fa
--- /dev/null
+++ b/extensions/transientstore/storeprovider.go
@@ -0,0 +1,33 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"github.com/hyperledger/fabric/common/flogging"
+	"github.com/hyperledger/fabric/core/transientstore"
+)
+
+var logger = flogging.MustGetLogger("transientstore")
+
+// Provider represents a trasientstore provider
+type Provider struct {
+}
+
+// NewStoreProvider instantiates a transient data storage provider backed by Memory
+func NewStoreProvider() *Provider {
+	logger.Debugf("constructing transient mem data storage provider")
+	return &Provider{}
+}
+
+// OpenStore creates a handle to the transient data store for the given ledger ID
+func (p *Provider) OpenStore(ledgerid string) (transientstore.Store, error) {
+	return newStore(), nil
+}
+
+// Close cleans up the provider
+func (p *Provider) Close() {
+}
diff --git a/extensions/transientstore/transientstore_test.go b/extensions/transientstore/transientstore_test.go
new file mode 100644
index 00000000..0b8eebdc
--- /dev/null
+++ b/extensions/transientstore/transientstore_test.go
@@ -0,0 +1,410 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package transientstore
+
+import (
+	"fmt"
+	"testing"
+
+	"github.com/golang/protobuf/proto"
+	"github.com/hyperledger/fabric/core/ledger"
+	origts "github.com/hyperledger/fabric/core/transientstore"
+	"github.com/hyperledger/fabric/extensions/mocks"
+	"github.com/stretchr/testify/require"
+)
+
+const (
+	txID1 = "txid1"
+	txID2 = "txid2"
+	txID3 = "txid3"
+	txID4 = "txid4"
+
+	ns1 = "ns1"
+
+	coll0 = "coll0"
+	coll1 = "coll1"
+	coll2 = "coll2"
+	coll3 = "coll3"
+
+	key1 = "key1"
+	key2 = "key2"
+	key3 = "key3"
+)
+
+func TestTransientStore(t *testing.T) {
+	s := NewStoreProvider()
+	require.NotNil(t, s, "Creating new store must not return nil")
+
+	transientStore, err := s.OpenStore("ledgerid")
+	require.NoError(t, err, "Opening transient store should not throw an error")
+	require.NotNil(t, transientStore, "TransientStore should not be nil")
+	ts := transientStore.(*store)
+	require.NotNil(t, ts.cache, "Transient store's cache should not be nil")
+	require.NotNil(t, ts.blockHeightCache, "Transient store's cache should not be nil")
+	require.NotNil(t, ts.txidCache, "Transient store's cache should not be nil")
+
+	b := mocks.NewPvtReadWriteSetBuilder()
+	b.Namespace(ns1).
+		Collection(coll1).
+		TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, []byte("value")).
+		WithMarshalError()
+
+	t.Run("Test Persist nil TxPvtReadWriteSet", func(t *testing.T) {
+		err = ts.Persist("12345", 1, nil)
+		require.Error(t, err, "Persist() call of transient store should throw an error with nil privateSimulationResults")
+	})
+
+	err = ts.Persist(txID1, 500, b.BuildReadWriteSet())
+	require.NoError(t, err, "Persist() call of transient store should not throw an error")
+	require.True(t, ts.cache.Has(txID1), "Transient store cache should have txId: %s following Persist(\"%s\", %d, &rwset.TxPvtReadWriteSet{}) cache content: %+v", txID1, txID1, 500, ts.cache.GetALL())
+	require.True(t, ts.blockHeightCache.Len() == 1, "Transient store blockHeightCache should have 1 item following Persist() call")
+	require.True(t, ts.blockHeightCache.Has(uint64(500)), "Transient store blockHeightCache should have blockHeight: %d following Persist(\"%s\", %d, &rwset.TxPvtReadWriteSet{}) cache content: %+v", 500, txID1, 500, ts.blockHeightCache.GetALL())
+
+	t.Run("Test GetTxPvtRWSetByTxid with TxPvtReadWriteSet", func(t *testing.T) {
+		var nilFilter ledger.PvtNsCollFilter
+		scanner, err := ts.GetTxPvtRWSetByTxid(txID1, nilFilter)
+		require.NoError(t, err, "GetTxPvtRWSetByTxid() call should not return error")
+		require.NotNil(t, scanner, "GetTxPvtRWSetByTxid() call should not return a nil scanner")
+		res, err := scanner.Next()
+		require.NoError(t, err, "scanner.Next() call should not return error")
+		require.NotNil(t, res, "scanner.Next() call should not return a nil EndorserPvtSimulationResults")
+		resWithConfig, err := scanner.NextWithConfig()
+		require.NoError(t, err, "scanner.NextWithConfig() call should return error for txid \"%s\"", txID1)
+		require.Nil(t, resWithConfig, "scanner.NextWithConfig() call should return a nil EndorserPvtSimulationResults for txid \"%s\" as it does not contain config in the results", txID1)
+		scanner.Close()
+	})
+
+	value1One := []byte("value1One")
+	value1Two := []byte("value1Two")
+
+	value2One := []byte("value2One")
+	value3One := []byte("value3One")
+
+	b = mocks.NewPvtReadWriteSetBuilder()
+	ns1Builder := b.Namespace(ns1)
+	coll0Builder := ns1Builder.Collection(coll0)
+	coll0Builder.
+		TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, value1One).
+		Write(key2, value1Two).
+		Write(key3, value3One)
+	coll1Builder := ns1Builder.Collection(coll1)
+	coll1Builder.
+		TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, value1One).
+		Write(key2, value1Two)
+	coll2Builder := ns1Builder.Collection(coll2)
+	coll2Builder.
+		TransientConfig("OR('Org1MSP.member')", 1, 2, "1m").
+		Write(key1, value2One).
+		Delete(key2)
+	coll3Builder := ns1Builder.Collection(coll3)
+	coll3Builder.
+		StaticConfig("OR('Org1MSP.member')", 1, 2, 100).
+		Write(key1, value3One)
+
+	err = ts.PersistWithConfig(txID2, 501, nil)
+	require.Error(t, err, "PersistWithConfig() call of transient store with nil config should throw an error")
+
+	txPvtReadWriteSetWithConfigInfo := b.Build()
+	err = ts.PersistWithConfig(txID2, 501, txPvtReadWriteSetWithConfigInfo)
+	require.NoError(t, err, "PersistWithConfig() call of transient store should not throw an error")
+	require.True(t, ts.cache.Has(txID2), "Transient store cache should have txId: %s following PersistWithConfig(\"%s\", %d, &pb.TxPvtReadWriteSetWithConfigInfo{}) cache content: %+v", txID2, txID2, 501, ts.cache.GetALL())
+	require.True(t, ts.blockHeightCache.Has(uint64(501)), "Transient store blockHeightCache should have blockHeight: %d following PersistWithConfig(\"%s\", %d, &pb.TxPvtReadWriteSetWithConfigInfo{}) cache content: %+v", 501, txID2, 501, ts.blockHeightCache.GetALL())
+
+	err = ts.PersistWithConfig(txID3, 502, txPvtReadWriteSetWithConfigInfo)
+	require.NoError(t, err, "PersistWithConfig() call of transient store should not throw an error")
+	require.True(t, ts.cache.Has(txID3), "Transient store cache should have txId: %s following PersistWithConfig(\"%s\", %d, &pb.TxPvtReadWriteSetWithConfigInfo{}) cache content: %+v", txID3, txID3, 502, ts.cache.GetALL())
+	require.True(t, ts.blockHeightCache.Has(uint64(502)), "Transient store blockHeightCache should have blockHeight: %d following PersistWithConfig(\"%s\", %d, &pb.TxPvtReadWriteSetWithConfigInfo{}) cache content: %+v", 502, txID3, 502, ts.blockHeightCache.GetALL())
+
+	err = ts.PersistWithConfig(txID4, 503, txPvtReadWriteSetWithConfigInfo)
+	require.NoError(t, err, "PersistWithConfig() call of transient store should not throw an error")
+	require.True(t, ts.cache.Has(txID4), "Transient store cache should have txId: %s following PersistWithConfig(\"%s\", %d, &pb.TxPvtReadWriteSetWithConfigInfo{}) cache content: %+v", txID4, txID4, 503, ts.cache.GetALL())
+	require.True(t, ts.blockHeightCache.Has(uint64(503)), "Transient store blockHeightCache should have blockHeight: %d following PersistWithConfig(\"%s\", %d, &pb.TxPvtReadWriteSetWithConfigInfo{}) cache content: %+v", 503, txID4, 503, ts.blockHeightCache.GetALL())
+
+	t.Run("Test GetTxPvtRWSetByTxid with TxPvtReadWriteSetWithConfigInfo", func(t *testing.T) {
+		filter := make(ledger.PvtNsCollFilter)
+		filter.Add(ns1, coll0)
+		scanner, err := ts.GetTxPvtRWSetByTxid(txID2, filter)
+		require.NoError(t, err, "GetTxPvtRWSetByTxid() call should not return error")
+		require.NotNil(t, scanner, "GetTxPvtRWSetByTxid() call should not return a nil scanner")
+		resWithConfig, err := scanner.NextWithConfig()
+		require.NoError(t, err, "scanner.NextWithConfig() call should return error for txid \"%s\"", txID2)
+		require.NotNil(t, resWithConfig, "scanner.NextWithConfig() call should return a nil EndorserPvtSimulationResults for txid \"%s\" as it does contain config in the results", txID2)
+	})
+
+	t.Run("Test GetMinTransientBlkHt, PurgeByTxids and PurgeByHeight", func(t *testing.T) {
+		minBlkHgt, err := ts.GetMinTransientBlkHt()
+		require.NoError(t, err, "GetMinTransientBlkHt() call of transient store should not throw an error")
+		require.Equal(t, uint64(500), minBlkHgt, "GetMinTransientBlkHt() call did not return expected minBlockHeight of 500")
+
+		txToPurge := []string{txID3, txID4}
+		err = ts.PurgeByTxids(txToPurge)
+		require.NoError(t, err, "PurgeByTxids() call of transient store should not throw an error when purging transactions %d and %d", txToPurge)
+		pvtRWSetData := ts.getTxPvtRWSetFromCache(txID3)
+		blkHgt := ts.getTxidsFromBlockHeightCache(502)
+		require.Empty(t, pvtRWSetData.m, "After purging 2 transactions, fetching pvt data should return empty set")
+		require.Empty(t, blkHgt.m, "After purging 2 transactions, fetching block should return empty block height")
+
+		err = ts.PurgeByHeight(minBlkHgt + 1) // purge minBlkHgt by setting 1 above the current minimum height
+		require.NoError(t, err, "PurgeByHeight() call of tansient store should not throw an error when purging transactions by height %d", minBlkHgt)
+		minBlkHgt, err = ts.GetMinTransientBlkHt()
+		// minBlkHgt should now be 501 as 500 was purged
+		require.NoError(t, err, "GetMinTransientBlkHt() call of transient store should not throw an error")
+		require.Equal(t, uint64(501), minBlkHgt, "GetMinTransientBlkHt() call did not return expected minBlockHeight of 501 (block 500 was purged already)")
+	})
+	s.Close()     // noop
+	ts.Shutdown() // noop
+}
+
+func TestPersistTransientStoreParallel(t *testing.T) {
+	s := NewStoreProvider()
+	require.NotNil(t, s, "Creating new store must not return nil")
+
+	transientStore, err := s.OpenStore("ledgerid")
+	require.NoError(t, err, "Opening transient store should not throw an error")
+	require.NotNil(t, transientStore, "TransientStore should not be nil")
+	tStore := transientStore.(*store)
+	require.NotNil(t, tStore.cache, "Transient store's cache should not be nil")
+	require.NotNil(t, tStore.blockHeightCache, "Transient store's cache should not be nil")
+	require.NotNil(t, tStore.txidCache, "Transient store's cache should not be nil")
+
+	samplePvtRWSetWithConfig := samplePvtDataWithConfigInfo(t)
+	// Create two private write set entry for txid-1
+	endorser0SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          10,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	endorser1SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          11,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	// Create one private write set entry for txid-2
+	endorser2SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          11,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	// Create three private write set entry for txid-3
+	endorser3SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	endorser4SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	endorser5SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          13,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	tcCreate := []struct {
+		pvtrwSet    *origts.EndorserPvtSimulationResultsWithConfig
+		txid        string
+		blockHeight uint64
+	}{
+		// Create two private write set entry for txid-1
+		{
+			endorser0SimulationResults,
+			"txid-1",
+			10,
+		},
+		{
+			endorser1SimulationResults,
+			"txid-1",
+			11,
+		},
+		// Create one private write set entry for txid-2
+		{
+			endorser2SimulationResults,
+			"txid-2",
+			11,
+		},
+		// Create three private write set entry for txid-3
+		{
+			endorser3SimulationResults,
+			"txid-3",
+			12,
+		},
+		{
+			endorser4SimulationResults,
+			"txid-3",
+			12,
+		},
+		{
+			endorser5SimulationResults,
+			"txid-3",
+			13,
+		},
+	}
+	for _, tt := range tcCreate {
+		tt := tt
+		t.Run(fmt.Sprintf("Testing parallel Create pvt transient data for txid: %s and block height: %d", tt.txid, tt.blockHeight), func(st *testing.T) {
+			st.Parallel()
+			err := tStore.PersistWithConfig(tt.txid, tt.pvtrwSet.ReceivedAtBlockHeight,
+				tt.pvtrwSet.PvtSimulationResultsWithConfig)
+			require.NoError(st, err, "PersistWithConfig should not fail")
+
+		})
+	}
+
+}
+func TestIterateTransientStoreParallel(t *testing.T) {
+	s := NewStoreProvider()
+	require.NotNil(t, s, "Creating new store must not return nil")
+
+	transientStore, err := s.OpenStore("ledgerid")
+	require.NoError(t, err, "Opening transient store should not throw an error")
+	require.NotNil(t, transientStore, "TransientStore should not be nil")
+	tStore := transientStore.(*store)
+	require.NotNil(t, tStore.cache, "Transient store's cache should not be nil")
+	require.NotNil(t, tStore.blockHeightCache, "Transient store's cache should not be nil")
+	require.NotNil(t, tStore.txidCache, "Transient store's cache should not be nil")
+
+	samplePvtRWSetWithConfig := samplePvtDataWithConfigInfo(t)
+	// Create two private write set entry for txid-1
+	endorser0SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          10,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	endorser1SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          11,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	// Create one private write set entry for txid-2
+	endorser2SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          11,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	// Create three private write set entry for txid-3
+	endorser3SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	endorser4SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          12,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+
+	endorser5SimulationResults := &origts.EndorserPvtSimulationResultsWithConfig{
+		ReceivedAtBlockHeight:          13,
+		PvtSimulationResultsWithConfig: samplePvtRWSetWithConfig,
+	}
+	// create sample data in transientstore (not in t.Run)
+	createSlice := []struct {
+		pvtrwSet    *origts.EndorserPvtSimulationResultsWithConfig
+		txid        string
+		blockHeight uint64
+	}{
+		// Create two private write set entry for txid-1
+		{
+			endorser0SimulationResults,
+			"txid-1",
+			10,
+		},
+		{
+			endorser1SimulationResults,
+			"txid-1",
+			11,
+		},
+		// Create one private write set entry for txid-2
+		{
+			endorser2SimulationResults,
+			"txid-2",
+			11,
+		},
+		// Create three private write set entry for txid-3
+		{
+			endorser3SimulationResults,
+			"txid-3",
+			12,
+		},
+		{
+			endorser4SimulationResults,
+			"txid-3",
+			12,
+		},
+		{
+			endorser5SimulationResults,
+			"txid-3",
+			13,
+		},
+	}
+	// notice there is no t.Run() call here as we want sample data to be stored prior to test the iterators later in the test
+	for _, tt := range createSlice {
+		tt := tt
+		err := tStore.PersistWithConfig(tt.txid, tt.pvtrwSet.ReceivedAtBlockHeight,
+			tt.pvtrwSet.PvtSimulationResultsWithConfig)
+		require.NoError(t, err, "PersistWithConfig should not fail")
+
+	}
+
+	// now start the scanner iterator test
+	tcGet := []struct {
+		txid string
+	}{
+
+		{"txid-1"},
+		{"txid-2"},
+		{"txid-3"},
+	}
+	for _, tt := range tcGet {
+		tt := tt
+		// prepare expectedEndorsersResults
+		var expectedEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+		switch tt.txid {
+		case "txid-1":
+			expectedEndorsersResults = append(expectedEndorsersResults, endorser0SimulationResults, endorser1SimulationResults)
+		case "txid-2":
+			expectedEndorsersResults = append(expectedEndorsersResults, endorser2SimulationResults)
+		case "txid-3":
+			expectedEndorsersResults = append(expectedEndorsersResults, endorser3SimulationResults, endorser4SimulationResults, endorser5SimulationResults)
+		default: // default txid-1
+			expectedEndorsersResults = append(expectedEndorsersResults, endorser0SimulationResults, endorser1SimulationResults)
+		}
+
+		t.Run(fmt.Sprintf("Testing parallel Get pvt transient data for txid: %s", tt.txid), func(st *testing.T) {
+			st.Parallel()
+			iter, err := tStore.GetTxPvtRWSetByTxid(tt.txid, nil)
+			require.NoError(st, err, "GetTxPvtRWSetByTxid should not fail")
+
+			// Check whether actual results and expected results are same
+			var actualEndorsersResults []*origts.EndorserPvtSimulationResultsWithConfig
+			for true {
+				result, err := iter.NextWithConfig()
+				require.NoError(st, err, "NextWithConfig should not fail")
+				if result == nil {
+					break
+				}
+				actualEndorsersResults = append(actualEndorsersResults, result)
+			}
+			iter.Close()
+
+			// Note that the ordering of actualRes and expectedRes is dependent on the uuid. Hence, we are sorting
+			// expectedRes and actualRes.
+			sortResults(expectedEndorsersResults)
+			sortResults(actualEndorsersResults)
+			require.Equal(st, len(expectedEndorsersResults), len(actualEndorsersResults), "%s must return %d results", tt.txid, len(expectedEndorsersResults))
+			for i, expected := range expectedEndorsersResults {
+				require.Equal(st, expected.ReceivedAtBlockHeight, actualEndorsersResults[i].ReceivedAtBlockHeight)
+				require.True(st, proto.Equal(expected.PvtSimulationResultsWithConfig, actualEndorsersResults[i].PvtSimulationResultsWithConfig))
+			}
+
+		})
+	}
+	//TODO add Purge parallel testing here..
+}
diff --git a/gossip/privdata/coordinator.go b/gossip/privdata/coordinator.go
index 89038246..42fc7549 100644
--- a/gossip/privdata/coordinator.go
+++ b/gossip/privdata/coordinator.go
@@ -21,6 +21,8 @@ import (
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
 	"github.com/hyperledger/fabric/core/transientstore"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	extensionscoord "github.com/hyperledger/fabric/extensions/gossip/coordinator"
 	"github.com/hyperledger/fabric/gossip/metrics"
 	privdatacommon "github.com/hyperledger/fabric/gossip/privdata/common"
 	"github.com/hyperledger/fabric/gossip/util"
@@ -113,15 +115,21 @@ type Support struct {
 	txvalidator.Validator
 	committer.Committer
 	TransientStore
+	CollDataStore storeapi.Store
 	Fetcher
 }
 
+type pvtDataStore interface {
+	StorePvtData(txID string, privData *transientstore2.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error
+}
+
 type coordinator struct {
 	selfSignedData common.SignedData
 	Support
 	transientBlockRetention uint64
 	metrics                 *metrics.PrivdataMetrics
 	pullRetryThreshold      time.Duration
+	pvtDataStore            pvtDataStore
 }
 
 type CoordinatorConfig struct {
@@ -129,17 +137,24 @@ type CoordinatorConfig struct {
 	PullRetryThreshold      time.Duration
 }
 
+// getPvtDataStore may be overridden by unit tests
+var getPvtDataStore = func(channelID string, transientStore TransientStore, collDataStore storeapi.Store) pvtDataStore {
+	return extensionscoord.New(channelID, transientStore, collDataStore)
+}
+
 // NewCoordinator creates a new instance of coordinator
 func NewCoordinator(support Support, selfSignedData common.SignedData, metrics *metrics.PrivdataMetrics,
 	config CoordinatorConfig) Coordinator {
 	return &coordinator{Support: support, selfSignedData: selfSignedData,
 		transientBlockRetention: config.TransientBlockRetention, metrics: metrics,
-		pullRetryThreshold: config.PullRetryThreshold}
+		pullRetryThreshold: config.PullRetryThreshold,
+		pvtDataStore:       getPvtDataStore(support.ChainID, support.TransientStore, support.CollDataStore),
+	}
 }
 
 // StorePvtData used to persist private date into transient store
 func (c *coordinator) StorePvtData(txID string, privData *transientstore2.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error {
-	return c.TransientStore.PersistWithConfig(txID, blkHeight, privData)
+	return c.pvtDataStore.StorePvtData(txID, privData, blkHeight)
 }
 
 // StoreBlock stores block with private data into the ledger
diff --git a/gossip/privdata/coordinator_test.go b/gossip/privdata/coordinator_test.go
index f7fb1411..4478164b 100644
--- a/gossip/privdata/coordinator_test.go
+++ b/gossip/privdata/coordinator_test.go
@@ -23,6 +23,8 @@ import (
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/rwsetutil"
 	"github.com/hyperledger/fabric/core/transientstore"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	kmocks "github.com/hyperledger/fabric/extensions/mocks"
 	"github.com/hyperledger/fabric/gossip/metrics"
 	gmetricsmocks "github.com/hyperledger/fabric/gossip/metrics/mocks"
 	privdatacommon "github.com/hyperledger/fabric/gossip/privdata/common"
@@ -34,6 +36,7 @@ import (
 	"github.com/hyperledger/fabric/protos/ledger/rwset/kvrwset"
 	"github.com/hyperledger/fabric/protos/msp"
 	"github.com/hyperledger/fabric/protos/peer"
+	tp "github.com/hyperledger/fabric/protos/transientstore"
 	transientstore2 "github.com/hyperledger/fabric/protos/transientstore"
 	"github.com/stretchr/testify/assert"
 	"github.com/stretchr/testify/mock"
@@ -416,6 +419,15 @@ func (cap *collectionAccessPolicy) AccessFilter() privdata.Filter {
 	}
 }
 
+type mockPvtDataStore struct {
+	transientStore TransientStore
+}
+
+// StorePvtData redirects the call to the transient store
+func (m *mockPvtDataStore) StorePvtData(txID string, privData *tp.TxPvtReadWriteSetWithConfigInfo, blkHeight uint64) error {
+	return m.transientStore.PersistWithConfig(txID, blkHeight, privData)
+}
+
 func TestPvtDataCollections_FailOnEmptyPayload(t *testing.T) {
 	collection := &util.PvtDataCollections{
 		&ledger.TxPvtData{
@@ -1350,18 +1362,24 @@ func TestPurgeByHeight(t *testing.T) {
 }
 
 func TestCoordinatorStorePvtData(t *testing.T) {
+	getPvtDataStore = func(channelID string, transientStore TransientStore, collDataStore storeapi.Store) pvtDataStore {
+		return &mockPvtDataStore{transientStore: transientStore}
+	}
+
 	metrics := metrics.NewGossipMetrics(&disabled.Provider{}).PrivdataMetrics
 	cs := createcollectionStore(common.SignedData{}).thatAcceptsAll()
 	committer := &mocks.Committer{}
 	store := &mockTransientStore{t: t}
 	store.On("PersistWithConfig", mock.Anything, uint64(5), mock.Anything).
 		expectRWSet("ns1", "c1", []byte("rws-pre-image")).Return(nil)
+	tdStore := kmocks.NewDataStore()
 	fetcher := &fetcherMock{t: t}
 	coordinator := NewCoordinator(Support{
 		CollectionStore: cs,
 		Committer:       committer,
 		Fetcher:         fetcher,
 		TransientStore:  store,
+		CollDataStore:   tdStore,
 		Validator:       &validatorMock{},
 	}, common.SignedData{}, metrics, testConfig)
 	pvtData := (&pvtDataFactory{}).addRWSet().addNSRWSet("ns1", "c1").create()
diff --git a/gossip/privdata/dissemination.go b/gossip/privdata/dissemination.go
new file mode 100644
index 00000000..86fdb089
--- /dev/null
+++ b/gossip/privdata/dissemination.go
@@ -0,0 +1,33 @@
+/*
+Copyright SecureKey Technologies Inc. All Rights Reserved.
+
+SPDX-License-Identifier: Apache-2.0
+*/
+
+package privdata
+
+import (
+	"github.com/hyperledger/fabric/core/common/privdata"
+	kdissemination "github.com/hyperledger/fabric/extensions/collections/dissemination"
+	"github.com/hyperledger/fabric/protos/common"
+	proto "github.com/hyperledger/fabric/protos/gossip"
+	"github.com/hyperledger/fabric/protos/ledger/rwset"
+)
+
+func (d *distributorImpl) disseminationPlanForExtensions(ns string, rwSet *rwset.CollectionPvtReadWriteSet, colCP *common.CollectionConfig, colAP privdata.CollectionAccessPolicy, pvtDataMsg *proto.SignedGossipMessage) ([]*dissemination, error) {
+	dissPlan, handled, err := kdissemination.ComputeDisseminationPlan(d.chainID, ns, rwSet, colCP, colAP, pvtDataMsg, d.gossipAdapter)
+	if err != nil {
+		return nil, err
+	}
+
+	if !handled {
+		// Use default dissemination plan
+		return d.disseminationPlanForMsg(colAP, colAP.AccessFilter(), pvtDataMsg)
+	}
+
+	dPlan := make([]*dissemination, len(dissPlan))
+	for i, dp := range dissPlan {
+		dPlan[i] = &dissemination{msg: dp.Msg, criteria: dp.Criteria}
+	}
+	return dPlan, nil
+}
diff --git a/gossip/privdata/distributor.go b/gossip/privdata/distributor.go
index 1e7cee78..c8c67039 100644
--- a/gossip/privdata/distributor.go
+++ b/gossip/privdata/distributor.go
@@ -46,6 +46,9 @@ type gossipAdapter interface {
 	// PeersOfChannel returns the NetworkMembers considered alive
 	// and also subscribed to the channel given
 	PeersOfChannel(gossipCommon.ChainID) []discovery.NetworkMember
+
+	// SelfMembershipInfo returns the peer's membership information
+	SelfMembershipInfo() discovery.NetworkMember
 }
 
 // PvtDataDistributor interface to defines API of distributing private data
diff --git a/gossip/privdata/distributor_test.go b/gossip/privdata/distributor_test.go
index 5629415e..1753b822 100644
--- a/gossip/privdata/distributor_test.go
+++ b/gossip/privdata/distributor_test.go
@@ -104,6 +104,10 @@ func (g *gossipMock) PeerFilter(channel gcommon.ChainID, messagePredicate api.Su
 	}, nil
 }
 
+func (g *gossipMock) SelfMembershipInfo() discovery.NetworkMember {
+	panic("not implemented")
+}
+
 func TestDistributor(t *testing.T) {
 	channelID := "test"
 
diff --git a/gossip/service/gossip_service.go b/gossip/service/gossip_service.go
index 8fb34292..bf03e570 100644
--- a/gossip/service/gossip_service.go
+++ b/gossip/service/gossip_service.go
@@ -13,8 +13,12 @@ import (
 	"github.com/hyperledger/fabric/core/committer"
 	"github.com/hyperledger/fabric/core/committer/txvalidator"
 	"github.com/hyperledger/fabric/core/common/privdata"
-	"github.com/hyperledger/fabric/core/deliverservice"
+	deliverclient "github.com/hyperledger/fabric/core/deliverservice"
 	"github.com/hyperledger/fabric/core/deliverservice/blocksprovider"
+	"github.com/hyperledger/fabric/core/ledger"
+	storeapi "github.com/hyperledger/fabric/extensions/collections/api/store"
+	kgossipapi "github.com/hyperledger/fabric/extensions/gossip/api"
+	"github.com/hyperledger/fabric/extensions/gossip/dispatcher"
 	"github.com/hyperledger/fabric/gossip/api"
 	gossipCommon "github.com/hyperledger/fabric/gossip/common"
 	"github.com/hyperledger/fabric/gossip/election"
@@ -214,6 +218,9 @@ type Support struct {
 	Store                privdata2.TransientStore
 	Cs                   privdata.CollectionStore
 	IdDeserializeFactory privdata2.IdentityDeserializerFactory
+	CollDataStore        storeapi.Store
+	Ledger               ledger.PeerLedger
+	BlockPublisher       kgossipapi.BlockPublisher
 }
 
 // DataStoreSupport aggregates interfaces capable
@@ -253,6 +260,7 @@ func (g *gossipServiceImpl) InitializeChannel(chainID string, endpoints []string
 		CollectionStore: support.Cs,
 		Validator:       support.Validator,
 		TransientStore:  support.Store,
+		CollDataStore:   support.CollDataStore,
 		Committer:       support.Committer,
 		Fetcher:         fetcher,
 	}, g.createSelfSignedData(), g.metrics.PrivdataMetrics, coordinatorConfig)
@@ -277,7 +285,8 @@ func (g *gossipServiceImpl) InitializeChannel(chainID string, endpoints []string
 	g.privateHandlers[chainID].reconciler.Start()
 
 	g.chains[chainID] = state.NewGossipStateProvider(chainID, servicesAdapter, coordinator,
-		g.metrics.StateMetrics, getStateConfiguration())
+		g.metrics.StateMetrics, getStateConfiguration(),
+		dispatcher.New(chainID, support.CollDataStore, servicesAdapter, support.Ledger, support.BlockPublisher))
 	if g.deliveryService[chainID] == nil {
 		var err error
 		g.deliveryService[chainID], err = g.deliveryFactory.Service(g, endpoints, g.mcs)
diff --git a/gossip/service/gossip_service_test.go b/gossip/service/gossip_service_test.go
index 62ce04ea..cb7b658f 100644
--- a/gossip/service/gossip_service_test.go
+++ b/gossip/service/gossip_service_test.go
@@ -18,10 +18,11 @@ import (
 	"github.com/hyperledger/fabric/common/localmsp"
 	"github.com/hyperledger/fabric/common/metrics/disabled"
 	"github.com/hyperledger/fabric/core/comm"
-	"github.com/hyperledger/fabric/core/deliverservice"
+	deliverclient "github.com/hyperledger/fabric/core/deliverservice"
 	"github.com/hyperledger/fabric/core/deliverservice/blocksprovider"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/transientstore"
+	kmocks "github.com/hyperledger/fabric/extensions/gossip/mocks"
 	"github.com/hyperledger/fabric/gossip/api"
 	gcomm "github.com/hyperledger/fabric/gossip/comm"
 	gossipCommon "github.com/hyperledger/fabric/gossip/common"
@@ -34,7 +35,7 @@ import (
 	"github.com/hyperledger/fabric/gossip/state"
 	"github.com/hyperledger/fabric/gossip/util"
 	"github.com/hyperledger/fabric/msp/mgmt"
-	"github.com/hyperledger/fabric/msp/mgmt/testtools"
+	msptesttools "github.com/hyperledger/fabric/msp/mgmt/testtools"
 	peergossip "github.com/hyperledger/fabric/peer/gossip"
 	"github.com/hyperledger/fabric/peer/gossip/mocks"
 	"github.com/hyperledger/fabric/protos/common"
@@ -148,8 +149,9 @@ func TestLeaderElectionWithDeliverClient(t *testing.T) {
 		deliverServiceFactory.service.running[channelName] = false
 
 		gossips[i].InitializeChannel(channelName, []string{"endpoint"}, Support{
-			Store:     &mockTransientStore{},
-			Committer: &mockLedgerInfo{1},
+			Store:          &mockTransientStore{},
+			Committer:      &mockLedgerInfo{1},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 		service, exist := gossips[i].(*gossipGRPC).gossipServiceImpl.leaderElection[channelName]
 		assert.True(t, exist, "Leader election service should be created for peer %d and channel %s", i, channelName)
@@ -206,8 +208,9 @@ func TestWithStaticDeliverClientLeader(t *testing.T) {
 		gossips[i].(*gossipGRPC).gossipServiceImpl.deliveryFactory = deliverServiceFactory
 		deliverServiceFactory.service.running[channelName] = false
 		gossips[i].InitializeChannel(channelName, []string{"endpoint"}, Support{
-			Committer: &mockLedgerInfo{1},
-			Store:     &mockTransientStore{},
+			Committer:      &mockLedgerInfo{1},
+			Store:          &mockTransientStore{},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 	}
 
@@ -220,8 +223,9 @@ func TestWithStaticDeliverClientLeader(t *testing.T) {
 	for i := 0; i < n; i++ {
 		deliverServiceFactory.service.running[channelName] = false
 		gossips[i].InitializeChannel(channelName, []string{"endpoint"}, Support{
-			Committer: &mockLedgerInfo{1},
-			Store:     &mockTransientStore{},
+			Committer:      &mockLedgerInfo{1},
+			Store:          &mockTransientStore{},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 	}
 
@@ -260,8 +264,9 @@ func TestWithStaticDeliverClientNotLeader(t *testing.T) {
 		gossips[i].(*gossipGRPC).gossipServiceImpl.deliveryFactory = deliverServiceFactory
 		deliverServiceFactory.service.running[channelName] = false
 		gossips[i].InitializeChannel(channelName, []string{"endpoint"}, Support{
-			Committer: &mockLedgerInfo{1},
-			Store:     &mockTransientStore{},
+			Committer:      &mockLedgerInfo{1},
+			Store:          &mockTransientStore{},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 	}
 
@@ -300,8 +305,9 @@ func TestWithStaticDeliverClientBothStaticAndLeaderElection(t *testing.T) {
 		gossips[i].(*gossipGRPC).gossipServiceImpl.deliveryFactory = deliverServiceFactory
 		assert.Panics(t, func() {
 			gossips[i].InitializeChannel(channelName, []string{"endpoint"}, Support{
-				Committer: &mockLedgerInfo{1},
-				Store:     &mockTransientStore{},
+				Committer:      &mockLedgerInfo{1},
+				Store:          &mockTransientStore{},
+				BlockPublisher: kmocks.NewBlockPublisher(),
 			})
 		}, "Dynamic leader election based and static connection to ordering service can't exist simultaneously")
 	}
diff --git a/gossip/service/integration_test.go b/gossip/service/integration_test.go
index 20cbf18d..68a49785 100644
--- a/gossip/service/integration_test.go
+++ b/gossip/service/integration_test.go
@@ -11,10 +11,11 @@ import (
 	"testing"
 	"time"
 
-	"github.com/hyperledger/fabric/core/deliverservice"
+	deliverclient "github.com/hyperledger/fabric/core/deliverservice"
 	"github.com/hyperledger/fabric/core/deliverservice/blocksprovider"
 	"github.com/hyperledger/fabric/core/ledger"
 	"github.com/hyperledger/fabric/core/transientstore"
+	kmocks "github.com/hyperledger/fabric/extensions/gossip/mocks"
 	"github.com/hyperledger/fabric/gossip/api"
 	"github.com/hyperledger/fabric/gossip/util"
 	"github.com/hyperledger/fabric/protos/ledger/rwset"
@@ -137,8 +138,9 @@ func TestLeaderYield(t *testing.T) {
 		gs.deliveryFactory = &embeddingDeliveryServiceFactory{&deliveryFactoryImpl{}}
 		gossipServiceInstance = gs
 		gs.InitializeChannel(channelName, []string{endpoint}, Support{
-			Committer: &mockLedgerInfo{1},
-			Store:     &transientStoreMock{},
+			Committer:      &mockLedgerInfo{1},
+			Store:          &transientStoreMock{},
+			BlockPublisher: kmocks.NewBlockPublisher(),
 		})
 		return gs
 	}
diff --git a/gossip/state/state.go b/gossip/state/state.go
index 0b99bdee..7f1c4dde 100644
--- a/gossip/state/state.go
+++ b/gossip/state/state.go
@@ -83,6 +83,12 @@ type GossipAdapter interface {
 	// PeersOfChannel returns the NetworkMembers considered alive
 	// and also subscribed to the channel given
 	PeersOfChannel(common2.ChainID) []discovery.NetworkMember
+
+	// SelfMembershipInfo returns the peer's membership information
+	SelfMembershipInfo() discovery.NetworkMember
+
+	// IdentityInfo returns information known peer identities
+	IdentityInfo() api.PeerIdentitySet
 }
 
 // MCSAdapter adapter of message crypto service interface to bound
@@ -129,6 +135,10 @@ type ServicesMediator struct {
 	MCSAdapter
 }
 
+type messageDispatcher interface {
+	Dispatch(msg proto.ReceivedMessage) bool
+}
+
 // GossipStateProviderImpl the implementation of the GossipStateProvider interface
 // the struct to handle in memory sliding window of
 // new ledger block to be acquired by hyper ledger
@@ -165,6 +175,8 @@ type GossipStateProviderImpl struct {
 	config *Configuration
 
 	stateMetrics *metrics.StateMetrics
+
+	msgDispatcher messageDispatcher
 }
 
 var logger = util.GetLogger(util.StateLogger, "")
@@ -188,7 +200,7 @@ func (v *stateRequestValidator) validate(request *proto.RemoteStateRequest, batc
 
 // NewGossipStateProvider creates state provider with coordinator instance
 // to orchestrate arrival of private rwsets and blocks before committing them into the ledger.
-func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger ledgerResources, stateMetrics *metrics.StateMetrics, config *Configuration) GossipStateProvider {
+func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger ledgerResources, stateMetrics *metrics.StateMetrics, config *Configuration, msgDispatcher messageDispatcher) GossipStateProvider {
 
 	gossipChan, _ := services.Accept(func(message interface{}) bool {
 		// Get only data messages
@@ -199,7 +211,8 @@ func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger l
 	remoteStateMsgFilter := func(message interface{}) bool {
 		receivedMsg := message.(proto.ReceivedMessage)
 		msg := receivedMsg.GetGossipMessage()
-		if !(msg.IsRemoteStateMessage() || msg.GetPrivateData() != nil) {
+		if !(msg.IsRemoteStateMessage() || msg.GetPrivateData() != nil ||
+			msg.GetCollDataReq() != nil || msg.GetCollDataRes() != nil) {
 			return false
 		}
 		// Ensure we deal only with messages that belong to this channel
@@ -269,6 +282,8 @@ func NewGossipStateProvider(chainID string, services *ServicesMediator, ledger l
 		config: config,
 
 		stateMetrics: stateMetrics,
+
+		msgDispatcher: msgDispatcher,
 	}
 
 	logger.Infof("Updating metadata information, "+
@@ -320,6 +335,10 @@ func (s *GossipStateProviderImpl) dispatch(msg proto.ReceivedMessage) {
 		logger.Debug("Handling private data collection message")
 		// Handling private data replication message
 		s.privateDataMessage(msg)
+	} else if s.msgDispatcher != nil {
+		if dispatched := s.msgDispatcher.Dispatch(msg); dispatched {
+			logger.Debug("Handled Extensions message")
+		}
 	}
 
 }
diff --git a/gossip/state/state_test.go b/gossip/state/state_test.go
index 2662d270..c9ef87f4 100644
--- a/gossip/state/state_test.go
+++ b/gossip/state/state_test.go
@@ -418,7 +418,7 @@ func newPeerNodeWithGossipWithValidatorWithMetrics(id int, committer committer.C
 		TransientStore: &mockTransientStore{},
 		Committer:      committer,
 	}, pcomm.SignedData{}, gossipMetrics.PrivdataMetrics, coordConfig)
-	sp := NewGossipStateProvider(util.GetTestChainID(), servicesAdapater, coord, gossipMetrics.StateMetrics, config)
+	sp := NewGossipStateProvider(util.GetTestChainID(), servicesAdapater, coord, gossipMetrics.StateMetrics, config, nil)
 	if sp == nil {
 		gRPCServer.Stop()
 		return nil, port
@@ -1465,7 +1465,7 @@ func TestTransferOfPrivateRWSet(t *testing.T) {
 
 	servicesAdapater := &ServicesMediator{GossipAdapter: g, MCSAdapter: &cryptoServiceMock{acceptor: noopPeerIdentityAcceptor}}
 	stateMetrics := metrics.NewGossipMetrics(&disabled.Provider{}).StateMetrics
-	st := NewGossipStateProvider(chainID, servicesAdapater, coord1, stateMetrics, config)
+	st := NewGossipStateProvider(chainID, servicesAdapater, coord1, stateMetrics, config, nil)
 	defer st.Stop()
 
 	// Mocked state request message
@@ -1699,11 +1699,11 @@ func TestTransferOfPvtDataBetweenPeers(t *testing.T) {
 	stateMetrics := metrics.NewGossipMetrics(&disabled.Provider{}).StateMetrics
 
 	mediator := &ServicesMediator{GossipAdapter: peers["peer1"], MCSAdapter: cryptoService}
-	peer1State := NewGossipStateProvider(chainID, mediator, peers["peer1"].coord, stateMetrics, config)
+	peer1State := NewGossipStateProvider(chainID, mediator, peers["peer1"].coord, stateMetrics, config, nil)
 	defer peer1State.Stop()
 
 	mediator = &ServicesMediator{GossipAdapter: peers["peer2"], MCSAdapter: cryptoService}
-	peer2State := NewGossipStateProvider(chainID, mediator, peers["peer2"].coord, stateMetrics, config)
+	peer2State := NewGossipStateProvider(chainID, mediator, peers["peer2"].coord, stateMetrics, config, nil)
 	defer peer2State.Stop()
 
 	// Make sure state was replicated
diff --git a/peer/node/start.go b/peer/node/start.go
index ba7329ce..9ccb0944 100644
--- a/peer/node/start.go
+++ b/peer/node/start.go
@@ -70,6 +70,8 @@ import (
 	ccsupport "github.com/hyperledger/fabric/discovery/support/chaincode"
 	"github.com/hyperledger/fabric/discovery/support/config"
 	"github.com/hyperledger/fabric/discovery/support/gossip"
+	supportapi "github.com/hyperledger/fabric/extensions/collections/api/support"
+	collretriever "github.com/hyperledger/fabric/extensions/collections/retriever"
 	gossipcommon "github.com/hyperledger/fabric/gossip/common"
 	"github.com/hyperledger/fabric/gossip/service"
 	"github.com/hyperledger/fabric/msp"
@@ -172,6 +174,16 @@ func serve(args []string) error {
 	flogging.Global.SetObserver(logObserver)
 
 	membershipInfoProvider := privdata.NewMembershipInfoProvider(createSelfSignedData(), identityDeserializerFactory)
+
+	transientDataProvider := collretriever.NewProvider(
+		peer.CollectionDataStoreFactory().StoreForChannel,
+		peer.GetLedger,
+		func() supportapi.GossipAdapter {
+			return service.GetGossipService()
+		},
+		peer.BlockPublisher.ForChannel,
+	)
+
 	//initialize resource management exit
 	ledgermgmt.Initialize(
 		&ledgermgmt.Initializer{
@@ -181,6 +193,7 @@ func serve(args []string) error {
 			MembershipInfoProvider:        membershipInfoProvider,
 			MetricsProvider:               metricsProvider,
 			HealthCheckRegistry:           opsSystem,
+			CollDataProvider:              transientDataProvider,
 		},
 	)
 
@@ -359,7 +372,7 @@ func serve(args []string) error {
 		}
 		cceventmgmt.GetMgr().Register(cid, sub)
 	}, ccp, sccp, txvalidator.MapBasedPluginMapper(validationPluginsByName),
-		pr, deployedCCInfoProvider, membershipInfoProvider, metricsProvider)
+		pr, deployedCCInfoProvider, membershipInfoProvider, metricsProvider, transientDataProvider)
 
 	if viper.GetBool("peer.discovery.enabled") {
 		registerDiscoveryService(peerServer, policyMgr, lifecycle)
diff --git a/protos/common/collection.pb.go b/protos/common/collection.pb.go
index e4d93634..d04d991b 100644
--- a/protos/common/collection.pb.go
+++ b/protos/common/collection.pb.go
@@ -18,6 +18,39 @@ var _ = math.Inf
 // proto package needs to be updated.
 const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package
 
+// CollectionType enumerates the various types of private data collections.
+type CollectionType int32
+
+const (
+	CollectionType_COL_UNKNOWN   CollectionType = 0
+	CollectionType_COL_PRIVATE   CollectionType = 1
+	CollectionType_COL_TRANSIENT CollectionType = 2
+	CollectionType_COL_OFFLEDGER CollectionType = 3
+	CollectionType_COL_DCAS      CollectionType = 4
+)
+
+var CollectionType_name = map[int32]string{
+	0: "COL_UNKNOWN",
+	1: "COL_PRIVATE",
+	2: "COL_TRANSIENT",
+	3: "COL_OFFLEDGER",
+	4: "COL_DCAS",
+}
+var CollectionType_value = map[string]int32{
+	"COL_UNKNOWN":   0,
+	"COL_PRIVATE":   1,
+	"COL_TRANSIENT": 2,
+	"COL_OFFLEDGER": 3,
+	"COL_DCAS":      4,
+}
+
+func (x CollectionType) String() string {
+	return proto.EnumName(CollectionType_name, int32(x))
+}
+func (CollectionType) EnumDescriptor() ([]byte, []int) {
+	return fileDescriptor_collection_c4f7076c0a86910e, []int{0}
+}
+
 // CollectionConfigPackage represents an array of CollectionConfig
 // messages; the extra struct is required because repeated oneof is
 // forbidden by the protobuf syntax
@@ -32,7 +65,7 @@ func (m *CollectionConfigPackage) Reset()         { *m = CollectionConfigPackage
 func (m *CollectionConfigPackage) String() string { return proto.CompactTextString(m) }
 func (*CollectionConfigPackage) ProtoMessage()    {}
 func (*CollectionConfigPackage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{0}
+	return fileDescriptor_collection_c4f7076c0a86910e, []int{0}
 }
 func (m *CollectionConfigPackage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionConfigPackage.Unmarshal(m, b)
@@ -75,7 +108,7 @@ func (m *CollectionConfig) Reset()         { *m = CollectionConfig{} }
 func (m *CollectionConfig) String() string { return proto.CompactTextString(m) }
 func (*CollectionConfig) ProtoMessage()    {}
 func (*CollectionConfig) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{1}
+	return fileDescriptor_collection_c4f7076c0a86910e, []int{1}
 }
 func (m *CollectionConfig) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionConfig.Unmarshal(m, b)
@@ -199,7 +232,15 @@ type StaticCollectionConfig struct {
 	// can read the private data (if set to true), or even non members can
 	// read the data (if set to false, for example if you want to implement more granular
 	// access logic in the chaincode)
-	MemberOnlyRead       bool     `protobuf:"varint,6,opt,name=member_only_read,json=memberOnlyRead,proto3" json:"member_only_read,omitempty"`
+	MemberOnlyRead bool `protobuf:"varint,6,opt,name=member_only_read,json=memberOnlyRead,proto3" json:"member_only_read,omitempty"`
+	// The type of collection.
+	Type CollectionType `protobuf:"varint,9900,opt,name=type,proto3,enum=common.CollectionType" json:"type,omitempty"`
+	// The time after which the collection data expires. For example,
+	// if the value is set to "10m" then the data will be purged
+	// 10 minutes after it was stored. An empty value indicates that
+	// the data should never be purged.
+	// The format of this string must be parseable by time.ParseDuration
+	TimeToLive           string   `protobuf:"bytes,9901,opt,name=time_to_live,json=timeToLive,proto3" json:"time_to_live,omitempty"`
 	XXX_NoUnkeyedLiteral struct{} `json:"-"`
 	XXX_unrecognized     []byte   `json:"-"`
 	XXX_sizecache        int32    `json:"-"`
@@ -209,7 +250,7 @@ func (m *StaticCollectionConfig) Reset()         { *m = StaticCollectionConfig{}
 func (m *StaticCollectionConfig) String() string { return proto.CompactTextString(m) }
 func (*StaticCollectionConfig) ProtoMessage()    {}
 func (*StaticCollectionConfig) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{2}
+	return fileDescriptor_collection_c4f7076c0a86910e, []int{2}
 }
 func (m *StaticCollectionConfig) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StaticCollectionConfig.Unmarshal(m, b)
@@ -271,6 +312,20 @@ func (m *StaticCollectionConfig) GetMemberOnlyRead() bool {
 	return false
 }
 
+func (m *StaticCollectionConfig) GetType() CollectionType {
+	if m != nil {
+		return m.Type
+	}
+	return CollectionType_COL_UNKNOWN
+}
+
+func (m *StaticCollectionConfig) GetTimeToLive() string {
+	if m != nil {
+		return m.TimeToLive
+	}
+	return ""
+}
+
 // Collection policy configuration. Initially, the configuration can only
 // contain a SignaturePolicy. In the future, the SignaturePolicy may be a
 // more general Policy. Instead of containing the actual policy, the
@@ -288,7 +343,7 @@ func (m *CollectionPolicyConfig) Reset()         { *m = CollectionPolicyConfig{}
 func (m *CollectionPolicyConfig) String() string { return proto.CompactTextString(m) }
 func (*CollectionPolicyConfig) ProtoMessage()    {}
 func (*CollectionPolicyConfig) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{3}
+	return fileDescriptor_collection_c4f7076c0a86910e, []int{3}
 }
 func (m *CollectionPolicyConfig) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionPolicyConfig.Unmarshal(m, b)
@@ -403,7 +458,7 @@ func (m *CollectionCriteria) Reset()         { *m = CollectionCriteria{} }
 func (m *CollectionCriteria) String() string { return proto.CompactTextString(m) }
 func (*CollectionCriteria) ProtoMessage()    {}
 func (*CollectionCriteria) Descriptor() ([]byte, []int) {
-	return fileDescriptor_collection_12a2cf6632dc7d83, []int{4}
+	return fileDescriptor_collection_c4f7076c0a86910e, []int{4}
 }
 func (m *CollectionCriteria) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_CollectionCriteria.Unmarshal(m, b)
@@ -457,40 +512,48 @@ func init() {
 	proto.RegisterType((*StaticCollectionConfig)(nil), "common.StaticCollectionConfig")
 	proto.RegisterType((*CollectionPolicyConfig)(nil), "common.CollectionPolicyConfig")
 	proto.RegisterType((*CollectionCriteria)(nil), "common.CollectionCriteria")
-}
-
-func init() { proto.RegisterFile("common/collection.proto", fileDescriptor_collection_12a2cf6632dc7d83) }
-
-var fileDescriptor_collection_12a2cf6632dc7d83 = []byte{
-	// 480 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x6c, 0x93, 0x51, 0x6b, 0xdb, 0x30,
-	0x10, 0xc7, 0xeb, 0x36, 0x4d, 0xe7, 0x0b, 0xdb, 0x32, 0x95, 0xa5, 0x66, 0x8c, 0x2e, 0x84, 0x3d,
-	0x18, 0x36, 0x9c, 0xd1, 0x7d, 0x83, 0x86, 0x41, 0xc7, 0x02, 0x0b, 0xea, 0x9e, 0xfa, 0x62, 0x14,
-	0xf9, 0xea, 0x88, 0xca, 0x92, 0x2b, 0x2b, 0x21, 0x7e, 0xdc, 0x97, 0xd9, 0xe7, 0x1c, 0x91, 0xec,
-	0x24, 0x0d, 0x79, 0xf3, 0xdd, 0xff, 0x77, 0xe7, 0xbb, 0xfb, 0xdb, 0x70, 0xc5, 0x75, 0x51, 0x68,
-	0x35, 0xe6, 0x5a, 0x4a, 0xe4, 0x56, 0x68, 0x95, 0x94, 0x46, 0x5b, 0x4d, 0xba, 0x5e, 0xf8, 0xf0,
-	0xbe, 0x01, 0x4a, 0x2d, 0x05, 0x17, 0x58, 0x79, 0x79, 0xf4, 0x0b, 0xae, 0x26, 0xdb, 0x92, 0x89,
-	0x56, 0x8f, 0x22, 0x9f, 0x31, 0xfe, 0xc4, 0x72, 0x24, 0xdf, 0xa0, 0xcb, 0x5d, 0x22, 0x0a, 0x86,
-	0x67, 0x71, 0xef, 0x26, 0x4a, 0x7c, 0x8b, 0xe4, 0xb0, 0x80, 0x36, 0xdc, 0xa8, 0x86, 0xfe, 0xa1,
-	0x46, 0x1e, 0x20, 0xaa, 0x2c, 0xb3, 0x82, 0xa7, 0xbb, 0xd1, 0xd2, 0x6d, 0xdf, 0x20, 0xee, 0xdd,
-	0x5c, 0xb7, 0x7d, 0xef, 0x1d, 0x77, 0xd8, 0xe1, 0xee, 0x84, 0x0e, 0xaa, 0xa3, 0xca, 0x6d, 0x08,
-	0x17, 0x25, 0xab, 0xa5, 0x66, 0xd9, 0xe8, 0xdf, 0x29, 0x0c, 0x8e, 0xd7, 0x13, 0x02, 0x1d, 0xc5,
-	0x0a, 0x74, 0x6f, 0x0b, 0xa9, 0x7b, 0x26, 0x53, 0x20, 0x05, 0x16, 0x73, 0x34, 0xa9, 0x36, 0x79,
-	0x95, 0xba, 0xa3, 0xd4, 0xd1, 0xe9, 0xcb, 0x79, 0x76, 0x9d, 0x66, 0x4e, 0x6f, 0xb6, 0xed, 0xfb,
-	0xca, 0xdf, 0x26, 0xaf, 0x7c, 0x9e, 0x24, 0x70, 0x69, 0xf0, 0x79, 0x29, 0x0c, 0x66, 0x69, 0x89,
-	0x68, 0x52, 0xae, 0x97, 0xca, 0x46, 0x67, 0xc3, 0x20, 0x3e, 0xa7, 0xef, 0x5a, 0x69, 0x86, 0x68,
-	0x26, 0x1b, 0x81, 0x7c, 0x05, 0x52, 0xb0, 0xb5, 0x28, 0x96, 0xc5, 0x3e, 0xde, 0x71, 0x78, 0xbf,
-	0x51, 0x76, 0xf4, 0x08, 0x5e, 0xcf, 0xa5, 0xe6, 0x4f, 0xa9, 0xd5, 0xa9, 0x14, 0x2b, 0x8c, 0xce,
-	0x87, 0x41, 0xdc, 0xa1, 0x3d, 0x97, 0xfc, 0xa3, 0xa7, 0x62, 0x85, 0x24, 0x86, 0x7e, 0xbb, 0x8f,
-	0x92, 0x75, 0x6a, 0x90, 0x65, 0x51, 0x77, 0x18, 0xc4, 0xaf, 0xe8, 0x9b, 0x66, 0x5a, 0x25, 0x6b,
-	0x8a, 0x2c, 0x1b, 0x3d, 0xc3, 0xe0, 0xf8, 0x5e, 0x64, 0x0a, 0xfd, 0x4a, 0xe4, 0x8a, 0xd9, 0xa5,
-	0xc1, 0xf6, 0x22, 0xde, 0xa1, 0x4f, 0x5b, 0x87, 0x5a, 0xdd, 0x17, 0xfe, 0x50, 0x2b, 0x94, 0xba,
-	0xc4, 0xbb, 0x13, 0xfa, 0xb6, 0x7a, 0x29, 0xed, 0x7b, 0xf3, 0x37, 0x00, 0xb2, 0xe7, 0x8a, 0x11,
-	0x16, 0x8d, 0x60, 0x24, 0x82, 0x0b, 0xbe, 0x60, 0x4a, 0xa1, 0x6c, 0xac, 0x69, 0x43, 0x72, 0x09,
-	0xe7, 0x76, 0x9d, 0x8a, 0xcc, 0x19, 0x12, 0xd2, 0x8e, 0x5d, 0xff, 0xcc, 0xc8, 0x35, 0xc0, 0xee,
-	0x0b, 0x72, 0xb7, 0x0d, 0xe9, 0x5e, 0x86, 0x7c, 0x84, 0x70, 0x63, 0x6d, 0x55, 0x32, 0x8e, 0xee,
-	0x96, 0x21, 0xdd, 0x25, 0x6e, 0xef, 0xe1, 0xb3, 0x36, 0x79, 0xb2, 0xa8, 0x4b, 0x34, 0x12, 0xb3,
-	0x1c, 0x4d, 0xf2, 0xc8, 0xe6, 0x46, 0x70, 0xff, 0x1f, 0x54, 0xcd, 0x86, 0x0f, 0x5f, 0x72, 0x61,
-	0x17, 0xcb, 0xf9, 0x26, 0x1c, 0xef, 0xc1, 0x63, 0x0f, 0x8f, 0x3d, 0x3c, 0xf6, 0xf0, 0xbc, 0xeb,
-	0xc2, 0xef, 0xff, 0x03, 0x00, 0x00, 0xff, 0xff, 0xf0, 0x3b, 0x7c, 0x15, 0x7d, 0x03, 0x00, 0x00,
+	proto.RegisterEnum("common.CollectionType", CollectionType_name, CollectionType_value)
+}
+
+func init() { proto.RegisterFile("common/collection.proto", fileDescriptor_collection_c4f7076c0a86910e) }
+
+var fileDescriptor_collection_c4f7076c0a86910e = []byte{
+	// 591 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x6c, 0x53, 0x5d, 0x4f, 0xdb, 0x30,
+	0x14, 0x25, 0x50, 0x0a, 0xbd, 0xe5, 0x23, 0x18, 0xad, 0x44, 0xd3, 0xc4, 0xba, 0x6a, 0x0f, 0xd1,
+	0x98, 0xda, 0x89, 0xfd, 0x02, 0x28, 0x65, 0x20, 0x4a, 0x5b, 0xb9, 0xdd, 0x26, 0xf1, 0x12, 0xb9,
+	0xc9, 0x25, 0x58, 0x24, 0x71, 0x70, 0x5c, 0x44, 0x1e, 0xf7, 0x7f, 0xb6, 0xbf, 0xb7, 0xe7, 0x29,
+	0x4e, 0xd2, 0x16, 0xc6, 0x5b, 0x7c, 0xce, 0xb9, 0xd7, 0xf7, 0x9e, 0xe3, 0xc0, 0x81, 0x2b, 0xc2,
+	0x50, 0x44, 0x1d, 0x57, 0x04, 0x01, 0xba, 0x8a, 0x8b, 0xa8, 0x1d, 0x4b, 0xa1, 0x04, 0xa9, 0xe6,
+	0xc4, 0xdb, 0x37, 0x85, 0x20, 0x16, 0x01, 0x77, 0x39, 0x26, 0x39, 0xdd, 0xba, 0x82, 0x83, 0xee,
+	0xbc, 0xa4, 0x2b, 0xa2, 0x5b, 0xee, 0x8f, 0x98, 0x7b, 0xcf, 0x7c, 0x24, 0x5f, 0xa0, 0xea, 0x6a,
+	0xc0, 0x32, 0x9a, 0x6b, 0x76, 0xfd, 0xd8, 0x6a, 0xe7, 0x2d, 0xda, 0x2f, 0x0b, 0x68, 0xa1, 0x6b,
+	0xa5, 0x60, 0xbe, 0xe4, 0xc8, 0x0d, 0x58, 0x89, 0x62, 0x8a, 0xbb, 0xce, 0x62, 0x34, 0x67, 0xde,
+	0xd7, 0xb0, 0xeb, 0xc7, 0x87, 0x65, 0xdf, 0xb1, 0xd6, 0xbd, 0xec, 0x70, 0xb1, 0x42, 0x1b, 0xc9,
+	0xab, 0xcc, 0x69, 0x0d, 0x36, 0x62, 0x96, 0x06, 0x82, 0x79, 0xad, 0xbf, 0xab, 0xd0, 0x78, 0xbd,
+	0x9e, 0x10, 0xa8, 0x44, 0x2c, 0x44, 0x7d, 0x5b, 0x8d, 0xea, 0x6f, 0xd2, 0x07, 0x12, 0x62, 0x38,
+	0x45, 0xe9, 0x08, 0xe9, 0x27, 0x8e, 0x36, 0x25, 0xb5, 0x56, 0x9f, 0xcf, 0xb3, 0xe8, 0x34, 0xd2,
+	0x7c, 0xb1, 0xad, 0x99, 0x57, 0x0e, 0xa5, 0x9f, 0xe4, 0x38, 0x69, 0xc3, 0xbe, 0xc4, 0x87, 0x19,
+	0x97, 0xe8, 0x39, 0x31, 0xa2, 0x74, 0x5c, 0x31, 0x8b, 0x94, 0xb5, 0xd6, 0x34, 0xec, 0x75, 0xba,
+	0x57, 0x52, 0x23, 0x44, 0xd9, 0xcd, 0x08, 0xf2, 0x19, 0x48, 0xc8, 0x9e, 0x78, 0x38, 0x0b, 0x97,
+	0xe5, 0x15, 0x2d, 0x37, 0x0b, 0x66, 0xa1, 0x6e, 0xc1, 0xf6, 0x34, 0x10, 0xee, 0xbd, 0xa3, 0x84,
+	0x13, 0xf0, 0x47, 0xb4, 0xd6, 0x9b, 0x86, 0x5d, 0xa1, 0x75, 0x0d, 0x4e, 0x44, 0x9f, 0x3f, 0x22,
+	0xb1, 0xc1, 0x2c, 0xf7, 0x89, 0x82, 0xd4, 0x91, 0xc8, 0x3c, 0xab, 0xda, 0x34, 0xec, 0x4d, 0xba,
+	0x53, 0x4c, 0x1b, 0x05, 0x29, 0x45, 0xe6, 0x91, 0x23, 0xa8, 0xa8, 0x34, 0x46, 0xeb, 0xf7, 0x75,
+	0xd3, 0xb0, 0x77, 0x8e, 0x1b, 0xff, 0x2f, 0x3b, 0x49, 0x63, 0xa4, 0x5a, 0x44, 0x3e, 0xc0, 0x96,
+	0xe2, 0x21, 0xce, 0x6f, 0xfe, 0x73, 0xad, 0x3d, 0x84, 0x0c, 0xcc, 0x6f, 0x6e, 0x3d, 0x40, 0xe3,
+	0x75, 0x9f, 0x48, 0x1f, 0xcc, 0x84, 0xfb, 0x11, 0x53, 0x33, 0x89, 0xa5, 0xc3, 0x79, 0xe2, 0xef,
+	0xe7, 0x89, 0x97, 0x7c, 0x5e, 0xd8, 0x8b, 0x1e, 0x31, 0x10, 0x31, 0x5e, 0xac, 0xd0, 0xdd, 0xe4,
+	0x39, 0xb5, 0x9c, 0xf5, 0x2f, 0x03, 0xc8, 0x52, 0xca, 0x92, 0x2b, 0x94, 0x9c, 0x11, 0x0b, 0x36,
+	0xdc, 0x3b, 0x16, 0x45, 0x18, 0x14, 0x51, 0x97, 0x47, 0xb2, 0x0f, 0xeb, 0xea, 0xc9, 0xe1, 0x9e,
+	0x0e, 0xb8, 0x46, 0x2b, 0xea, 0xe9, 0xd2, 0x23, 0x87, 0x00, 0x8b, 0x17, 0xa9, 0xb3, 0xaa, 0xd1,
+	0x25, 0x84, 0xbc, 0x83, 0x5a, 0xf6, 0x54, 0x92, 0x98, 0xb9, 0xa8, 0xb3, 0xa9, 0xd1, 0x05, 0xf0,
+	0xe9, 0x16, 0x76, 0x9e, 0x3b, 0x46, 0x76, 0xa1, 0xde, 0x1d, 0xf6, 0x9d, 0xef, 0x83, 0xab, 0xc1,
+	0xf0, 0xe7, 0xc0, 0x5c, 0x29, 0x81, 0x11, 0xbd, 0xfc, 0x71, 0x32, 0xe9, 0x99, 0x06, 0xd9, 0x83,
+	0xed, 0x0c, 0x98, 0xd0, 0x93, 0xc1, 0xf8, 0xb2, 0x37, 0x98, 0x98, 0xab, 0x25, 0x34, 0x3c, 0x3f,
+	0xef, 0xf7, 0xce, 0xbe, 0xf5, 0xa8, 0xb9, 0x46, 0xb6, 0x60, 0x33, 0x83, 0xce, 0xba, 0x27, 0x63,
+	0xb3, 0x72, 0x3a, 0x86, 0x8f, 0x42, 0xfa, 0xed, 0xbb, 0x34, 0x46, 0x19, 0xa0, 0xe7, 0xa3, 0x6c,
+	0xdf, 0xb2, 0xa9, 0xe4, 0x6e, 0xfe, 0xff, 0x26, 0x85, 0x93, 0x37, 0x47, 0x3e, 0x57, 0x77, 0xb3,
+	0x69, 0x76, 0xec, 0x2c, 0x89, 0x3b, 0xb9, 0xb8, 0x93, 0x8b, 0x3b, 0xb9, 0x78, 0x5a, 0xd5, 0xc7,
+	0xaf, 0xff, 0x02, 0x00, 0x00, 0xff, 0xff, 0xf7, 0x1a, 0x4f, 0xba, 0x35, 0x04, 0x00, 0x00,
 }
diff --git a/protos/common/collection.proto b/protos/common/collection.proto
index 321bfd72..cf0d0862 100644
--- a/protos/common/collection.proto
+++ b/protos/common/collection.proto
@@ -13,6 +13,15 @@ option java_package = "org.hyperledger.fabric.protos.common";
 
 package common;
 
+// CollectionType enumerates the various types of private data collections.
+enum CollectionType {
+    COL_UNKNOWN = 0;    // Unspecified type - will use PRIVATE as default.
+    COL_PRIVATE = 1;    // Persisted private data collection.
+    COL_TRANSIENT = 2;  // Transient private data collection.
+    COL_OFFLEDGER = 3;  // Off-ledger private data collection.
+    COL_DCAS = 4;       // Distributed Content Addressable Store (CAS) private data collection.
+}
+
 // CollectionConfigPackage represents an array of CollectionConfig
 // messages; the extra struct is required because repeated oneof is
 // forbidden by the protobuf syntax
@@ -56,6 +65,14 @@ message StaticCollectionConfig {
     // read the data (if set to false, for example if you want to implement more granular
     // access logic in the chaincode)
     bool member_only_read = 6;
+    // The type of collection.
+    CollectionType type = 9900;
+    // The time after which the collection data expires. For example,
+    // if the value is set to "10m" then the data will be purged
+    // 10 minutes after it was stored. An empty value indicates that
+    // the data should never be purged.
+    // The format of this string must be parseable by time.ParseDuration
+    string time_to_live = 9901;
 }
 
 
diff --git a/protos/gossip/message.pb.go b/protos/gossip/message.pb.go
index 310eabc1..6dc899c3 100644
--- a/protos/gossip/message.pb.go
+++ b/protos/gossip/message.pb.go
@@ -6,6 +6,7 @@ package gossip // import "github.com/hyperledger/fabric/protos/gossip"
 import proto "github.com/golang/protobuf/proto"
 import fmt "fmt"
 import math "math"
+import timestamp "github.com/golang/protobuf/ptypes/timestamp"
 import common "github.com/hyperledger/fabric/protos/common"
 
 import (
@@ -47,7 +48,7 @@ func (x PullMsgType) String() string {
 	return proto.EnumName(PullMsgType_name, int32(x))
 }
 func (PullMsgType) EnumDescriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{0}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{0}
 }
 
 type GossipMessage_Tag int32
@@ -82,7 +83,7 @@ func (x GossipMessage_Tag) String() string {
 	return proto.EnumName(GossipMessage_Tag_name, int32(x))
 }
 func (GossipMessage_Tag) EnumDescriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{3, 0}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{3, 0}
 }
 
 // Envelope contains a marshalled
@@ -102,7 +103,7 @@ func (m *Envelope) Reset()         { *m = Envelope{} }
 func (m *Envelope) String() string { return proto.CompactTextString(m) }
 func (*Envelope) ProtoMessage()    {}
 func (*Envelope) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{0}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{0}
 }
 func (m *Envelope) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Envelope.Unmarshal(m, b)
@@ -160,7 +161,7 @@ func (m *SecretEnvelope) Reset()         { *m = SecretEnvelope{} }
 func (m *SecretEnvelope) String() string { return proto.CompactTextString(m) }
 func (*SecretEnvelope) ProtoMessage()    {}
 func (*SecretEnvelope) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{1}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{1}
 }
 func (m *SecretEnvelope) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_SecretEnvelope.Unmarshal(m, b)
@@ -210,7 +211,7 @@ func (m *Secret) Reset()         { *m = Secret{} }
 func (m *Secret) String() string { return proto.CompactTextString(m) }
 func (*Secret) ProtoMessage()    {}
 func (*Secret) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{2}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{2}
 }
 func (m *Secret) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Secret.Unmarshal(m, b)
@@ -339,6 +340,8 @@ type GossipMessage struct {
 	//	*GossipMessage_PrivateReq
 	//	*GossipMessage_PrivateRes
 	//	*GossipMessage_PrivateData
+	//	*GossipMessage_CollDataReq
+	//	*GossipMessage_CollDataRes
 	Content              isGossipMessage_Content `protobuf_oneof:"content"`
 	XXX_NoUnkeyedLiteral struct{}                `json:"-"`
 	XXX_unrecognized     []byte                  `json:"-"`
@@ -349,7 +352,7 @@ func (m *GossipMessage) Reset()         { *m = GossipMessage{} }
 func (m *GossipMessage) String() string { return proto.CompactTextString(m) }
 func (*GossipMessage) ProtoMessage()    {}
 func (*GossipMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{3}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{3}
 }
 func (m *GossipMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_GossipMessage.Unmarshal(m, b)
@@ -478,6 +481,14 @@ type GossipMessage_PrivateData struct {
 	PrivateData *PrivateDataMessage `protobuf:"bytes,25,opt,name=private_data,json=privateData,proto3,oneof"`
 }
 
+type GossipMessage_CollDataReq struct {
+	CollDataReq *RemoteCollDataRequest `protobuf:"bytes,90,opt,name=collDataReq,proto3,oneof"`
+}
+
+type GossipMessage_CollDataRes struct {
+	CollDataRes *RemoteCollDataResponse `protobuf:"bytes,91,opt,name=collDataRes,proto3,oneof"`
+}
+
 func (*GossipMessage_AliveMsg) isGossipMessage_Content() {}
 
 func (*GossipMessage_MemReq) isGossipMessage_Content() {}
@@ -520,6 +531,10 @@ func (*GossipMessage_PrivateRes) isGossipMessage_Content() {}
 
 func (*GossipMessage_PrivateData) isGossipMessage_Content() {}
 
+func (*GossipMessage_CollDataReq) isGossipMessage_Content() {}
+
+func (*GossipMessage_CollDataRes) isGossipMessage_Content() {}
+
 func (m *GossipMessage) GetContent() isGossipMessage_Content {
 	if m != nil {
 		return m.Content
@@ -674,6 +689,20 @@ func (m *GossipMessage) GetPrivateData() *PrivateDataMessage {
 	return nil
 }
 
+func (m *GossipMessage) GetCollDataReq() *RemoteCollDataRequest {
+	if x, ok := m.GetContent().(*GossipMessage_CollDataReq); ok {
+		return x.CollDataReq
+	}
+	return nil
+}
+
+func (m *GossipMessage) GetCollDataRes() *RemoteCollDataResponse {
+	if x, ok := m.GetContent().(*GossipMessage_CollDataRes); ok {
+		return x.CollDataRes
+	}
+	return nil
+}
+
 // XXX_OneofFuncs is for the internal use of the proto package.
 func (*GossipMessage) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer) error, func(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error), func(msg proto.Message) (n int), []interface{}) {
 	return _GossipMessage_OneofMarshaler, _GossipMessage_OneofUnmarshaler, _GossipMessage_OneofSizer, []interface{}{
@@ -698,6 +727,8 @@ func (*GossipMessage) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer)
 		(*GossipMessage_PrivateReq)(nil),
 		(*GossipMessage_PrivateRes)(nil),
 		(*GossipMessage_PrivateData)(nil),
+		(*GossipMessage_CollDataReq)(nil),
+		(*GossipMessage_CollDataRes)(nil),
 	}
 }
 
@@ -810,6 +841,16 @@ func _GossipMessage_OneofMarshaler(msg proto.Message, b *proto.Buffer) error {
 		if err := b.EncodeMessage(x.PrivateData); err != nil {
 			return err
 		}
+	case *GossipMessage_CollDataReq:
+		b.EncodeVarint(90<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.CollDataReq); err != nil {
+			return err
+		}
+	case *GossipMessage_CollDataRes:
+		b.EncodeVarint(91<<3 | proto.WireBytes)
+		if err := b.EncodeMessage(x.CollDataRes); err != nil {
+			return err
+		}
 	case nil:
 	default:
 		return fmt.Errorf("GossipMessage.Content has unexpected type %T", x)
@@ -988,6 +1029,22 @@ func _GossipMessage_OneofUnmarshaler(msg proto.Message, tag, wire int, b *proto.
 		err := b.DecodeMessage(msg)
 		m.Content = &GossipMessage_PrivateData{msg}
 		return true, err
+	case 90: // content.collDataReq
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(RemoteCollDataRequest)
+		err := b.DecodeMessage(msg)
+		m.Content = &GossipMessage_CollDataReq{msg}
+		return true, err
+	case 91: // content.collDataRes
+		if wire != proto.WireBytes {
+			return true, proto.ErrInternalBadWireType
+		}
+		msg := new(RemoteCollDataResponse)
+		err := b.DecodeMessage(msg)
+		m.Content = &GossipMessage_CollDataRes{msg}
+		return true, err
 	default:
 		return false, nil
 	}
@@ -1102,6 +1159,16 @@ func _GossipMessage_OneofSizer(msg proto.Message) (n int) {
 		n += 2 // tag and wire
 		n += proto.SizeVarint(uint64(s))
 		n += s
+	case *GossipMessage_CollDataReq:
+		s := proto.Size(x.CollDataReq)
+		n += 2 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
+	case *GossipMessage_CollDataRes:
+		s := proto.Size(x.CollDataRes)
+		n += 2 // tag and wire
+		n += proto.SizeVarint(uint64(s))
+		n += s
 	case nil:
 	default:
 		panic(fmt.Sprintf("proto: unexpected type %T in oneof", x))
@@ -1128,7 +1195,7 @@ func (m *StateInfo) Reset()         { *m = StateInfo{} }
 func (m *StateInfo) String() string { return proto.CompactTextString(m) }
 func (*StateInfo) ProtoMessage()    {}
 func (*StateInfo) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{4}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{4}
 }
 func (m *StateInfo) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StateInfo.Unmarshal(m, b)
@@ -1189,7 +1256,7 @@ func (m *Properties) Reset()         { *m = Properties{} }
 func (m *Properties) String() string { return proto.CompactTextString(m) }
 func (*Properties) ProtoMessage()    {}
 func (*Properties) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{5}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{5}
 }
 func (m *Properties) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Properties.Unmarshal(m, b)
@@ -1242,7 +1309,7 @@ func (m *StateInfoSnapshot) Reset()         { *m = StateInfoSnapshot{} }
 func (m *StateInfoSnapshot) String() string { return proto.CompactTextString(m) }
 func (*StateInfoSnapshot) ProtoMessage()    {}
 func (*StateInfoSnapshot) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{6}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{6}
 }
 func (m *StateInfoSnapshot) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StateInfoSnapshot.Unmarshal(m, b)
@@ -1285,7 +1352,7 @@ func (m *StateInfoPullRequest) Reset()         { *m = StateInfoPullRequest{} }
 func (m *StateInfoPullRequest) String() string { return proto.CompactTextString(m) }
 func (*StateInfoPullRequest) ProtoMessage()    {}
 func (*StateInfoPullRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{7}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{7}
 }
 func (m *StateInfoPullRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_StateInfoPullRequest.Unmarshal(m, b)
@@ -1328,7 +1395,7 @@ func (m *ConnEstablish) Reset()         { *m = ConnEstablish{} }
 func (m *ConnEstablish) String() string { return proto.CompactTextString(m) }
 func (*ConnEstablish) ProtoMessage()    {}
 func (*ConnEstablish) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{8}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{8}
 }
 func (m *ConnEstablish) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_ConnEstablish.Unmarshal(m, b)
@@ -1385,7 +1452,7 @@ func (m *PeerIdentity) Reset()         { *m = PeerIdentity{} }
 func (m *PeerIdentity) String() string { return proto.CompactTextString(m) }
 func (*PeerIdentity) ProtoMessage()    {}
 func (*PeerIdentity) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{9}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{9}
 }
 func (m *PeerIdentity) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PeerIdentity.Unmarshal(m, b)
@@ -1441,7 +1508,7 @@ func (m *DataRequest) Reset()         { *m = DataRequest{} }
 func (m *DataRequest) String() string { return proto.CompactTextString(m) }
 func (*DataRequest) ProtoMessage()    {}
 func (*DataRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{10}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{10}
 }
 func (m *DataRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataRequest.Unmarshal(m, b)
@@ -1497,7 +1564,7 @@ func (m *GossipHello) Reset()         { *m = GossipHello{} }
 func (m *GossipHello) String() string { return proto.CompactTextString(m) }
 func (*GossipHello) ProtoMessage()    {}
 func (*GossipHello) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{11}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{11}
 }
 func (m *GossipHello) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_GossipHello.Unmarshal(m, b)
@@ -1553,7 +1620,7 @@ func (m *DataUpdate) Reset()         { *m = DataUpdate{} }
 func (m *DataUpdate) String() string { return proto.CompactTextString(m) }
 func (*DataUpdate) ProtoMessage()    {}
 func (*DataUpdate) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{12}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{12}
 }
 func (m *DataUpdate) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataUpdate.Unmarshal(m, b)
@@ -1609,7 +1676,7 @@ func (m *DataDigest) Reset()         { *m = DataDigest{} }
 func (m *DataDigest) String() string { return proto.CompactTextString(m) }
 func (*DataDigest) ProtoMessage()    {}
 func (*DataDigest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{13}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{13}
 }
 func (m *DataDigest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataDigest.Unmarshal(m, b)
@@ -1662,7 +1729,7 @@ func (m *DataMessage) Reset()         { *m = DataMessage{} }
 func (m *DataMessage) String() string { return proto.CompactTextString(m) }
 func (*DataMessage) ProtoMessage()    {}
 func (*DataMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{14}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{14}
 }
 func (m *DataMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_DataMessage.Unmarshal(m, b)
@@ -1703,7 +1770,7 @@ func (m *PrivateDataMessage) Reset()         { *m = PrivateDataMessage{} }
 func (m *PrivateDataMessage) String() string { return proto.CompactTextString(m) }
 func (*PrivateDataMessage) ProtoMessage()    {}
 func (*PrivateDataMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{15}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{15}
 }
 func (m *PrivateDataMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PrivateDataMessage.Unmarshal(m, b)
@@ -1744,7 +1811,7 @@ func (m *Payload) Reset()         { *m = Payload{} }
 func (m *Payload) String() string { return proto.CompactTextString(m) }
 func (*Payload) ProtoMessage()    {}
 func (*Payload) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{16}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{16}
 }
 func (m *Payload) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Payload.Unmarshal(m, b)
@@ -1804,7 +1871,7 @@ func (m *PrivatePayload) Reset()         { *m = PrivatePayload{} }
 func (m *PrivatePayload) String() string { return proto.CompactTextString(m) }
 func (*PrivatePayload) ProtoMessage()    {}
 func (*PrivatePayload) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{17}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{17}
 }
 func (m *PrivatePayload) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PrivatePayload.Unmarshal(m, b)
@@ -1881,7 +1948,7 @@ func (m *AliveMessage) Reset()         { *m = AliveMessage{} }
 func (m *AliveMessage) String() string { return proto.CompactTextString(m) }
 func (*AliveMessage) ProtoMessage()    {}
 func (*AliveMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{18}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{18}
 }
 func (m *AliveMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_AliveMessage.Unmarshal(m, b)
@@ -1937,7 +2004,7 @@ func (m *LeadershipMessage) Reset()         { *m = LeadershipMessage{} }
 func (m *LeadershipMessage) String() string { return proto.CompactTextString(m) }
 func (*LeadershipMessage) ProtoMessage()    {}
 func (*LeadershipMessage) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{19}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{19}
 }
 func (m *LeadershipMessage) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_LeadershipMessage.Unmarshal(m, b)
@@ -1991,7 +2058,7 @@ func (m *PeerTime) Reset()         { *m = PeerTime{} }
 func (m *PeerTime) String() string { return proto.CompactTextString(m) }
 func (*PeerTime) ProtoMessage()    {}
 func (*PeerTime) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{20}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{20}
 }
 func (m *PeerTime) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PeerTime.Unmarshal(m, b)
@@ -2039,7 +2106,7 @@ func (m *MembershipRequest) Reset()         { *m = MembershipRequest{} }
 func (m *MembershipRequest) String() string { return proto.CompactTextString(m) }
 func (*MembershipRequest) ProtoMessage()    {}
 func (*MembershipRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{21}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{21}
 }
 func (m *MembershipRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_MembershipRequest.Unmarshal(m, b)
@@ -2086,7 +2153,7 @@ func (m *MembershipResponse) Reset()         { *m = MembershipResponse{} }
 func (m *MembershipResponse) String() string { return proto.CompactTextString(m) }
 func (*MembershipResponse) ProtoMessage()    {}
 func (*MembershipResponse) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{22}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{22}
 }
 func (m *MembershipResponse) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_MembershipResponse.Unmarshal(m, b)
@@ -2135,7 +2202,7 @@ func (m *Member) Reset()         { *m = Member{} }
 func (m *Member) String() string { return proto.CompactTextString(m) }
 func (*Member) ProtoMessage()    {}
 func (*Member) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{23}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{23}
 }
 func (m *Member) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Member.Unmarshal(m, b)
@@ -2187,7 +2254,7 @@ func (m *Empty) Reset()         { *m = Empty{} }
 func (m *Empty) String() string { return proto.CompactTextString(m) }
 func (*Empty) ProtoMessage()    {}
 func (*Empty) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{24}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{24}
 }
 func (m *Empty) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Empty.Unmarshal(m, b)
@@ -2221,7 +2288,7 @@ func (m *RemoteStateRequest) Reset()         { *m = RemoteStateRequest{} }
 func (m *RemoteStateRequest) String() string { return proto.CompactTextString(m) }
 func (*RemoteStateRequest) ProtoMessage()    {}
 func (*RemoteStateRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{25}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{25}
 }
 func (m *RemoteStateRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemoteStateRequest.Unmarshal(m, b)
@@ -2268,7 +2335,7 @@ func (m *RemoteStateResponse) Reset()         { *m = RemoteStateResponse{} }
 func (m *RemoteStateResponse) String() string { return proto.CompactTextString(m) }
 func (*RemoteStateResponse) ProtoMessage()    {}
 func (*RemoteStateResponse) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{26}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{26}
 }
 func (m *RemoteStateResponse) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemoteStateResponse.Unmarshal(m, b)
@@ -2308,7 +2375,7 @@ func (m *RemotePvtDataRequest) Reset()         { *m = RemotePvtDataRequest{} }
 func (m *RemotePvtDataRequest) String() string { return proto.CompactTextString(m) }
 func (*RemotePvtDataRequest) ProtoMessage()    {}
 func (*RemotePvtDataRequest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{27}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{27}
 }
 func (m *RemotePvtDataRequest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemotePvtDataRequest.Unmarshal(m, b)
@@ -2351,7 +2418,7 @@ func (m *PvtDataDigest) Reset()         { *m = PvtDataDigest{} }
 func (m *PvtDataDigest) String() string { return proto.CompactTextString(m) }
 func (*PvtDataDigest) ProtoMessage()    {}
 func (*PvtDataDigest) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{28}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{28}
 }
 func (m *PvtDataDigest) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PvtDataDigest.Unmarshal(m, b)
@@ -2419,7 +2486,7 @@ func (m *RemotePvtDataResponse) Reset()         { *m = RemotePvtDataResponse{} }
 func (m *RemotePvtDataResponse) String() string { return proto.CompactTextString(m) }
 func (*RemotePvtDataResponse) ProtoMessage()    {}
 func (*RemotePvtDataResponse) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{29}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{29}
 }
 func (m *RemotePvtDataResponse) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_RemotePvtDataResponse.Unmarshal(m, b)
@@ -2459,7 +2526,7 @@ func (m *PvtDataElement) Reset()         { *m = PvtDataElement{} }
 func (m *PvtDataElement) String() string { return proto.CompactTextString(m) }
 func (*PvtDataElement) ProtoMessage()    {}
 func (*PvtDataElement) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{30}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{30}
 }
 func (m *PvtDataElement) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PvtDataElement.Unmarshal(m, b)
@@ -2509,7 +2576,7 @@ func (m *PvtDataPayload) Reset()         { *m = PvtDataPayload{} }
 func (m *PvtDataPayload) String() string { return proto.CompactTextString(m) }
 func (*PvtDataPayload) ProtoMessage()    {}
 func (*PvtDataPayload) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{31}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{31}
 }
 func (m *PvtDataPayload) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_PvtDataPayload.Unmarshal(m, b)
@@ -2554,7 +2621,7 @@ func (m *Acknowledgement) Reset()         { *m = Acknowledgement{} }
 func (m *Acknowledgement) String() string { return proto.CompactTextString(m) }
 func (*Acknowledgement) ProtoMessage()    {}
 func (*Acknowledgement) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{32}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{32}
 }
 func (m *Acknowledgement) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Acknowledgement.Unmarshal(m, b)
@@ -2596,7 +2663,7 @@ func (m *Chaincode) Reset()         { *m = Chaincode{} }
 func (m *Chaincode) String() string { return proto.CompactTextString(m) }
 func (*Chaincode) ProtoMessage()    {}
 func (*Chaincode) Descriptor() ([]byte, []int) {
-	return fileDescriptor_message_7c42328ef5ef9997, []int{33}
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{33}
 }
 func (m *Chaincode) XXX_Unmarshal(b []byte) error {
 	return xxx_messageInfo_Chaincode.Unmarshal(m, b)
@@ -2637,6 +2704,283 @@ func (m *Chaincode) GetMetadata() []byte {
 	return nil
 }
 
+// ValidationResultsMessage is the message containing block validation results
+type ValidationResultsMessage struct {
+	SeqNum               uint64   `protobuf:"varint,1,opt,name=seq_num,json=seqNum,proto3" json:"seq_num,omitempty"`
+	TxFlags              []byte   `protobuf:"bytes,2,opt,name=txFlags,proto3" json:"txFlags,omitempty"`
+	Signature            []byte   `protobuf:"bytes,3,opt,name=signature,proto3" json:"signature,omitempty"`
+	Identity             []byte   `protobuf:"bytes,4,opt,name=identity,proto3" json:"identity,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *ValidationResultsMessage) Reset()         { *m = ValidationResultsMessage{} }
+func (m *ValidationResultsMessage) String() string { return proto.CompactTextString(m) }
+func (*ValidationResultsMessage) ProtoMessage()    {}
+func (*ValidationResultsMessage) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{34}
+}
+func (m *ValidationResultsMessage) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_ValidationResultsMessage.Unmarshal(m, b)
+}
+func (m *ValidationResultsMessage) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_ValidationResultsMessage.Marshal(b, m, deterministic)
+}
+func (dst *ValidationResultsMessage) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_ValidationResultsMessage.Merge(dst, src)
+}
+func (m *ValidationResultsMessage) XXX_Size() int {
+	return xxx_messageInfo_ValidationResultsMessage.Size(m)
+}
+func (m *ValidationResultsMessage) XXX_DiscardUnknown() {
+	xxx_messageInfo_ValidationResultsMessage.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_ValidationResultsMessage proto.InternalMessageInfo
+
+func (m *ValidationResultsMessage) GetSeqNum() uint64 {
+	if m != nil {
+		return m.SeqNum
+	}
+	return 0
+}
+
+func (m *ValidationResultsMessage) GetTxFlags() []byte {
+	if m != nil {
+		return m.TxFlags
+	}
+	return nil
+}
+
+func (m *ValidationResultsMessage) GetSignature() []byte {
+	if m != nil {
+		return m.Signature
+	}
+	return nil
+}
+
+func (m *ValidationResultsMessage) GetIdentity() []byte {
+	if m != nil {
+		return m.Identity
+	}
+	return nil
+}
+
+// RemoteCollDataRequest message used to request
+// collection data
+type RemoteCollDataRequest struct {
+	Nonce                uint64            `protobuf:"varint,1,opt,name=nonce,proto3" json:"nonce,omitempty"`
+	Digests              []*CollDataDigest `protobuf:"bytes,2,rep,name=digests,proto3" json:"digests,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}          `json:"-"`
+	XXX_unrecognized     []byte            `json:"-"`
+	XXX_sizecache        int32             `json:"-"`
+}
+
+func (m *RemoteCollDataRequest) Reset()         { *m = RemoteCollDataRequest{} }
+func (m *RemoteCollDataRequest) String() string { return proto.CompactTextString(m) }
+func (*RemoteCollDataRequest) ProtoMessage()    {}
+func (*RemoteCollDataRequest) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{35}
+}
+func (m *RemoteCollDataRequest) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_RemoteCollDataRequest.Unmarshal(m, b)
+}
+func (m *RemoteCollDataRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_RemoteCollDataRequest.Marshal(b, m, deterministic)
+}
+func (dst *RemoteCollDataRequest) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_RemoteCollDataRequest.Merge(dst, src)
+}
+func (m *RemoteCollDataRequest) XXX_Size() int {
+	return xxx_messageInfo_RemoteCollDataRequest.Size(m)
+}
+func (m *RemoteCollDataRequest) XXX_DiscardUnknown() {
+	xxx_messageInfo_RemoteCollDataRequest.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_RemoteCollDataRequest proto.InternalMessageInfo
+
+func (m *RemoteCollDataRequest) GetNonce() uint64 {
+	if m != nil {
+		return m.Nonce
+	}
+	return 0
+}
+
+func (m *RemoteCollDataRequest) GetDigests() []*CollDataDigest {
+	if m != nil {
+		return m.Digests
+	}
+	return nil
+}
+
+// CollDataDigest defines a digest of collection data
+type CollDataDigest struct {
+	Namespace            string   `protobuf:"bytes,1,opt,name=namespace,proto3" json:"namespace,omitempty"`
+	Collection           string   `protobuf:"bytes,2,opt,name=collection,proto3" json:"collection,omitempty"`
+	Key                  string   `protobuf:"bytes,3,opt,name=key,proto3" json:"key,omitempty"`
+	EndorsedAtTxID       string   `protobuf:"bytes,4,opt,name=endorsedAtTxID,proto3" json:"endorsedAtTxID,omitempty"`
+	XXX_NoUnkeyedLiteral struct{} `json:"-"`
+	XXX_unrecognized     []byte   `json:"-"`
+	XXX_sizecache        int32    `json:"-"`
+}
+
+func (m *CollDataDigest) Reset()         { *m = CollDataDigest{} }
+func (m *CollDataDigest) String() string { return proto.CompactTextString(m) }
+func (*CollDataDigest) ProtoMessage()    {}
+func (*CollDataDigest) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{36}
+}
+func (m *CollDataDigest) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_CollDataDigest.Unmarshal(m, b)
+}
+func (m *CollDataDigest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_CollDataDigest.Marshal(b, m, deterministic)
+}
+func (dst *CollDataDigest) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_CollDataDigest.Merge(dst, src)
+}
+func (m *CollDataDigest) XXX_Size() int {
+	return xxx_messageInfo_CollDataDigest.Size(m)
+}
+func (m *CollDataDigest) XXX_DiscardUnknown() {
+	xxx_messageInfo_CollDataDigest.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_CollDataDigest proto.InternalMessageInfo
+
+func (m *CollDataDigest) GetNamespace() string {
+	if m != nil {
+		return m.Namespace
+	}
+	return ""
+}
+
+func (m *CollDataDigest) GetCollection() string {
+	if m != nil {
+		return m.Collection
+	}
+	return ""
+}
+
+func (m *CollDataDigest) GetKey() string {
+	if m != nil {
+		return m.Key
+	}
+	return ""
+}
+
+func (m *CollDataDigest) GetEndorsedAtTxID() string {
+	if m != nil {
+		return m.EndorsedAtTxID
+	}
+	return ""
+}
+
+// RemoteCollDataResponse message used to respond to
+// collection data request
+type RemoteCollDataResponse struct {
+	Nonce                uint64             `protobuf:"varint,1,opt,name=nonce,proto3" json:"nonce,omitempty"`
+	Elements             []*CollDataElement `protobuf:"bytes,2,rep,name=elements,proto3" json:"elements,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
+	XXX_unrecognized     []byte             `json:"-"`
+	XXX_sizecache        int32              `json:"-"`
+}
+
+func (m *RemoteCollDataResponse) Reset()         { *m = RemoteCollDataResponse{} }
+func (m *RemoteCollDataResponse) String() string { return proto.CompactTextString(m) }
+func (*RemoteCollDataResponse) ProtoMessage()    {}
+func (*RemoteCollDataResponse) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{37}
+}
+func (m *RemoteCollDataResponse) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_RemoteCollDataResponse.Unmarshal(m, b)
+}
+func (m *RemoteCollDataResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_RemoteCollDataResponse.Marshal(b, m, deterministic)
+}
+func (dst *RemoteCollDataResponse) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_RemoteCollDataResponse.Merge(dst, src)
+}
+func (m *RemoteCollDataResponse) XXX_Size() int {
+	return xxx_messageInfo_RemoteCollDataResponse.Size(m)
+}
+func (m *RemoteCollDataResponse) XXX_DiscardUnknown() {
+	xxx_messageInfo_RemoteCollDataResponse.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_RemoteCollDataResponse proto.InternalMessageInfo
+
+func (m *RemoteCollDataResponse) GetNonce() uint64 {
+	if m != nil {
+		return m.Nonce
+	}
+	return 0
+}
+
+func (m *RemoteCollDataResponse) GetElements() []*CollDataElement {
+	if m != nil {
+		return m.Elements
+	}
+	return nil
+}
+
+// CollDataElement contains the collection data digest and value
+type CollDataElement struct {
+	Digest               *CollDataDigest      `protobuf:"bytes,1,opt,name=digest,proto3" json:"digest,omitempty"`
+	Value                []byte               `protobuf:"bytes,2,opt,name=value,proto3" json:"value,omitempty"`
+	ExpiryTime           *timestamp.Timestamp `protobuf:"bytes,3,opt,name=expiryTime,proto3" json:"expiryTime,omitempty"`
+	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
+	XXX_unrecognized     []byte               `json:"-"`
+	XXX_sizecache        int32                `json:"-"`
+}
+
+func (m *CollDataElement) Reset()         { *m = CollDataElement{} }
+func (m *CollDataElement) String() string { return proto.CompactTextString(m) }
+func (*CollDataElement) ProtoMessage()    {}
+func (*CollDataElement) Descriptor() ([]byte, []int) {
+	return fileDescriptor_message_9b7c5a83d1e435d7, []int{38}
+}
+func (m *CollDataElement) XXX_Unmarshal(b []byte) error {
+	return xxx_messageInfo_CollDataElement.Unmarshal(m, b)
+}
+func (m *CollDataElement) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
+	return xxx_messageInfo_CollDataElement.Marshal(b, m, deterministic)
+}
+func (dst *CollDataElement) XXX_Merge(src proto.Message) {
+	xxx_messageInfo_CollDataElement.Merge(dst, src)
+}
+func (m *CollDataElement) XXX_Size() int {
+	return xxx_messageInfo_CollDataElement.Size(m)
+}
+func (m *CollDataElement) XXX_DiscardUnknown() {
+	xxx_messageInfo_CollDataElement.DiscardUnknown(m)
+}
+
+var xxx_messageInfo_CollDataElement proto.InternalMessageInfo
+
+func (m *CollDataElement) GetDigest() *CollDataDigest {
+	if m != nil {
+		return m.Digest
+	}
+	return nil
+}
+
+func (m *CollDataElement) GetValue() []byte {
+	if m != nil {
+		return m.Value
+	}
+	return nil
+}
+
+func (m *CollDataElement) GetExpiryTime() *timestamp.Timestamp {
+	if m != nil {
+		return m.ExpiryTime
+	}
+	return nil
+}
+
 func init() {
 	proto.RegisterType((*Envelope)(nil), "gossip.Envelope")
 	proto.RegisterType((*SecretEnvelope)(nil), "gossip.SecretEnvelope")
@@ -2672,6 +3016,11 @@ func init() {
 	proto.RegisterType((*PvtDataPayload)(nil), "gossip.PvtDataPayload")
 	proto.RegisterType((*Acknowledgement)(nil), "gossip.Acknowledgement")
 	proto.RegisterType((*Chaincode)(nil), "gossip.Chaincode")
+	proto.RegisterType((*ValidationResultsMessage)(nil), "gossip.ValidationResultsMessage")
+	proto.RegisterType((*RemoteCollDataRequest)(nil), "gossip.RemoteCollDataRequest")
+	proto.RegisterType((*CollDataDigest)(nil), "gossip.CollDataDigest")
+	proto.RegisterType((*RemoteCollDataResponse)(nil), "gossip.RemoteCollDataResponse")
+	proto.RegisterType((*CollDataElement)(nil), "gossip.CollDataElement")
 	proto.RegisterEnum("gossip.PullMsgType", PullMsgType_name, PullMsgType_value)
 	proto.RegisterEnum("gossip.GossipMessage_Tag", GossipMessage_Tag_name, GossipMessage_Tag_value)
 }
@@ -2818,126 +3167,140 @@ var _Gossip_serviceDesc = grpc.ServiceDesc{
 	Metadata: "gossip/message.proto",
 }
 
-func init() { proto.RegisterFile("gossip/message.proto", fileDescriptor_message_7c42328ef5ef9997) }
-
-var fileDescriptor_message_7c42328ef5ef9997 = []byte{
-	// 1874 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x58, 0x5b, 0x6f, 0xe3, 0xc6,
-	0x15, 0x16, 0x6d, 0x5d, 0x8f, 0x2e, 0x96, 0xc7, 0xde, 0x5d, 0xc6, 0x49, 0x13, 0x87, 0xed, 0x26,
-	0xdb, 0x7a, 0x23, 0x6f, 0x9d, 0x16, 0x0d, 0x90, 0xb6, 0x0b, 0x5b, 0x52, 0x2c, 0x21, 0x2b, 0xad,
-	0x4b, 0x7b, 0xd1, 0xba, 0x2f, 0xc4, 0x98, 0x1c, 0x53, 0xac, 0xc9, 0x21, 0xcd, 0x19, 0x3b, 0xf6,
-	0x63, 0xd1, 0x87, 0x00, 0x7d, 0xe9, 0x6f, 0xe8, 0x53, 0xff, 0x66, 0x31, 0x33, 0xbc, 0x4a, 0xf6,
-	0x02, 0x1b, 0x20, 0x6f, 0x3c, 0xf7, 0x99, 0x33, 0x67, 0xbe, 0x73, 0x86, 0xb0, 0xed, 0x86, 0x8c,
-	0x79, 0xd1, 0x7e, 0x40, 0x18, 0xc3, 0x2e, 0x19, 0x44, 0x71, 0xc8, 0x43, 0x54, 0x57, 0xdc, 0x9d,
-	0x67, 0x76, 0x18, 0x04, 0x21, 0xdd, 0xb7, 0x43, 0xdf, 0x27, 0x36, 0xf7, 0x42, 0xaa, 0x14, 0x8c,
-	0x7f, 0x69, 0xd0, 0x1c, 0xd3, 0x5b, 0xe2, 0x87, 0x11, 0x41, 0x3a, 0x34, 0x22, 0x7c, 0xef, 0x87,
-	0xd8, 0xd1, 0xb5, 0x5d, 0xed, 0x45, 0xc7, 0x4c, 0x49, 0xf4, 0x09, 0xb4, 0x98, 0xe7, 0x52, 0xcc,
-	0x6f, 0x62, 0xa2, 0xaf, 0x49, 0x59, 0xce, 0x40, 0xaf, 0x61, 0x83, 0x11, 0x3b, 0x26, 0xdc, 0x22,
-	0x89, 0x2b, 0x7d, 0x7d, 0x57, 0x7b, 0xd1, 0x3e, 0x78, 0x3a, 0x50, 0xf1, 0x07, 0xa7, 0x52, 0x9c,
-	0x06, 0x32, 0x7b, 0xac, 0x44, 0x1b, 0x13, 0xe8, 0x95, 0x35, 0x7e, 0xea, 0x52, 0x8c, 0x43, 0xa8,
-	0x2b, 0x4f, 0xe8, 0x25, 0xf4, 0x3d, 0xca, 0x49, 0x4c, 0xb1, 0x3f, 0xa6, 0x4e, 0x14, 0x7a, 0x94,
-	0x4b, 0x57, 0xad, 0x49, 0xc5, 0x5c, 0x91, 0x1c, 0xb5, 0xa0, 0x61, 0x87, 0x94, 0x13, 0xca, 0x8d,
-	0x1f, 0xdb, 0xd0, 0x3d, 0x96, 0xcb, 0x9e, 0xa9, 0x5c, 0xa2, 0x6d, 0xa8, 0xd1, 0x90, 0xda, 0x44,
-	0xda, 0x57, 0x4d, 0x45, 0x88, 0x25, 0xda, 0x0b, 0x4c, 0x29, 0xf1, 0x93, 0x65, 0xa4, 0x24, 0xda,
-	0x83, 0x75, 0x8e, 0x5d, 0x99, 0x83, 0xde, 0xc1, 0x47, 0x69, 0x0e, 0x4a, 0x3e, 0x07, 0x67, 0xd8,
-	0x35, 0x85, 0x16, 0xfa, 0x1a, 0x5a, 0xd8, 0xf7, 0x6e, 0x89, 0x15, 0x30, 0x57, 0xaf, 0xc9, 0xb4,
-	0x6d, 0xa7, 0x26, 0x87, 0x42, 0x90, 0x58, 0x4c, 0x2a, 0x66, 0x53, 0x2a, 0xce, 0x98, 0x8b, 0x7e,
-	0x07, 0x8d, 0x80, 0x04, 0x56, 0x4c, 0xae, 0xf5, 0xba, 0x34, 0xc9, 0xa2, 0xcc, 0x48, 0x70, 0x41,
-	0x62, 0xb6, 0xf0, 0x22, 0x93, 0x5c, 0xdf, 0x10, 0xc6, 0x27, 0x15, 0xb3, 0x1e, 0x90, 0xc0, 0x24,
-	0xd7, 0xe8, 0xf7, 0xa9, 0x15, 0xd3, 0x1b, 0xd2, 0x6a, 0xe7, 0x21, 0x2b, 0x16, 0x85, 0x94, 0x91,
-	0xcc, 0x8c, 0xa1, 0x57, 0xd0, 0x74, 0x30, 0xc7, 0x72, 0x81, 0x4d, 0x69, 0xb7, 0x95, 0xda, 0x8d,
-	0x30, 0xc7, 0xf9, 0xfa, 0x1a, 0x42, 0x4d, 0x2c, 0x6f, 0x0f, 0x6a, 0x0b, 0xe2, 0xfb, 0xa1, 0xde,
-	0x2a, 0xab, 0xab, 0x14, 0x4c, 0x84, 0x68, 0x52, 0x31, 0x95, 0x0e, 0xda, 0x4f, 0xdc, 0x3b, 0x9e,
-	0xab, 0x83, 0xd4, 0x47, 0x45, 0xf7, 0x23, 0xcf, 0x55, 0xbb, 0x90, 0xde, 0x47, 0x9e, 0x9b, 0xad,
-	0x47, 0xec, 0xbe, 0xbd, 0xba, 0x9e, 0x7c, 0xdf, 0xd2, 0x42, 0x6d, 0xbc, 0x2d, 0x2d, 0x6e, 0x22,
-	0x07, 0x73, 0xa2, 0x77, 0x56, 0xa3, 0xbc, 0x93, 0x92, 0x49, 0xc5, 0x04, 0x27, 0xa3, 0xd0, 0x73,
-	0xa8, 0x91, 0x20, 0xe2, 0xf7, 0x7a, 0x57, 0x1a, 0x74, 0x53, 0x83, 0xb1, 0x60, 0x8a, 0x0d, 0x48,
-	0x29, 0xda, 0x83, 0xaa, 0x1d, 0x52, 0xaa, 0xf7, 0xa4, 0xd6, 0x93, 0x54, 0x6b, 0x18, 0x52, 0x3a,
-	0x66, 0x1c, 0x5f, 0xf8, 0x1e, 0x5b, 0x4c, 0x2a, 0xa6, 0x54, 0x42, 0x07, 0x00, 0x8c, 0x63, 0x4e,
-	0x2c, 0x8f, 0x5e, 0x86, 0xfa, 0x86, 0x34, 0xd9, 0xcc, 0xae, 0x89, 0x90, 0x4c, 0xe9, 0xa5, 0xc8,
-	0x4e, 0x8b, 0xa5, 0x04, 0x3a, 0x82, 0x9e, 0xb2, 0x61, 0x14, 0x47, 0x6c, 0x11, 0x72, 0xbd, 0x5f,
-	0x3e, 0xf4, 0xcc, 0xee, 0x34, 0x51, 0x98, 0x54, 0xcc, 0xae, 0x34, 0x49, 0x19, 0x68, 0x06, 0x5b,
-	0x79, 0x5c, 0x2b, 0xba, 0xf1, 0x7d, 0x99, 0xbf, 0x4d, 0xe9, 0xe8, 0x93, 0x15, 0x47, 0x27, 0x37,
-	0xbe, 0x9f, 0x27, 0xb2, 0xcf, 0x96, 0xf8, 0xe8, 0x10, 0x94, 0x7f, 0xe1, 0x44, 0x28, 0xe9, 0xa8,
-	0x5c, 0x50, 0x26, 0x09, 0x42, 0x4e, 0xa4, 0xbb, 0xdc, 0x4d, 0x87, 0x15, 0x68, 0x34, 0x4a, 0x77,
-	0x15, 0x27, 0x25, 0xa7, 0x6f, 0x49, 0x1f, 0x1f, 0x3f, 0xe8, 0x23, 0xab, 0xca, 0x2e, 0x2b, 0x32,
-	0x44, 0x6e, 0x7c, 0x82, 0x1d, 0x55, 0xbc, 0xb2, 0x44, 0xb7, 0xcb, 0xb9, 0x79, 0x93, 0x49, 0xf3,
-	0x42, 0xed, 0xe6, 0x26, 0xa2, 0x5c, 0xbf, 0x85, 0x6e, 0x44, 0x48, 0x6c, 0x79, 0x0e, 0xa1, 0xdc,
-	0xe3, 0xf7, 0xfa, 0x93, 0xf2, 0x35, 0x3c, 0x21, 0x24, 0x9e, 0x26, 0x32, 0xb1, 0x8d, 0xa8, 0x40,
-	0x8b, 0xcb, 0x8e, 0xed, 0x2b, 0xfd, 0xa9, 0x34, 0x79, 0x96, 0xdd, 0x5c, 0xfb, 0x8a, 0x86, 0x3f,
-	0xf8, 0xc4, 0x71, 0x49, 0x40, 0xa8, 0xd8, 0xbc, 0xd0, 0x42, 0x7f, 0x06, 0x88, 0x62, 0xef, 0x56,
-	0x65, 0x41, 0x7f, 0x56, 0x4e, 0xbe, 0xda, 0xef, 0xc9, 0x2d, 0x2f, 0x57, 0x71, 0xc1, 0x02, 0xbd,
-	0x2e, 0xd8, 0x33, 0x5d, 0x97, 0xf6, 0xbf, 0x78, 0xc4, 0x3e, 0xcb, 0x58, 0xc1, 0x04, 0xbd, 0x86,
-	0x4e, 0x42, 0x59, 0xa2, 0xd0, 0xf5, 0x8f, 0xca, 0xc7, 0x76, 0xa2, 0x64, 0xe5, 0x6b, 0xdd, 0x8e,
-	0x72, 0xae, 0x61, 0xc1, 0xfa, 0x19, 0x76, 0x51, 0x17, 0x5a, 0xef, 0xe6, 0xa3, 0xf1, 0x77, 0xd3,
-	0xf9, 0x78, 0xd4, 0xaf, 0xa0, 0x16, 0xd4, 0xc6, 0xb3, 0x93, 0xb3, 0xf3, 0xbe, 0x86, 0x3a, 0xd0,
-	0x7c, 0x6b, 0x1e, 0x5b, 0x6f, 0xe7, 0x6f, 0xce, 0xfb, 0x6b, 0x42, 0x6f, 0x38, 0x39, 0x9c, 0x2b,
-	0x72, 0x1d, 0xf5, 0xa1, 0x23, 0xc9, 0xc3, 0xf9, 0xc8, 0x7a, 0x6b, 0x1e, 0xf7, 0xab, 0x68, 0x03,
-	0xda, 0x4a, 0xc1, 0x94, 0x8c, 0x5a, 0x11, 0x89, 0xff, 0xa7, 0x41, 0x2b, 0xab, 0x48, 0x34, 0x80,
-	0x16, 0xf7, 0x02, 0xc2, 0x38, 0x0e, 0x22, 0x89, 0xb8, 0xed, 0x83, 0x7e, 0xf1, 0x84, 0xce, 0xbc,
-	0x80, 0x98, 0xb9, 0x0a, 0x7a, 0x02, 0xf5, 0xe8, 0xca, 0xb3, 0x3c, 0x47, 0x02, 0x71, 0xc7, 0xac,
-	0x45, 0x57, 0xde, 0xd4, 0x41, 0x9f, 0x41, 0x3b, 0xc1, 0x69, 0x6b, 0x76, 0x38, 0xd4, 0xab, 0x52,
-	0x06, 0x09, 0x6b, 0x76, 0x38, 0x14, 0x37, 0x34, 0x8a, 0xc3, 0x88, 0xc4, 0xdc, 0x23, 0x2c, 0x41,
-	0x64, 0x94, 0x27, 0x28, 0x95, 0x98, 0x05, 0x2d, 0xe3, 0x47, 0x0d, 0x20, 0x17, 0xa1, 0x5f, 0x42,
-	0x57, 0x1e, 0x7d, 0x6c, 0x2d, 0x88, 0xe7, 0x2e, 0x78, 0xd2, 0x38, 0x3a, 0x8a, 0x39, 0x91, 0x3c,
-	0xf4, 0x39, 0x74, 0x7c, 0x72, 0xc9, 0xad, 0x62, 0x13, 0x69, 0x9a, 0x6d, 0xc1, 0x1b, 0x26, 0x8d,
-	0xe4, 0xb7, 0x20, 0x16, 0xe6, 0x51, 0x3b, 0x74, 0x08, 0xd3, 0xd7, 0x77, 0xd7, 0x8b, 0x60, 0x31,
-	0x4c, 0x25, 0x66, 0x41, 0xc9, 0x38, 0x84, 0xcd, 0x15, 0x34, 0x40, 0x2f, 0xa1, 0x49, 0x7c, 0x59,
-	0x88, 0x4c, 0xd7, 0xa4, 0x97, 0x2c, 0x73, 0x59, 0x4f, 0xce, 0x34, 0x8c, 0x3f, 0xc0, 0xf6, 0x43,
-	0x38, 0xb0, 0x9c, 0x39, 0x6d, 0x39, 0x73, 0xc6, 0x25, 0x74, 0x4b, 0xa0, 0x57, 0x38, 0x02, 0xad,
-	0x78, 0x04, 0x3b, 0xd0, 0xcc, 0xae, 0x9a, 0x6a, 0x9d, 0x19, 0x8d, 0x0c, 0xe8, 0x72, 0x9f, 0x59,
-	0x36, 0x89, 0xb9, 0xb5, 0xc0, 0x6c, 0x91, 0x1c, 0x5e, 0x9b, 0xfb, 0x6c, 0x48, 0x62, 0x3e, 0xc1,
-	0x6c, 0x61, 0xbc, 0x83, 0x4e, 0xf1, 0x4a, 0x3e, 0x16, 0x06, 0x41, 0x55, 0xb8, 0x49, 0x42, 0xc8,
-	0x6f, 0x11, 0x3a, 0x20, 0x1c, 0xcb, 0xda, 0x57, 0x9e, 0x33, 0xda, 0x08, 0xa0, 0x5d, 0xb8, 0x79,
-	0x8f, 0x77, 0x7d, 0x47, 0x76, 0x24, 0xa6, 0xaf, 0xed, 0xae, 0x8b, 0xae, 0x9f, 0x90, 0x68, 0x00,
-	0xcd, 0x80, 0xb9, 0x16, 0xbf, 0x4f, 0xc6, 0x9f, 0x5e, 0xde, 0x96, 0x44, 0x16, 0x67, 0xcc, 0x3d,
-	0xbb, 0x8f, 0x88, 0xd9, 0x08, 0xd4, 0x87, 0x11, 0x42, 0xbb, 0xd0, 0x0f, 0x1f, 0x09, 0x57, 0x5c,
-	0xef, 0x5a, 0x79, 0xbd, 0x1f, 0x1c, 0xf0, 0x0e, 0x20, 0x6f, 0x75, 0x8f, 0xc4, 0xfb, 0x15, 0x54,
-	0x93, 0x58, 0x0f, 0x57, 0x49, 0xf5, 0x27, 0x45, 0xf6, 0x55, 0x64, 0xd5, 0xca, 0x7f, 0xf6, 0xc4,
-	0x7e, 0xa3, 0xce, 0x31, 0x9d, 0xde, 0x7e, 0x5d, 0x1e, 0x25, 0xdb, 0x07, 0x1b, 0x99, 0xb5, 0x62,
-	0x67, 0xb3, 0xa5, 0xf1, 0x1d, 0xa0, 0x55, 0x04, 0x44, 0xaf, 0x96, 0x1d, 0x3c, 0x5d, 0x82, 0xcb,
-	0x15, 0x3f, 0xe7, 0xd0, 0x48, 0x78, 0xe8, 0x19, 0x34, 0x18, 0xb9, 0xb6, 0xe8, 0x4d, 0x90, 0x6c,
-	0xb7, 0xce, 0xc8, 0xf5, 0xfc, 0x26, 0x10, 0xd5, 0x59, 0x38, 0x55, 0x95, 0xd7, 0xcf, 0x97, 0xd0,
-	0x79, 0x5d, 0x26, 0xa2, 0x84, 0xbf, 0xff, 0x59, 0x83, 0x5e, 0x39, 0x2c, 0xfa, 0x12, 0x36, 0xf2,
-	0xb9, 0xde, 0xa2, 0x38, 0x50, 0x99, 0x6d, 0x99, 0xbd, 0x9c, 0x3d, 0xc7, 0x01, 0x11, 0xa3, 0xb3,
-	0x90, 0xb2, 0x08, 0xdb, 0x6a, 0x74, 0x6e, 0x99, 0x39, 0x03, 0x6d, 0x41, 0x8d, 0xdf, 0xa5, 0x70,
-	0xd9, 0x32, 0xab, 0xfc, 0x6e, 0xea, 0x08, 0x24, 0x4b, 0x57, 0x14, 0xff, 0xc0, 0x08, 0x4f, 0xf0,
-	0x32, 0x5d, 0xa6, 0x29, 0x78, 0xe8, 0x25, 0xa0, 0x54, 0x89, 0x79, 0x41, 0x8a, 0x79, 0x35, 0xb9,
-	0xdd, 0x7e, 0x22, 0x39, 0xf5, 0x82, 0x04, 0xf7, 0xe6, 0x80, 0x0a, 0xcb, 0xb5, 0x43, 0x7a, 0xe9,
-	0xb9, 0x2c, 0x19, 0x63, 0x3f, 0x1b, 0xa8, 0x87, 0xca, 0x60, 0x98, 0x69, 0x0c, 0xa5, 0xc2, 0x09,
-	0xb6, 0xaf, 0xb0, 0x4b, 0xcc, 0x4d, 0x7b, 0x49, 0xc0, 0x8c, 0x7f, 0x6b, 0xd0, 0x29, 0x0e, 0xca,
-	0x68, 0x00, 0x10, 0x64, 0xf3, 0x6c, 0x72, 0x64, 0xbd, 0xf2, 0xa4, 0x6b, 0x16, 0x34, 0x3e, 0xb8,
-	0xb1, 0x14, 0xe1, 0xab, 0x5a, 0x86, 0x2f, 0xe3, 0x9f, 0x1a, 0x6c, 0xae, 0x4c, 0x1c, 0x8f, 0x01,
-	0xd4, 0x87, 0x06, 0x7e, 0x0e, 0x3d, 0x8f, 0x59, 0x0e, 0xb1, 0x7d, 0x1c, 0x63, 0x91, 0x02, 0x79,
-	0x54, 0x4d, 0xb3, 0xeb, 0xb1, 0x51, 0xce, 0x34, 0xfe, 0x08, 0xcd, 0xd4, 0x5a, 0x94, 0x9f, 0x47,
-	0xed, 0x62, 0xf9, 0x79, 0xd4, 0x16, 0xe5, 0x57, 0xa8, 0xcb, 0xb5, 0x62, 0x5d, 0x1a, 0x97, 0xb0,
-	0xb9, 0xf2, 0x86, 0x40, 0xdf, 0x42, 0x9f, 0x11, 0xff, 0x52, 0x0e, 0x8f, 0x71, 0xa0, 0x62, 0x6b,
-	0xe5, 0x05, 0x67, 0x10, 0xb1, 0x21, 0x34, 0xa7, 0xb9, 0xa2, 0xb8, 0xef, 0x62, 0x18, 0xa2, 0xc9,
-	0xbd, 0x56, 0x84, 0x71, 0x01, 0x68, 0xf5, 0xd5, 0x81, 0xbe, 0x80, 0x9a, 0x7c, 0xe4, 0x3c, 0xda,
-	0xa6, 0x94, 0x58, 0xe2, 0x14, 0xc1, 0xce, 0x7b, 0x70, 0x8a, 0x60, 0xc7, 0xf8, 0x2b, 0xd4, 0x55,
-	0x0c, 0x71, 0x66, 0xa4, 0xf4, 0x0a, 0x34, 0x33, 0xfa, 0xbd, 0x18, 0xfb, 0xf0, 0x10, 0x61, 0x34,
-	0xa0, 0x26, 0x1f, 0x01, 0xc6, 0xdf, 0x00, 0xad, 0x8e, 0xba, 0xa2, 0x89, 0x31, 0x8e, 0x63, 0x6e,
-	0x95, 0xaf, 0x7e, 0x5b, 0x32, 0x4f, 0xd5, 0xfd, 0xff, 0x14, 0xda, 0x84, 0x3a, 0x56, 0xf9, 0x10,
-	0x5a, 0x84, 0x3a, 0x4a, 0x6e, 0x1c, 0xc1, 0xd6, 0x03, 0x03, 0x30, 0xda, 0x83, 0x66, 0x82, 0x32,
-	0x69, 0x2b, 0x5f, 0x81, 0xb3, 0x4c, 0xc1, 0x38, 0x86, 0xed, 0x87, 0x86, 0x4a, 0xb4, 0x9f, 0x63,
-	0xad, 0xf2, 0x91, 0x3d, 0x5a, 0x12, 0x45, 0x85, 0xd4, 0x19, 0x04, 0x1b, 0xff, 0xd5, 0xa0, 0x5b,
-	0x12, 0xe5, 0x68, 0xa1, 0x15, 0xd0, 0xe2, 0xfd, 0x00, 0xf3, 0x29, 0x40, 0x7e, 0x7b, 0x13, 0x94,
-	0x29, 0x70, 0xd0, 0xc7, 0xd0, 0xba, 0xf0, 0x43, 0xfb, 0x4a, 0xe4, 0x44, 0x5e, 0xac, 0xaa, 0xd9,
-	0x94, 0x8c, 0x53, 0x72, 0x8d, 0x76, 0xa1, 0x23, 0x52, 0xe5, 0x51, 0x4b, 0xb2, 0x12, 0x74, 0x01,
-	0x46, 0xae, 0xa7, 0xf4, 0x48, 0x70, 0x8c, 0xef, 0xe1, 0xc9, 0x83, 0x13, 0x30, 0x3a, 0x58, 0x99,
-	0x7e, 0x9e, 0x2e, 0x6d, 0x77, 0xac, 0xc4, 0x85, 0x19, 0xe8, 0x1c, 0x7a, 0x65, 0x19, 0xfa, 0x0a,
-	0xea, 0x2a, 0x1b, 0x49, 0xe1, 0x3f, 0x92, 0xb2, 0x44, 0xa9, 0xf8, 0x03, 0x23, 0x69, 0x67, 0x69,
-	0x73, 0xf8, 0x4b, 0xe6, 0x3a, 0x05, 0xf0, 0xe7, 0xb0, 0xc1, 0xef, 0xac, 0xd2, 0xf6, 0x92, 0x81,
-	0x91, 0xdf, 0x9d, 0x66, 0x1b, 0x2c, 0xbb, 0x2c, 0xfe, 0x13, 0x31, 0xbe, 0x84, 0x8d, 0xa5, 0x07,
-	0x87, 0xb8, 0x74, 0x24, 0x8e, 0xc3, 0x38, 0x39, 0x1f, 0x45, 0x18, 0xef, 0xa0, 0x95, 0x8d, 0x8d,
-	0xa2, 0x03, 0x15, 0x9a, 0x85, 0xfc, 0x16, 0x31, 0x6e, 0x49, 0xcc, 0xc4, 0x01, 0xa9, 0xf3, 0x4b,
-	0xc9, 0xf7, 0x4d, 0x4e, 0xbf, 0xf9, 0x13, 0xb4, 0x0b, 0x9d, 0x78, 0xf9, 0x71, 0xd0, 0x85, 0xd6,
-	0xd1, 0x9b, 0xb7, 0xc3, 0xef, 0xad, 0xd9, 0xe9, 0x71, 0x5f, 0x13, 0x6f, 0x80, 0xe9, 0x68, 0x3c,
-	0x3f, 0x9b, 0x9e, 0x9d, 0x4b, 0xce, 0xda, 0xc1, 0x3f, 0xa0, 0xae, 0x26, 0x21, 0xf4, 0x0d, 0x74,
-	0xd4, 0xd7, 0x29, 0x8f, 0x09, 0x0e, 0xd0, 0xca, 0xc5, 0xde, 0x59, 0xe1, 0x18, 0x95, 0x17, 0xda,
-	0x2b, 0x0d, 0x7d, 0x01, 0xd5, 0x13, 0x8f, 0xba, 0xa8, 0xfc, 0x48, 0xdf, 0x29, 0x93, 0x46, 0xe5,
-	0xe8, 0xab, 0xbf, 0xef, 0xb9, 0x1e, 0x5f, 0xdc, 0x5c, 0x88, 0x4e, 0xb3, 0xbf, 0xb8, 0x8f, 0x48,
-	0xac, 0xa6, 0xf2, 0xfd, 0x4b, 0x7c, 0x11, 0x7b, 0xf6, 0xbe, 0xfc, 0x2f, 0xc6, 0xf6, 0x95, 0xd9,
-	0x45, 0x5d, 0x92, 0x5f, 0xff, 0x3f, 0x00, 0x00, 0xff, 0xff, 0xdd, 0x1d, 0xb3, 0x7e, 0x5f, 0x13,
-	0x00, 0x00,
+func init() { proto.RegisterFile("gossip/message.proto", fileDescriptor_message_9b7c5a83d1e435d7) }
+
+var fileDescriptor_message_9b7c5a83d1e435d7 = []byte{
+	// 2106 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x18, 0x5b, 0x73, 0xdb, 0x58,
+	0x39, 0x4a, 0xec, 0xd8, 0xfe, 0x7c, 0x89, 0x73, 0x9a, 0xb6, 0xda, 0xec, 0xd2, 0x76, 0x05, 0xed,
+	0x16, 0xda, 0x75, 0x4a, 0x0a, 0xc3, 0x0e, 0x0b, 0x74, 0x1c, 0x3b, 0xad, 0x3d, 0xdb, 0xa4, 0x41,
+	0x49, 0x81, 0x2e, 0x0f, 0x1a, 0x45, 0x3a, 0x96, 0x45, 0x74, 0x8b, 0xce, 0x71, 0x36, 0x79, 0x64,
+	0x78, 0x58, 0x86, 0x07, 0x78, 0xe0, 0x17, 0xf0, 0xc4, 0xdf, 0x64, 0xce, 0x45, 0xd2, 0x91, 0x2f,
+	0x99, 0xe9, 0xce, 0xf0, 0xa6, 0xef, 0x7a, 0xbe, 0xef, 0x3b, 0xdf, 0xed, 0x08, 0x76, 0xbc, 0x98,
+	0x10, 0x3f, 0xd9, 0x0b, 0x31, 0x21, 0xb6, 0x87, 0x7b, 0x49, 0x1a, 0xd3, 0x18, 0x6d, 0x0a, 0xec,
+	0xee, 0x7d, 0x27, 0x0e, 0xc3, 0x38, 0xda, 0x73, 0xe2, 0x20, 0xc0, 0x0e, 0xf5, 0xe3, 0x48, 0x30,
+	0xec, 0x3e, 0xf4, 0xe2, 0xd8, 0x0b, 0xf0, 0x1e, 0x87, 0xce, 0x67, 0x93, 0x3d, 0xea, 0x87, 0x98,
+	0x50, 0x3b, 0x4c, 0x04, 0x83, 0xf1, 0x37, 0x0d, 0xea, 0x87, 0xd1, 0x15, 0x0e, 0xe2, 0x04, 0x23,
+	0x1d, 0x6a, 0x89, 0x7d, 0x13, 0xc4, 0xb6, 0xab, 0x6b, 0x8f, 0xb4, 0xa7, 0x2d, 0x33, 0x03, 0xd1,
+	0x67, 0xd0, 0x20, 0xbe, 0x17, 0xd9, 0x74, 0x96, 0x62, 0x7d, 0x9d, 0xd3, 0x0a, 0x04, 0x7a, 0x05,
+	0x5b, 0x04, 0x3b, 0x29, 0xa6, 0x16, 0x96, 0xaa, 0xf4, 0x8d, 0x47, 0xda, 0xd3, 0xe6, 0xfe, 0xbd,
+	0x9e, 0x30, 0xb0, 0x77, 0xca, 0xc9, 0xd9, 0x41, 0x66, 0x87, 0x94, 0x60, 0x63, 0x04, 0x9d, 0x32,
+	0xc7, 0x0f, 0x35, 0xc5, 0xe8, 0xc3, 0xa6, 0xd0, 0x84, 0x9e, 0x43, 0xd7, 0x8f, 0x28, 0x4e, 0x23,
+	0x3b, 0x38, 0x8c, 0xdc, 0x24, 0xf6, 0x23, 0xca, 0x55, 0x35, 0x46, 0x6b, 0xe6, 0x02, 0xe5, 0xa0,
+	0x01, 0x35, 0x27, 0x8e, 0x28, 0x8e, 0xa8, 0xf1, 0xcf, 0x16, 0xb4, 0xdf, 0x70, 0xb3, 0x8f, 0x44,
+	0xb0, 0xd1, 0x0e, 0x54, 0xa3, 0x38, 0x72, 0x30, 0x97, 0xaf, 0x98, 0x02, 0x60, 0x26, 0x3a, 0x53,
+	0x3b, 0x8a, 0x70, 0x20, 0xcd, 0xc8, 0x40, 0xf4, 0x0c, 0x36, 0xa8, 0xed, 0xf1, 0x18, 0x74, 0xf6,
+	0x3f, 0xc9, 0x62, 0x50, 0xd2, 0xd9, 0x3b, 0xb3, 0x3d, 0x93, 0x71, 0xa1, 0x97, 0xd0, 0xb0, 0x03,
+	0xff, 0x0a, 0x5b, 0x21, 0xf1, 0xf4, 0x2a, 0x0f, 0xdb, 0x4e, 0x26, 0xd2, 0x67, 0x04, 0x29, 0x31,
+	0x5a, 0x33, 0xeb, 0x9c, 0xf1, 0x88, 0x78, 0xe8, 0x17, 0x50, 0x0b, 0x71, 0x68, 0xa5, 0xf8, 0x52,
+	0xdf, 0xe4, 0x22, 0xf9, 0x29, 0x47, 0x38, 0x3c, 0xc7, 0x29, 0x99, 0xfa, 0x89, 0x89, 0x2f, 0x67,
+	0x98, 0xd0, 0xd1, 0x9a, 0xb9, 0x19, 0xe2, 0xd0, 0xc4, 0x97, 0xe8, 0x97, 0x99, 0x14, 0xd1, 0x6b,
+	0x5c, 0x6a, 0x77, 0x99, 0x14, 0x49, 0xe2, 0x88, 0xe0, 0x5c, 0x8c, 0xa0, 0x17, 0x50, 0x77, 0x6d,
+	0x6a, 0x73, 0x03, 0xeb, 0x5c, 0xee, 0x4e, 0x26, 0x37, 0xb4, 0xa9, 0x5d, 0xd8, 0x57, 0x63, 0x6c,
+	0xcc, 0xbc, 0x67, 0x50, 0x9d, 0xe2, 0x20, 0x88, 0xf5, 0x46, 0x99, 0x5d, 0x84, 0x60, 0xc4, 0x48,
+	0xa3, 0x35, 0x53, 0xf0, 0xa0, 0x3d, 0xa9, 0xde, 0xf5, 0x3d, 0x1d, 0x38, 0x3f, 0x52, 0xd5, 0x0f,
+	0x7d, 0x4f, 0x78, 0xc1, 0xb5, 0x0f, 0x7d, 0x2f, 0xb7, 0x87, 0x79, 0xdf, 0x5c, 0xb4, 0xa7, 0xf0,
+	0x9b, 0x4b, 0x08, 0xc7, 0x9b, 0x5c, 0x62, 0x96, 0xb8, 0x36, 0xc5, 0x7a, 0x6b, 0xf1, 0x94, 0xf7,
+	0x9c, 0x32, 0x5a, 0x33, 0xc1, 0xcd, 0x21, 0xf4, 0x18, 0xaa, 0x38, 0x4c, 0xe8, 0x8d, 0xde, 0xe6,
+	0x02, 0xed, 0x4c, 0xe0, 0x90, 0x21, 0x99, 0x03, 0x9c, 0x8a, 0x9e, 0x41, 0xc5, 0x89, 0xa3, 0x48,
+	0xef, 0x70, 0xae, 0xbb, 0x19, 0xd7, 0x20, 0x8e, 0xa2, 0x43, 0x42, 0xed, 0xf3, 0xc0, 0x27, 0xd3,
+	0xd1, 0x9a, 0xc9, 0x99, 0xd0, 0x3e, 0x00, 0xa1, 0x36, 0xc5, 0x96, 0x1f, 0x4d, 0x62, 0x7d, 0x8b,
+	0x8b, 0x6c, 0xe7, 0x65, 0xc2, 0x28, 0xe3, 0x68, 0xc2, 0xa2, 0xd3, 0x20, 0x19, 0x80, 0x0e, 0xa0,
+	0x23, 0x64, 0x48, 0x64, 0x27, 0x64, 0x1a, 0x53, 0xbd, 0x5b, 0xbe, 0xf4, 0x5c, 0xee, 0x54, 0x32,
+	0x8c, 0xd6, 0xcc, 0x36, 0x17, 0xc9, 0x10, 0xe8, 0x08, 0xee, 0x14, 0xe7, 0x5a, 0xc9, 0x2c, 0x08,
+	0x78, 0xfc, 0xb6, 0xb9, 0xa2, 0xcf, 0x16, 0x14, 0x9d, 0xcc, 0x82, 0xa0, 0x08, 0x64, 0x97, 0xcc,
+	0xe1, 0x51, 0x1f, 0x84, 0x7e, 0xa6, 0x84, 0x31, 0xe9, 0xa8, 0x9c, 0x50, 0x26, 0x0e, 0x63, 0x8a,
+	0xb9, 0xba, 0x42, 0x4d, 0x8b, 0x28, 0x30, 0x1a, 0x66, 0x5e, 0xa5, 0x32, 0xe5, 0xf4, 0x3b, 0x5c,
+	0xc7, 0xa7, 0x4b, 0x75, 0xe4, 0x59, 0xd9, 0x26, 0x2a, 0x82, 0xc5, 0x26, 0xc0, 0xb6, 0x2b, 0x92,
+	0x97, 0xa7, 0xe8, 0x4e, 0x39, 0x36, 0x6f, 0x73, 0x6a, 0x91, 0xa8, 0xed, 0x42, 0x84, 0xa5, 0xeb,
+	0xd7, 0xd0, 0x4e, 0x30, 0x4e, 0x2d, 0xdf, 0xc5, 0x11, 0xf5, 0xe9, 0x8d, 0x7e, 0xb7, 0x5c, 0x86,
+	0x27, 0x18, 0xa7, 0x63, 0x49, 0x63, 0x6e, 0x24, 0x0a, 0xcc, 0x8a, 0xdd, 0x76, 0x2e, 0xf4, 0x7b,
+	0x5c, 0xe4, 0x7e, 0x5e, 0xb9, 0xce, 0x45, 0x14, 0x7f, 0x17, 0x60, 0xd7, 0xc3, 0x21, 0x8e, 0x98,
+	0xf3, 0x8c, 0x0b, 0xfd, 0x0e, 0x20, 0x49, 0xfd, 0x2b, 0x11, 0x05, 0xfd, 0x7e, 0x39, 0xf8, 0xc2,
+	0xdf, 0x93, 0x2b, 0x5a, 0xce, 0x62, 0x45, 0x02, 0xbd, 0x52, 0xe4, 0x89, 0xae, 0x73, 0xf9, 0x1f,
+	0xad, 0x90, 0xcf, 0x23, 0xa6, 0x88, 0xa0, 0x57, 0xd0, 0x92, 0x90, 0xc5, 0x12, 0x5d, 0xff, 0xa4,
+	0x7c, 0x6d, 0x27, 0x82, 0x56, 0x2e, 0xeb, 0x66, 0x52, 0x60, 0x51, 0x1f, 0x9a, 0x6c, 0xca, 0x48,
+	0x13, 0xf5, 0x6f, 0x97, 0x99, 0x30, 0x28, 0x18, 0xa4, 0x0f, 0xaa, 0x0c, 0x3a, 0x50, 0x55, 0x10,
+	0xfd, 0xcf, 0x5c, 0xc5, 0x83, 0x55, 0x2a, 0x72, 0x37, 0x54, 0x21, 0xc3, 0x82, 0x8d, 0x33, 0xdb,
+	0x43, 0x6d, 0x68, 0xbc, 0x3f, 0x1e, 0x1e, 0xbe, 0x1e, 0x1f, 0x1f, 0x0e, 0xbb, 0x6b, 0xa8, 0x01,
+	0xd5, 0xc3, 0xa3, 0x93, 0xb3, 0x0f, 0x5d, 0x0d, 0xb5, 0xa0, 0xfe, 0xce, 0x7c, 0x63, 0xbd, 0x3b,
+	0x7e, 0xfb, 0xa1, 0xbb, 0xce, 0xf8, 0x06, 0xa3, 0xfe, 0xb1, 0x00, 0x37, 0x50, 0x17, 0x5a, 0x1c,
+	0xec, 0x1f, 0x0f, 0xad, 0x77, 0xe6, 0x9b, 0x6e, 0x05, 0x6d, 0x41, 0x53, 0x30, 0x98, 0x1c, 0x51,
+	0x55, 0x07, 0xc2, 0x7f, 0x35, 0x68, 0xe4, 0x85, 0x81, 0x7a, 0xd0, 0xc8, 0x87, 0x28, 0x6f, 0xfc,
+	0xcd, 0xfd, 0xae, 0x9a, 0x28, 0x67, 0x7e, 0x88, 0xcd, 0x82, 0x05, 0xdd, 0x85, 0xcd, 0xe4, 0xc2,
+	0xb7, 0x7c, 0x97, 0xcf, 0x83, 0x96, 0x59, 0x4d, 0x2e, 0xfc, 0xb1, 0x8b, 0x1e, 0x42, 0x53, 0x8e,
+	0x0b, 0xeb, 0xa8, 0x3f, 0xd0, 0x2b, 0x9c, 0x06, 0x12, 0x75, 0xd4, 0x1f, 0xb0, 0x46, 0x91, 0xa4,
+	0x71, 0x82, 0x53, 0xea, 0x63, 0x22, 0x07, 0x03, 0x2a, 0xee, 0x29, 0xa3, 0x98, 0x0a, 0x97, 0xf1,
+	0xbd, 0x06, 0x50, 0x90, 0xd0, 0x8f, 0xa1, 0xcd, 0x33, 0x30, 0xb5, 0xa6, 0xd8, 0xf7, 0xa6, 0x54,
+	0xce, 0xaf, 0x96, 0x40, 0x8e, 0x38, 0x0e, 0x7d, 0x0e, 0xad, 0x00, 0x4f, 0xa8, 0xa5, 0xce, 0xb2,
+	0xba, 0xd9, 0x64, 0xb8, 0x81, 0x9c, 0x67, 0x3f, 0x07, 0x66, 0x98, 0x1f, 0x39, 0xb1, 0x8b, 0x89,
+	0xbe, 0xf1, 0x68, 0x43, 0xed, 0x59, 0x83, 0x8c, 0x62, 0x2a, 0x4c, 0x46, 0x1f, 0xb6, 0x17, 0x9a,
+	0x12, 0x7a, 0x0e, 0x75, 0x1c, 0xf0, 0x7a, 0x20, 0xba, 0xc6, 0xb5, 0xe4, 0x91, 0xcb, 0x57, 0x83,
+	0x9c, 0xc3, 0xf8, 0x15, 0xec, 0x2c, 0x6b, 0x47, 0xf3, 0x91, 0xd3, 0xe6, 0x23, 0x67, 0x4c, 0xa0,
+	0x5d, 0xea, 0xbd, 0xca, 0x15, 0x68, 0xea, 0x15, 0xec, 0x42, 0x3d, 0xaf, 0x78, 0x31, 0xc1, 0x73,
+	0x18, 0x19, 0xd0, 0xa6, 0x01, 0xb1, 0x1c, 0x9c, 0x52, 0x6b, 0x6a, 0x93, 0xa9, 0xbc, 0xbc, 0x26,
+	0x0d, 0xc8, 0x00, 0xa7, 0x74, 0x64, 0x93, 0xa9, 0xf1, 0x1e, 0x5a, 0x6a, 0x67, 0x58, 0x75, 0x0c,
+	0x82, 0x0a, 0x53, 0x23, 0x8f, 0xe0, 0xdf, 0xec, 0xe8, 0x10, 0x53, 0x9b, 0x97, 0xa0, 0xd0, 0x9c,
+	0xc3, 0x46, 0x08, 0x4d, 0xa5, 0x78, 0x56, 0x2f, 0x1f, 0x2e, 0x1f, 0x8c, 0x44, 0x5f, 0x7f, 0xb4,
+	0xc1, 0x96, 0x0f, 0x09, 0xa2, 0x1e, 0xd4, 0x43, 0xe2, 0x59, 0xf4, 0x46, 0x6e, 0x61, 0x9d, 0x62,
+	0x3a, 0xb2, 0x28, 0x1e, 0x11, 0xef, 0xec, 0x26, 0xc1, 0x66, 0x2d, 0x14, 0x1f, 0x46, 0x0c, 0x4d,
+	0x65, 0x2c, 0xaf, 0x38, 0x4e, 0xb5, 0x77, 0xbd, 0x6c, 0xef, 0x47, 0x1f, 0x78, 0x0d, 0x50, 0x4c,
+	0xdc, 0x15, 0xe7, 0xfd, 0x04, 0x2a, 0xf2, 0xac, 0xe5, 0x59, 0x52, 0xf9, 0x41, 0x27, 0x07, 0xe2,
+	0x64, 0xb1, 0x51, 0xfc, 0xdf, 0x03, 0xfb, 0x95, 0xb8, 0xc7, 0x6c, 0x89, 0xfc, 0x69, 0x79, 0xa3,
+	0x6d, 0xee, 0x6f, 0xe5, 0xd2, 0x02, 0x9d, 0xaf, 0xb8, 0xc6, 0x6b, 0x40, 0x8b, 0x8d, 0x18, 0xbd,
+	0x98, 0x57, 0x70, 0x6f, 0xae, 0x6b, 0x2f, 0xe8, 0xf9, 0x00, 0x35, 0x89, 0x43, 0xf7, 0xa1, 0x46,
+	0xf0, 0xa5, 0x15, 0xcd, 0x42, 0xe9, 0xee, 0x26, 0xc1, 0x97, 0xc7, 0xb3, 0x90, 0x65, 0xa7, 0x72,
+	0xab, 0x22, 0xae, 0x9f, 0xcf, 0x0d, 0x89, 0x0d, 0x1e, 0x08, 0x75, 0x0c, 0x18, 0xff, 0x5a, 0x87,
+	0x4e, 0xf9, 0x58, 0xf4, 0x05, 0x6c, 0x15, 0xef, 0x0f, 0x2b, 0xb2, 0x43, 0x11, 0xd9, 0x86, 0xd9,
+	0x29, 0xd0, 0xc7, 0x76, 0x88, 0xd9, 0x06, 0xcf, 0xa8, 0x24, 0xb1, 0x1d, 0xb1, 0xc1, 0x37, 0xcc,
+	0x02, 0x81, 0xee, 0x40, 0x95, 0x5e, 0x67, 0xed, 0xb2, 0x61, 0x56, 0xe8, 0xf5, 0xd8, 0x65, 0x9d,
+	0x2c, 0xb3, 0x28, 0xfd, 0x8e, 0x60, 0x2a, 0xfb, 0x65, 0x66, 0xa6, 0xc9, 0x70, 0xe8, 0x39, 0xa0,
+	0x8c, 0x89, 0xf8, 0x61, 0xd6, 0xf3, 0xaa, 0xdc, 0xdd, 0xae, 0xa4, 0x9c, 0xfa, 0xa1, 0xec, 0x7b,
+	0xc7, 0x80, 0x14, 0x73, 0x9d, 0x38, 0x9a, 0xf8, 0x1e, 0x91, 0xdb, 0xf4, 0xc3, 0x9e, 0x78, 0x50,
+	0xf5, 0x06, 0x39, 0xc7, 0x80, 0x33, 0x9c, 0xd8, 0xce, 0x85, 0xed, 0x61, 0x73, 0xdb, 0x99, 0x23,
+	0x10, 0xe3, 0x1f, 0x1a, 0xb4, 0xd4, 0x7d, 0x1d, 0xf5, 0x00, 0xc2, 0x7c, 0xad, 0x96, 0x57, 0xd6,
+	0x29, 0x2f, 0xdc, 0xa6, 0xc2, 0xf1, 0xd1, 0x83, 0x45, 0x6d, 0x5f, 0x95, 0x72, 0xfb, 0x32, 0xfe,
+	0xaa, 0xc1, 0xf6, 0xc2, 0xe2, 0xb3, 0xaa, 0x41, 0x7d, 0xec, 0xc1, 0x8f, 0xa1, 0xe3, 0x13, 0xcb,
+	0xc5, 0x4e, 0x60, 0xa7, 0x36, 0x0b, 0x01, 0xbf, 0xaa, 0xba, 0xd9, 0xf6, 0xc9, 0xb0, 0x40, 0x1a,
+	0xbf, 0x81, 0x7a, 0x26, 0xcd, 0xd2, 0xcf, 0x8f, 0x1c, 0x35, 0xfd, 0xfc, 0xc8, 0x61, 0xe9, 0xa7,
+	0xe4, 0xe5, 0xba, 0x9a, 0x97, 0xc6, 0x04, 0xb6, 0x17, 0x9e, 0x32, 0xe8, 0x6b, 0xe8, 0x12, 0x1c,
+	0x4c, 0xf8, 0x0e, 0x9b, 0x86, 0xe2, 0x6c, 0xad, 0x6c, 0x70, 0xde, 0x22, 0xb6, 0x18, 0xe7, 0xb8,
+	0x60, 0x64, 0xf5, 0xce, 0x76, 0xb2, 0x48, 0xd6, 0xb5, 0x00, 0x8c, 0x73, 0x40, 0x8b, 0x8f, 0x1f,
+	0xf4, 0x04, 0xaa, 0xfc, 0xad, 0xb5, 0x72, 0x4c, 0x09, 0x32, 0xef, 0x53, 0xd8, 0x76, 0x6f, 0xe9,
+	0x53, 0xd8, 0x76, 0x8d, 0x3f, 0xc2, 0xa6, 0x38, 0x83, 0xdd, 0x19, 0x2e, 0x3d, 0x46, 0xcd, 0x1c,
+	0xbe, 0xb5, 0xc7, 0x2e, 0x5f, 0x22, 0x8c, 0x1a, 0x54, 0xf9, 0x5b, 0xc4, 0xf8, 0x13, 0xa0, 0xc5,
+	0x8d, 0x9b, 0x0d, 0x31, 0x42, 0xed, 0x94, 0x5a, 0xe5, 0xd2, 0x6f, 0x72, 0xe4, 0xa9, 0xa8, 0xff,
+	0x07, 0xd0, 0xc4, 0x91, 0x6b, 0x95, 0x2f, 0xa1, 0x81, 0x23, 0x57, 0xd0, 0x8d, 0x03, 0xb8, 0xb3,
+	0x64, 0x0f, 0x47, 0xcf, 0xa0, 0x2e, 0xbb, 0x4c, 0x36, 0xca, 0x17, 0xda, 0x59, 0xce, 0x60, 0xbc,
+	0x81, 0x9d, 0x65, 0xbb, 0x2d, 0xda, 0x2b, 0x7a, 0xad, 0xd0, 0x91, 0xbf, 0x9d, 0x24, 0xa3, 0xe8,
+	0xd4, 0x79, 0x0b, 0x36, 0xfe, 0xa3, 0x41, 0xbb, 0x44, 0x2a, 0xba, 0x85, 0xa6, 0x74, 0x8b, 0xdb,
+	0x1b, 0xcc, 0x03, 0x80, 0xa2, 0x7a, 0x65, 0x97, 0x51, 0x30, 0xe8, 0x53, 0x68, 0x9c, 0x07, 0xb1,
+	0x73, 0xc1, 0x62, 0xc2, 0x0b, 0xab, 0x62, 0xd6, 0x39, 0xe2, 0x14, 0x5f, 0xa2, 0x47, 0xd0, 0x62,
+	0xa1, 0xf2, 0x23, 0x8b, 0xa3, 0x64, 0x77, 0x01, 0x82, 0x2f, 0xc7, 0xd1, 0x01, 0xc3, 0x18, 0xdf,
+	0xc0, 0xdd, 0xa5, 0x8b, 0x38, 0xda, 0x5f, 0xd8, 0x7e, 0xee, 0xcd, 0xb9, 0x7b, 0x28, 0xc8, 0xca,
+	0x0e, 0xf4, 0x01, 0x3a, 0x65, 0x1a, 0xfa, 0x12, 0x36, 0x45, 0x34, 0x64, 0xe2, 0xaf, 0x08, 0x99,
+	0x64, 0x52, 0xff, 0xa3, 0xc8, 0x71, 0x96, 0x0d, 0x87, 0xdf, 0xe7, 0xaa, 0xb3, 0x06, 0xfe, 0x18,
+	0xb6, 0xe8, 0xb5, 0x55, 0x72, 0x4f, 0x2e, 0x8c, 0xf4, 0xfa, 0x34, 0x77, 0xb0, 0xac, 0x52, 0xfd,
+	0x35, 0x63, 0x7c, 0x01, 0x5b, 0x73, 0xef, 0x1e, 0x56, 0x74, 0x38, 0x4d, 0xe3, 0x54, 0xde, 0x8f,
+	0x00, 0x8c, 0xf7, 0xd0, 0xc8, 0xd7, 0x46, 0x36, 0x81, 0x94, 0x61, 0xc1, 0xbf, 0xd9, 0x19, 0x57,
+	0x38, 0x25, 0xec, 0x82, 0xc4, 0xfd, 0x65, 0xe0, 0xad, 0x9b, 0xd3, 0xf7, 0x1a, 0xe8, 0x7f, 0xb0,
+	0x03, 0xdf, 0xe5, 0x05, 0x6f, 0x62, 0x32, 0x0b, 0x28, 0xc9, 0x9a, 0xdf, 0xca, 0x09, 0xa8, 0x43,
+	0x8d, 0x5e, 0xbf, 0x0e, 0x6c, 0x8f, 0x64, 0xfe, 0x48, 0xb0, 0xfc, 0xab, 0x69, 0x63, 0xfe, 0xaf,
+	0xd7, 0x6d, 0xfd, 0xd7, 0xca, 0x92, 0x60, 0xee, 0x29, 0xb4, 0x62, 0xe9, 0x78, 0x51, 0x5e, 0x3a,
+	0x94, 0xcc, 0xc8, 0xe4, 0xe7, 0x2b, 0xe1, 0xef, 0x1a, 0x74, 0xca, 0xb4, 0x72, 0xd6, 0x6b, 0xb7,
+	0x67, 0xfd, 0xfa, 0x42, 0xd6, 0x77, 0x61, 0xe3, 0x02, 0xdf, 0xc8, 0x72, 0x60, 0x9f, 0xe8, 0x09,
+	0x74, 0x70, 0xe4, 0xc6, 0x29, 0xc1, 0x6e, 0x9f, 0x9e, 0x5d, 0x8f, 0x87, 0xdc, 0xcb, 0x86, 0x39,
+	0x87, 0x35, 0x1c, 0xb8, 0xb7, 0xfc, 0xcd, 0xb6, 0xc2, 0xd9, 0x97, 0x4a, 0x1d, 0x08, 0x6f, 0xef,
+	0xcf, 0x7b, 0xbb, 0x58, 0x08, 0xff, 0xd6, 0x60, 0x6b, 0x8e, 0x8a, 0x7a, 0x73, 0xa5, 0xb0, 0x2a,
+	0x68, 0x59, 0x2d, 0xec, 0x40, 0xf5, 0xca, 0x0e, 0x66, 0xd9, 0x5f, 0x43, 0x01, 0xa0, 0x5f, 0x03,
+	0xe0, 0xeb, 0xc4, 0x4f, 0x6f, 0xd8, 0xa0, 0x92, 0xff, 0x2d, 0xd9, 0x7b, 0x38, 0xf6, 0x02, 0xf9,
+	0x9b, 0xf5, 0x7c, 0x36, 0xe9, 0x9d, 0x65, 0xd3, 0xcf, 0x54, 0xb8, 0x7f, 0xf6, 0x5b, 0x68, 0x2a,
+	0xab, 0xdf, 0xfc, 0x6b, 0xb4, 0x0d, 0x8d, 0x83, 0xb7, 0xef, 0x06, 0xdf, 0x58, 0x47, 0xa7, 0x6f,
+	0xba, 0x1a, 0x7b, 0x74, 0x8e, 0x87, 0x87, 0xc7, 0x67, 0xe3, 0xb3, 0x0f, 0x1c, 0xb3, 0xbe, 0xff,
+	0x17, 0xd8, 0x14, 0xab, 0x37, 0xfa, 0x0a, 0x5a, 0xe2, 0xeb, 0x94, 0xa6, 0xd8, 0x0e, 0xd1, 0xc2,
+	0x24, 0xd9, 0x5d, 0xc0, 0x18, 0x6b, 0x4f, 0xb5, 0x17, 0x1a, 0x7a, 0x02, 0x95, 0x13, 0x3f, 0xf2,
+	0x50, 0xf9, 0xe7, 0xd4, 0x6e, 0x19, 0x34, 0xd6, 0x0e, 0xbe, 0xfc, 0xf6, 0x99, 0xe7, 0xd3, 0xe9,
+	0xec, 0x9c, 0xad, 0x36, 0x7b, 0xd3, 0x9b, 0x04, 0xa7, 0xe2, 0x19, 0xb8, 0x37, 0xb1, 0xcf, 0x53,
+	0xdf, 0x11, 0xbf, 0x88, 0xc9, 0x9e, 0x10, 0x3b, 0xdf, 0xe4, 0xe0, 0xcb, 0xff, 0x05, 0x00, 0x00,
+	0xff, 0xff, 0xa8, 0x94, 0x0a, 0xc3, 0x78, 0x16, 0x00, 0x00,
 }
diff --git a/protos/gossip/message.proto b/protos/gossip/message.proto
index 56cb0cd0..f9ea4a89 100644
--- a/protos/gossip/message.proto
+++ b/protos/gossip/message.proto
@@ -9,6 +9,7 @@ option go_package = "github.com/hyperledger/fabric/protos/gossip" ;
 package gossip;
 
 import "common/collection.proto";
+import "google/protobuf/timestamp.proto";
 
 // Gossip
 service Gossip {
@@ -130,6 +131,12 @@ message GossipMessage {
         // Encapsulates private data used to distribute
         // private rwset after the endorsement
         PrivateDataMessage private_data = 25;
+
+        // Used to request collection data
+        RemoteCollDataRequest collDataReq = 90;
+
+        // Used to respond to collection data requests
+        RemoteCollDataResponse collDataRes = 91;
     }
 }
 
@@ -369,4 +376,41 @@ message Chaincode {
     string name = 1;
     string version = 2;
     bytes metadata = 3;
-}
\ No newline at end of file
+}
+
+// ValidationResultsMessage is the message containing block validation results
+message ValidationResultsMessage {
+    uint64 seq_num = 1;
+    bytes txFlags  = 2;
+    bytes signature = 3;
+    bytes identity = 4;
+}
+
+// RemoteCollDataRequest message used to request
+// collection data
+message RemoteCollDataRequest {
+    uint64 nonce = 1;
+    repeated CollDataDigest digests = 2;
+}
+
+// CollDataDigest defines a digest of collection data
+message CollDataDigest {
+    string namespace = 1;
+    string collection = 2;
+    string key = 3;
+    string endorsedAtTxID = 4;
+}
+
+// RemoteCollDataResponse message used to respond to
+// collection data request
+message RemoteCollDataResponse {
+    uint64 nonce = 1;
+    repeated CollDataElement elements = 2;
+}
+
+// CollDataElement contains the collection data digest and value
+message CollDataElement {
+    CollDataDigest digest = 1;
+    bytes value = 2;
+    google.protobuf.Timestamp expiryTime = 3;
+}
diff --git a/vendor/github.com/bluele/gcache/LICENSE b/vendor/github.com/bluele/gcache/LICENSE
new file mode 100644
index 00000000..d1e7b03e
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/LICENSE
@@ -0,0 +1,21 @@
+The MIT License (MIT)
+
+Copyright (c) 2017 Jun Kimura
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
diff --git a/vendor/github.com/bluele/gcache/arc.go b/vendor/github.com/bluele/gcache/arc.go
new file mode 100644
index 00000000..5215dca9
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/arc.go
@@ -0,0 +1,452 @@
+package gcache
+
+import (
+	"container/list"
+	"time"
+)
+
+// Constantly balances between LRU and LFU, to improve the combined result.
+type ARC struct {
+	baseCache
+	items map[interface{}]*arcItem
+
+	part int
+	t1   *arcList
+	t2   *arcList
+	b1   *arcList
+	b2   *arcList
+}
+
+func newARC(cb *CacheBuilder) *ARC {
+	c := &ARC{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *ARC) init() {
+	c.items = make(map[interface{}]*arcItem)
+	c.t1 = newARCList()
+	c.t2 = newARCList()
+	c.b1 = newARCList()
+	c.b2 = newARCList()
+}
+
+func (c *ARC) replace(key interface{}) {
+	if !c.isCacheFull() {
+		return
+	}
+	var old interface{}
+	if c.t1.Len() > 0 && ((c.b2.Has(key) && c.t1.Len() == c.part) || (c.t1.Len() > c.part)) {
+		old = c.t1.RemoveTail()
+		c.b1.PushFront(old)
+	} else if c.t2.Len() > 0 {
+		old = c.t2.RemoveTail()
+		c.b2.PushFront(old)
+	} else {
+		old = c.t1.RemoveTail()
+		c.b1.PushFront(old)
+	}
+	item, ok := c.items[old]
+	if ok {
+		delete(c.items, old)
+		if c.evictedFunc != nil {
+			c.evictedFunc(item.key, item.value)
+		}
+	}
+}
+
+func (c *ARC) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *ARC) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*arcItem).expiration = &t
+	return nil
+}
+
+func (c *ARC) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	item, ok := c.items[key]
+	if ok {
+		item.value = value
+	} else {
+		item = &arcItem{
+			clock: c.clock,
+			key:   key,
+			value: value,
+		}
+		c.items[key] = item
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	defer func() {
+		if c.addedFunc != nil {
+			c.addedFunc(key, value)
+		}
+	}()
+
+	if c.t1.Has(key) || c.t2.Has(key) {
+		return item, nil
+	}
+
+	if elt := c.b1.Lookup(key); elt != nil {
+		c.setPart(minInt(c.size, c.part+maxInt(c.b2.Len()/c.b1.Len(), 1)))
+		c.replace(key)
+		c.b1.Remove(key, elt)
+		c.t2.PushFront(key)
+		return item, nil
+	}
+
+	if elt := c.b2.Lookup(key); elt != nil {
+		c.setPart(maxInt(0, c.part-maxInt(c.b1.Len()/c.b2.Len(), 1)))
+		c.replace(key)
+		c.b2.Remove(key, elt)
+		c.t2.PushFront(key)
+		return item, nil
+	}
+
+	if c.isCacheFull() && c.t1.Len()+c.b1.Len() == c.size {
+		if c.t1.Len() < c.size {
+			c.b1.RemoveTail()
+			c.replace(key)
+		} else {
+			pop := c.t1.RemoveTail()
+			item, ok := c.items[pop]
+			if ok {
+				delete(c.items, pop)
+				if c.evictedFunc != nil {
+					c.evictedFunc(item.key, item.value)
+				}
+			}
+		}
+	} else {
+		total := c.t1.Len() + c.b1.Len() + c.t2.Len() + c.b2.Len()
+		if total >= c.size {
+			if total == (2 * c.size) {
+				if c.b2.Len() > 0 {
+					c.b2.RemoveTail()
+				} else {
+					c.b1.RemoveTail()
+				}
+			}
+			c.replace(key)
+		}
+	}
+	c.t1.PushFront(key)
+	return item, nil
+}
+
+// Get a value from cache pool using key if it exists. If not exists and it has LoaderFunc, it will generate the value using you have specified LoaderFunc method returns value.
+func (c *ARC) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *ARC) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, err
+}
+
+func (c *ARC) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *ARC) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	if elt := c.t1.Lookup(key); elt != nil {
+		c.t1.Remove(key, elt)
+		item := c.items[key]
+		if !item.IsExpired(nil) {
+			c.t2.PushFront(key)
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return item.value, nil
+		} else {
+			delete(c.items, key)
+			c.b1.PushFront(key)
+			if c.evictedFunc != nil {
+				c.evictedFunc(item.key, item.value)
+			}
+		}
+	}
+	if elt := c.t2.Lookup(key); elt != nil {
+		item := c.items[key]
+		if !item.IsExpired(nil) {
+			c.t2.MoveToFront(elt)
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return item.value, nil
+		} else {
+			delete(c.items, key)
+			c.t2.Remove(key, elt)
+			c.b2.PushFront(key)
+			if c.evictedFunc != nil {
+				c.evictedFunc(item.key, item.value)
+			}
+		}
+	}
+
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *ARC) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*arcItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+// Has checks if key exists in cache
+func (c *ARC) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *ARC) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *ARC) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *ARC) remove(key interface{}) bool {
+	if elt := c.t1.Lookup(key); elt != nil {
+		c.t1.Remove(key, elt)
+		item := c.items[key]
+		delete(c.items, key)
+		c.b1.PushFront(key)
+		if c.evictedFunc != nil {
+			c.evictedFunc(key, item.value)
+		}
+		return true
+	}
+
+	if elt := c.t2.Lookup(key); elt != nil {
+		c.t2.Remove(key, elt)
+		item := c.items[key]
+		delete(c.items, key)
+		c.b2.PushFront(key)
+		if c.evictedFunc != nil {
+			c.evictedFunc(key, item.value)
+		}
+		return true
+	}
+
+	return false
+}
+
+func (c *ARC) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *ARC) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *ARC) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *ARC) Len() int {
+	return len(c.GetALL())
+}
+
+// Purge is used to completely clear the cache
+func (c *ARC) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for _, item := range c.items {
+			c.purgeVisitorFunc(item.key, item.value)
+		}
+	}
+
+	c.init()
+}
+
+func (c *ARC) setPart(p int) {
+	if c.isCacheFull() {
+		c.part = p
+	}
+}
+
+func (c *ARC) isCacheFull() bool {
+	return (c.t1.Len() + c.t2.Len()) == c.size
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (it *arcItem) IsExpired(now *time.Time) bool {
+	if it.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := it.clock.Now()
+		now = &t
+	}
+	return it.expiration.Before(*now)
+}
+
+type arcList struct {
+	l    *list.List
+	keys map[interface{}]*list.Element
+}
+
+type arcItem struct {
+	clock      Clock
+	key        interface{}
+	value      interface{}
+	expiration *time.Time
+}
+
+func newARCList() *arcList {
+	return &arcList{
+		l:    list.New(),
+		keys: make(map[interface{}]*list.Element),
+	}
+}
+
+func (al *arcList) Has(key interface{}) bool {
+	_, ok := al.keys[key]
+	return ok
+}
+
+func (al *arcList) Lookup(key interface{}) *list.Element {
+	elt := al.keys[key]
+	return elt
+}
+
+func (al *arcList) MoveToFront(elt *list.Element) {
+	al.l.MoveToFront(elt)
+}
+
+func (al *arcList) PushFront(key interface{}) {
+	if elt, ok := al.keys[key]; ok {
+		al.l.MoveToFront(elt)
+		return
+	}
+	elt := al.l.PushFront(key)
+	al.keys[key] = elt
+}
+
+func (al *arcList) Remove(key interface{}, elt *list.Element) {
+	delete(al.keys, key)
+	al.l.Remove(elt)
+}
+
+func (al *arcList) RemoveTail() interface{} {
+	elt := al.l.Back()
+	al.l.Remove(elt)
+
+	key := elt.Value
+	delete(al.keys, key)
+
+	return key
+}
+
+func (al *arcList) Len() int {
+	return al.l.Len()
+}
diff --git a/vendor/github.com/bluele/gcache/cache.go b/vendor/github.com/bluele/gcache/cache.go
new file mode 100644
index 00000000..0ac85572
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/cache.go
@@ -0,0 +1,205 @@
+package gcache
+
+import (
+	"errors"
+	"fmt"
+	"sync"
+	"time"
+)
+
+const (
+	TYPE_SIMPLE = "simple"
+	TYPE_LRU    = "lru"
+	TYPE_LFU    = "lfu"
+	TYPE_ARC    = "arc"
+)
+
+var KeyNotFoundError = errors.New("Key not found.")
+
+type Cache interface {
+	Set(interface{}, interface{}) error
+	SetWithExpire(interface{}, interface{}, time.Duration) error
+	Get(interface{}) (interface{}, error)
+	GetIFPresent(interface{}) (interface{}, error)
+	GetALL() map[interface{}]interface{}
+	Has(interface{}) bool
+	get(interface{}, bool) (interface{}, error)
+	Remove(interface{}) bool
+	Purge()
+	Keys() []interface{}
+	Len() int
+
+	statsAccessor
+}
+
+type baseCache struct {
+	clock            Clock
+	size             int
+	loaderExpireFunc LoaderExpireFunc
+	evictedFunc      EvictedFunc
+	purgeVisitorFunc PurgeVisitorFunc
+	addedFunc        AddedFunc
+	deserializeFunc  DeserializeFunc
+	serializeFunc    SerializeFunc
+	expiration       *time.Duration
+	mu               sync.RWMutex
+	loadGroup        Group
+	*stats
+}
+
+type (
+	LoaderFunc       func(interface{}) (interface{}, error)
+	LoaderExpireFunc func(interface{}) (interface{}, *time.Duration, error)
+	EvictedFunc      func(interface{}, interface{})
+	PurgeVisitorFunc func(interface{}, interface{})
+	AddedFunc        func(interface{}, interface{})
+	DeserializeFunc  func(interface{}, interface{}) (interface{}, error)
+	SerializeFunc    func(interface{}, interface{}) (interface{}, error)
+)
+
+type CacheBuilder struct {
+	clock            Clock
+	tp               string
+	size             int
+	loaderExpireFunc LoaderExpireFunc
+	evictedFunc      EvictedFunc
+	purgeVisitorFunc PurgeVisitorFunc
+	addedFunc        AddedFunc
+	expiration       *time.Duration
+	deserializeFunc  DeserializeFunc
+	serializeFunc    SerializeFunc
+}
+
+func New(size int) *CacheBuilder {
+	return &CacheBuilder{
+		clock: NewRealClock(),
+		tp:    TYPE_SIMPLE,
+		size:  size,
+	}
+}
+
+func (cb *CacheBuilder) Clock(clock Clock) *CacheBuilder {
+	cb.clock = clock
+	return cb
+}
+
+// Set a loader function.
+// loaderFunc: create a new value with this function if cached value is expired.
+func (cb *CacheBuilder) LoaderFunc(loaderFunc LoaderFunc) *CacheBuilder {
+	cb.loaderExpireFunc = func(k interface{}) (interface{}, *time.Duration, error) {
+		v, err := loaderFunc(k)
+		return v, nil, err
+	}
+	return cb
+}
+
+// Set a loader function with expiration.
+// loaderExpireFunc: create a new value with this function if cached value is expired.
+// If nil returned instead of time.Duration from loaderExpireFunc than value will never expire.
+func (cb *CacheBuilder) LoaderExpireFunc(loaderExpireFunc LoaderExpireFunc) *CacheBuilder {
+	cb.loaderExpireFunc = loaderExpireFunc
+	return cb
+}
+
+func (cb *CacheBuilder) EvictType(tp string) *CacheBuilder {
+	cb.tp = tp
+	return cb
+}
+
+func (cb *CacheBuilder) Simple() *CacheBuilder {
+	return cb.EvictType(TYPE_SIMPLE)
+}
+
+func (cb *CacheBuilder) LRU() *CacheBuilder {
+	return cb.EvictType(TYPE_LRU)
+}
+
+func (cb *CacheBuilder) LFU() *CacheBuilder {
+	return cb.EvictType(TYPE_LFU)
+}
+
+func (cb *CacheBuilder) ARC() *CacheBuilder {
+	return cb.EvictType(TYPE_ARC)
+}
+
+func (cb *CacheBuilder) EvictedFunc(evictedFunc EvictedFunc) *CacheBuilder {
+	cb.evictedFunc = evictedFunc
+	return cb
+}
+
+func (cb *CacheBuilder) PurgeVisitorFunc(purgeVisitorFunc PurgeVisitorFunc) *CacheBuilder {
+	cb.purgeVisitorFunc = purgeVisitorFunc
+	return cb
+}
+
+func (cb *CacheBuilder) AddedFunc(addedFunc AddedFunc) *CacheBuilder {
+	cb.addedFunc = addedFunc
+	return cb
+}
+
+func (cb *CacheBuilder) DeserializeFunc(deserializeFunc DeserializeFunc) *CacheBuilder {
+	cb.deserializeFunc = deserializeFunc
+	return cb
+}
+
+func (cb *CacheBuilder) SerializeFunc(serializeFunc SerializeFunc) *CacheBuilder {
+	cb.serializeFunc = serializeFunc
+	return cb
+}
+
+func (cb *CacheBuilder) Expiration(expiration time.Duration) *CacheBuilder {
+	cb.expiration = &expiration
+	return cb
+}
+
+func (cb *CacheBuilder) Build() Cache {
+	if cb.size <= 0 && cb.tp != TYPE_SIMPLE {
+		panic("gcache: Cache size <= 0")
+	}
+
+	return cb.build()
+}
+
+func (cb *CacheBuilder) build() Cache {
+	switch cb.tp {
+	case TYPE_SIMPLE:
+		return newSimpleCache(cb)
+	case TYPE_LRU:
+		return newLRUCache(cb)
+	case TYPE_LFU:
+		return newLFUCache(cb)
+	case TYPE_ARC:
+		return newARC(cb)
+	default:
+		panic("gcache: Unknown type " + cb.tp)
+	}
+}
+
+func buildCache(c *baseCache, cb *CacheBuilder) {
+	c.clock = cb.clock
+	c.size = cb.size
+	c.loaderExpireFunc = cb.loaderExpireFunc
+	c.expiration = cb.expiration
+	c.addedFunc = cb.addedFunc
+	c.deserializeFunc = cb.deserializeFunc
+	c.serializeFunc = cb.serializeFunc
+	c.evictedFunc = cb.evictedFunc
+	c.purgeVisitorFunc = cb.purgeVisitorFunc
+	c.stats = &stats{}
+}
+
+// load a new value using by specified key.
+func (c *baseCache) load(key interface{}, cb func(interface{}, *time.Duration, error) (interface{}, error), isWait bool) (interface{}, bool, error) {
+	v, called, err := c.loadGroup.Do(key, func() (v interface{}, e error) {
+		defer func() {
+			if r := recover(); r != nil {
+				e = fmt.Errorf("Loader panics: %v", r)
+			}
+		}()
+		return cb(c.loaderExpireFunc(key))
+	}, isWait)
+	if err != nil {
+		return nil, called, err
+	}
+	return v, called, nil
+}
diff --git a/vendor/github.com/bluele/gcache/clock.go b/vendor/github.com/bluele/gcache/clock.go
new file mode 100644
index 00000000..3acc3f0d
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/clock.go
@@ -0,0 +1,53 @@
+package gcache
+
+import (
+	"sync"
+	"time"
+)
+
+type Clock interface {
+	Now() time.Time
+}
+
+type RealClock struct{}
+
+func NewRealClock() Clock {
+	return RealClock{}
+}
+
+func (rc RealClock) Now() time.Time {
+	t := time.Now()
+	return t
+}
+
+type FakeClock interface {
+	Clock
+
+	Advance(d time.Duration)
+}
+
+func NewFakeClock() FakeClock {
+	return &fakeclock{
+		// Taken from github.com/jonboulle/clockwork: use a fixture that does not fulfill Time.IsZero()
+		now: time.Date(1984, time.April, 4, 0, 0, 0, 0, time.UTC),
+	}
+}
+
+type fakeclock struct {
+	now time.Time
+
+	mutex sync.RWMutex
+}
+
+func (fc *fakeclock) Now() time.Time {
+	fc.mutex.RLock()
+	defer fc.mutex.RUnlock()
+	t := fc.now
+	return t
+}
+
+func (fc *fakeclock) Advance(d time.Duration) {
+	fc.mutex.Lock()
+	defer fc.mutex.Unlock()
+	fc.now = fc.now.Add(d)
+}
diff --git a/vendor/github.com/bluele/gcache/lfu.go b/vendor/github.com/bluele/gcache/lfu.go
new file mode 100644
index 00000000..915ac8cc
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/lfu.go
@@ -0,0 +1,335 @@
+package gcache
+
+import (
+	"container/list"
+	"time"
+)
+
+// Discards the least frequently used items first.
+type LFUCache struct {
+	baseCache
+	items    map[interface{}]*lfuItem
+	freqList *list.List // list for freqEntry
+}
+
+func newLFUCache(cb *CacheBuilder) *LFUCache {
+	c := &LFUCache{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *LFUCache) init() {
+	c.freqList = list.New()
+	c.items = make(map[interface{}]*lfuItem, c.size+1)
+	c.freqList.PushFront(&freqEntry{
+		freq:  0,
+		items: make(map[*lfuItem]struct{}),
+	})
+}
+
+// Set a new key-value pair
+func (c *LFUCache) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *LFUCache) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*lfuItem).expiration = &t
+	return nil
+}
+
+func (c *LFUCache) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	// Check for existing item
+	item, ok := c.items[key]
+	if ok {
+		item.value = value
+	} else {
+		// Verify size not exceeded
+		if len(c.items) >= c.size {
+			c.evict(1)
+		}
+		item = &lfuItem{
+			clock:       c.clock,
+			key:         key,
+			value:       value,
+			freqElement: nil,
+		}
+		el := c.freqList.Front()
+		fe := el.Value.(*freqEntry)
+		fe.items[item] = struct{}{}
+
+		item.freqElement = el
+		c.items[key] = item
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	if c.addedFunc != nil {
+		c.addedFunc(key, value)
+	}
+
+	return item, nil
+}
+
+// Get a value from cache pool using key if it exists.
+// If it dose not exists key and has LoaderFunc,
+// generate a value using `LoaderFunc` method returns value.
+func (c *LFUCache) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *LFUCache) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, err
+}
+
+func (c *LFUCache) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *LFUCache) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	item, ok := c.items[key]
+	if ok {
+		if !item.IsExpired(nil) {
+			c.increment(item)
+			v := item.value
+			c.mu.Unlock()
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return v, nil
+		}
+		c.removeItem(item)
+	}
+	c.mu.Unlock()
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *LFUCache) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*lfuItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+func (c *LFUCache) increment(item *lfuItem) {
+	currentFreqElement := item.freqElement
+	currentFreqEntry := currentFreqElement.Value.(*freqEntry)
+	nextFreq := currentFreqEntry.freq + 1
+	delete(currentFreqEntry.items, item)
+
+	nextFreqElement := currentFreqElement.Next()
+	if nextFreqElement == nil {
+		nextFreqElement = c.freqList.InsertAfter(&freqEntry{
+			freq:  nextFreq,
+			items: make(map[*lfuItem]struct{}),
+		}, currentFreqElement)
+	}
+	nextFreqElement.Value.(*freqEntry).items[item] = struct{}{}
+	item.freqElement = nextFreqElement
+}
+
+// evict removes the least frequence item from the cache.
+func (c *LFUCache) evict(count int) {
+	entry := c.freqList.Front()
+	for i := 0; i < count; {
+		if entry == nil {
+			return
+		} else {
+			for item, _ := range entry.Value.(*freqEntry).items {
+				if i >= count {
+					return
+				}
+				c.removeItem(item)
+				i++
+			}
+			entry = entry.Next()
+		}
+	}
+}
+
+// Has checks if key exists in cache
+func (c *LFUCache) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *LFUCache) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *LFUCache) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *LFUCache) remove(key interface{}) bool {
+	if item, ok := c.items[key]; ok {
+		c.removeItem(item)
+		return true
+	}
+	return false
+}
+
+// removeElement is used to remove a given list element from the cache
+func (c *LFUCache) removeItem(item *lfuItem) {
+	delete(c.items, item.key)
+	delete(item.freqElement.Value.(*freqEntry).items, item)
+	if c.evictedFunc != nil {
+		c.evictedFunc(item.key, item.value)
+	}
+}
+
+func (c *LFUCache) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *LFUCache) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *LFUCache) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *LFUCache) Len() int {
+	return len(c.GetALL())
+}
+
+// Completely clear the cache
+func (c *LFUCache) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for key, item := range c.items {
+			c.purgeVisitorFunc(key, item.value)
+		}
+	}
+
+	c.init()
+}
+
+type freqEntry struct {
+	freq  uint
+	items map[*lfuItem]struct{}
+}
+
+type lfuItem struct {
+	clock       Clock
+	key         interface{}
+	value       interface{}
+	freqElement *list.Element
+	expiration  *time.Time
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (it *lfuItem) IsExpired(now *time.Time) bool {
+	if it.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := it.clock.Now()
+		now = &t
+	}
+	return it.expiration.Before(*now)
+}
diff --git a/vendor/github.com/bluele/gcache/lru.go b/vendor/github.com/bluele/gcache/lru.go
new file mode 100644
index 00000000..10061b08
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/lru.go
@@ -0,0 +1,301 @@
+package gcache
+
+import (
+	"container/list"
+	"time"
+)
+
+// Discards the least recently used items first.
+type LRUCache struct {
+	baseCache
+	items     map[interface{}]*list.Element
+	evictList *list.List
+}
+
+func newLRUCache(cb *CacheBuilder) *LRUCache {
+	c := &LRUCache{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *LRUCache) init() {
+	c.evictList = list.New()
+	c.items = make(map[interface{}]*list.Element, c.size+1)
+}
+
+func (c *LRUCache) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	// Check for existing item
+	var item *lruItem
+	if it, ok := c.items[key]; ok {
+		c.evictList.MoveToFront(it)
+		item = it.Value.(*lruItem)
+		item.value = value
+	} else {
+		// Verify size not exceeded
+		if c.evictList.Len() >= c.size {
+			c.evict(1)
+		}
+		item = &lruItem{
+			clock: c.clock,
+			key:   key,
+			value: value,
+		}
+		c.items[key] = c.evictList.PushFront(item)
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	if c.addedFunc != nil {
+		c.addedFunc(key, value)
+	}
+
+	return item, nil
+}
+
+// set a new key-value pair
+func (c *LRUCache) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *LRUCache) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*lruItem).expiration = &t
+	return nil
+}
+
+// Get a value from cache pool using key if it exists.
+// If it dose not exists key and has LoaderFunc,
+// generate a value using `LoaderFunc` method returns value.
+func (c *LRUCache) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *LRUCache) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, err
+}
+
+func (c *LRUCache) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *LRUCache) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	item, ok := c.items[key]
+	if ok {
+		it := item.Value.(*lruItem)
+		if !it.IsExpired(nil) {
+			c.evictList.MoveToFront(item)
+			v := it.value
+			c.mu.Unlock()
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return v, nil
+		}
+		c.removeElement(item)
+	}
+	c.mu.Unlock()
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *LRUCache) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*lruItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+// evict removes the oldest item from the cache.
+func (c *LRUCache) evict(count int) {
+	for i := 0; i < count; i++ {
+		ent := c.evictList.Back()
+		if ent == nil {
+			return
+		} else {
+			c.removeElement(ent)
+		}
+	}
+}
+
+// Has checks if key exists in cache
+func (c *LRUCache) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *LRUCache) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.Value.(*lruItem).IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *LRUCache) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *LRUCache) remove(key interface{}) bool {
+	if ent, ok := c.items[key]; ok {
+		c.removeElement(ent)
+		return true
+	}
+	return false
+}
+
+func (c *LRUCache) removeElement(e *list.Element) {
+	c.evictList.Remove(e)
+	entry := e.Value.(*lruItem)
+	delete(c.items, entry.key)
+	if c.evictedFunc != nil {
+		entry := e.Value.(*lruItem)
+		c.evictedFunc(entry.key, entry.value)
+	}
+}
+
+func (c *LRUCache) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *LRUCache) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *LRUCache) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *LRUCache) Len() int {
+	return len(c.GetALL())
+}
+
+// Completely clear the cache
+func (c *LRUCache) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for key, item := range c.items {
+			it := item.Value.(*lruItem)
+			v := it.value
+			c.purgeVisitorFunc(key, v)
+		}
+	}
+
+	c.init()
+}
+
+type lruItem struct {
+	clock      Clock
+	key        interface{}
+	value      interface{}
+	expiration *time.Time
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (it *lruItem) IsExpired(now *time.Time) bool {
+	if it.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := it.clock.Now()
+		now = &t
+	}
+	return it.expiration.Before(*now)
+}
diff --git a/vendor/github.com/bluele/gcache/simple.go b/vendor/github.com/bluele/gcache/simple.go
new file mode 100644
index 00000000..befc0368
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/simple.go
@@ -0,0 +1,289 @@
+package gcache
+
+import "time"
+
+// SimpleCache has no clear priority for evict cache. It depends on key-value map order.
+type SimpleCache struct {
+	baseCache
+	items map[interface{}]*simpleItem
+}
+
+func newSimpleCache(cb *CacheBuilder) *SimpleCache {
+	c := &SimpleCache{}
+	buildCache(&c.baseCache, cb)
+
+	c.init()
+	c.loadGroup.cache = c
+	return c
+}
+
+func (c *SimpleCache) init() {
+	if c.size <= 0 {
+		c.items = make(map[interface{}]*simpleItem)
+	} else {
+		c.items = make(map[interface{}]*simpleItem, c.size)
+	}
+}
+
+// Set a new key-value pair
+func (c *SimpleCache) Set(key, value interface{}) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	_, err := c.set(key, value)
+	return err
+}
+
+// Set a new key-value pair with an expiration time
+func (c *SimpleCache) SetWithExpire(key, value interface{}, expiration time.Duration) error {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	item, err := c.set(key, value)
+	if err != nil {
+		return err
+	}
+
+	t := c.clock.Now().Add(expiration)
+	item.(*simpleItem).expiration = &t
+	return nil
+}
+
+func (c *SimpleCache) set(key, value interface{}) (interface{}, error) {
+	var err error
+	if c.serializeFunc != nil {
+		value, err = c.serializeFunc(key, value)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	// Check for existing item
+	item, ok := c.items[key]
+	if ok {
+		item.value = value
+	} else {
+		// Verify size not exceeded
+		if (len(c.items) >= c.size) && c.size > 0 {
+			c.evict(1)
+		}
+		item = &simpleItem{
+			clock: c.clock,
+			value: value,
+		}
+		c.items[key] = item
+	}
+
+	if c.expiration != nil {
+		t := c.clock.Now().Add(*c.expiration)
+		item.expiration = &t
+	}
+
+	if c.addedFunc != nil {
+		c.addedFunc(key, value)
+	}
+
+	return item, nil
+}
+
+// Get a value from cache pool using key if it exists.
+// If it dose not exists key and has LoaderFunc,
+// generate a value using `LoaderFunc` method returns value.
+func (c *SimpleCache) Get(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, true)
+	}
+	return v, err
+}
+
+// GetIFPresent gets a value from cache pool using key if it exists.
+// If it dose not exists key, returns KeyNotFoundError.
+// And send a request which refresh value for specified key if cache object has LoaderFunc.
+func (c *SimpleCache) GetIFPresent(key interface{}) (interface{}, error) {
+	v, err := c.get(key, false)
+	if err == KeyNotFoundError {
+		return c.getWithLoader(key, false)
+	}
+	return v, nil
+}
+
+func (c *SimpleCache) get(key interface{}, onLoad bool) (interface{}, error) {
+	v, err := c.getValue(key, onLoad)
+	if err != nil {
+		return nil, err
+	}
+	if c.deserializeFunc != nil {
+		return c.deserializeFunc(key, v)
+	}
+	return v, nil
+}
+
+func (c *SimpleCache) getValue(key interface{}, onLoad bool) (interface{}, error) {
+	c.mu.Lock()
+	item, ok := c.items[key]
+	if ok {
+		if !item.IsExpired(nil) {
+			v := item.value
+			c.mu.Unlock()
+			if !onLoad {
+				c.stats.IncrHitCount()
+			}
+			return v, nil
+		}
+		c.remove(key)
+	}
+	c.mu.Unlock()
+	if !onLoad {
+		c.stats.IncrMissCount()
+	}
+	return nil, KeyNotFoundError
+}
+
+func (c *SimpleCache) getWithLoader(key interface{}, isWait bool) (interface{}, error) {
+	if c.loaderExpireFunc == nil {
+		return nil, KeyNotFoundError
+	}
+	value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) {
+		if e != nil {
+			return nil, e
+		}
+		c.mu.Lock()
+		defer c.mu.Unlock()
+		item, err := c.set(key, v)
+		if err != nil {
+			return nil, err
+		}
+		if expiration != nil {
+			t := c.clock.Now().Add(*expiration)
+			item.(*simpleItem).expiration = &t
+		}
+		return v, nil
+	}, isWait)
+	if err != nil {
+		return nil, err
+	}
+	return value, nil
+}
+
+func (c *SimpleCache) evict(count int) {
+	now := c.clock.Now()
+	current := 0
+	for key, item := range c.items {
+		if current >= count {
+			return
+		}
+		if item.expiration == nil || now.After(*item.expiration) {
+			defer c.remove(key)
+			current++
+		}
+	}
+}
+
+// Has checks if key exists in cache
+func (c *SimpleCache) Has(key interface{}) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	now := time.Now()
+	return c.has(key, &now)
+}
+
+func (c *SimpleCache) has(key interface{}, now *time.Time) bool {
+	item, ok := c.items[key]
+	if !ok {
+		return false
+	}
+	return !item.IsExpired(now)
+}
+
+// Remove removes the provided key from the cache.
+func (c *SimpleCache) Remove(key interface{}) bool {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	return c.remove(key)
+}
+
+func (c *SimpleCache) remove(key interface{}) bool {
+	item, ok := c.items[key]
+	if ok {
+		delete(c.items, key)
+		if c.evictedFunc != nil {
+			c.evictedFunc(key, item.value)
+		}
+		return true
+	}
+	return false
+}
+
+// Returns a slice of the keys in the cache.
+func (c *SimpleCache) keys() []interface{} {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	keys := make([]interface{}, len(c.items))
+	var i = 0
+	for k := range c.items {
+		keys[i] = k
+		i++
+	}
+	return keys
+}
+
+// Keys returns a slice of the keys in the cache.
+func (c *SimpleCache) Keys() []interface{} {
+	keys := []interface{}{}
+	for _, k := range c.keys() {
+		_, err := c.GetIFPresent(k)
+		if err == nil {
+			keys = append(keys, k)
+		}
+	}
+	return keys
+}
+
+// GetALL returns all key-value pairs in the cache.
+func (c *SimpleCache) GetALL() map[interface{}]interface{} {
+	m := make(map[interface{}]interface{})
+	for _, k := range c.keys() {
+		v, err := c.GetIFPresent(k)
+		if err == nil {
+			m[k] = v
+		}
+	}
+	return m
+}
+
+// Len returns the number of items in the cache.
+func (c *SimpleCache) Len() int {
+	return len(c.GetALL())
+}
+
+// Completely clear the cache
+func (c *SimpleCache) Purge() {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+
+	if c.purgeVisitorFunc != nil {
+		for key, item := range c.items {
+			c.purgeVisitorFunc(key, item.value)
+		}
+	}
+
+	c.init()
+}
+
+type simpleItem struct {
+	clock      Clock
+	value      interface{}
+	expiration *time.Time
+}
+
+// IsExpired returns boolean value whether this item is expired or not.
+func (si *simpleItem) IsExpired(now *time.Time) bool {
+	if si.expiration == nil {
+		return false
+	}
+	if now == nil {
+		t := si.clock.Now()
+		now = &t
+	}
+	return si.expiration.Before(*now)
+}
diff --git a/vendor/github.com/bluele/gcache/singleflight.go b/vendor/github.com/bluele/gcache/singleflight.go
new file mode 100644
index 00000000..2c6285e8
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/singleflight.go
@@ -0,0 +1,82 @@
+package gcache
+
+/*
+Copyright 2012 Google Inc.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+// This module provides a duplicate function call suppression
+// mechanism.
+
+import "sync"
+
+// call is an in-flight or completed Do call
+type call struct {
+	wg  sync.WaitGroup
+	val interface{}
+	err error
+}
+
+// Group represents a class of work and forms a namespace in which
+// units of work can be executed with duplicate suppression.
+type Group struct {
+	cache Cache
+	mu    sync.Mutex            // protects m
+	m     map[interface{}]*call // lazily initialized
+}
+
+// Do executes and returns the results of the given function, making
+// sure that only one execution is in-flight for a given key at a
+// time. If a duplicate comes in, the duplicate caller waits for the
+// original to complete and receives the same results.
+func (g *Group) Do(key interface{}, fn func() (interface{}, error), isWait bool) (interface{}, bool, error) {
+	g.mu.Lock()
+	v, err := g.cache.get(key, true)
+	if err == nil {
+		g.mu.Unlock()
+		return v, false, nil
+	}
+	if g.m == nil {
+		g.m = make(map[interface{}]*call)
+	}
+	if c, ok := g.m[key]; ok {
+		g.mu.Unlock()
+		if !isWait {
+			return nil, false, KeyNotFoundError
+		}
+		c.wg.Wait()
+		return c.val, false, c.err
+	}
+	c := new(call)
+	c.wg.Add(1)
+	g.m[key] = c
+	g.mu.Unlock()
+	if !isWait {
+		go g.call(c, key, fn)
+		return nil, false, KeyNotFoundError
+	}
+	v, err = g.call(c, key, fn)
+	return v, true, err
+}
+
+func (g *Group) call(c *call, key interface{}, fn func() (interface{}, error)) (interface{}, error) {
+	c.val, c.err = fn()
+	c.wg.Done()
+
+	g.mu.Lock()
+	delete(g.m, key)
+	g.mu.Unlock()
+
+	return c.val, c.err
+}
diff --git a/vendor/github.com/bluele/gcache/stats.go b/vendor/github.com/bluele/gcache/stats.go
new file mode 100644
index 00000000..ca0bf318
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/stats.go
@@ -0,0 +1,53 @@
+package gcache
+
+import (
+	"sync/atomic"
+)
+
+type statsAccessor interface {
+	HitCount() uint64
+	MissCount() uint64
+	LookupCount() uint64
+	HitRate() float64
+}
+
+// statistics
+type stats struct {
+	hitCount  uint64
+	missCount uint64
+}
+
+// increment hit count
+func (st *stats) IncrHitCount() uint64 {
+	return atomic.AddUint64(&st.hitCount, 1)
+}
+
+// increment miss count
+func (st *stats) IncrMissCount() uint64 {
+	return atomic.AddUint64(&st.missCount, 1)
+}
+
+// HitCount returns hit count
+func (st *stats) HitCount() uint64 {
+	return atomic.LoadUint64(&st.hitCount)
+}
+
+// MissCount returns miss count
+func (st *stats) MissCount() uint64 {
+	return atomic.LoadUint64(&st.missCount)
+}
+
+// LookupCount returns lookup count
+func (st *stats) LookupCount() uint64 {
+	return st.HitCount() + st.MissCount()
+}
+
+// HitRate returns rate for cache hitting
+func (st *stats) HitRate() float64 {
+	hc, mc := st.HitCount(), st.MissCount()
+	total := hc + mc
+	if total == 0 {
+		return 0.0
+	}
+	return float64(hc) / float64(total)
+}
diff --git a/vendor/github.com/bluele/gcache/utils.go b/vendor/github.com/bluele/gcache/utils.go
new file mode 100644
index 00000000..1f784e4c
--- /dev/null
+++ b/vendor/github.com/bluele/gcache/utils.go
@@ -0,0 +1,15 @@
+package gcache
+
+func minInt(x, y int) int {
+	if x < y {
+		return x
+	}
+	return y
+}
+
+func maxInt(x, y int) int {
+	if x > y {
+		return x
+	}
+	return y
+}
diff --git a/vendor/github.com/btcsuite/btcutil/LICENSE b/vendor/github.com/btcsuite/btcutil/LICENSE
new file mode 100644
index 00000000..3e7b1679
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/LICENSE
@@ -0,0 +1,16 @@
+ISC License
+
+Copyright (c) 2013-2017 The btcsuite developers
+Copyright (c) 2016-2017 The Lightning Network Developers
+
+Permission to use, copy, modify, and distribute this software for any
+purpose with or without fee is hereby granted, provided that the above
+copyright notice and this permission notice appear in all copies.
+
+THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
+ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
+ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
+OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
diff --git a/vendor/github.com/btcsuite/btcutil/base58/alphabet.go b/vendor/github.com/btcsuite/btcutil/base58/alphabet.go
new file mode 100644
index 00000000..6bb39fef
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/alphabet.go
@@ -0,0 +1,49 @@
+// Copyright (c) 2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+// AUTOGENERATED by genalphabet.go; do not edit.
+
+package base58
+
+const (
+	// alphabet is the modified base58 alphabet used by Bitcoin.
+	alphabet = "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz"
+
+	alphabetIdx0 = '1'
+)
+
+var b58 = [256]byte{
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 0, 1, 2, 3, 4, 5, 6,
+	7, 8, 255, 255, 255, 255, 255, 255,
+	255, 9, 10, 11, 12, 13, 14, 15,
+	16, 255, 17, 18, 19, 20, 21, 255,
+	22, 23, 24, 25, 26, 27, 28, 29,
+	30, 31, 32, 255, 255, 255, 255, 255,
+	255, 33, 34, 35, 36, 37, 38, 39,
+	40, 41, 42, 43, 255, 44, 45, 46,
+	47, 48, 49, 50, 51, 52, 53, 54,
+	55, 56, 57, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+	255, 255, 255, 255, 255, 255, 255, 255,
+}
diff --git a/vendor/github.com/btcsuite/btcutil/base58/base58.go b/vendor/github.com/btcsuite/btcutil/base58/base58.go
new file mode 100644
index 00000000..19a72de2
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/base58.go
@@ -0,0 +1,75 @@
+// Copyright (c) 2013-2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+package base58
+
+import (
+	"math/big"
+)
+
+//go:generate go run genalphabet.go
+
+var bigRadix = big.NewInt(58)
+var bigZero = big.NewInt(0)
+
+// Decode decodes a modified base58 string to a byte slice.
+func Decode(b string) []byte {
+	answer := big.NewInt(0)
+	j := big.NewInt(1)
+
+	scratch := new(big.Int)
+	for i := len(b) - 1; i >= 0; i-- {
+		tmp := b58[b[i]]
+		if tmp == 255 {
+			return []byte("")
+		}
+		scratch.SetInt64(int64(tmp))
+		scratch.Mul(j, scratch)
+		answer.Add(answer, scratch)
+		j.Mul(j, bigRadix)
+	}
+
+	tmpval := answer.Bytes()
+
+	var numZeros int
+	for numZeros = 0; numZeros < len(b); numZeros++ {
+		if b[numZeros] != alphabetIdx0 {
+			break
+		}
+	}
+	flen := numZeros + len(tmpval)
+	val := make([]byte, flen)
+	copy(val[numZeros:], tmpval)
+
+	return val
+}
+
+// Encode encodes a byte slice to a modified base58 string.
+func Encode(b []byte) string {
+	x := new(big.Int)
+	x.SetBytes(b)
+
+	answer := make([]byte, 0, len(b)*136/100)
+	for x.Cmp(bigZero) > 0 {
+		mod := new(big.Int)
+		x.DivMod(x, bigRadix, mod)
+		answer = append(answer, alphabet[mod.Int64()])
+	}
+
+	// leading zero bytes
+	for _, i := range b {
+		if i != 0 {
+			break
+		}
+		answer = append(answer, alphabetIdx0)
+	}
+
+	// reverse
+	alen := len(answer)
+	for i := 0; i < alen/2; i++ {
+		answer[i], answer[alen-1-i] = answer[alen-1-i], answer[i]
+	}
+
+	return string(answer)
+}
diff --git a/vendor/github.com/btcsuite/btcutil/base58/base58check.go b/vendor/github.com/btcsuite/btcutil/base58/base58check.go
new file mode 100644
index 00000000..7cdafeee
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/base58check.go
@@ -0,0 +1,52 @@
+// Copyright (c) 2013-2014 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+package base58
+
+import (
+	"crypto/sha256"
+	"errors"
+)
+
+// ErrChecksum indicates that the checksum of a check-encoded string does not verify against
+// the checksum.
+var ErrChecksum = errors.New("checksum error")
+
+// ErrInvalidFormat indicates that the check-encoded string has an invalid format.
+var ErrInvalidFormat = errors.New("invalid format: version and/or checksum bytes missing")
+
+// checksum: first four bytes of sha256^2
+func checksum(input []byte) (cksum [4]byte) {
+	h := sha256.Sum256(input)
+	h2 := sha256.Sum256(h[:])
+	copy(cksum[:], h2[:4])
+	return
+}
+
+// CheckEncode prepends a version byte and appends a four byte checksum.
+func CheckEncode(input []byte, version byte) string {
+	b := make([]byte, 0, 1+len(input)+4)
+	b = append(b, version)
+	b = append(b, input[:]...)
+	cksum := checksum(b)
+	b = append(b, cksum[:]...)
+	return Encode(b)
+}
+
+// CheckDecode decodes a string that was encoded with CheckEncode and verifies the checksum.
+func CheckDecode(input string) (result []byte, version byte, err error) {
+	decoded := Decode(input)
+	if len(decoded) < 5 {
+		return nil, 0, ErrInvalidFormat
+	}
+	version = decoded[0]
+	var cksum [4]byte
+	copy(cksum[:], decoded[len(decoded)-4:])
+	if checksum(decoded[:len(decoded)-4]) != cksum {
+		return nil, 0, ErrChecksum
+	}
+	payload := decoded[1 : len(decoded)-4]
+	result = append(result, payload...)
+	return
+}
diff --git a/vendor/github.com/btcsuite/btcutil/base58/doc.go b/vendor/github.com/btcsuite/btcutil/base58/doc.go
new file mode 100644
index 00000000..9a2c0e6e
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/doc.go
@@ -0,0 +1,29 @@
+// Copyright (c) 2014 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+/*
+Package base58 provides an API for working with modified base58 and Base58Check
+encodings.
+
+Modified Base58 Encoding
+
+Standard base58 encoding is similar to standard base64 encoding except, as the
+name implies, it uses a 58 character alphabet which results in an alphanumeric
+string and allows some characters which are problematic for humans to be
+excluded.  Due to this, there can be various base58 alphabets.
+
+The modified base58 alphabet used by Bitcoin, and hence this package, omits the
+0, O, I, and l characters that look the same in many fonts and are therefore
+hard to humans to distinguish.
+
+Base58Check Encoding Scheme
+
+The Base58Check encoding scheme is primarily used for Bitcoin addresses at the
+time of this writing, however it can be used to generically encode arbitrary
+byte arrays into human-readable strings along with a version byte that can be
+used to differentiate the same payload.  For Bitcoin addresses, the extra
+version is used to differentiate the network of otherwise identical public keys
+which helps prevent using an address intended for one network on another.
+*/
+package base58
diff --git a/vendor/github.com/btcsuite/btcutil/base58/genalphabet.go b/vendor/github.com/btcsuite/btcutil/base58/genalphabet.go
new file mode 100644
index 00000000..010cbee3
--- /dev/null
+++ b/vendor/github.com/btcsuite/btcutil/base58/genalphabet.go
@@ -0,0 +1,79 @@
+// Copyright (c) 2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+//+build ignore
+
+package main
+
+import (
+	"bytes"
+	"io"
+	"log"
+	"os"
+	"strconv"
+)
+
+var (
+	start = []byte(`// Copyright (c) 2015 The btcsuite developers
+// Use of this source code is governed by an ISC
+// license that can be found in the LICENSE file.
+
+// AUTOGENERATED by genalphabet.go; do not edit.
+
+package base58
+
+const (
+	// alphabet is the modified base58 alphabet used by Bitcoin.
+	alphabet = "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz"
+
+	alphabetIdx0 = '1'
+)
+
+var b58 = [256]byte{`)
+
+	end = []byte(`}`)
+
+	alphabet = []byte("123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz")
+	tab      = []byte("\t")
+	invalid  = []byte("255")
+	comma    = []byte(",")
+	space    = []byte(" ")
+	nl       = []byte("\n")
+)
+
+func write(w io.Writer, b []byte) {
+	_, err := w.Write(b)
+	if err != nil {
+		log.Fatal(err)
+	}
+}
+
+func main() {
+	fi, err := os.Create("alphabet.go")
+	if err != nil {
+		log.Fatal(err)
+	}
+	defer fi.Close()
+
+	write(fi, start)
+	write(fi, nl)
+	for i := byte(0); i < 32; i++ {
+		write(fi, tab)
+		for j := byte(0); j < 8; j++ {
+			idx := bytes.IndexByte(alphabet, i*8+j)
+			if idx == -1 {
+				write(fi, invalid)
+			} else {
+				write(fi, strconv.AppendInt(nil, int64(idx), 10))
+			}
+			write(fi, comma)
+			if j != 7 {
+				write(fi, space)
+			}
+		}
+		write(fi, nl)
+	}
+	write(fi, end)
+	write(fi, nl)
+}
-- 
2.15.0

